Computer file
From Wikipedia, the free encyclopedia
This article is about computer files and file systems in general terms. For a more detailed and technical article, see File system.

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2009) (Learn how and when to remove this template message)
A computer file is a resource for storing information, which is available to a computer program and is usually based on some kind of durable storage. A file is "durable" in the sense that it remains available for other programs to use after the program that created it has finished executing. Computer files can be considered as the information technology counterpart of paper documents which traditionally are kept in office and library files, and this is the source of the term.

Contents  [hide] 
1	Etymology
2	File contents
2.1	File size
2.2	Organizing the data in a file
2.3	File operations
3	Identifying and organizing files
4	Protecting files
5	Storing files
6	Backing up files
7	File systems and file managers
8	See also
9	Notes
10	External links
Etymology[edit]

A punched card file

The twin disk files of an IBM 305 system
The word "file" was used publicly in the context of computer storage as early as February, 1950. In an RCA (Radio Corporation of America) advertisement in Popular Science Magazine[1] describing a new "memory" vacuum tube it had developed, RCA stated:

"...the results of countless computations can be kept "on file" and taken out again. Such a "file" now exists in a "memory" tube developed at RCA Laboratories. Electronically it retains figures fed into calculating machines, holds them in storage while it memorizes new ones - speeds intelligent solutions through mazes of mathematics."
In 1952, "file" was used in referring to information stored on punched cards.[2]

In early usage, people regarded the underlying hardware (rather than the contents) as a file. For example, the IBM 350 disk drives were called "disk files".[3] In about 1961 the Burroughs MCP and the MIT Compatible Time-Sharing System introduced the concept of a "file system", which managed several virtual "files" on one storage device, giving the term its present-day meaning. Although the current term "register file" shows the early concept of files, it has largely disappeared.

The word ultimately comes from the Latin filum "a thread".[4]

File contents[edit]
On most modern operating systems, files are organized into one-dimensional arrays of bytes. The format of a file is defined by its content since a file is solely a container for data, although, on some platforms the format is usually indicated by its filename extension, specifying the rules for how the bytes must be organized and interpreted meaningfully. For example, the bytes of a plain text file (.txt in Windows) are associated with either ASCII or UTF-8 characters, while the bytes of image, video, and audio files are interpreted otherwise. Most file types also allocate a few bytes for metadata, which allows a file to carry some basic information about itself.

Some file systems can store arbitrary (not interpreted by the file system) file-specific data outside of the file format, but linked to the file, for example extended attributes or forks. On other file systems this can be done via sidecar files or software-specific databases. All those methods, however, are more susceptible to loss of metadata than are container and archive file formats.

File size[edit]
Main article: File size
At any instant in time, a file might have a size, normally expressed as number of bytes, that indicates how much storage is associated with the file. In most modern operating systems the size can be any non-negative whole number of bytes up to a system limit. Many older operating systems kept track only of the number of blocks or tracks occupied by a file on a physical storage device. In such systems, software employed other methods to track the exact byte count (e.g., CP/M used a special control character, Ctrl-Z, to signal the end of text files).

The general definition of a file does not require that its size have any real meaning, however, unless the data within the file happens to correspond to data within a pool of persistent storage. A special case is a zero byte file; these files can be newly created files that have not yet had any data written to them, or may serve as some kind of flag in the file system, or are accidents (the results of aborted disk operations). For example, the file to which the link /bin/ls points in a typical Unix-like system probably has a defined size that seldom changes. Compare this with /dev/null which is also a file, but its size may be obscure. (This is misleading because /dev/null is not really a file: in Unix-like systems, all resources, including devices, are accessed like files, but there is still a real distinction between files and devices--at core, they behave differently--and the obscurity of the "size" of /dev/null is one manifestation of this. As a character device, /dev/null has no size.)

Organizing the data in a file[edit]
Information in a computer file can consist of smaller packets of information (often called "records" or "lines") that are individually different but share some common traits. For example, a payroll file might contain information concerning all the employees in a company and their payroll details; each record in the payroll file concerns just one employee, and all the records have the common trait of being related to payroll—this is very similar to placing all payroll information into a specific filing cabinet in an office that does not have a computer. A text file may contain lines of text, corresponding to printed lines on a piece of paper. Alternatively, a file may contain an arbitrary binary image (a BLOB) or it may contain an executable.

The way information is grouped into a file is entirely up to how it is designed. This has led to a plethora of more or less standardized file structures for all imaginable purposes, from the simplest to the most complex. Most computer files are used by computer programs which create, modify or delete the files for their own use on an as-needed basis. The programmers who create the programs decide what files are needed, how they are to be used and (often) their names.

In some cases, computer programs manipulate files that are made visible to the computer user. For example, in a word-processing program, the user manipulates document files that the user personally names. Although the content of the document file is arranged in a format that the word-processing program understands, the user is able to choose the name and location of the file and provide the bulk of the information (such as words and text) that will be stored in the file.

Many applications pack all their data files into a single file called an archive file, using internal markers to discern the different types of information contained within. The benefits of the archive file are to lower the number of files for easier transfer, to reduce storage usage, or just to organize outdated files. The archive file must often be unpacked before next using.

File operations[edit]
The most basic operations that programs can perform on a file are:

Create a new file
Change the access permissions and attributes of a file
Open a file, which makes the file contents available to the program
Read data from a file
Write data to a file
Close a file, terminating the association between it and the program
Files on a computer can be created, moved, modified, grown, shrunk, and deleted. In most cases, computer programs that are executed on the computer handle these operations, but the user of a computer can also manipulate files if necessary. For instance, Microsoft Word files are normally created and modified by the Microsoft Word program in response to user commands, but the user can also move, rename, or delete these files directly by using a file manager program such as Windows Explorer (on Windows computers) or by command lines (CLI).

In Unix-like systems, user-space processes do not normally deal with files at all; the operating system provides a level of abstraction which means that almost all interaction with files from user-space is through hard links. For example, a user space program cannot delete a file; it can delete a link to a file, and if the kernel determines that there are no hard links to the file, it may then allow the memory location for the deleted file to be allocated for another file. The resulting free space, is commonly considered a security risk due to the existence of file recovery software. Such a risk has given rise to secure deletion programs. Only the kernel deals with files, but it handles all user-space interaction with (virtual) files in a manner that is transparent to the user-space programs.

Identifying and organizing files[edit]

Files and folders arranged in a hierarchy
In modern computer systems, files are typically accessed using names (filenames). In some operating systems, the name is associated with the file itself. In others, the file is anonymous, and is pointed to by links that have names. In the latter case, a user can identify the name of the link with the file itself, but this is a false analogue, especially where there exists more than one link to the same file.

Files (or links to files) can be located in directories. However, more generally, a directory can contain either a list of files or a list of links to files. Within this definition, it is of paramount importance that the term "file" includes directories. This permits the existence of directory hierarchies, i.e., directories containing sub-directories. A name that refers to a file within a directory must be typically unique. In other words, there must be no identical names within a directory. However, in some operating systems, a name may include a specification of type that means a directory can contain an identical name for more than one type of object such as a directory and a file.

In environments in which a file is named, a file's name and the path to the file's directory must uniquely identify it among all other files in the computer system—no two files can have the same name and path. Where a file is anonymous, named references to it will exist within a namespace. In most cases, any name within the namespace will refer to exactly zero or one file. However, any file may be represented within any namespace by zero, one or more names.

Any string of characters may or may not be a well-formed name for a file or a link depending upon the context of application. Whether or not a name is well-formed depends on the type of computer system being used. Early computers permitted only a few letters or digits in the name of a file, but modern computers allow long names (some up to 255 characters) containing almost any combination of unicode letters or unicode digits, making it easier to understand the purpose of a file at a glance. Some computer systems allow file names to contain spaces; others do not. Case-sensitivity of file names is determined by the file system. Unix file systems are usually case sensitive and allow user-level applications to create files whose names differ only in the case of characters. Microsoft Windows supports multiple file systems, each with different policies[which?] regarding case-sensitivity. The common FAT file system can have multiple files whose names differ only in case if the user uses a disk editor to edit the file names in the directory entries. User applications, however, will usually not allow the user to create multiple files with the same name but differing in case.

Most computers organize files into hierarchies using folders, directories, or catalogs. The concept is the same irrespective of the terminology used. Each folder can contain an arbitrary number of files, and it can also contain other folders. These other folders are referred to as subfolders. Subfolders can contain still more files and folders and so on, thus building a tree-like structure in which one "master folder" (or "root folder" — the name varies from one operating system to another) can contain any number of levels of other folders and files. Folders can be named just as files can (except for the root folder, which often does not have a name). The use of folders makes it easier to organize files in a logical way.

When a computer allows the use of folders, each file and folder has not only a name of its own, but also a path, which identifies the folder or folders in which a file or folder resides. In the path, some sort of special character—such as a slash—is used to separate the file and folder names. For example, in the illustration shown in this article, the path /Payroll/Salaries/Managers uniquely identifies a file called Managers in a folder called Salaries, which in turn is contained in a folder called Payroll. The folder and file names are separated by slashes in this example; the topmost or root folder has no name, and so the path begins with a slash (if the root folder had a name, it would precede this first slash).

Many (but not all) computer systems use extensions in file names to help identify what they contain, also known as the file type. On Windows computers, extensions consist of a dot (period) at the end of a file name, followed by a few letters to identify the type of file. An extension of .txt identifies a text file; a .doc extension identifies any type of document or documentation, commonly in the Microsoft Word file format; and so on. Even when extensions are used in a computer system, the degree to which the computer system recognizes and heeds them can vary; in some systems, they are required, while in other systems, they are completely ignored if they are presented.

Protecting files[edit]
Many modern computer systems provide methods for protecting files against accidental and deliberate damage. Computers that allow for multiple users implement file permissions to control who may or may not modify, delete, or create files and folders. For example, a given user may be granted only permission to read a file or folder, but not to modify or delete it; or a user may be given permission to read and modify files or folders, but not to execute them. Permissions may also be used to allow only certain users to see the contents of a file or folder. Permissions protect against unauthorized tampering or destruction of information in files, and keep private information confidential from unauthorized users.

Another protection mechanism implemented in many computers is a read-only flag. When this flag is turned on for a file (which can be accomplished by a computer program or by a human user), the file can be examined, but it cannot be modified. This flag is useful for critical information that must not be modified or erased, such as special files that are used only by internal parts of the computer system. Some systems also include a hidden flag to make certain files invisible; this flag is used by the computer system to hide essential system files that users should not alter.

Storing files[edit]
The discussion above describes a file as a concept presented to a user or a high-level operating system. However, any file that has any useful purpose, outside of a thought experiment, must have some physical manifestation. That is, a file (an abstract concept) in a real computer system must have a real physical analogue if it is to exist at all.

In physical terms, most computer files are stored on some type of data storage device. For example, there is a hard disk, from which most operating systems run and on which most store their files. Hard disks have been the ubiquitous form of non-volatile storage since the early 1960s.[5] Where files contain only temporary information, they may be stored in RAM. Computer files can be also stored on other media in some cases, such as magnetic tapes, compact discs, Digital Versatile Discs, Zip drives, USB flash drives, etc. The use of solid state drives is also beginning to rival the hard disk drive.

In Unix-like operating systems, many files have no direct association with a physical storage device: /dev/null is a prime example, as are just about all files under /dev, /proc and /sys. These can be accessed as files in user space. They are really virtual files that exist, in reality, as objects within the operating system kernel.

As seen by a running user program, files are usually represented either by a File Control Block or by a file handle. A File Control Block (FCB) is an area of memory which is manipulated to establish a filename etc. and then passed to the operating system as a parameter, it was used by older IBM operating systems and by early PC operating systems including CP/M and early versions of MS-DOS. A file handle is generally either an opaque data type or an integer, it was introduced in around 1961 by the ALGOL-based Burroughs MCP running on the Burroughs B5000 but is now ubiquitous.

Backing up files[edit]
When computer files contain information that is extremely important, a back-up process is used to protect against disasters that might destroy the files. Backing up files simply means making copies of the files in a separate location so that they can be restored if something happens to the computer, or if they are deleted accidentally.

There are many ways to back up files. Most computer systems provide utility programs to assist in the back-up process, which can become very time-consuming if there are many files to safeguard. Files are often copied to removable media such as writable CDs or cartridge tapes. Copying files to another hard disk in the same computer protects against failure of one disk, but if it is necessary to protect against failure or destruction of the entire computer, then copies of the files must be made on other media that can be taken away from the computer and stored in a safe, distant location.

The grandfather-father-son backup method automatically makes three back-ups; the grandfather file is the oldest copy of the file and the son is the current copy.

File systems and file managers[edit]
The way a computer organizes, names, stores and manipulates files is globally referred to as its file system. Most computers have at least one file system. Some computers allow the use of several different file systems. For instance, on newer MS Windows computers, the older FAT-type file systems of MS-DOS and old versions of Windows are supported, in addition to the NTFS file system that is the normal file system for recent versions of Windows. Each system has its own advantages and disadvantages. Standard FAT allows only eight-character file names (plus a three-character extension) with no spaces, for example, whereas NTFS allows much longer names that can contain spaces. You can call a file "Payroll records" in NTFS, but in FAT you would be restricted to something like payroll.dat (unless you were using VFAT, a FAT extension allowing long file names).

File manager programs are utility programs that allow users to manipulate files directly. They allow you to move, create, delete and rename files and folders, although they do not actually allow you to read the contents of a file or store information in it. Every computer system provides at least one file-manager program for its native file system. For example, File Explorer (formerly Windows Explorer) is commonly used in Microsoft Windows operating systems, and Nautilus is common under several distributions of Linux.

See also[edit]
Block (data storage)
Computer file management
Data hierarchy
File Camouflage
File copying
File conversion
File deletion
File directory
File manager
File system
Filename
Flat file database
Object composition
Soft copy
Block (data storage)
From Wikipedia, the free encyclopedia
This article is about the computer input/output technique. For the process scheduling concept, see Blocking (computing).

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (April 2014) (Learn how and when to remove this template message)
In computing (specifically data transmission and data storage), a block, sometimes called a physical record, is a sequence of bytes or bits, usually containing some whole number of records, having a maximum length, a block size.[1] Data thus structured are said to be blocked. The process of putting data into blocks is called blocking, while deblocking is the process of extracting data from blocks. Blocked data is normally stored in a data buffer and read or written a whole block at a time. Blocking reduces the overhead and speeds up the handling of the data-stream.[2] For some devices such as magnetic tape and CKD disk devices blocking reduces the amount of external storage required for the data. Blocking is almost universally employed when storing data to 9-track magnetic tape, to NAND flash memory, and to rotating media such as floppy disks, hard disks, and optical discs.

Most file systems are based on a block device, which is a level of abstraction for the hardware responsible for storing and retrieving specified blocks of data, though the block size in file systems may be a multiple of the physical block size. This leads to space inefficiency due to internal fragmentation, since file lengths are often not integer multiples of block size, and thus the last block of a file may remain partially empty. This will create slack space, which averages half a block per file. Some newer file systems attempt to solve this through techniques called block suballocation and tail merging.

Block storage is normally abstracted by a file system or database management system (DBMS) for use by applications and end users. The physical or logical volumes accessed via block I/O may be devices internal to a server, directly attached via SCSI or Fibre Channel, or distant devices accessed via a storage area network (SAN) using a protocol such as iSCSI, or AoE. DBMSes often use their own block I/O for improved performance and recoverability as compared to layering the DBMS on top of a file system.

See also[edit]
Block (telecommunications)
Disk sector
Data cluster
Extent (file systems)
Object storage
DEBLOCK (CONFIG.SYS directive) - disk deblocking configuration under DR-DOS
File manager
From Wikipedia, the free encyclopedia
  (Redirected from Computer file management)
"File Manager" redirects here. It is not to be confused with File Manager (Windows).
A file manager or file browser is a computer program that provides a user interface to manage files and folders. The most common operations performed on files or groups of files include creating, opening (e.g. viewing, playing, editing or printing), renaming, moving or copying, deleting and searching for files, as well as modifying file attributes, properties and file permissions. Folders and files may be displayed in a hierarchical tree based on their directory structure. Some file managers contain features inspired by web browsers, including forward and back navigational buttons.

Some file managers provide network connectivity via protocols, such as FTP, NFS, SMB or WebDAV. This is achieved by allowing the user to browse for a file server (connecting and accessing the server's file system like a local file system) or by providing its own full client implementations for file server protocols.

Contents  [hide] 
1	Directory editors
2	File-list file manager
3	Orthodox file managers
3.1	Features
3.1.1	Tabbed panels
3.1.2	Portability
3.1.3	Dual-pane managers
4	Navigational file manager
4.1	Concepts
5	Spatial file manager
6	3D file managers
7	Web-based file managers
8	See also
9	References
10	External links
Directory editors[edit]
A term that predates the usage of file manager is directory editor. The first directory editor, DIRED, was invented circa 1974 at the Stanford Artificial Intelligence Lab by Stan Kugell[1][2]

A directory editor was written for EXEC 8 at the University of Maryland, and was available to other users at that time. The term was used by other developers, including Jay Lepreau, who wrote the dired program in 1980, [3] which ran on BSD. This was in turn inspired by an older program with the same name running on TOPS-20. Dired inspired other programs, including dired, the editor script (for emacs and similar editors), and ded. [4]

File-list file manager[edit]
File-list file managers are lesser known and older than orthodox file managers.

One such file manager is flist, which was first used in 1981 on the Conversational Monitor System.[5][6] This is a variant of fulist, which originated before late 1978, according to comments by its author, Theo Alkema.[7]

The flist program provided a list of files in the user's minidisk,[8] and allowed sorting by any file attribute. The file attributes could be passed to scripts or function-key definitions, making it simple to use flist as part of CMS EXEC, EXEC 2 or XEDIT scripts.

This program ran only on IBM VM/SP CMS, but was the inspiration for other programs, including filelist[9][10][11] (a script run via the Xedit editor), and programs running on other operating systems, including a program also called flist, which ran on OpenVMS,[12] and fulist (from the name of the corresponding internal IBM program),[13] which runs on Unix.[14]

Orthodox file managers[edit]

Midnight Commander, an orthodox file manager with a text-based user interface
Orthodox file managers (OFM) or command-based file managers have three windows (two panels and one command line window). Orthodox file managers are one of the older families of file managers. Developers create applications that duplicate and extend the interface that was introduced by PathMinder and John Socha's famous Norton Commander for DOS.[citation needed] The concept is more than thirty years old—PathMinder was released in 1984, and Norton Commander version 1.0 was released in 1986. Despite the age of this concept, file managers based on Norton Commander are actively developed, and dozens of implementations exist for DOS, Unix, and Microsoft Windows. Nikolai Bezroukov publishes his own set of criteria for an OFM standard (version 1.2 dated June 1997).[15]

Features[edit]
An orthodox file manager typically has three windows. Two of the windows are called panels and are positioned symmetrically at the top of the screen. The third is the command line, which is essentially a minimized command (shell) window that can be expanded to full screen. Only one of the panels is active at a given time. The active panel contains the "file cursor". Panels are resizable and can be hidden. Files in the active panel serve as the source of file operations performed by the manager. For example, files can be copied or moved from the active panel to the location represented in the passive panel. This scheme is most effective for systems in which the keyboard is the primary or sole input device. The active panel shows information about the current working directory and the files that it contains. The passive (inactive) panel shows the content of the same or another directory (the default target for file operations). Users may customize the display of columns that show relevant file information. The active panel and passive panel can be switched (often by pressing the tab key).

The following features describe the class of orthodox file managers.

They present the user with a two-panel directory view with a command line below. Either panel may be selected to be active; the other becomes passive. The active panel becomes the working area for delete and rename operations, while the passive panel serves as a target for copy and move operations. Panels may be shrunk, exposing the terminal window hidden behind them. Normally, only the last line of the terminal window (the command line) is visible.
They provide close integration with an underlying OS shell via command line, using the associated terminal window that permits viewing the results of executing shell commands entered on the command line (e.g., via Ctrl-O shortcut in Norton Commander).
They provide the user with extensive keyboard shortcuts.
The file manager frees the user from having to use the mouse.
Users can create their own file associations and scripts that are invoked for certain file types and organize these scripts into a hierarchical tree (e.g., as a user script library or user menu).[citation needed]
Users can extend the functionality of the manager via a so-called User menu or Start menu and extensions menu.
Other common features include:

Information on the "active" and "passive" panels may be used for constructing commands on the command line. Examples include current file, path to left panel, path to right panel, etc.
They provide a built-in viewer for (at least) the most basic file types.
They have a built-in editor. In many cases, the editor can extract certain elements of the panels into the text being edited.
Many support virtual file systems (VFS) such as viewing compressed archives, or working with files via an FTP connection.
They often have the word commander in the name, after Norton Commander.
Path: shows the source/destination location of the directory in use
Information about directory size, disk usage and disk name (usually at the bottom of the panels)
Panel with information about file name, extension, date and time of creation, last modification, and permissions (attributes).
Info panel with the number of files in directory, and the sum of the sizes of selected files.
Tabbed interface (usually GUI file managers)
Function keys: F1–F10 have all the same functions under all orthodox file managers. Examples: F5 always copies file(s) from the active to the inactive panel, while F6 moves the file.
Tabbed panels[edit]
The introduction of tabbed panels in some file managers (for example Total Commander) made it possible to manipulate more than one active and passive directory at a time.

Portability[edit]
Orthodox file managers[16] are among the most portable file managers. Examples are available on almost any platform, with both command-line and graphical interfaces. This is unusual among command line managers in that something purporting to be a standard for the interface is published. They are also actively supported by developers. This makes it possible to do the same work on different platforms without much relearning of the interface.

Dual-pane managers[edit]
Sometimes they are called dual-pane managers, a term that is typically used for programs such as the Windows File Explorer (see below). But they have three panes including a command line pane below (or hidden behind) two symmetric panes. Furthermore, most of these programs allow using just one of the two larger panes with the second hidden. Some also add an item to the Context Menu in Windows to "Open two Explorers, side by side".

Notable ones include:

Altap Salamander
Commander One
Demos Commander
Directory Opus
DOS Navigator
Double Commander
emelFM2
Far Manager
File Commander
GNOME Commander
Krusader
Midnight Commander
muCommander
Norton Commander
PowerDesk
PathMinder
SE-Explorer
Total Commander
Volkov Commander
WinSCP
XTree
ZTreeWin
Navigational file manager[edit]
A navigational file manager is a newer type of file manager. Since the advent of GUIs, it has become the dominant type of file manager for desktop computers.[citation needed]

Typically, it has two panes, with the filesystem tree in the left pane and the contents of the current directory in the right pane. For Mac OS X, one view in the Finder is an example of a navigational file manager.[dubious – discuss]


The Miller Column browser from GNUstep is a type of Navigational file manager.
Concepts[edit]
The window displays the location currently being viewed.
The location being viewed (the current directory) can be changed by the user by opening directories, pressing a back button, typing a location, or using the additional pane with the navigation tree representing all or part of the filesystem.
Icons represent files, programs, and directories.
The interface in a navigational file manager often resembles a web browser, complete with back and forward buttons, and often reload buttons. Most also contain an address bar into which the file or directory path (or URI) can be typed.

Most navigational file managers have two panes, the left pane being a tree view of the filesystem. This means that unlike orthodox file managers, the two panes are asymmetrical in their content and use.

Selecting a directory in the Navigation pane on the left designates it as the current directory, displaying its contents in the Contents pane on the right. However, expanding (+) or collapsing (-) a portion of the tree without selecting a directory will not alter the contents of the right pane. The exception to this behavior applies when collapsing a parent of the current directory, in which case the selection is refocused on the collapsed parent directory, thus altering the list in the Contents pane.

The process of moving from one location to another need not open a new window. Several instances of the file manager can be opened simultaneously and communicate with each other via drag-and-drop and clipboard operations, so it is possible to view several directories simultaneously and perform cut-and paste operations between instances.

File operations are based on drag-and-drop and editor metaphors: users can select and copy files or directories onto the clipboard and then paste them in a different place in the filesystem or even in a different instance of the file manager.

Notable examples of navigational file managers include:

Directory Opus
Dolphin in KDE
File Manager in Windows
Mac OS X Finder
Nautilus in GNOME (default since v2.30)
Windows Explorer
XTree / ZTreeWin[relevant? – discuss]
Spatial file manager[edit]

The Nautilus file manager had a spatial mode, which was removed with the arrival of GNOME (and with it Nautilus) version 3.x. Each of these windows displays an open directory.
Spatial file managers use a spatial metaphor to represent files and directories as if they were actual physical objects. A spatial file manager imitates the way people interact with physical objects.

Some ideas behind the concept of a spatial file manager are:

A single window represents each opened directory
Each window is unambiguously and irrevocably tied to a particular directory.
Stability: files, directories, and windows go where the user moves them, stay where the user puts them ("preserve their spatial state"), and retain all their other "physical" characteristics (such as size, shape, color and location).
The same item can only be viewed in one window at a time.
As in navigational file managers, when a directory is opened, the icon representing the directory changes—perhaps from an image showing a closed drawer to an opened one, perhaps the directory's icon turns into a silhouette filled with a pattern—and a new window is opened to represent that directory.

Examples of file managers that use a spatial metaphor to some extent include:

Apple's Finder 5 to 9 (In the Mac OS X Finder the spatial metaphor can still be used in general by enabling Finder's preference "Always open folders in a new window" [17] or on demand by holding the command key (?) while double-clicking a folder [18])
Konqueror has the option to turn into spatial mode
RISC OS Filer
Amiga's Workbench
GNOME's Nautilus from version 2.6 (default until 2.29, completely removed in 3.0)
MATE's Caja (though the default mode is navigational)
BeOS's Tracker
Haiku's Tracker
OS/2's Workplace Shell
Digital Research's GEM (implemented in Atari TOS and as a somewhat reduced version for PCs)
ROX-Filer file manager (ROX Desktop)
E17 file manager
Dysfunctional spatial file managers:

Windows Explorer in Windows 95 was set as a spatial file manager model by default; because it also worked as a navigational file manager, directories could be opened in multiple windows, which made it fail all the above criteria. Later versions gradually abandoned the spatial model.
Apple's Finder in Mac OS X was designed with a similar integration of spatial and navigational modes, meaning that the spatial mode did not actually work.[19]
3D file managers[edit]

This section may contain excessive, poor, or irrelevant examples. Please improve the article by adding more descriptive text and removing less pertinent examples. See Wikipedia's guide to writing better articles for further suggestions. (September 2013)

File System Visualizer, one example of a 3D file manager

In Cubix, files sharing the same attributes are represented by cubes in a 3D environment.
Some projects have attempted to implement a three-dimensional method of displaying files and directory structures. Three-dimensional file browsing has not yet become popular; the exact implementation tends to differ between projects, and there are no common standards to follow.

Examples of three-dimensional file managers include:

fsn, for Silicon Graphics' IRIX systems, notably featured prominently in one scene from the film Jurassic Park, as a representation of Unix systems.
File System Visualizer, or fsv, an open-source clone of fsn for modern Unix-like systems.
tdfsb,[20] an open-source 3D file browser, where one enters directories by flying into them (using WASD). Runs on Linux, FreeBSD and BeOS.
BumpTop, a file manager using a three-dimensional representation of a desktop with realistic physics, intended for use with a stylus and touchscreen.
Real Desktop,[21] a desktop replacement with similarities to BumpTop.
Cubix 3D Filer [22] is a Windows file manager which organizes files according to different attributes.
GopherVR, a 3D visualisation of networked Gopher resources.
Web-based file managers[edit]
Web-based file managers are typically scripts written in either PHP, Ajax, Perl, ASP or another server-side language. When installed on a local server or on a remote server, they allow files and directories located there to be managed and edited, using a web browser, without the need for FTP Access.

More advanced, and usually commercially distributed, web-based file management scripts allow the administrator of the file manager to configure secure, individual user accounts, each with individual account permissions. Authorized users have access to documents stored on the server or in their individual user directories anytime, from anywhere, via a web browser.

A web-based file manager can serve as an organization's digital repository. For example, documents, digital media, publishing layouts, and presentations can be stored, managed, and shared between customers, suppliers, and remote workers, or just internally.

Web-based file managers are becoming increasingly popular due to the rise in popularity of dynamic web content management systems (CMS) and the need for non-technical website moderators to manage media on their websites powered by these platforms.

An example is net2ftp, a PHP- and JavaScript-based FTP client.

See also[edit]
Batch renaming
Comparison of file managers
Disk space analyzer
Desktop metaphor
Miller columns
Spatial navigation
Data hierarchy
From Wikipedia, the free encyclopedia

This article may require cleanup to meet Wikipedia's quality standards. The specific problem is: this disaster is not fit for Wikipedia the way it is Please help improve this article if you can. (January 2015)
Data hierarchy refers to the systematic organization of data, often in a hierarchical form. Data organization involves fields, records, files and so on.[1]

A data field holds a single fact or attribute of an entity. Consider a date field, e.g. "September 19, 2004". This can be treated as a single date field (e.g. birthdate), or 3 fields, namely, month, day of month and year.

A record is a collection of related fields. An Employee record may contain a name field(s), address fields, birthdate field and so on.

A file is a collection of related records. If there are 100 employees, then each employee would have a record (e.g. called Employee Personal Details record) and the collection of 100 such records would constitute a file (in this case, called Employee Personal Details file).

Files are integrated into a database.[2] This is done using a Database Management System.[3] If there are other facets of employee data that we wish to capture, then other files such as Employee Training History file and Employee Work History file could be created as well.

An illustration of the above description is shown in this diagram below.

Data Hierarchy diagram

The following terms are for better clarity.

With reference to the example in the above diagram.

Data field label = Employee Name or EMP_NAME

Data field value = Jeffrey Tan

The above description is a view of data as understood by a user e.g. a person working in Human Resource Department.

The above structure can be seen in the hierarchical model, which is one way to organize data in a database.[3]

In terms of data storage, data fields are made of bytes and these in turn are made up of bits.

Steganography
From Wikipedia, the free encyclopedia
  (Redirected from File Camouflage)
For the process of writing in shorthand, see Stenography. For the prefix "Stego-" as used in taxonomy, see List of commonly used taxonomic affixes.
Steganography (US Listeni/?st?.???n??.?r?.fi/, UK /?st??.??n??.r?.fi/) is the practice of concealing a file, message, image, or video within another file, message, image, or video. The word steganography combines the Greek words steganos (????????), meaning "covered, concealed, or protected", and graphein (???????) meaning "writing".

The first recorded use of the term was in 1499 by Johannes Trithemius in his Steganographia, a treatise on cryptography and steganography, disguised as a book on magic. Generally, the hidden messages appear to be (or be part of) something else: images, articles, shopping lists, or some other cover text. For example, the hidden message may be in invisible ink between the visible lines of a private letter. Some implementations of steganography that lack a shared secret are forms of security through obscurity, whereas key-dependent steganographic schemes adhere to Kerckhoffs's principle.[1]

The advantage of steganography over cryptography alone is that the intended secret message does not attract attention to itself as an object of scrutiny. Plainly visible encrypted messages—no matter how unbreakable—arouse interest, and may in themselves be incriminating in countries where encryption is illegal.[2] Thus, whereas cryptography is the practice of protecting the contents of a message alone, steganography is concerned with concealing the fact that a secret message is being sent, as well as concealing the contents of the message.

Steganography includes the concealment of information within computer files. In digital steganography, electronic communications may include steganographic coding inside of a transport layer, such as a document file, image file, program or protocol. Media files are ideal for steganographic transmission because of their large size. For example, a sender might start with an innocuous image file and adjust the color of every 100th pixel to correspond to a letter in the alphabet, a change so subtle that someone not specifically looking for it is unlikely to notice it.

Contents  [hide] 
1	History
2	Techniques
2.1	Physical
2.2	Digital messages
2.2.1	Digital text
2.2.2	Social steganography
2.3	Network
2.4	Printed
2.5	Using puzzles
3	Additional terminology
4	Countermeasures and detection
5	Applications
5.1	Use in modern printers
5.2	Example from modern practice
5.3	Alleged use by intelligence services
5.4	Distributed steganography
5.5	Online challenge
6	See also
7	Citations
8	References
9	External links
History[edit]

A chart from Johannes Trithemius's Steganographia copied by Dr John Dee in 1591
The first recorded uses of steganography can be traced back to 440 BC when Herodotus mentions two examples in his Histories.[3] Demaratus sent a warning about a forthcoming attack to Greece by writing it directly on the wooden backing of a wax tablet before applying its beeswax surface. Wax tablets were in common use then as reusable writing surfaces, sometimes used for shorthand.

In his work Polygraphiae Johannes Trithemius developed his so-called "Ave-Maria-Cipher" that can hide information in a Latin praise of God. "Auctor Sapientissimus Conseruans Angelica Deferat Nobis Charitas Potentissimi Creatoris" for example contains the concealed word VICIPEDIA.[4]

Techniques[edit]

Deciphering the code. Steganographia
Physical[edit]
Steganography has been widely used, including in recent historical times and the present day. Known examples include:

Hidden messages within wax tablet—in ancient Greece, people wrote messages on wood and covered it with wax that bore an innocent covering message.
Hidden messages on messenger's body—also used in ancient Greece. Herodotus tells the story of a message tattooed on the shaved head of a slave of Histiaeus, hidden by the hair that afterwards grew over it, and exposed by shaving the head. The message allegedly carried a warning to Greece about Persian invasion plans. This method has obvious drawbacks, such as delayed transmission while waiting for the slave's hair to grow, and restrictions on the number and size of messages that can be encoded on one person's scalp.
During World War II, the French Resistance sent some messages written on the backs of couriers in invisible ink.
Hidden messages on paper written in secret inks, under other messages or on the blank parts of other messages
Messages written in Morse code on yarn and then knitted into a piece of clothing worn by a courier.
Messages written on envelopes in the area covered by postage stamps.
In the early days of the printing press, it was common to mix different typefaces on a printed page due to the printer not having enough copies of some letters in one typeface. Because of this, a message could be hidden using two (or more) different typefaces, such as normal or italic.
During and after World War II, espionage agents used photographically produced microdots to send information back and forth. Microdots were typically minute (less than the size of the period produced by a typewriter). World War II microdots were embedded in the paper and covered with an adhesive, such as collodion. This was reflective, and thus detectable by viewing against glancing light. Alternative techniques included inserting microdots into slits cut into the edge of post cards.
During WWII, Velvalee Dickinson, a spy for Japan in New York City, sent information to accommodation addresses in neutral South America. She was a dealer in dolls, and her letters discussed the quantity and type of doll to ship. The stegotext was the doll orders, while the concealed "plaintext" was itself encoded and gave information about ship movements, etc. Her case became somewhat famous and she became known as the Doll Woman.
Jeremiah Denton repeatedly blinked his eyes in Morse Code during the 1966 televised press conference that he was forced into as an American POW by his North Vietnamese captors, spelling out "T-O-R-T-U-R-E". This confirmed for the first time to the U.S. Military (naval intelligence) and Americans that the North Vietnamese were torturing American POWs.
Cold War counter-propaganda. In 1968, crew members of the USS Pueblo intelligence ship held as prisoners by North Korea, communicated in sign language during staged photo opportunities, informing the United States they were not defectors, but captives of the North Koreans. In other photos presented to the US, crew members gave "the finger" to the unsuspecting North Koreans, in an attempt to discredit photos that showed them smiling and comfortable.
Digital messages[edit]

Image of a tree with a steganographically hidden image. The hidden image is revealed by removing all but the two least significant bits of each color component and a subsequent normalization. The hidden image is shown below.

Image of a cat extracted from the tree image above.
Modern steganography entered the world in 1985 with the advent of personal computers being applied to classical steganography problems.[5] Development following that was very slow, but has since taken off, going by the large number of steganography software available:

Concealing messages within the lowest bits of noisy images or sound files.
Concealing data within encrypted data or within random data. The message to conceal is encrypted, then used to overwrite part of a much larger block of encrypted data or a block of random data (an unbreakable cipher like the one-time pad generates ciphertexts that look perfectly random without the private key).
Chaffing and winnowing.
Mimic functions convert one file to have the statistical profile of another. This can thwart statistical methods that help brute-force attacks identify the right solution in a ciphertext-only attack.
Concealed messages in tampered executable files, exploiting redundancy in the targeted instruction set.
Pictures embedded in video material (optionally played at slower or faster speed).
Injecting imperceptible delays to packets sent over the network from the keyboard. Delays in keypresses in some applications (telnet or remote desktop software) can mean a delay in packets, and the delays in the packets can be used to encode data.
Changing the order of elements in a set.
Content-Aware Steganography hides information in the semantics a human user assigns to a datagram. These systems offer security against a nonhuman adversary/warden.
Blog-Steganography. Messages are fractionalized and the (encrypted) pieces are added as comments of orphaned web-logs (or pin boards on social network platforms). In this case the selection of blogs is the symmetric key that sender and recipient are using; the carrier of the hidden message is the whole blogosphere.
Modifying the echo of a sound file (Echo Steganography).[6]
Steganography for audio signals.[7]
Image bit-plane complexity segmentation steganography
Including data in ignored sections of a file, such as after the logical end of the carrier file.
Digital text[edit]
Making text the same color as the background in word processor documents, e-mails, and forum posts.
Using Unicode characters that look like the standard ASCII character set. On most systems, there is no visual difference from ordinary text. Some systems may display the fonts differently, and the extra information would then be easily spotted, of course.
Using hidden (control) characters, and redundant use of markup (e.g., empty bold, underline or italics) to embed information within HTML, which is visible by examining the document source. HTML pages can contain code for extra blank spaces and tabs at the end of lines, and colours, fonts and sizes, which are not visible when displayed.
Using non-printing Unicode characters Zero-Width Joiner (ZWJ) and Zero-Width Non-Joiner (ZWNJ).[8] These characters are used for joining and disjoining letters in Arabic and Persian, but can be used in Roman alphabets for hiding information because they have no meaning in Roman alphabets: because they are "zero-width" they are not displayed. ZWJ and ZWNJ can represent "1" and "0".
Social steganography[edit]
In communities with social or government taboos or censorship, people use cultural steganography—hiding messages in idiom, pop culture references, and other messages they share publicly and assume are monitored. This relies on social context to make the underlying messages visible only to certain readers.[9][10] Examples include:

Hiding a message in the title and context of a shared video or image
Misspelling names or words that are popular in the media in a given week, to suggest an alternate meaning
Hiding a Picture which can be traced by using Paint or any other Drawing tool.
Network[edit]
All information hiding techniques that may be used to exchange steganograms in telecommunication networks can be classified under the general term of network steganography. This nomenclature was originally introduced by Krzysztof Szczypiorski in 2003.[11] Contrary to typical steganographic methods that use digital media (images, audio and video files) to hide data, network steganography uses communication protocols' control elements and their intrinsic functionality. As a result, such methods are harder to detect and eliminate.[12]

Typical network steganography methods involve modification of the properties of a single network protocol. Such modification can be applied to the PDU (Protocol Data Unit),[13][14][15] to the time relations between the exchanged PDUs,[16] or both (hybrid methods).[17]

Moreover, it is feasible to utilize the relation between two or more different network protocols to enable secret communication. These applications fall under the term inter-protocol steganography.[18]

Network steganography covers a broad spectrum of techniques, which include, among others:

Steganophony — the concealment of messages in Voice-over-IP conversations, e.g. the employment of delayed or corrupted packets that would normally be ignored by the receiver (this method is called LACK — Lost Audio Packets Steganography), or, alternatively, hiding information in unused header fields.[19]
WLAN Steganography – transmission of steganograms in Wireless Local Area Networks. A practical example of WLAN Steganography is the HICCUPS system (Hidden Communication System for Corrupted Networks)[20]
Printed[edit]
Digital steganography output may be in the form of printed documents. A message, the plaintext, may be first encrypted by traditional means, producing a ciphertext. Then, an innocuous covertext is modified in some way so as to contain the ciphertext, resulting in the stegotext. For example, the letter size, spacing, typeface, or other characteristics of a covertext can be manipulated to carry the hidden message. Only a recipient who knows the technique used can recover the message and then decrypt it. Francis Bacon developed Bacon's cipher as such a technique.

The ciphertext produced by most digital steganography methods, however, is not printable. Traditional digital methods rely on perturbing noise in the channel file to hide the message, as such, the channel file must be transmitted to the recipient with no additional noise from the transmission. Printing introduces much noise in the ciphertext, generally rendering the message unrecoverable. There are techniques that address this limitation, one notable example is ASCII Art Steganography.[21]

Using puzzles[edit]
The art of concealing data in a puzzle can take advantage of the degrees of freedom in stating the puzzle, using the starting information to encode a key within the puzzle / puzzle image.

For instance, steganography using sudoku puzzles has as many keys as there are possible solutions of a sudoku puzzle, which is 6.71?1021. This is equivalent to around 70 bits, making it much stronger than the DES method, which uses a 56 bit key.[22]

Additional terminology[edit]
Discussions of steganography generally use terminology analogous to (and consistent with) conventional radio and communications technology. However, some terms show up in software specifically, and are easily confused. These are most relevant to digital steganographic systems.

The payload is the data covertly communicated. The carrier is the signal, stream, or data file that hides the payload—which differs from the channel (which typically means the type of input, such as a JPEG image). The resulting signal, stream, or data file with the encoded payload is sometimes called the package, stego file, or covert message. The percentage of bytes, samples, or other signal elements modified to encode the payload is called the encoding density, and is typically expressed as a number between 0 and 1.

In a set of files, those files considered likely to contain a payload are suspects. A suspect identified through some type of statistical analysis might be referred to as a candidate.

Countermeasures and detection[edit]
Detecting physical steganography requires careful physical examination—including the use of magnification, developer chemicals and ultraviolet light. It is a time-consuming process with obvious resource implications, even in countries that employ large numbers of people to spy on their fellow nationals. However, it is feasible to screen mail of certain suspected individuals or institutions, such as prisons or prisoner-of-war (POW) camps.

During World War II, prisoner of war camps gave prisoners specially treated paper that would reveal invisible ink. An article in the 24 June 1948 issue of Paper Trade Journal by the Technical Director of the United States Government Printing Office, Morris S. Kantrowitz, describes, in general terms, the development of this paper. They used three prototype papers named Sensicoat, Anilith, and Coatalith. These were for the manufacture of post cards and stationery provided to German prisoners of war in the US and Canada. If POWs tried to write a hidden message, the special paper rendered it visible. The U.S. granted at least two patents related to this technology—one to Kantrowitz, U.S. Patent 2,515,232, "Water-Detecting paper and Water-Detecting Coating Composition Therefor," patented 18 July 1950, and an earlier one, "Moisture-Sensitive Paper and the Manufacture Thereof", U.S. Patent 2,445,586, patented 20 July 1948. A similar strategy is to issue prisoners with writing paper ruled with a water-soluble ink that runs in contact with water-based invisible ink.

In computing, steganographically encoded package detection is called steganalysis. The simplest method to detect modified files, however, is to compare them to known originals. For example, to detect information being moved through the graphics on a website, an analyst can maintain known clean-copies of these materials and compare them against the current contents of the site. The differences, assuming the carrier is the same, comprise the payload. In general, using extremely high compression rates makes steganography difficult, but not impossible. Compression errors provide a hiding place for data, but high compression reduces the amount of data available to hold the payload, raising the encoding density, which facilitates easier detection (in extreme cases, even by casual observation).

Applications[edit]
Use in modern printers[edit]
Main article: Printer steganography
Some modern computer printers use steganography, including HP and Xerox brand color laser printers. These printers add tiny yellow dots to each page. The barely-visible dots contain encoded printer serial numbers and date and time stamps.[23]

Example from modern practice[edit]
The larger the cover message (in binary data, the number of bits) relative to the hidden message, the easier it is to hide the latter. For this reason, digital pictures (which contain large amounts of data) are used to hide messages on the Internet and on other communication media. It is not clear how common this actually is. For example: a 24-bit bitmap uses 8 bits to represent each of the three color values (red, green, and blue) at each pixel. The blue alone has 28 different levels of blue intensity. The difference between 11111111 and 11111110 in the value for blue intensity is likely to be undetectable by the human eye. Therefore, the least significant bit can be used more or less undetectably for something else other than color information. If this is repeated for the green and the red elements of each pixel as well, it is possible to encode one letter of ASCII text for every three pixels.

Stated somewhat more formally, the objective for making steganographic encoding difficult to detect is to ensure that the changes to the carrier (the original signal) due to the injection of the payload (the signal to covertly embed) are visually (and ideally, statistically) negligible; that is to say, the changes are indistinguishable from the noise floor of the carrier. Any medium can be a carrier, but media with a large amount of redundant or compressible information are better suited.

From an information theoretical point of view, this means that the channel must have more capacity than the "surface" signal requires; that is, there must be redundancy. For a digital image, this may be noise from the imaging element; for digital audio, it may be noise from recording techniques or amplification equipment. In general, electronics that digitize an analog signal suffer from several noise sources such as thermal noise, flicker noise, and shot noise. This noise provides enough variation in the captured digital information that it can be exploited as a noise cover for hidden data. In addition, lossy compression schemes (such as JPEG) always introduce some error into the decompressed data; it is possible to exploit this for steganographic use as well.

Steganography can be used for digital watermarking, where a message (being simply an identifier) is hidden in an image so that its source can be tracked or verified (for example, Coded Anti-Piracy), or even just to identify an image (as in the EURion constellation).

Alleged use by intelligence services[edit]
In 2010, the Federal Bureau of Investigation alleged that the Russian foreign intelligence service uses customized steganography software for embedding encrypted text messages inside image files for certain communications with "illegal agents" (agents under non-diplomatic cover) stationed abroad.[24]

Distributed steganography[edit]
There are distributed steganography methods,[25] including methodologies that distribute the payload through multiple carrier files in diverse locations to make detection more difficult. For example, U.S. Patent 8,527,779 by cryptographer William Easttom (Chuck Easttom).

Online challenge[edit]
The puzzles presented by Cicada 3301 incorporates steganography with cryptography and other solving techniques since 2012.[26]

See also[edit]
Steganography tools
Cryptography
BPCS-Steganography
Camera/Shy
Canary trap
Covert channel
Deniable encryption
Invisible ink
Polybius square
Security engineering
Semiotics
Steganographic file system
Digital watermarking
Watermark detection
Acrostic
File copying
From Wikipedia, the free encyclopedia

This article does not cite any sources. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2009) (Learn how and when to remove this template message)
In the realm of computer file management, file copying is the creation of a new file which has the same content as an existing file.

All computer operating systems include file copying provisions in the user interface, like the command, "cp" in Unix and "copy" in MS-DOS; operating systems with a graphical user interface, or GUI, usually provide copy-and-paste or drag-and-drop methods of file copying.  File manager applications, too, provide an easy way of copying files.

Internally, however, while some systems have specialized application programming interfaces (APIs) for copying files (like CopyFile and CopyFileEx in Windows API), others (like Unix and MS-DOS) fall back to simply reading the contents of the old file and writing it to the new file. This makes little difference with local files (those on the computer's hard drive), but provides an interesting situation when both the source and target files are located on a remote file server.  Operating systems with specialized file copying APIs are usually able to tell the server to perform the copying locally, without sending file contents over the network, thus greatly improving performance.  Those systems that have no comparable APIs, however, have to read the file contents over the network, and then send them back again, over the network.  Sometimes, remote file copying is performed with a specialized command, like "ncopy" in MS-DOS clients for Novell NetWare.

An even more complicated situation arises when one needs to copy files between two remote servers. The simple way is to read data from one server, and then to write the data to the second server. 

Data conversion
From Wikipedia, the free encyclopedia
  (Redirected from File conversion)

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2014) (Learn how and when to remove this template message)
Data conversion is the conversion of computer data from one format to another. Throughout a computer environment, data is encoded in a variety of ways. For example, computer hardware is built on the basis of certain standards, which requires that data contains, for example, parity bit checks. Similarly, the operating system is predicated on certain standards for data and file handling. Furthermore, each computer program handles data in a different manner. Whenever any one of these variables is changed, data must be converted in some way before it can be used by a different computer, operating system or program. Even different versions of these elements usually involve different data structures. For example, the changing of bits from one format to another, usually for the purpose of application interoperability or of capability of using new features, is merely a data conversion. Data conversions may be as simple as the conversion of a text file from one character encoding system to another; or more complex, such as the conversion of office file formats, or the conversion of image and audio file formats.

There are many ways in which data is converted within the computer environment. This may be seamless, as in the case of upgrading to a newer version of a computer program. Alternatively, the conversion may require processing by the use of a special conversion program, or it may involve a complex process of going through intermediary stages, or involving complex "exporting" and "importing" procedures, which may include converting to and from a tab-delimited or comma-separated text file. In some cases, a program may recognize several data file formats at the data input stage and then is also capable of storing the output data in a number of different formats. Such a program may be used to convert a file format. If the source format or target format is not recognized, then at times a third program may be available which permits the conversion to an intermediate format, which can then be reformatted using the first program. There are many possible scenarios.

Contents  [hide] 
1	Information basics
2	Pivotal conversion
3	Lost and inexact data conversion
4	Open vs. secret specifications
5	Electronics
6	See also
7	References
Information basics[edit]
Before any data conversion is carried out, the user or application programmer should keep a few basics of computing and information theory in mind. These include:

Information can easily be discarded by the computer, but adding information takes effort.
The computer can add information only in a rule-based fashion.[citation needed]
Upsampling the data or converting to a more feature-rich format does not add information; it merely makes room for that addition, which usually a human must do.
Data stored in an electronic format can be quickly modified and analyzed.
For example, a true color image can easily be converted to grayscale, while the opposite conversion is a painstaking process. Converting a Unix text file to a Microsoft (DOS/Windows) text file involves adding characters, but this does not increase the entropy since it is rule-based; whereas the addition of color information to a grayscale image cannot be done programmatically, since only a human[citation needed] knows which colors are needed for each section of the picture–there are no rules that can be used to automate that process. Converting a 24-bit PNG to a 48-bit one does not add information to it, it only pads existing RGB pixel values with zeroes[citation needed], so that a pixel with a value of FF C3 56, for example, becomes FF00 C300 5600. The conversion makes it possible to change a pixel to have a value of, for instance, FF80 C340 56A0, but the conversion itself does not do that, only further manipulation of the image can. Converting an image or audio file in a lossy format (like JPEG or Vorbis) to a lossless (like PNG or FLAC) or uncompressed (like BMP or WAV) format only wastes space, since the same image with its loss of original information (the artifacts of lossy compression) becomes the target. A JPEG image can never be restored to the quality of the original image from which it was made, no matter how much the user tries the "JPEG Artifact Removal" feature of his or her image manipulation program.

Automatic restoration of information that was lost through a lossy compression process would probably require important advances in artificial intelligence.

Because of these realities of computing and information theory, data conversion is often a complex and error-prone process that requires the help of experts.

Pivotal conversion[edit]
Data conversion can occur directly from one format to another, but many applications that convert between multiple formats use a pivotal encoding by way of which any source format is converted to its target. For example, it is possible to convert Cyrillic text from KOI8-R to Windows-1251 using a lookup table between the two encodings, but the modern approach is to convert the KOI8-R file to Unicode first and from that to Windows-1251. This is a more manageable approach; rather than needing lookup tables for all possible pairs of character encodings, an application needs only one lookup table for each character set, which it uses to convert to and from Unicode, thereby scaling the number of tables down from hundreds to a few tens.

Pivotal conversion is similarly used in other areas. Office applications, when employed to convert between office file formats, use their internal, default file format as a pivot. For example, a word processor may convert an RTF file to a WordPerfect file by converting the RTF to OpenDocument and then that to WordPerfect format. An image conversion program does not convert a PCX image to PNG directly; instead, when loading the PCX image, it decodes it to a simple bitmap format for internal use in memory, and when commanded to convert to PNG, that memory image is converted to the target format. An audio converter that converts from FLAC to AAC decodes the source file to raw PCM data in memory first, and then performs the lossy AAC compression on that memory image to produce the target file.

Lost and inexact data conversion[edit]
The objective of data conversion is to maintain all of the data, and as much of the embedded information as possible. This can only be done if the target format supports the same features and data structures present in the source file. Conversion of a word processing document to a plain text file necessarily involves loss of formatting information, because plain text format does not support word processing constructs such as marking a word as boldface. For this reason, conversion from one format to another which does not support a feature which is important to the user is rarely carried out, though it may be necessary for interoperability, e.g. converting a file from one version of Microsoft Word to an earlier version to enable transfer and use by other users who do not have the same later version of Word installed on their computer.

Loss of information can be mitigated by approximation in the target format. There is no way of converting a character like ? to ASCII, since the ASCII standard lacks it, but the information may be retained by approximating the character as ae. Of course, this is not an optimal solution, and can impact operations like searching and copying; and if a language makes a distinction between ? and ae, then that approximation does involve loss of information.

Data conversion can also suffer from inexactitude, the result of converting between formats that are conceptually different. The WYSIWYG paradigm, extant in word processors and desktop publishing applications, versus the structural-descriptive paradigm, found in SGML, XML and many applications derived therefrom, like HTML and MathML, is one example. Using a WYSIWYG HTML editor conflates the two paradigms, and the result is HTML files with suboptimal, if not nonstandard, code. In the WYSIWYG paradigm a double linebreak signifies a new paragraph, as that is the visual cue for such a construct, but a WYSIWYG HTML editor will usually convert such a sequence to <BR><BR>, which is structurally no new paragraph at all. As another example, converting from PDF to an editable word processor format is a tough chore, because PDF records the textual information like engraving on stone, with each character given a fixed position and linebreaks hard-coded, whereas word processor formats accommodate text reflow. PDF does not know of a word space character—the space between two letters and the space between two words differ only in quantity. Therefore, a title with ample letter-spacing for effect will usually end up with spaces in the word processor file, for example INTRODUCTION with spacing of 1 em as I N T R O D U C T I O N on the word processor.

Open vs. secret specifications[edit]
Successful data conversion requires thorough knowledge of the workings of both source and target formats. In the case where the specification of a format is unknown, reverse engineering will be needed to carry out conversion. Reverse engineering can achieve close approximation of the original specifications, but errors and missing features can still result.

File deletion
From Wikipedia, the free encyclopedia
For deleting a file in Wikipedia, see Wikipedia:Files for deletion.

This article includes a list of references, related reading or external links, but its sources remain unclear because it lacks inline citations. Please improve this article by introducing more precise citations. (March 2015) (Learn how and when to remove this template message)

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2015) (Learn how and when to remove this template message)
File deletion is a way of removing a file from a computer's file system.

Examples of reasons for deleting files are:

Freeing the disk space
Removing duplicate or unnecessary data to avoid confusion
Making sensitive information unavailable to others
All operating systems include commands for deleting files (rm on Unix, era in CP/M and DR-DOS, del/erase in MS-DOS/PC DOS, DR-DOS, Microsoft Windows etc.). File managers also provide a convenient way of deleting files. Files may be deleted one-by-one, or a whole directory tree may be deleted.

Contents  [hide] 
1	Problem with accidental removal
2	Problem with sensitive data
3	See also
4	References
Problem with accidental removal[edit]
Main article: Data recovery
The common problem with deleting files is accidental removal of information that later proves to be important. One way to deal with this is to back up files regularly. Erroneously deleted files may then be found in archives.

Another technique often used is not to delete files instantly, but to move them to a temporary directory whose contents can then be deleted at will. This is how the "recycle bin" or "trash can" works. Microsoft Windows and Apple's Mac OS X, as well as some Linux distributions, all employ this strategy.

In MS-DOS, one can use the undelete command. In MS-DOS the "deleted" files are not really deleted, but only marked as deleted—so they could be undeleted during some time, until the disk blocks they used are eventually taken up by other files. This is how data recovery programs work, by scanning for files that have been marked as deleted. As the space is freed up per byte, rather than per file, this can sometimes cause data to be recovered incompletely. Defragging a drive may prevent undeletion, as the blocks used by deleted file might be overwritten since they are marked as "empty".

Another precautionary measure is to mark important files as read-only. Many operating systems will warn the user trying to delete such files. Where file system permissions exist, users who lack the necessary permissions are only able to delete their own files, preventing the erasure of other people's work or critical system files.

Under Unix-like operating systems, in order to delete a file, one must usually have write permission to the parent directory of that file.

Problem with sensitive data[edit]
Main article: Data remanence
The common problem with sensitive data is that deleted files are not really erased and so may be recovered by interested parties. Most file systems only remove the link to data (see undelete, above). But even overwriting parts of the disk with something else or formatting it may not guarantee that the sensitive data is completely unrecoverable. Special software is available that overwrites data, and modern (post-2001) ATA drives include a secure erase command in firmware. However, high security applications and high-security enterprises can sometimes require that a disk drive be physically destroyed to ensure data is not recoverable, as microscopic changes in head alignment and other effects can mean even such measures are not guaranteed.

Directory (computing)
From Wikipedia, the free encyclopedia
  (Redirected from File directory)

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (November 2014) (Learn how and when to remove this template message)

Screenshot of a Windows command prompt window showing a directory listing.
In computing, a directory is a file system cataloging structure which contains references to other computer files, and possibly other directories. On many computers, directories are known as folders, catalogs (catalog was used on the Apple II, the Commodore 128 and some other early home computers as a command for displaying disk contents; the filesystems used by these did not support hierarchal directories), or drawers[1] to provide some relevancy to a workbench or the traditional office file cabinet.

Files are organized by storing related files in the same directory. In a hierarchical filesystem (that is, one in which files and directories are organized in a manner that resembles a tree), a directory contained inside another directory is called a subdirectory. The terms parent and child are often used to describe the relationship between a subdirectory and the directory in which it is cataloged, the latter being the parent. The top-most directory in such a filesystem, which does not have a parent of its own, is called the root directory.

Contents  [hide] 
1	Overview
1.1	Folder metaphor
2	Lookup cache
3	See also
4	References
5	External links
Overview[edit]

Diagram of a hierarchal directory tree. The root directory is here called 'MFD', for Master File Directory.
Historically, and even on some modern embedded systems, the file systems either had no support for directories at all or only had a "flat" directory structure, meaning subdirectories were not supported; there were only a group of top-level directories each containing files.

Most modern Unix-like systems, especially Linux, have a standard directory structure defined by the Filesystem Hierarchy Standard.

In many operating systems, programs have an associated working directory in which they execute. Typically, file names accessed by the program are assumed to reside within this directory if the file names are not specified with an explicit directory name.

Some operating systems restrict a user's access to only their home directory or project directory, thus isolating their activities from all other users. In early versions of Unix the root directory was the home directory of the root user, but modern Unix usually uses another directory such as /root for this purpose.

In keeping with Unix philosophy, Unix systems treat directories as a type of file.[2]

Folder metaphor[edit]

Sample folder icon (from KDE).
The name folder, presenting an analogy to the file folder used in offices, and used in a hierarchical file system design for the Electronic Recording Machine, Accounting (ERMA) Mark 1 published in 1958[3] as well as by Xerox Star,[4] is used in almost all modern operating systems' desktop environments. Folders are often depicted with icons which visually resemble physical file folders.

There is a difference between a directory, which is a file system concept, and the graphical user interface metaphor that is used to represent it (a folder). For example, Microsoft Windows uses the concept of special folders to help present the contents of the computer to the user in a fairly consistent way that frees the user from having to deal with absolute directory paths, which can vary between versions of Windows, and between individual installations. Many operating systems also have the concept of "smart folders" that reflect the results of a file system search or other operation. These folders do not represent a directory in the file hierarchy. Many email clients allow the creation of folders to organize email. These folders have no corresponding representation in the filesystem structure.

If one is referring to a container of documents, the term folder is more appropriate. The term directory refers to the way a structured list of document files and folders is stored on the computer. The distinction can be due to the way a directory is accessed; on Unix systems, /usr/bin/ is usually referred to as a directory when viewed in a command line console, but if accessed through a graphical file manager, users may sometimes call it a folder.

Lookup cache[edit]
[icon]	This section requires expansion. (December 2013)
Operating systems that support hierarchical filesystems (practically all modern ones) implement a form of caching to RAM of recent pathnames lookups. In the Unix world, this is usually called Directory Name Lookup Cache (DNLC), although it is called dcache on Linux.[5]

For local filesystems, DNLC entries normally expire only under pressure from other more recent entries. For network file systems a coherence mechanism is necessary to ensure that entries have not been invalidated by other clients.[5]
File system
From Wikipedia, the free encyclopedia
This article is about the way computers store files on disk. For library and office filing systems, see Library classification.
In computing, a file system (or filesystem) is used to control how data is stored and retrieved. Without a file system, information placed in a storage area would be one large body of data with no way to tell where one piece of information stops and the next begins. By separating the data into pieces and giving each piece a name, the information is easily isolated and identified. Taking its name from the way paper-based information systems are named, each group of data is called a "file". The structure and logic rules used to manage the groups of information and their names is called a "file system".

There are many different kinds of file systems. Each one has different structure and logic, properties of speed, flexibility, security, size and more. Some file systems have been designed to be used for specific applications. For example, the ISO 9660 file system is designed specifically for optical discs.

File systems can be used on numerous different types of storage devices that use different kinds of media. The most common storage device in use today is a hard disk drive, in which the media is a disc that has been coated with a magnetic film. The film has ones and zeros "written" on it by sending electrical pulses to a magnetic "read-write" head. Other kinds of media that are used include flash memory, magnetic tapes, and optical discs. In some cases, such as with tmpfs, the computer's main memory (random-access memory, RAM) is used to create a temporary file system for short-term use.

Some file systems are used on local data storage devices;[1] others provide file access via a network protocol (for example, NFS,[2] SMB, or 9P clients). Some file systems are "virtual", meaning that the supplied "files" (called virtual files) are computed on request (e.g. procfs) or are merely a mapping into a different file system used as a backing store. The file system manages access to both the content of files and the metadata about those files. It is responsible for arranging storage space; reliability, efficiency, and tuning with regard to the physical storage medium are important design considerations.

Contents  [hide] 
1	Aspects of file systems
1.1	Space management
1.2	Filenames
1.3	Directories
1.4	Metadata
1.5	File system as an abstract user interface
1.6	Utilities
1.7	Restricting and permitting access
1.8	Maintaining integrity
1.9	User data
1.10	Using a file system
1.11	Multiple file systems within a single system
1.12	Design limitations
2	Types of file systems
2.1	Disk file systems
2.1.1	Optical discs
2.2	Flash file systems
2.3	Tape file systems
2.3.1	Tape formatting
2.4	Database file systems
2.5	Transactional file systems
2.6	Network file systems
2.7	Shared disk file systems
2.8	Special file systems
2.8.1	Device file systems
2.8.2	Other special file systems
2.9	Minimal file system / audio-cassette storage
2.10	Flat file systems
3	File systems and operating systems
3.1	Unix and Unix-like operating systems
3.1.1	Linux
3.1.2	Solaris
3.1.3	OS X
3.2	PC-BSD
3.3	Plan 9
3.4	Microsoft Windows
3.4.1	FAT
3.4.2	NTFS
3.4.3	exFAT
3.5	OpenVMS
3.6	MVS [IBM Mainframe]
3.7	AS/400 file system
3.8	Other file systems
4	Limitations
4.1	Converting the type of a file system
4.1.1	In-place conversion
4.1.2	Migrating to a different file system
4.2	Long file paths and long file names
5	See also
6	Notes
7	References
7.1	Further reading
8	Further reading
8.1	Books
8.2	Online
9	External links
Aspects of file systems[edit]
Space management[edit]
Note: this only applies to file systems used in storage devices.


An example of slack space, demonstrated with 4,096-byte NTFS clusters: 100,000 files, each five bytes per file, which equal to 500,000 bytes of actual data but require 409,600,000 bytes of disk space to store
File systems allocate space in a granular manner, usually multiple physical units on the device. The file system is responsible for organizing files and directories, and keeping track of which areas of the media belong to which file and which are not being used. For example, in Apple DOS of the early 1980s, 256-byte sectors on 140 kilobyte floppy disk used a track/sector map.[citation needed]

This results in unused space when a file is not an exact multiple of the allocation unit, sometimes referred to as slack space. For a 512-byte allocation, the average unused space is 256 bytes. For 64 KB clusters, the average unused space is 32 KB. The size of the allocation unit is chosen when the file system is created. Choosing the allocation size based on the average size of the files expected to be in the file system can minimize the amount of unusable space. Frequently the default allocation may provide reasonable usage. Choosing an allocation size that is too small results in excessive overhead if the file system will contain mostly very large files.

File system fragmentation occurs when unused space or single files are not contiguous. As a file system is used, files are created, modified and deleted. When a file is created the file system allocates space for the data. Some file systems permit or require specifying an initial space allocation and subsequent incremental allocations as the file grows. As files are deleted the space they were allocated eventually is considered available for use by other files. This creates alternating used and unused areas of various sizes. This is free space fragmentation. When a file is created and there is not an area of contiguous space available for its initial allocation the space must be assigned in fragments. When a file is modified such that it becomes larger it may exceed the space initially allocated to it, another allocation must be assigned elsewhere and the file becomes fragmented.

Filenames[edit]
Main article: Filename
A filename (or file name) is used to identify a storage location in the file system. Most file systems have restrictions on the length of filenames. In some file systems, filenames are not case sensitive (i.e., filenames such as FOO and foo refer to the same file); in others, filenames are case sensitive (i.e., the names FOO, Foo and foo refer to three separate files).

Most modern file systems allow filenames to contain a wide range of characters from the Unicode character set. However, they may have restrictions on the use of certain special characters, disallowing them within filenames; those characters might be used to indicate a device, device type, directory prefix, file path separator, or file type.

Directories[edit]
Main article: Directory (file systems)
File systems typically have directories (also called folders) which allow the user to group files into separate collections. This may be implemented by associating the file name with an index in a table of contents or an inode in a Unix-like file system. Directory structures may be flat (i.e. linear), or allow hierarchies where directories may contain subdirectories. The first file system to support arbitrary hierarchies of directories was used in the Multics operating system.[3] The native file systems of Unix-like systems also support arbitrary directory hierarchies, as do, for example, Apple's Hierarchical File System, and its successor HFS+ in classic Mac OS (HFS+ is still used in Mac OS X), the FAT file system in MS-DOS 2.0 and later versions of MS-DOS and in Microsoft Windows, the NTFS file system in the Windows NT family of operating systems, and the ODS-2 (On-Disk Structure-2) and higher levels of the Files-11 file system in OpenVMS.

Metadata[edit]
Other bookkeeping information is typically associated with each file within a file system. The length of the data contained in a file may be stored as the number of blocks allocated for the file or as a byte count. The time that the file was last modified may be stored as the file's timestamp. File systems might store the file creation time, the time it was last accessed, the time the file's metadata was changed, or the time the file was last backed up. Other information can include the file's device type (e.g. block, character, socket, subdirectory, etc.), its owner user ID and group ID, its access permissions and other file attributes (e.g. whether the file is read-only, executable, etc.).

A file system stores all the metadata associated with the file—including the file name, the length of the contents of a file, and the location of the file in the folder hierarchy—separate from the contents of the file.

Most file systems store the names of all the files in one directory in one place—the directory table for that directory—which is often stored like any other file. Many file systems put only some of the metadata for a file in the directory table, and the rest of the metadata for that file in a completely separate structure, such as the inode.

Most file systems also store metadata not associated with any one particular file. Such metadata includes information about unused regions—free space bitmap, block availability map—and information about bad sectors. Often such information about an allocation group is stored inside the allocation group itself.

Additional attributes can be associated on file systems, such as NTFS, XFS, ext2, ext3, some versions of UFS, and HFS+, using extended file attributes. Some file systems provide for user defined attributes such as the author of the document, the character encoding of a document or the size of an image.

Some file systems allow for different data collections to be associated with one file name. These separate collections may be referred to as streams or forks. Apple has long used a forked file system on the Macintosh, and Microsoft supports streams in NTFS. Some file systems maintain multiple past revisions of a file under a single file name; the filename by itself retrieves the most recent version, while prior saved version can be accessed using a special naming convention such as "filename;4" or "filename(-4)" to access the version four saves ago.

See comparison of file systems#Metadata for details on which file systems support which kinds of metadata.

File system as an abstract user interface[edit]
In some cases, a file system may not make use of a storage device but can be used to organize and represent access to any data, whether it is stored or dynamically generated (e.g. procfs).

Utilities[edit]
File systems include utilities to initialize, alter parameters of and remove an instance of the file system. Some include the ability to extend or truncate the space allocated to the file system.

Directory utilities may be used to create, rename and delete directory entries, which are also known as dentries (singular: dentry),[4] and to alter metadata associated with a directory. Directory utilities may also include capabilities to create additional links to a directory (hard links in Unix), to rename parent links (".." in Unix-like operating systems),[clarification needed] and to create bidirectional links to files.

File utilities create, list, copy, move and delete files, and alter metadata. They may be able to truncate data, truncate or extend space allocation, append to, move, and modify files in-place. Depending on the underlying structure of the file system, they may provide a mechanism to prepend to, or truncate from, the beginning of a file, insert entries into the middle of a file or delete entries from a file.

Utilities to free space for deleted files, if the file system provides an undelete function, also belong to this category.

Some file systems defer operations such as reorganization of free space, secure erasing of free space, and rebuilding of hierarchical structures by providing utilities to perform these functions at times of minimal activity. An example is the file system defragmentation utilities.

Some of the most important features of file system utilities involve supervisory activities which may involve bypassing ownership or direct access to the underlying device. These include high-performance backup and recovery, data replication and reorganization of various data structures and allocation tables within the file system.

Restricting and permitting access[edit]
See also: Computer security, Password cracking, Filesystem-level encryption, and Encrypting File System
There are several mechanisms used by file systems to control access to data. Usually the intent is to prevent reading or modifying files by a user or group of users. Another reason is to ensure data is modified in a controlled way so access may be restricted to a specific program. Examples include passwords stored in the metadata of the file or elsewhere and file permissions in the form of permission bits, access control lists, or capabilities. The need for file system utilities to be able to access the data at the media level to reorganize the structures and provide efficient backup usually means that these are only effective for polite users but are not effective against intruders.

Methods for encrypting file data are sometimes included in the file system. This is very effective since there is no need for file system utilities to know the encryption seed to effectively manage the data. The risks of relying on encryption include the fact that an attacker can copy the data and use brute force to decrypt the data. Losing the seed means losing the data.

Maintaining integrity[edit]
One significant responsibility of a file system is to ensure that, regardless of the actions by programs accessing the data, the structure remains consistent. This includes actions taken if a program modifying data terminates abnormally or neglects to inform the file system that it has completed its activities. This may include updating the metadata, the directory entry and handling any data that was buffered but not yet updated on the physical storage media.

Other failures which the file system must deal with include media failures or loss of connection to remote systems.

In the event of an operating system failure or "soft" power failure, special routines in the file system must be invoked similar to when an individual program fails.

The file system must also be able to correct damaged structures. These may occur as a result of an operating system failure for which the OS was unable to notify the file system, power failure or reset.

The file system must also record events to allow analysis of systemic issues as well as problems with specific files or directories.

User data[edit]
The most important purpose of a file system is to manage user data. This includes storing, retrieving and updating data.

Some file systems accept data for storage as a stream of bytes which are collected and stored in a manner efficient for the media. When a program retrieves the data, it specifies the size of a memory buffer and the file system transfers data from the media to the buffer. A runtime library routine may sometimes allow the user program to define a record based on a library call specifying a length. When the user program reads the data, the library retrieves data via the file system and returns a record.

Some file systems allow the specification of a fixed record length which is used for all writes and reads. This facilitates locating the nth record as well as updating records.

An identification for each record, also known as a key, makes for a more sophisticated file system. The user program can read, write and update records without regard to their location. This requires complicated management of blocks of media usually separating key blocks and data blocks. Very efficient algorithms can be developed with pyramid structure for locating records.[5]

Using a file system[edit]
Utilities, language specific run-time libraries and user programs use file system APIs to make requests of the file system. These include data transfer, positioning, updating metadata, managing directories, managing access specifications, and removal.

Multiple file systems within a single system[edit]
Frequently, retail systems are configured with a single file system occupying the entire storage device.

Another approach is to partition the disk so that several file systems with different attributes can be used. One file system, for use as browser cache, might be configured with a small allocation size. This has the additional advantage of keeping the frantic activity of creating and deleting files typical of browser activity in a narrow area of the disk and not interfering with allocations of other files. A similar partition might be created for email. Another partition, and file system might be created for the storage of audio or video files with a relatively large allocation. One of the file systems may normally be set read-only and only periodically be set writable.

A third approach, which is mostly used in cloud systems, is to use "disk images" to house additional file systems, with the same attributes or not, within another (host) file system as a file. A common example is virtualization: one user can run an experimental Linux distribution (using the ext4 file system) in a virtual machine under his/her production Windows environment (using NTFS). The ext4 file system resides in a disk image, which is treated as a file (or multiple files, depending on the hypervisor and settings) in the NTFS host file system.

Having multiple file systems on a single system has the additional benefit that in the event of a corruption of a single partition, the remaining file systems will frequently still be intact. This includes virus destruction of the system partition or even a system that will not boot. File system utilities which require dedicated access can be effectively completed piecemeal. In addition, defragmentation may be more effective. Several system maintenance utilities, such as virus scans and backups, can also be processed in segments. For example, it is not necessary to backup the file system containing videos along with all the other files if none have been added since the last backup. As for the image files, one can easily "spin off" differential images which contain only "new" data written to the master (original) image. Differential images can be used for both safety concerns (as a "disposable" system - can be quickly restored if destroyed or contaminated by a virus, as the old image can be removed and a new image can be created in matter of seconds, even without automated procedures) and quick virtual machine deployment (since the differential images can be quickly spawned using a script in batches).

Design limitations[edit]
All file systems have some functional limit that defines the maximum storable data capacity within that system. These functional limits are a best-guess effort by the designer based on how large the storage systems are right now and how large storage systems are likely to become in the future. Disk storage has continued to increase at near exponential rates (see Moore's law), so after a few years, file systems have kept reaching design limitations that require computer users to repeatedly move to a newer system with ever-greater capacity.

File system complexity typically varies proportionally with the available storage capacity. The file systems of early 1980s home computers with 50 KB to 512 KB of storage would not be a reasonable choice for modern storage systems with hundreds of gigabytes of capacity. Likewise, modern file systems would not be a reasonable choice for these early systems, since the complexity of modern file system structures would quickly consume or even exceed the very limited capacity of the early storage systems.

Types of file systems[edit]
File system types can be classified into disk/tape file systems, network file systems and special-purpose file systems.

Disk file systems[edit]
A disk file system takes advantages of the ability of disk storage media to randomly address data in a short amount of time. Additional considerations include the speed of accessing data following that initially requested and the anticipation that the following data may also be requested. This permits multiple users (or processes) access to various data on the disk without regard to the sequential location of the data. Examples include FAT (FAT12, FAT16, FAT32), exFAT, NTFS, HFS and HFS+, HPFS, UFS, ext2, ext3, ext4, XFS, btrfs, ISO 9660, Files-11, Veritas File System, VMFS, ZFS, ReiserFS and UDF. Some disk file systems are journaling file systems or versioning file systems.

Optical discs[edit]
ISO 9660 and Universal Disk Format (UDF) are two common formats that target Compact Discs, DVDs and Blu-ray discs. Mount Rainier is an extension to UDF supported since 2.6 series of the Linux kernel and since Windows Vista that facilitates rewriting to DVDs.

Flash file systems[edit]
Main article: Flash file system
A flash file system considers the special abilities, performance and restrictions of flash memory devices. Frequently a disk file system can use a flash memory device as the underlying storage media but it is much better to use a file system specifically designed for a flash device.

Tape file systems[edit]
A tape file system is a file system and tape format designed to store files on tape in a self-describing form. Magnetic tapes are sequential storage media with significantly longer random data access times than disks, posing challenges to the creation and efficient management of a general-purpose file system.

In a disk file system there is typically a master file directory, and a map of used and free data regions. Any file additions, changes, or removals require updating the directory and the used/free maps. Random access to data regions is measured in milliseconds so this system works well for disks.

Tape requires linear motion to wind and unwind potentially very long reels of media. This tape motion may take several seconds to several minutes to move the read/write head from one end of the tape to the other.

Consequently, a master file directory and usage map can be extremely slow and inefficient with tape. Writing typically involves reading the block usage map to find free blocks for writing, updating the usage map and directory to add the data, and then advancing the tape to write the data in the correct spot. Each additional file write requires updating the map and directory and writing the data, which may take several seconds to occur for each file.

Tape file systems instead typically allow for the file directory to be spread across the tape intermixed with the data, referred to as streaming, so that time-consuming and repeated tape motions are not required to write new data.

However, a side effect of this design is that reading the file directory of a tape usually requires scanning the entire tape to read all the scattered directory entries. Most data archiving software that works with tape storage will store a local copy of the tape catalog on a disk file system, so that adding files to a tape can be done quickly without having to rescan the tape media. The local tape catalog copy is usually discarded if not used for a specified period of time, at which point the tape must be re-scanned if it is to be used in the future.

IBM has developed a file system for tape called the Linear Tape File System. The IBM implementation of this file system has been released as the open-source IBM Linear Tape File System — Single Drive Edition (LTFS-SDE) product. The Linear Tape File System uses a separate partition on the tape to record the index meta-data, thereby avoiding the problems associated with scattering directory entries across the entire tape.

Tape formatting[edit]
Writing data to a tape, erasing, or formatting a tape is often a significantly time-consuming process and can take several hours on large tapes[a]. With many data tape technologies it is not necessary to format the tape before over-writing new data to the tape. This is due to the inherently destructive nature of overwriting data on sequential media.

Because of the time it can take to format a tape, typically tapes are pre-formatted so that the tape user does not need to spend time preparing each new tape for use. All that is usually necessary is to write an identifying media label to the tape before use, and even this can be automatically written by software when a new tape is used for the first time.

Database file systems[edit]
Another concept for file management is the idea of a database-based file system. Instead of, or in addition to, hierarchical structured management, files are identified by their characteristics, like type of file, topic, author, or similar rich metadata.[6]

IBM DB2 for i [7] (formerly known as DB2/400 and DB2 for i5/OS) is a database file system as part of the object based IBM i [8] operating system (formerly known as OS/400 and i5/OS), incorporating a single level store and running on IBM Power Systems (formerly known as AS/400 and iSeries), designed by Frank G. Soltis IBM's former chief scientist for IBM i. Around 1978 to 1988 Frank G. Soltis and his team at IBM Rochester have successfully designed and applied technologies like the database file system where others like Microsoft later failed to accomplish.[9] These technologies are informally known as 'Fortress Rochester'[citation needed] and were in few basic aspects extended from early Mainframe technologies but in many ways more advanced from a technological perspective[citation needed].

Some other projects that aren't "pure" database file systems but that use some aspects of a database file system:

Many Web content management systems use a relational DBMS to store and retrieve files. For example, XHTML files are stored as XML or text fields, while image files are stored as blob fields; SQL SELECT (with optional XPath) statements retrieve the files, and allow the use of a sophisticated logic and more rich information associations than "usual file systems". Many CMSs also have the option of storing only metadata within the database, with the standard filesystem used to store the content of files.
Very large file systems, embodied by applications like Apache Hadoop and Google File System, use some database file system concepts.
Transactional file systems[edit]
Some programs need to update multiple files "all at once". For example, a software installation may write program binaries, libraries, and configuration files. If the software installation fails, the program may be unusable. If the installation is upgrading a key system utility, such as the command shell, the entire system may be left in an unusable state.

Transaction processing introduces the isolation guarantee, which states that operations within a transaction are hidden from other threads on the system until the transaction commits, and that interfering operations on the system will be properly serialized with the transaction. Transactions also provide the atomicity guarantee, ensuring that operations inside of a transaction are either all committed or the transaction can be aborted and the system discards all of its partial results. This means that if there is a crash or power failure, after recovery, the stored state will be consistent. Either the software will be completely installed or the failed installation will be completely rolled back, but an unusable partial install will not be left on the system.

Windows, beginning with Vista, added transaction support to NTFS, in a feature called Transactional NTFS, but its use is now discouraged.[10] There are a number of research prototypes of transactional file systems for UNIX systems, including the Valor file system,[11] Amino,[12] LFS,[13] and a transactional ext3 file system on the TxOS kernel,[14] as well as transactional file systems targeting embedded systems, such as TFFS.[15]

Ensuring consistency across multiple file system operations is difficult, if not impossible, without file system transactions. File locking can be used as a concurrency control mechanism for individual files, but it typically does not protect the directory structure or file metadata. For instance, file locking cannot prevent TOCTTOU race conditions on symbolic links. File locking also cannot automatically roll back a failed operation, such as a software upgrade; this requires atomicity.

Journaling file systems are one technique used to introduce transaction-level consistency to file system structures. Journal transactions are not exposed to programs as part of the OS API; they are only used internally to ensure consistency at the granularity of a single system call.

Data backup systems typically do not provide support for direct backup of data stored in a transactional manner, which makes recovery of reliable and consistent data sets difficult. Most backup software simply notes what files have changed since a certain time, regardless of the transactional state shared across multiple files in the overall dataset. As a workaround, some database systems simply produce an archived state file containing all data up to that point, and the backup software only backs that up and does not interact directly with the active transactional databases at all. Recovery requires separate recreation of the database from the state file, after the file has been restored by the backup software.

Network file systems[edit]
Main article: Distributed file system
A network file system is a file system that acts as a client for a remote file access protocol, providing access to files on a server. Programs using local interfaces can transparently create, manage and access hierarchical directories and files in remote network-connected computers. Examples of network file systems include clients for the NFS, AFS, SMB protocols, and file-system-like clients for FTP and WebDAV.

Shared disk file systems[edit]
Main article: Shared disk file system
A shared disk file system is one in which a number of machines (usually servers) all have access to the same external disk subsystem (usually a SAN). The file system arbitrates access to that subsystem, preventing write collisions. Examples include GFS2 from Red Hat, GPFS from IBM, SFS from DataPlow, CXFS from SGI and StorNext from Quantum Corporation.

Special file systems [edit]
A special file system presents non-file elements of an operating system as files so they can be acted on using file system APIs. This is most commonly done in Unix-like operating systems, but devices are given file names in some non-Unix-like operating systems as well.

Device file systems [edit]
A device file system represents I/O devices and pseudo-devices as files, called device files. Examples in Unix-like systems include devfs and, in Linux 2.6 systems, udev. In non-Unix-like systems, such as TOPS-10 and other operating systems influenced by it, where the full filename or pathname of a file can include a device prefix, devices other than those containing file systems are referred to by a device prefix specifying the device, without anything following it.

Other special file systems[edit]
In the Linux kernel, configfs and sysfs provide files that can be used to query the kernel for information and configure entities in the kernel.
procfs maps processes and, on Linux, other operating system structures into a filespace.
Minimal file system / audio-cassette storage[edit]
The late 1970s saw the development of the microcomputer. Disk and digital tape devices were too expensive for hobbyists. An inexpensive basic data storage system was devised that used common audio cassette tape.

When the system needed to write data, the user was notified to press "RECORD" on the cassette recorder, then press "RETURN" on the keyboard to notify the system that the cassette recorder was recording. The system wrote a sound to provide time synchronization, then modulated sounds that encoded a prefix, the data, a checksum and a suffix. When the system needed to read data, the user was instructed to press "PLAY" on the cassette recorder. The system would listen to the sounds on the tape waiting until a burst of sound could be recognized as the synchronization. The system would then interpret subsequent sounds as data. When the data read was complete, the system would notify the user to press "STOP" on the cassette recorder. It was primitive, but it worked (a lot of the time). Data was stored sequentially, usually in an unnamed format, although some systems (such as the Commodore PET series of computers) did allow the files to be named. Multiple sets of data could be written and located by fast-forwarding the tape and observing at the tape counter to find the approximate start of the next data region on the tape. The user might have to listen to the sounds to find the right spot to begin playing the next data region. Some implementations even included audible sounds interspersed with the data.

Flat file systems[edit]
In a flat file system, there are no subdirectories.

When floppy disk media was first available this type of file system was adequate due to the relatively small amount of data space available. CP/M machines featured a flat file system, where files could be assigned to one of 16 user areas and generic file operations narrowed to work on one instead of defaulting to work on all of them. These user areas were no more than special attributes associated with the files, that is, it was not necessary to define specific quota for each of these areas and files could be added to groups for as long as there was still free storage space on the disk. The early Apple Macintosh also featured a flat file system, the Macintosh File System. It was unusual in that the file management program (Macintosh Finder) created the illusion of a partially hierarchical filing system on top of EMFS. This structure required every file to have a unique name, even if it appeared to be in a separate folder.

While simple, flat file systems become awkward as the number of files grows and makes it difficult to organize data into related groups of files.

A recent addition to the flat file system family is Amazon's S3, a remote storage service, which is intentionally simplistic to allow users the ability to customize how their data is stored. The only constructs are buckets (imagine a disk drive of unlimited size) and objects (similar, but not identical to the standard concept of a file). Advanced file management is allowed by being able to use nearly any character (including '/') in the object's name, and the ability to select subsets of the bucket's content based on identical prefixes.

File systems and operating systems[edit]
Many operating systems include support for more than one file system. Sometimes the OS and the file system are so tightly interwoven it is difficult to separate out file system functions.

There needs to be an interface provided by the operating system software between the user and the file system. This interface can be textual (such as provided by a command line interface, such as the Unix shell, or OpenVMS DCL) or graphical (such as provided by a graphical user interface, such as file browsers). If graphical, the metaphor of the folder, containing documents, other files, and nested folders is often used (see also: directory and folder).

Unix and Unix-like operating systems[edit]
Unix-like operating systems create a virtual file system, which makes all the files on all the devices appear to exist in a single hierarchy. This means, in those systems, there is one root directory, and every file existing on the system is located under it somewhere. Unix-like systems can use a RAM disk or network shared resource as its root directory.

Unix-like systems assign a device name to each device, but this is not how the files on that device are accessed. Instead, to gain access to files on another device, the operating system must first be informed where in the directory tree those files should appear. This process is called mounting a file system. For example, to access the files on a CD-ROM, one must tell the operating system "Take the file system from this CD-ROM and make it appear under such-and-such directory". The directory given to the operating system is called the mount point – it might, for example, be /media. The /media directory exists on many Unix systems (as specified in the Filesystem Hierarchy Standard) and is intended specifically for use as a mount point for removable media such as CDs, DVDs, USB drives or floppy disks. It may be empty, or it may contain subdirectories for mounting individual devices. Generally, only the administrator (i.e. root user) may authorize the mounting of file systems.

Unix-like operating systems often include software and tools that assist in the mounting process and provide it new functionality. Some of these strategies have been coined "auto-mounting" as a reflection of their purpose.

In many situations, file systems other than the root need to be available as soon as the operating system has booted. All Unix-like systems therefore provide a facility for mounting file systems at boot time. System administrators define these file systems in the configuration file fstab (vfstab in Solaris), which also indicates options and mount points.
In some situations, there is no need to mount certain file systems at boot time, although their use may be desired thereafter. There are some utilities for Unix-like systems that allow the mounting of predefined file systems upon demand.
Removable media have become very common with microcomputer platforms. They allow programs and data to be transferred between machines without a physical connection. Common examples include USB flash drives, CD-ROMs, and DVDs. Utilities have therefore been developed to detect the presence and availability of a medium and then mount that medium without any user intervention.
Progressive Unix-like systems have also introduced a concept called supermounting; see, for example, the Linux supermount-ng project. For example, a floppy disk that has been supermounted can be physically removed from the system. Under normal circumstances, the disk should have been synchronized and then unmounted before its removal. Provided synchronization has occurred, a different disk can be inserted into the drive. The system automatically notices that the disk has changed and updates the mount point contents to reflect the new medium.
An automounter will automatically mount a file system when a reference is made to the directory atop which it should be mounted. This is usually used for file systems on network servers, rather than relying on events such as the insertion of media, as would be appropriate for removable media.
Linux[edit]
Linux supports numerous file systems, but common choices for the system disk on a block device include the ext* family (ext2, ext3 and ext4), XFS, JFS, ReiserFS and btrfs. For raw flash without a flash translation layer (FTL) or Memory Technology Device (MTD), there are UBIFS, JFFS2 and YAFFS, among others. SquashFS is a common compressed read-only file system.

Solaris[edit]
Solaris in earlier releases defaulted to (non-journaled or non-logging) UFS for bootable and supplementary file systems. Solaris defaulted to, supported, and extended UFS.

Support for other file systems and significant enhancements were added over time, including Veritas Software Corp. (Journaling) VxFS, Sun Microsystems (Clustering) QFS, Sun Microsystems (Journaling) UFS, and Sun Microsystems (open source, poolable, 128 bit compressible, and error-correcting) ZFS.

Kernel extensions were added to Solaris to allow for bootable Veritas VxFS operation. Logging or Journaling was added to UFS in Sun's Solaris 7. Releases of Solaris 10, Solaris Express, OpenSolaris, and other open source variants of the Solaris operating system later supported bootable ZFS.

Logical Volume Management allows for spanning a file system across multiple devices for the purpose of adding redundancy, capacity, and/or throughput. Legacy environments in Solaris may use Solaris Volume Manager (formerly known as Solstice DiskSuite). Multiple operating systems (including Solaris) may use Veritas Volume Manager. Modern Solaris based operating systems eclipse the need for Volume Management through leveraging virtual storage pools in ZFS.

OS X[edit]
OS X uses a file system inherited from classic Mac OS called HFS Plus. Apple also uses the term "Mac OS Extended".[16][17] HFS Plus is a metadata-rich and case-preserving but (usually) case-insensitive file system. Due to the Unix roots of OS X, Unix permissions were added to HFS Plus. Later versions of HFS Plus added journaling to prevent corruption of the file system structure and introduced a number of optimizations to the allocation algorithms in an attempt to defragment files automatically without requiring an external defragmenter.

Filenames can be up to 255 characters. HFS Plus uses Unicode to store filenames. On OS X, the filetype can come from the type code, stored in file's metadata, or the filename extension.

HFS Plus has three kinds of links: Unix-style hard links, Unix-style symbolic links and aliases. Aliases are designed to maintain a link to their original file even if they are moved or renamed; they are not interpreted by the file system itself, but by the File Manager code in userland.

OS X also supported the UFS file system, derived from the BSD Unix Fast File System via NeXTSTEP. However, as of Mac OS X Leopard, OS X could no longer be installed on a UFS volume, nor can a pre-Leopard system installed on a UFS volume be upgraded to Leopard.[18] As of Mac OS X Lion UFS support was completely dropped.

Newer versions of OS X are capable of reading and writing to the legacy FAT file systems (16 & 32) common on Windows. They are also capable of reading the newer NTFS file systems for Windows. In order to write to NTFS file systems on OS X versions prior to 10.6 (Snow Leopard) third party software is necessary. Mac OS X 10.6 (Snow Leopard) and later allows writing to NTFS file systems, but only after a non-trivial system setting change (third party software exists that automates this).[19]

Finally, OS X supports reading and writing of the exFAT file system since Mac OS X Snow Leopard, starting from version 10.6.5.[20]

PC-BSD[edit]
PC-BSD is a desktop version of FreeBSD, which inherits FreeBSD's ZFS support, similarly to FreeNAS. The new graphical installer of PC-BSD can handle / (root) on ZFS and RAID-Z pool installs and disk encryption using Geli right from the start in an easy convenient (GUI) way. The current PC-BSD 9.0+ 'Isotope Edition' has ZFS filesystem version 5 and ZFS storage pool version 28.

Plan 9[edit]
Plan 9 from Bell Labs treats everything as a file and accesses all objects as a file would be accessed (i.e., there is no ioctl or mmap): networking, graphics, debugging, authentication, capabilities, encryption, and other services are accessed via I/O operations on file descriptors. The 9P protocol removes the difference between local and remote files. File systems in Plan 9 are organized with the help of private, per-process namespaces, allowing each process to have a different view of the many file systems that provide resources in a distributed system.

The Inferno operating system shares these concepts with Plan 9.

Microsoft Windows[edit]

Directory listing in a Windows command shell
Windows makes use of the FAT, NTFS, exFAT, Live File System and ReFS file systems (the last of these is only supported and usable in Windows Server 2012; Windows cannot boot from it).

Windows uses a drive letter abstraction at the user level to distinguish one disk or partition from another. For example, the path C:\WINDOWS represents a directory WINDOWS on the partition represented by the letter C. Drive C: is most commonly used for the primary hard disk drive partition, on which Windows is usually installed and from which it boots. This "tradition" has become so firmly ingrained that bugs exist in many applications which make assumptions that the drive that the operating system is installed on is C. The use of drive letters, and the tradition of using "C" as the drive letter for the primary hard disk drive partition, can be traced to MS-DOS, where the letters A and B were reserved for up to two floppy disk drives. This in turn derived from CP/M in the 1970s, and ultimately from IBM's CP/CMS of 1967.

FAT[edit]
Main article: File Allocation Table
The family of FAT file systems is supported by almost all operating systems for personal computers, including all versions of Windows and MS-DOS/PC DOS and DR-DOS. (PC DOS is an OEM version of MS-DOS, MS-DOS was originally based on SCP's 86-DOS. DR-DOS was based on Digital Research's Concurrent DOS, a successor of CP/M-86.) The FAT file systems are therefore well-suited as a universal exchange format between computers and devices of most any type and age.

The FAT file system traces its roots back to an (incompatible) 8-bit FAT precursor in Standalone Disk BASIC and the short-lived MDOS/MIDAS project.[citation needed]

Over the years, the file system has been expanded from FAT12 to FAT16 and FAT32. Various features have been added to the file system including subdirectories, codepage support, extended attributes, and long filenames. Third parties such as Digital Research have incorporated optional support for deletion tracking, and volume/directory/file-based multi-user security schemes to support file and directory passwords and permissions such as read/write/execute/delete access rights. Most of these extensions are not supported by Windows.

The FAT12 and FAT16 file systems had a limit on the number of entries in the root directory of the file system and had restrictions on the maximum size of FAT-formatted disks or partitions.

FAT32 addresses the limitations in FAT12 and FAT16, except for the file size limit of close to 4 GB, but it remains limited compared to NTFS.

FAT12, FAT16 and FAT32 also have a limit of eight characters for the file name, and three characters for the extension (such as .exe). This is commonly referred to as the 8.3 filename limit. VFAT, an optional extension to FAT12, FAT16 and FAT32, introduced in Windows 95 and Windows NT 3.5, allowed long file names (LFN) to be stored in the FAT file system in a backwards compatible fashion.

NTFS[edit]
Main article: NTFS
NTFS, introduced with the Windows NT operating system in 1993, allowed ACL-based permission control. Other features also supported by NTFS include hard links, multiple file streams, attribute indexing, quota tracking, sparse files, encryption, compression, and reparse points (directories working as mount-points for other file systems, symlinks, junctions, remote storage links).

exFAT[edit]
Main article: exFAT
exFAT is a proprietary and patent-protected file system with certain advantages over NTFS with regard to file system overhead.

exFAT is not backward compatible with FAT file systems such as FAT12, FAT16 or FAT32. The file system is supported with newer Windows systems, such as Windows Server 2003, Windows Vista, Windows 2008, Windows 7, Windows 8, and more recently, support has been added for Windows XP.[21]

exFAT is supported in OS X starting with version 10.6.5 (Snow Leopard).[20] Support in other operating systems is sparse since Microsoft has not published the specifications of the file system and implementing support for exFAT requires a license. exFAT is the only file system that is fully supported on both OS X and Windows that can hold files bigger than 4 GB.[citation needed]

OpenVMS[edit]
Main article: Files-11
MVS [IBM Mainframe][edit]
Main article: MVS  MVS filesystem
AS/400 file system[edit]
Data on the AS/400 and its successors consists of system objects mapped into the system virtual address space in a single-level store. Many types of AS/400 objects are defined including the directories and files found in other file systems. File objects, along with other types of objects, form the basis of the As/400's support for an integrated relational database.

Other file systems[edit]
The Prospero File System is a file system based on the Virtual System Model.[22] The system was created by Dr. B. Clifford Neuman of the Information Sciences Institute at the University of Southern California.[23]
RSRE FLEX file system - written in ALGOL 68
The file system of the Michigan Terminal System (MTS) is interesting because: (i) it provides "line files" where record lengths and line numbers are associated as metadata with each record in the file, lines can be added, replaced, updated with the same or different length records, and deleted anywhere in the file without the need to read and rewrite the entire file; (ii) using program keys files may be shared or permitted to commands and programs in addition to users and groups; and (iii) there is a comprehensive file locking mechanism that protects both the file's data and its metadata.[24][25]
Limitations[edit]
Converting the type of a file system[edit]
It may be advantageous or necessary to have files in a different file system than they currently exist. Reasons include the need for an increase in the space requirements beyond the limits of the current file system. The depth of path may need to be increased beyond the restrictions of the file system. There may be performance or reliability considerations. Providing access to another operating system which does not support existing file system is another reason.

In-place conversion[edit]
In some cases conversion can be done in-place, although migrating the file system is more conservative, as it involves a creating a copy of the data and is recommended.[26] On Windows, FAT and FAT32 file systems can be converted to NTFS via the convert.exe utility, but not the reverse.[26] On Linux, ext2 can be converted to ext3 (and converted back), and ext3 can be converted to ext4 (but not back),[27] and both ext3 and ext4 can be converted to btrfs, and converted back until the undo information is deleted.[28] These conversions are possible due to using the same format for the file data itself, and relocating the metadata into empty space, in some cases using sparse file support.[28]

Migrating to a different file system[edit]
Migration has the disadvantage of requiring additional space although it may be faster. The best case is if there is unused space on media which will contain the final file system.

For example, to migrate a FAT32 file system to an ext2 file system. First create a new ext2 file system, then copy the data to the file system, then delete the FAT32 file system.

An alternative, when there is not sufficient space to retain the original file system until the new one is created, is to use a work area (such as a removable media). This takes longer but a backup of the data is a nice side effect.

Long file paths and long file names[edit]
In hierarchical file systems, files are accessed by means of a path that is a branching list of directories containing the file. Different file systems have different limits on the depth of the path. File systems also have a limit on the length of an individual filename.

Copying files with long names or located in paths of significant depth from one file system to another may cause undesirable results. This depends on how the utility doing the copying handles the discrepancy.
Filename
From Wikipedia, the free encyclopedia

Screenshot of a Windows command shell showing filenames in a directory

filename list, with long filenames, foreign letters, comma, dot and space characters as they appear in a software displaying filenames
A filename (also written as two words, file name) is a name used to uniquely identify a computer file stored in a file system. Different file systems impose different restrictions on filename lengths and the allowed characters within filenames.

A filename may include one or more of these components:

host (or node or server) – network device that contains the file
device (or drive) – hardware device or drive
directory (or path) – directory tree (e.g., /usr/bin, \TEMP, [USR.LIB.SRC], etc.)
file – base name of the file
type (format or extension) – indicates the content type of the file (e.g. .txt, .exe, .COM, etc.)
version – revision or generation number of the file
The components required to identify a file varies across operating systems, as does the syntax and format for a valid filename.

Discussions of filenames are complicated by a lack of standardisation of the term. Sometimes "filename" is used to mean the entire name, such as the Windows name c:\directory\myfile.txt. Sometimes, it will be used to refer to the components, so the filename in this case would be myfile.txt. Sometimes, it is a reference that excludes an extension, so the filename would be just myfile. Such ambiguity is widespread and this article does not attempt to define any one meaning, and indeed may be using any of these meanings. Some systems will adopt their own standardised nomenclature like "path name" but these too are not standardised across systems.

Contents  [hide] 
1	History
1.1	Unicode migration
2	References: absolute vs relative
3	Number of names per file
4	Length restrictions
5	Filename extensions
6	Encoding interoperability
6.1	Encoding indication interoperability
6.2	Unicode interoperability
6.3	Perspectives
7	Uniqueness
8	Letter case preservation
9	Reserved characters and words
10	Comparison of filename limitations
11	See also
12	References
13	External links
History[edit]
[icon]	This section requires expansion. (July 2012)
Around 1962, the Compatible Time-Sharing System introduced the concept of a file (i.e. non-paper file).[citation needed]

Around this same time appeared the dot (period or full-stop) as a filename extension separator, and the limit to three letter extensions might have come from RAD50 16-bit limits.[1]

Traditionally, filenames allowed only alphanumeric characters, but as time progressed, the number of characters allowed increased. This led to compatibility problems when moving files from one file system to another.[2]

Around 1995, VFAT, an extension to the FAT filesystem, was introduced in Windows 95 and Windows NT 3.5. It allowed mixed-case Unicode long filenames (LFNs), in addition to classic "8.3" names.

In 1985, RFC 959 officially defined a pathname to be the character string which must be entered into a file system by a user in order to identify a file.[3]

Unicode migration[edit]
[icon]	This section requires expansion. (July 2012)
One issue was migration to Unicode. For this purpose, several software companies provided software for migrating filenames to the new Unicode encoding.

Microsoft provided migration transparent for the user throughout the vfat technology
Apple provided "File Name Encoding Repair Utility v1.0".[4]
The Linux community provided “convmv”.[5]
OS X 10.3 marked Apple's adoption of Unicode 3.2 character decomposition, superseding the Unicode 2.1 decomposition used previously. This change caused problems for developers writing software for OS X.[6]

References: absolute vs relative[edit]
Main article: Path (computing)
An absolute reference includes all directory levels. In some systems, a filename reference that does not include the complete directory path defaults to the current working directory. This is a relative reference. One advantage of using a relative reference in program configuration files or scripts is that different instances of the script or program can use different files.

This makes an absolute or relative path composed of a sequence of filenames.

Number of names per file[edit]
Unix-like file systems allow a file to have more than one name; in traditional Unix-style file systems, the names are hard links to the file's inode or equivalent. Windows supports hard links on NTFS file systems, and provides the command fsutil in Windows XP, and mklink in later versions, for creating them.[7][8] Hard links are different from Windows shortcuts, Mac OS aliases, or symbolic links. The introduction of LFNs with VFAT allowed filename aliases. For example, longfi~1.??? with a maximum of eight plus three characters was a filename alias of "long file name.???" as a way to conform to 8.3 limitations for older programs.

This property was used by the move command algorithm which first creates a second filename and then only removes the first filename.

Other filesystems, by design, provide only one filename per file, which guarantees that alteration of one filename's file does not alter the other filename's file.

Length restrictions[edit]
Some filesystems restrict the length of filenames. In some cases, these lengths apply to the entire file name, as in 44 characters on IBM S/370.[9] In other cases, the length limits may apply to particular portions of the filename, such as the name of a file in a directory, or a directory name. For example, 9 (e.g., 8-bit FAT in Standalone Disk BASIC), 11 (e.g. FAT12, FAT16, FAT32 in DOS), 14 (e.g. early Unix), 21 (Human68K), 31, 30 (e.g. Apple DOS 3.2 and 3.3), 15 (e.g. Apple ProDOS), 44 (e.g. IBM S/370),[9] or 255 (e.g. early Berkeley Unix) characters or bytes. Length limits often result from assigning fixed space in a filesystem to storing components of names, so increasing limits often requires an incompatible change, as well as reserving more space.

A particular issue with filesystems that store information in nested directories is that it may be possible to create a file whose total name exceeds implementation limits, since length checking may apply only to individual parts of the name rather than the entire name. Many Windows applications are limited to a MAX_PATH value of 260, but Windows file names can easily exceed this limit [1].

Filename extensions[edit]
Many file systems, including FAT, NTFS, and VMS systems, allow a filename extension that consists of one or more characters following the last period in the filename, dividing the filename into two parts: a base name or stem and an extension or suffix used by some applications to indicate the file type. Multiple output files created by an application use the same basename and various extensions. For example, a compiler might use the extension FOR for source input file (for Fortran code), OBJ for the object output and LST for the listing. Although there are some common extensions, they are arbitrary and a different application might use REL and RPT. On filesystems that do not segregate the extension, files will often have a longer extension such as html.

Encoding interoperability[edit]
There is no general encoding standard for filenames.

Because file names have to be exchanged between software environments (think network file transfer, file system storage, backup and file synchronization software, configuration management, data compression and archiving, etc.), it is very important not to lose file name information between applications. This led to wide adoption of Unicode as a standard for encoding file names, although legacy software might be non-Unicode-aware.

Encoding indication interoperability[edit]
Crystal Clear app kedit.svg
This section may need to be rewritten entirely to comply with Wikipedia's quality standards. You can help. The discussion page may contain suggestions. (September 2012)
Traditionally, filenames allowed any character in their filenames as long as they were file system safe.[2] Although this permitted the use of any encoding, and thus allowed the representation of any local text on any local system, it caused many interoperability issues.

A filename could be stored using different byte strings in distinct systems within a single country, such as if one used Japanese Shift JIS encoding and another Japanese EUC encoding. Conversion was not possible as most systems did not expose a description of the encoding used for a filename as part of the extended file information. This forced costly filename encoding guessing with each file access.[2]

A solution was to adopt Unicode as the encoding for filenames.

In the Mac OS however, encoding of the filename was stored with the filename attributes.[2]

Unicode interoperability[edit]
The Unicode standard solves the encoding determination issue.

Nonetheless, some limited interoperability issues remain, such as normalization (equivalence), or the Unicode version in use. For instance, UDF is limited to Unicode 2.0; Mac OS applies NFD Unicode normalization and is optionally case-sensitive (case-insensitive by default.) Filename maximum length is not standard and might depend on the code unit size. Although it is a serious issue, in most cases this is a limited one.[2]

On Linux, this means the filename is not enough to open a file: additionally, the exact byte representation of the filename on the storage device is needed. This can be solved at the application level, with some tricky normalization calls.[10]

The issue of Unicode equivalence is known as "normalized-name collision". A solution is the Non-normalizing Unicode Composition Awareness used in the Subversion and Apache technical communities.[11] This solution does not normalize paths in the repository. Paths are only normalized for the purpose of comparisons. Nonetheless, some communities have patented this strategy, forbidding its use by other communities.[clarification needed]

Perspectives[edit]
To limit interoperability issues, some ideas described by Sun are to:

use one Unicode encoding (such as UTF-8)
do transparent code conversions on filenames
store no normalized filenames
check for canonical equivalence among filenames, to avoid two canonically equivalent filenames in the same directory.[2]
Those considerations create a limitation not allowing a switch to a future encoding different from UTF-8.

Uniqueness[edit]
Within a single directory, filenames must be unique. Since the filename syntax also applies for directories, it is not possible to create a file and directory entries with the same name in a single directory. Multiple files in different directories may have the same name.

Uniqueness approach may differ both on the case sensitivity and on the Unicode normalization form such as NFC, NFD. This means two separate files might be created with the same text filename and a different byte implementation of the filename, such as L"\x00C0.txt" (UTF-16, NFC) (Latin capital A with grave) and L"\x0041\x0300.txt" (UTF-16, NFD) (Latin capital A, grave combining).[12]

Letter case preservation[edit]
Some filesystems, such as FAT, store filenames as upper-case regardless of the letter case used to create them. For example, a file created with the name "MyName.Txt" or "myname.txt" would be stored with the filename "MYNAME.TXT". Any variation of upper and lower case can be used to refer to the same file. These kinds of file systems are called case-insensitive and are not case-preserving. Some filesystems prohibit the use of lower case letters in filenames altogether.

Some file systems store filenames in the form that they were originally created; these are referred to as case-retentive or case-preserving. Such a file system can be case-sensitive or case-insensitive. If case-sensitive, then "MyName.Txt" and "myname.txt" may refer to two different files in the same directory, and each file must be referenced by the exact capitalisation by which it is named. On a case-insensitive, case-preserving file system, on the other hand, only one of "MyName.Txt", "myname.txt" and "Myname.TXT" can be the name of a file in a given directory at a given time, and a file with one of these names can be referenced by any capitalisation of the name.

From its original inception, Unix and its derivative systems were case-preserving. However, not all Unix-like file systems are case-sensitive; by default, HFS+ in Mac OS X is case-insensitive, and SMB servers usually provide case-insensitive behavior (even when the underlying file system is case-sensitive, e.g. Samba on most Unix-like systems), and SMB client file systems provide case-insensitive behavior. File system case sensitivity is a considerable challenge for software such as Samba and Wine, which must interoperate efficiently with both systems that treat uppercase and lowercase files as different and with systems that treat them the same.[13]

Reserved characters and words[edit]
File systems have not always provided the same character set for composing a filename. Before Unicode became a de facto standard, file systems mostly used a locale-dependent character set. By contrast, some new systems permit a filename to be composed of almost any character of the Unicode repertoire, and even certain non-Unicode byte sequences. Limitations may be imposed by the file system, operating system, application, or requirements for interoperability with other systems.

Many file system utilities prohibit control characters from appearing in filenames. In Unix-like file systems, the null character[14] and the path separator / are prohibited.

Some file system utilities and naming conventions prohibit particular characters from appearing in filenames:[15]
Note 1: While they are allowed in Unix file and folder names, most Unix shells require certain characters such as spaces, <, >, |, \, and sometimes :, (, ), &, ;, #, as well as wildcards such as ? and *, to be quoted or escaped:

five\ and\ six\<seven (example of escaping)
'five and six<seven' or "five and six<seven" (examples of quoting)

The character 0xE5 was not allowed as the first letter in a filename under 86-DOS and MS-DOS/PC DOS 1.x-2.x, but can be used in later versions.

In Windows utilities, the space and the period are not allowed as the final character of a filename.[16] The period is allowed as the first character, but certain Windows applications, such as Windows Explorer, forbid creating or renaming such files (despite this convention being used in Unix-like systems to describe hidden files and directories). Workarounds include appending a dot when renaming the file (which is then automatically removed afterwards), using alternative file managers, or saving a file with the desired filename from within an application.[17]

Some file systems on a given operating system (especially file systems originally implemented on other operating systems), and particular applications on that operating system, may apply further restrictions and interpretations. See comparison of file systems for more details on restrictions.

In Unix-like systems, DOS, and Windows, the filenames "." and ".." have special meanings (current and parent directory respectively). Windows 95/98/ME also uses names like "...", "...." and so on to denote grandparent or great-grandparent directories.[18] All Windows versions forbid creation of filenames that consist of only dots, although names consist of three dots ("...") or more are legal in Unix.

In addition, in Windows and DOS utilities, some words are also reserved and cannot be used as filenames.[17] For example, DOS device files:[19]

CON, PRN, AUX, CLOCK$, NUL
COM1, COM2, COM3, COM4
LPT1, LPT2, LPT3, LPT4 (LPT4 only in some versions of DR-DOS)
LST (only in 86-DOS and DOS 1.xx)
KEYBD$, SCREEN$ (only in multitasking MS-DOS 4.0)
$IDLE$ (only in Concurrent DOS 386, Multiuser DOS and DR DOS 5.0 and higher)
CONFIG$ (only in MS-DOS 7.0-8.0)
Systems that have these restrictions cause incompatibilities with some other filesystems. For example, Windows will fail to handle, or raise error reports for, these legal UNIX filenames: aux.c, q"uote"s.txt, or NUL.txt.

NTFS filenames that are used internally include:

Flat file database
From Wikipedia, the free encyclopedia

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2015) (Learn how and when to remove this template message)

This article possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (March 2015)

Example of a flat file model[1]
A flat file database is a database which is stored on its host computer system as an ordinary "flat file". To access the structure of the data and manipulate it, the file must be read in its entirety into the computer's memory. Upon completion of the database operations, the file is again written out in its entirety to the host's file system. In this stored mode the database is "flat", which means it has no structure for indexing and there are usually no structural relationships between the records. A flat file can be a plain text file or a binary file.

The term has generally implied a small, simple database. As computer memory has become cheaper, more sophisticated databases can now be entirely held in memory for faster access. These newer databases would not generally be referred to as flat-file databases.

Contents  [hide] 
1	Overview
2	History
2.1	Contemporary implementations
3	Data transfer operations
4	Terminology
5	Example database
6	References
Overview[edit]
Plain text files usually contain one record per line,[2] There are different conventions for depicting data. In comma-separated values and delimiter-separated values files, fields can be separated by delimiters such as comma or tab characters. In other cases, each field may have a fixed length; short values may be padded with space characters. Extra formatting may be needed to avoid delimiter collision. More complex solutions are markup languages and programming languages.

Using delimiters incurs some overhead in locating them every time they are processed (unlike fixed-width formatting), which may have performance implications. However, use of character delimiters (especially commas) is also a crude form of data compression which may assist overall performance by reducing data volumes — especially for data transmission purposes. Use of character delimiters which include a length component (Declarative notation) is comparatively rare but vastly reduces the overhead associated with locating the extent of each field.

Typical examples of flat files are /etc/passwd and /etc/group on Unix-like operating systems. Another example of a flat file is a name-and-address list with the fields Name, Address, and Phone Number.

A list of names, addresses, and phone numbers written by hand on a sheet of paper is a flat file database. This can also be done with any typewriter or word processor. A spreadsheet or text editor program may be used to implement a flat file database, which may then be printed or used online for improved search capabilities.

History[edit]
Herman Hollerith conceived the idea that data could be represented by holes punched in paper cards then tabulated by machine. He implemented this concept for the US Census Bureau; thus the 1890 United States Census processing created the first database—consisting of thousands of boxes full of punched cards.

Hollerith's enterprise grew into the computer giant IBM, which dominated the data processing market for most of the 20th century. IBM's fixed-length field, 80-column punch cards became the ubiquitous means of inputting electronic data until the 1970s.

In the 1980s, configurable flat-file database computer applications were popular on DOS and the Macintosh. These programs were designed to make it easy for individuals to design and use their own databases, and were almost on par with word processors and spreadsheets in popularity.[citation needed] Examples of flat-file database products were early versions of FileMaker and the shareware PC-File. Some of these, like dBase II, offered limited relational capabilities, allowing some data to be shared between files.

In the 2010s flat file databases were used in content management systems. Instead of using a database, web developers were able to change the content directly in the file system or at the command line.

Contemporary implementations[edit]
FairCom's c-tree is an example of a modern enterprise-level solution, and spreadsheet software and text editors can be used for this purpose. WebDNA is a scripting language designed for the World Wide Web, with a hybrid flat file in-memory database system making it easy to build resilient database-driven websites. With the in-memory concept, WebDNA searches and database updates are almost realtime while the data is stored as text files within the website itself. Otherwise, flat file database is implemented in Microsoft Works and Apple Works. Over time, products like Borland's Paradox, and Microsoft's Access started offering some relational capabilities, as well as built-in programming languages. Database Management Systems (DBMS) like MySQL or Oracle generally require programmers to build applications.

Faceless flat file database engines are used internally by Mac OS X, Firefox, and other computer software to store configuration data. Programs to manage collections of books or appointments and address book are essentially single-purpose flat file database applications, allowing users to store and retrieve information from flat files using a predefined set of fields. As of 2011, one of the most popular flat file database engines is SQLite,[dubious – discuss] which is the engine used by Firefox and Android and is part of the standard library for PHP and Python languages.

Data transfer operations[edit]
Flat files are used not only as data storage tools in DB and CMS systems, but also as data transfer tools to remote servers (in which case they become known as information streams).

In recent years, this latter implementation has been replaced with XML files, which not only contain but also describe the data. Those still using flat files to transfer information are mainframes employing specific procedures which are too expensive to modify.

One criticism often raised against the XML format as a way to perform mass data transfer operations is that file size is significantly larger than that of flat files, which is generally reduced to the bare minimum. The solution to this problem consists in XML file compression (a solution that applies equally well to flat files), which has nowadays gained EXI standards (i.e., Efficient XML Interchange, which is often used by mobile devices).

It is advisable that transfer data be performed via EXI rather than flat files because defining the compression method is not required, because libraries reading the file contents are readily available, and because there is no need for the two communicating systems to preliminarily establish a protocol describing data properties such as position, alignment, type, and format. However, in those circumstances where the sheer mass of data and/or the inadequacy of legacy systems becomes a problem, the only viable solution remains the use of flat files. In order to successfully handle those problems connected with data communication, format, validation, control and much else (be it a flat file or an XML file data source), it is advisable to adopt a Data Quality Firewall.

Terminology[edit]
"Flat file database" may be defined very narrowly, or more broadly. The narrower interpretation is correct in database theory; the broader covers the term as generally used.

Strictly, a flat file database should consist of nothing but data and, if records vary in length, delimiters. More broadly, the term refers to any database which exists in a single file in the form of rows and columns, with no relationships or links between records and fields except the table structure.

Terms used to describe different aspects of a database and its tools differ from one implementation to the next, but the concepts remain the same. FileMaker uses the term "Find", while MySQL uses the term "Query"; but the concept is the same. FileMaker "files", in version 7 and above, are equivalent to MySQL "databases", and so forth. To avoid confusing the reader, one consistent set of terms is used throughout this article.

However, the basic terms "record" and "field" are used in nearly every flat file database implementation.

Example database[edit]
The following example illustrates the basic elements of a flat-file database. The data arrangement consists of a series of columns and rows organized into a tabular format. This specific example uses only one table.

The columns include: name (a person's name, second column); team (the name of an athletic team supported by the person, third column); and a numeric unique ID, (used to uniquely identify records, first column).

Here is an example textual representation of the described data:

id    name    team
1     Amy     Blues
2     Bob     Reds
3     Chuck   Blues
4     Dick    Blues
5     Ethel   Reds
6     Fred    Blues
7     Gilly   Blues
8     Hank    Reds
9     Hank    Blues
This type of data representation is quite standard for a flat-file database, although there are some additional considerations that are not readily apparent from the text:

Data types: each column in a database table such as the one above is ordinarily restricted to a specific data type. Such restrictions are usually established by convention, but not formally indicated unless the data is transferred to a relational database system.
Separated columns: In the above example, individual columns are separated using whitespace characters. This is also called indentation or "fixed-width" data formatting. Another common convention is to separate columns using one or more delimiter characters. More complex solutions are markup and programming languages.
Relational algebra: Each row or record in the above table meets the standard definition of a tuple under relational algebra (the above example depicts a series of 3-tuples). Additionally, the first row specifies the field names that are associated with the values of each row.
Database management system: Since the formal operations possible with a text file are usually more limited than desired, the text in the above example would ordinarily represent an intermediary state of the data prior to being transferred into a database management system.

Object composition
From Wikipedia, the free encyclopedia

This article may contain excessive, poor, or irrelevant examples. Please improve the article by adding more descriptive text and removing less pertinent examples. See Wikipedia's guide to writing better articles for further suggestions. (August 2010)
In computer science, object composition (not to be confused with function composition) is a way to combine simple objects or data types into more complex ones. Compositions are a critical building block of many basic data structures, including the tagged union, the linked list, and the binary tree, as well as the object used in object-oriented programming.[1]

Contents  [hide] 
1	Details
2	UML notation
3	Composite types in C
4	Recursive composition
5	Timeline of composition in various languages
6	Aggregation
6.1	Aggregation in COM
7	Containment
8	See also
9	References
10	External links
Details[edit]
In a programming language, when objects are typed, types can often be divided into composite and noncomposite types, and composition can be regarded as a relationship between types: an object of a composite type (e.g. car) "has an" object of a simpler type (e.g. wheel).

Consider the relation of an automobile to its parts, specifically: the automobile has or is composed from objects including steering wheel, seat, gearbox and engine. This relation might be fixed in a computer program as a composition relation. However, the real world relation is softer and fuzzier. The engine in one automobile may be replaced by the engine from another automobile, meaning the relation is better described as an aggregation than a composition. Generally speaking, as time passes, what look like compositions tend to turn into aggregations, and aggregations tend to turn into looser associations.

Composition must be distinguished from subtyping, which is the process of adding detail to a general data type to create a more specific data type. For instance, cars may be a specific type of vehicle: car is a vehicle. Subtyping doesn't describe a relationship between different objects, but instead, says that objects of a type are simultaneously objects of another type.

In programming languages, composite objects are usually expressed by means of references from one object to another; depending on the language, such references may be known as attributes, fields, members or properties, and the resulting composition as composite type, storage record, structure, tuple, or a user-defined type (UDT). Fields are given a unique name so that each one can be distinguished from the others. However, having such references doesn't necessarily mean that an object is a composite. It is only called composite, if the objects it refers to are really its parts, i.e. have no independent existence. For details, see the aggregation section below.

UML notation[edit]
In UML, there are two ways of modelling composition: Composition and aggregation. Beware that in UML, composition has a more narrow meaning than in ordinary language:

Composition is depicted as a filled diamond and a solid line.

Composition is a kind of association where the composite object has sole responsibility for the disposition of the component parts. The relationship between the composite and the component is a strong “has a” relationship, as the composite object takes ownership of the component. This means the composite is responsible for the creation and destruction of the component parts. An object may only be part of one composite. If the composite object is destroyed, all the component parts must be destroyed. The part has no life of itself and cannot be transferred to another object. Composition enforces encapsulation as the component parts usually are members of the composite object.


UML notation for a composition (upper) and an aggregation (lower) (Note that the examples show abstracted data models, disregarding the fact that in a real world carburetor ? car association, there is a market for spare parts.)
The more general form, aggregation, is depicted as an unfilled diamond and a solid line.

Aggregation is a kind of association that specifies a whole/part relationship between the aggregate (whole) and component part. This relationship between the aggregate and component is a weak “has a” relationship, as the component may survive the aggregate object. The component object may be accessed through other objects without going through the aggregate object. The aggregate object does not take part in the lifecycle of the component object, meaning the component object may outlive the aggregate object. The state of the component object still forms part of the aggregate object.

The C++ code below shows what the source code is likely to look like.

// Composition
class Car
{
private:

  // Car is the owner of carburetor.
  // Carburetor is created when Car is created,
  // it is destroyed when Car is destroyed.
  Carburetor carb;

};
// Aggregation
class Pond
{
private:

  // Pond is not the owner of ducks,
  // it has references on other ducks managed somewhere else
  std::vector<Duck*> ducks;
};
Composite types in C[edit]
This is an example of composition in C.

struct Person
{
  int age;
  char *name;
  enum {job_seeking, professional, non_professional, retired, student} employment;
};
In this example, the primitive types int, char *, and enum {job_seeking, professional, non_professional, retired, student} are combined to form the composite structure Person. Each Person structure then "has an" age, name, and an employment type.

Recursive composition[edit]
Objects can be composited recursively with the use of recursive types or references. Consider a tree. Each node in a tree may be a branch or leaf; in other words, each node is a tree at the same time when it belongs to another tree.

One implementation for the recursive composition is to let each object have references to others of the same type. In C, for example, a binary tree can be defined like:

struct bintree
{
  struct bintree *left, *right;
  // some data
};
If pointers left and right are valid, the node is thought to be a branch referring to each tree to which left and right point. If not, the node is a leaf. In this way, the recursion can be terminated.

Another is to use a tagged union. For e.g. see tagged union.

Timeline of composition in various languages[edit]
C calls a record a struct or structure; object-oriented languages such as Java, Smalltalk, and C++ often keep their records hidden inside objects (class instances); languages in the ML family simply call them records. COBOL was the first widespread programming language to support records directly;[2] ALGOL 68 got it from COBOL and Pascal got it, more or less indirectly, from ALGOL 68. Common Lisp provides structures and classes (the latter via the Common Lisp Object System).

1959 – COBOL
      01  customer-record.
        03  customer-number     pic 9(8) comp.
        03  customer-name.
          05  given-names       pic x(15).
          05  initial-2         pic x.
          05  surname           pic x(15).
        03  customer-address.
          05  street.
            07  street-name     pic x(15).
              09  house-number  pic 999 comp.
          05  city              pic x(10).
          05  country-code      pic x(3).
          05  postcode          pic x(8).
        03  amount-owing        pic 9(8) comp.
1960 – ALGOL 60
Arrays were the only composite data type in Algol 60.

1964 – PL/I
dcl 1 newtypet based (P);
 2 (a, b, c) fixed bin(31),
 2 (i, j, k) float,
 2 r ptr;
allocate newtypet;
1968 – ALGOL 68
int max = 99;
mode newtypet = [0..9] [0..max]struct (
 long real a, b, c, short int i, j, k, ref real r
);
newtypet newarrayt = (1, 2, 3, 4, 5, 6, heap real := 7)
For an example of all this, here is the traditional linked list declaration:

mode node = union (real, int, compl, string),
 list = struct (node val, ref list next);
Note that for ALGOL 68 only the newtype name appears to the left of the equality, and most notably the construction is made – and can be read – from left to right without regard to priorities.

1970 – Pascal
type
 a = array [1..10] of integer;
 b = record
 a, b, c: real;
 i, j, k: integer;
 end;
1972 – K&R C
#define max 99
struct newtypet {
  double a, b, c;
  float r;
  short i, j, k;
} newarrayt[10] [max + 1];
1977 – FORTRAN 77
Fortran 77 has arrays, but lacked any formal record/structure definitions. Typically compound structures were built up using EQUIVALENCE or COMMON statements:

       CHARACTER NAME*32, ADDR*32, PHONE*16
       REAL OWING
       COMMON /CUST/NAME, ADDR, PHONE, OWING
1983 – ADA
type Cust is
 record
 Name : Name_Type;
 Addr : Addr_Type;
 Phone : Phone_Type;
 Owing : Integer range 1..999999;
 end record;
1983 – C++
const int max = 99;
class {
  public:
  double a, b, c;
  float &r;
  short i, j, k;
}newtypet[10] [max + 1];
1991 – Python
max = 99
class NewTypeT:
    def __init__(self):
        self.a = self.b = self.c = 0
        self.i = self.j = self.k = 0.0
# Initialise an example array of this class.
newarrayt = [[NewTypeT() for i in range(max + 1)] for j in range(10)]
1992 – FORTRAN 90
Arrays and strings were inherited from FORTRAN 77, and a new reserved word was introduced: type

type newtypet
 double precision a, b, c
 integer*2 i, j, k
* No pointer type REF REAL R
 end type

type (newtypet) t(10, 100)
FORTRAN 90 updated and included FORTRAN IV's concept called NAMELIST.

INTEGER :: jan = 1, feb = 2, mar = 3, apr = 4
NAMELIST / week / jan, feb, mar, apr
1994 – ANSI Common Lisp
Common Lisp provides structures and the ANSI Common Lisp standard added CLOS classes.

(defclass some-class ()
 ((f :type float)
 (i :type integer)
 (a :type (array integer (10)))))
For more details about composition in C/C++, see Composite type.

Aggregation[edit]
Aggregation differs from ordinary composition in that it does not imply ownership. In composition, when the owning object is destroyed, so are the contained objects. In aggregation, this is not necessarily true. For example, a university owns various departments (e.g., chemistry), and each department has a number of professors. If the university closes, the departments will no longer exist, but the professors in those departments will continue to exist. Therefore, a University can be seen as a composition of departments, whereas departments have an aggregation of professors. In addition, a Professor could work in more than one department, but a department could not be part of more than one university.

Composition is usually implemented such that an object contains another object. For example, in C++:

class Professor;

class Department
{
  // Aggregation: vector of pointers to Professor objects living outside the Department
  std::vector<Professor*> members;
};

class University
{
  std::vector<Department> faculty;

  University()  // constructor
  {
    // Composition: Departments exist as long as the University exists
    faculty.push_back(Department("chemistry"));
    faculty.push_back(Department("physics"));
    faculty.push_back(Department("arts"));
  }
};
In aggregation, the object may only contain a reference or pointer to the object (and not have lifetime responsibility for it).

Sometimes aggregation is referred to as composition when the distinction between ordinary composition and aggregation is unimportant.

The above code would transform into the following UML Class diagram:

Aggregation-Composition3.png

Aggregation in COM[edit]

Aggregation in COM
In Microsoft's Component Object Model, aggregation means that an object exports, as if it were their owner, one or several interfaces of another object it owns. Formally, this is more similar to composition or encapsulation than aggregation. However, instead of implementing the exported interfaces by calling the interfaces of the owned object, the interfaces of the owned object themselves are exported. The owned object is responsible for assuring that methods of those interfaces inherited from IUnknown actually invoke the corresponding methods of the owner. This is to guarantee that the reference count of the owner is correct and all interfaces of the owner are accessible through the exported interface, while no other (private) interfaces of the owned object are accessible.[3]

Containment[edit]
Main article: Containment (computer programming)
Composition that is used to store several instances of the composited data type is referred to as containment. Examples of such containers are arrays, associative arrays, binary trees, and linked lists.

In UML, containment is depicted with a multiplicity of 1 or 0..n (depending on the issue of ownership), indicating that the data type is composed of an unknown number of instances of the composited data type.

See also[edit]
C++ structure
Composite type
Composition over inheritance
Delegation (programming)
Has-a
Implementation inheritance
Inheritance semantics
Law of Demeter
Virtual inheritance

Soft copy
From Wikipedia, the free encyclopedia

This article does not cite any sources. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2008) (Learn how and when to remove this template message)
A soft copy is the unprinted digital document file. This term is often contrasted with hard copy. It can usually be viewed through an appropriate editing program, such as word processing programs, database programs, or presentation software, depending on the file type.

It can be transported from one computer to another through file transfer/downloading mechanisms such as FTP or HTTP, as an email attachment, or through USB drives and other disk drives. Keeping a digital copy of a document can allow easy editing of it later on. See hard copy for information about printed documents.

Using soft copies of work over traditional printed documents eliminates the need for paper and ink. Multiple copies of the same document can be kept in different versions, allowing the user to easily backtrack to an earlier version. Also, soft copies are more easily manipulated by users than hard copies, which can be both an advantage and a disadvantage.

When soft copies are kept on storage devices, they conserve office space. Soft copy documents are more portable compared to hard copy because it is not bulky like hard copy.

Hard disk drive
From Wikipedia, the free encyclopedia
  (Redirected from Hard disk)
"Hard drive" redirects here. For other uses, see Hard drive (disambiguation).
Hard disk drive
Laptop-hard-drive-exposed.jpg
Internals of a 2.5-inch SATA hard disk drive
Date invented	24 December 1954; 61 years ago[a]
Invented by	IBM team led by Rey Johnson

A disassembled and labeled 1997 HDD lying atop a mirror
File:Harddrive-engineerguy.ogv
An overview of how HDDs work
A hard disk drive (HDD), hard disk, hard drive or fixed disk[b] is a data storage device used for storing and retrieving digital information using one or more rigid ("hard") rapidly rotating disks (platters) coated with magnetic material. The platters are paired with magnetic heads arranged on a moving actuator arm, which read and write data to the platter surfaces.[2] Data is accessed in a random-access manner, meaning that individual blocks of data can be stored or retrieved in any order and not only sequentially. HDDs are a type of non-volatile memory, retaining stored data even when powered off.

Introduced by IBM in 1956,[3] HDDs became the dominant secondary storage device for general-purpose computers by the early 1960s. Continuously improved, HDDs have maintained this position into the modern era of servers and personal computers. More than 200 companies have produced HDD units, though most current units are manufactured by Seagate, Toshiba and Western Digital. As of 2015, HDD production (exabytes per year) and areal density are growing, although unit shipments are declining.

The primary characteristics of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of 1000: a 1-terabyte (TB) drive has a capacity of 1,000 gigabytes (GB; where 1 gigabyte = 1 billion bytes). Typically, some of an HDD's capacity is unavailable to the user because it is used by the file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. Performance is specified by the time required to move the heads to a track or cylinder (average access time) plus the time it takes for the desired sector to move under the head (average latency, which is a function of the physical rotational speed in revolutions per minute), and finally the speed at which the data is transmitted (data rate).

The two most common form factors for modern HDDs are 3.5-inch, for desktop computers, and 2.5-inch, primarily for laptops. HDDs are connected to systems by standard interface cables such as PATA (Parallel ATA), SATA (Serial ATA), USB or SAS (Serial attached SCSI) cables.

As of 2016, the primary competing technology for secondary storage is flash memory in the form of solid-state drives (SSDs), which have higher data transfer rates, better reliability,[4] and significantly lower latency and access times, but HDDs remain the dominant medium for secondary storage due to advantages in cost per unit of storage.[5][6] However, SSDs are replacing HDDs where speed, power consumption and durability are more important considerations.[7][8] Hybrid drive products, also known by the initialism SSHD, have been available since 2007,[9] made as a combination of HDD and SSD technology in a single device.

Contents  [hide] 
1	History
2	Technology
2.1	Magnetic recording
2.2	Components
2.3	Error rates and handling
2.4	Future development
3	Capacity
3.1	Calculation
3.2	System use
3.3	Units
4	Price evolution
5	Form factors
6	Performance characteristics
6.1	Time to access data
6.2	Seek time
6.3	Latency
6.4	Data transfer rate
6.5	Other considerations
7	Access and interfaces
8	Integrity and failure
9	Market segments
10	Manufacturers and sales
11	External hard disk drives
12	Visual representation
13	See also
14	Notes
15	References
16	Further reading
17	External links
History[edit]
File:HardDisk1.ogg
Video of modern HDD operation (cover removed)
Main article: History of hard disk drives
Improvement of HDD characteristics over time
Parameter	Started with	Developed to	Improvement
Capacity
(formatted)	3.75 megabytes[10]	10 terabytes[11]	2.7-million-to-one
Physical volume	68 cubic feet (1.9 m3)[c][3]	2.1 cubic inches (34 cm3)[12]	56,000-to-one
Weight	2,000 pounds (910 kg)[3]	2.2 ounces (62 g)[12]	15,000-to-one
Average access time	about 600 milliseconds[3]	a few milliseconds	about
200-to-one
Price	US$9,200 per megabyte (1961)[13]	$0.032 per gigabyte by 2015[14]	300-million-to-one
Areal density	2,000 bits per square inch[15]	826 gigabits per square inch in 2014[16]	greater than 400-million-to-one
Hard disk drives were introduced in 1956 as data storage for an IBM real-time transaction processing computer and were developed for use with general-purpose mainframe and minicomputers. The first IBM drive, the 350 RAMAC, was approximately the size of two refrigerators and stored five million six-bit characters (3.75 megabytes)[10] on a stack of 50 disks.[17]

The IBM 350 RAMAC disk storage unit was superseded by the IBM 1301 disk storage unit,[18] which consisted of 50 platters, each about 1/8-inch thick and 24 inches in diameter.[19] Whereas the IBM 350 used two read/write heads, pneumatically actuated[17] and moving through two dimensions, the 1301 was one of the first disk storage units to use an array of heads, one per platter, moving as a single unit. Cylinder-mode read/write operations were supported, while the heads flew about 250 micro-inches above the platter surface. Motion of the head array depended upon a binary adder system of hydraulic actuators which assured repeatable positioning. The 1301 cabinet was about the size of three home refrigerators placed side by side, storing the equivalent of about 21 million eight-bit bytes. Access time was about 200 milliseconds.

In 1962, IBM introduced the model 1311 disk drive, which was about the size of a washing machine and stored two million characters on a removable disk pack. Users could buy additional packs and interchange them as needed, much like reels of magnetic tape. Later models of removable pack drives, from IBM and others, became the norm in most computer installations and reached capacities of 300 megabytes by the early 1980s. Non-removable HDDs were called "fixed disk" drives.

Some high-performance HDDs were manufactured with one head per track (e.g. IBM 2305) so that no time was lost physically moving the heads to a track.[20] Known as fixed-head or head-per-track disk drives they were very expensive and are no longer in production.[21]

In 1973, IBM introduced a new type of HDD codenamed "Winchester". Its primary distinguishing feature was that the disk heads were not withdrawn completely from the stack of disk platters when the drive was powered down. Instead, the heads were allowed to "land" on a special area of the disk surface upon spin-down, "taking off" again when the disk was later powered on. This greatly reduced the cost of the head actuator mechanism, but precluded removing just the disks from the drive as was done with the disk packs of the day. Instead, the first models of "Winchester technology" drives featured a removable disk module, which included both the disk pack and the head assembly, leaving the actuator motor in the drive upon removal. Later "Winchester" drives abandoned the removable media concept and returned to non-removable platters.

Like the first removable pack drive, the first "Winchester" drives used platters 14 inches (360 mm) in diameter. A few years later, designers were exploring the possibility that physically smaller platters might offer advantages. Drives with non-removable eight-inch platters appeared, and then drives that used a 5 1?4 in (130 mm) form factor (a mounting width equivalent to that used by contemporary floppy disk drives). The latter were primarily intended for the then-fledgling personal computer (PC) market.

As the 1980s began, HDDs were a rare and very expensive additional feature in PCs, but by the late 1980s their cost had been reduced to the point where they were standard on all but the cheapest computers.

Most HDDs in the early 1980s were sold to PC end users as an external, add-on subsystem. The subsystem was not sold under the drive manufacturer's name but under the subsystem manufacturer's name such as Corvus Systems and Tallgrass Technologies, or under the PC system manufacturer's name such as the Apple ProFile. The IBM PC/XT in 1983 included an internal 10 MB HDD, and soon thereafter internal HDDs proliferated on personal computers.

External HDDs remained popular for much longer on the Apple Macintosh. Many Macintosh computers made between 1986 and 1998 featured a SCSI port on the back, making external expansion simple. Older compact Macintosh computers did not have user-accessible hard drive bays (indeed, the Macintosh 128K, Macintosh 512K, and Macintosh Plus did not feature a hard drive bay at all), so on those models external SCSI disks were the only reasonable option for expanding upon any internal storage.

The 2011 Thailand floods damaged the manufacturing plants and impacted hard disk drive cost adversely between 2011 and 2013.[22]

Driven by ever increasing areal density since their invention, HDDs have continuously improved their characteristics; a few highlights are listed in the table above. At the same time, market application expanded from mainframe computers of the late 1950s to most mass storage applications including computers and consumer applications such as storage of entertainment content.

Technology[edit]

Magnetic cross section & frequency modulation encoded binary data
Magnetic recording[edit]
See also: Magnetic storage
An HDD records data by magnetizing a thin film of ferromagnetic material[d] on a disk. Sequential changes in the direction of magnetization represent binary data bits. The data is read from the disk by detecting the transitions in magnetization. User data is encoded using an encoding scheme, such as run-length limited encoding,[e] which determines how the data is represented by the magnetic transitions.

A typical HDD design consists of a spindle that holds flat circular disks, also called platters, which hold the recorded data. The platters are made from a non-magnetic material, usually aluminum alloy, glass, or ceramic, and are coated with a shallow layer of magnetic material typically 10–20 nm in depth, with an outer layer of carbon for protection.[24][25][26] For reference, a standard piece of copy paper is 0.07–0.18 millimeters (70,000–180,000 nm).[27]


Diagram labeling the major components of a computer HDD

Recording of single magnetisations of bits on a 200 MB HDD-platter (recording made visible using CMOS-MagView).[28]

Longitudinal recording (standard) & perpendicular recording diagram
The platters in contemporary HDDs are spun at speeds varying from 4,200 rpm in energy-efficient portable devices, to 15,000 rpm for high-performance servers.[29] The first HDDs spun at 1,200 rpm[3] and, for many years, 3,600 rpm was the norm.[30] As of December 2013, the platters in most consumer-grade HDDs spin at either 5,400 rpm or 7,200 rpm.[31]

Information is written to and read from a platter as it rotates past devices called read-and-write heads that are positioned to operate very close to the magnetic surface, with their flying height often in the range of tens of nanometers. The read-and-write head is used to detect and modify the magnetization of the material passing immediately under it.

In modern drives, there is one head for each magnetic platter surface on the spindle, mounted on a common arm. An actuator arm (or access arm) moves the heads on an arc (roughly radially) across the platters as they spin, allowing each head to access almost the entire surface of the platter as it spins. The arm is moved using a voice coil actuator or in some older designs a stepper motor. Early hard disk drives wrote data at some constant bits per second, resulting in all tracks having the same amount of data per track but modern drives (since the 1990s) use zone bit recording—increasing the write speed from inner to outer zone and thereby storing more data per track in the outer zones.

In modern drives, the small size of the magnetic regions creates the danger that their magnetic state might be lost because of thermal effects, thermally induced magnetic instability which is commonly known as the "superparamagnetic limit". To counter this, the platters are coated with two parallel magnetic layers, separated by a 3-atom layer of the non-magnetic element ruthenium, and the two layers are magnetized in opposite orientation, thus reinforcing each other.[32] Another technology used to overcome thermal effects to allow greater recording densities is perpendicular recording, first shipped in 2005,[33] and as of 2007 the technology was used in many HDDs.[34][35][36]

In 2004, a new concept was introduced to allow further increase of the data density in magnetic recording, using recording media consisting of coupled soft and hard magnetic layers. That so-called exchange spring media, also known as exchange coupled composite media, allows good writability due to the write-assist nature of the soft layer. However, the thermal stability is determined only by the hardest layer and not influenced by the soft layer.[37][38]

Components[edit]

HDD with disks and motor hub removed exposing copper colored stator coils surrounding a bearing in the center of the spindle motor. Orange stripe along the side of the arm is thin printed-circuit cable, spindle bearing is in the center and the actuator is in the upper left
A typical HDD has two electric motors; a spindle motor that spins the disks and an actuator (motor) that positions the read/write head assembly across the spinning disks. The disk motor has an external rotor attached to the disks; the stator windings are fixed in place. Opposite the actuator at the end of the head support arm is the read-write head; thin printed-circuit cables connect the read-write heads to amplifier electronics mounted at the pivot of the actuator. The head support arm is very light, but also stiff; in modern drives, acceleration at the head reaches 550 g.


Head stack with an actuator coil on the left and read/write heads on the right
The actuator is a permanent magnet and moving coil motor that swings the heads to the desired position. A metal plate supports a squat neodymium-iron-boron (NIB) high-flux magnet. Beneath this plate is the moving coil, often referred to as the voice coil by analogy to the coil in loudspeakers, which is attached to the actuator hub, and beneath that is a second NIB magnet, mounted on the bottom plate of the motor (some drives only have one magnet).

The voice coil itself is shaped rather like an arrowhead, and made of doubly coated copper magnet wire. The inner layer is insulation, and the outer is thermoplastic, which bonds the coil together after it is wound on a form, making it self-supporting. The portions of the coil along the two sides of the arrowhead (which point to the actuator bearing center) interact with the magnetic field, developing a tangential force that rotates the actuator. Current flowing radially outward along one side of the arrowhead and radially inward on the other produces the tangential force. If the magnetic field were uniform, each side would generate opposing forces that would cancel each other out. Therefore, the surface of the magnet is half north pole and half south pole, with the radial dividing line in the middle, causing the two sides of the coil to see opposite magnetic fields and produce forces that add instead of canceling. Currents along the top and bottom of the coil produce radial forces that do not rotate the head.

The HDD's electronics control the movement of the actuator and the rotation of the disk, and perform reads and writes on demand from the disk controller. Feedback of the drive electronics is accomplished by means of special segments of the disk dedicated to servo feedback. These are either complete concentric circles (in the case of dedicated servo technology), or segments interspersed with real data (in the case of embedded servo technology). The servo feedback optimizes the signal to noise ratio of the GMR sensors by adjusting the voice-coil of the actuated arm. The spinning of the disk also uses a servo motor. Modern disk firmware is capable of scheduling reads and writes efficiently on the platter surfaces and remapping sectors of the media which have failed.

Error rates and handling[edit]
Modern drives make extensive use of error correction codes (ECCs), particularly Reed–Solomon error correction. These techniques store extra bits, determined by mathematical formulas, for each block of data; the extra bits allow many errors to be corrected invisibly. The extra bits themselves take up space on the HDD, but allow higher recording densities to be employed without causing uncorrectable errors, resulting in much larger storage capacity.[39] For example, a typical 1 TB hard disk with 512-byte sectors provides additional capacity of about 93 GB for the ECC data.[40]

In the newest drives, as of 2009,[needs update?] low-density parity-check codes (LDPC) were supplanting Reed-Solomon; LDPC codes enable performance close to the Shannon Limit and thus provide the highest storage density available.[41]

Typical hard disk drives attempt to "remap" the data in a physical sector that is failing to a spare physical sector provided by the drive's "spare sector pool" (also called "reserve pool"),[42] while relying on the ECC to recover stored data while the amount of errors in a bad sector is still low enough. The S.M.A.R.T (Self-Monitoring, Analysis and Reporting Technology) feature counts the total number of errors in the entire HDD fixed by ECC (although not on all hard drives as the related S.M.A.R.T attributes "Hardware ECC Recovered" and "Soft ECC Correction" are not consistently supported), and the total number of performed sector remappings, as the occurrence of many such errors may predict an HDD failure.

The "No-ID Format", developed by IBM in the mid-1990s, contains information about which sectors are bad and where remapped sectors have been located.[43]

Only a tiny fraction of the detected errors ends up as not correctable. For example, specification for an enterprise SAS disk (a model from 2013) estimates this fraction to be one uncorrected error in every 1016 bits,[44] and another SAS enterprise disk from 2013 specifies similar error rates.[45] Another modern (as of 2013) enterprise SATA disk specifies an error rate of less than 10 non-recoverable read errors in every 1016 bits.[46] An enterprise disk with a Fibre Channel interface, which uses 520 byte sectors to support the Data Integrity Field standard to combat data corruption, specifies similar error rates in 2005.[47]

The worst type of errors are those that go unnoticed, and are not even detected by the disk firmware or the host operating system. These errors are known as silent data corruption, some of which may be caused by hard disk drive malfunctions.[48]

Future development[edit]

Leading-edge hard disk drive areal densities from 1956 through 2009 compared to Moore's law
The rate of areal density advancement was similar to Moore's law (doubling every two years) through 2010: 60% per year during 1988–1996, 100% during 1996–2003 and 30% during 2003–2010.[49] Gordon Moore (1997) called the increase "flabbergasting,"[50] while observing later that growth cannot continue forever.[51] Areal density advancement slowed to 10% per year during 2011–2014,[49] due to difficulty in migrating from perpendicular recording to newer technologies.[52]

Areal density is the inverse of bit cell size, so an increase in areal density corresponds to a decrease in bit cell size. In 2013, a production desktop 3 TB HDD (with four platters) would have had an areal density of about 500 Gbit/in2 which would have amounted to a bit cell comprising about 18 magnetic grains (11 by 1.6 grains).[53] Since the mid-2000s areal density progress has increasingly been challenged by a superparamagnetic trilemma involving grain size, grain magnetic strength and ability of the head to write.[54] In order to maintain acceptable signal to noise smaller grains are required; smaller grains may self-reverse (thermal instability) unless their magnetic strength is increased, but known write head materials are unable to generate a magnetic field sufficient to write the medium. Several new magnetic storage technologies are being developed to overcome or at least abate this trilemma and thereby maintain the competitiveness of HDDs with respect to products such as flash memory-based solid-state drives (SSDs).

In 2013, Seagate introduced one such technology, shingled magnetic recording (SMR).[55] Additionally, SMR comes with design complexities that may cause reduced write performance.[56][57] Other new recording technologies that, as of 2016, still remain under development include heat-assisted magnetic recording (HAMR),[58][59] microwave-assisted magnetic recording (MAMR),[60][61] two-dimensional magnetic recording (TDMR),[53][62] bit-patterned recording (BPR),[63] and "current perpendicular to plane" giant magnetoresistance (CPP/GMR) heads.[64][65][66]

The rate of areal density growth has dropped below the historical Moore's law rate of 40% per year, and the deceleration is expected to persist through at least 2020. Depending upon assumptions on feasibility and timing of these technologies, the median forecast by industry observers and analysts for 2020 and beyond for areal density growth is 20% per year with a range of 10–30%.[67][68][69][70] The achievable limit for the HAMR technology in combination with BPR and SMR may be 10 Tbit/in2,[71] which would be 20 times higher than the 500 Gbit/in2 represented by 2013 production desktop HDDs. As of 2015, HAMR HDDs have been delayed several years, and are expected in 2018. They require a different architecture, with redesigned media and read/write heads, new lasers, and new near-field optical transducers.[72]

Capacity[edit]
The capacity of a hard disk drive, as reported by an operating system to the end user, is smaller than the amount stated by the manufacturer, which has several reasons: the operating system using some space, use for data redundancy, and use for file system structures. The difference in capacity reported in true SI-based units vs. binary prefixes can lead to a false impression of missing capacity.

Calculation[edit]
Modern hard disk drives appear to their host controller as a contiguous set of logical blocks. The gross drive capacity is calculated by multiplying the number of blocks by the block size. This information is available from the manufacturer's product specification, and from the drive itself through use of operating system functions that invoke low-level drive commands.[73][74]

The gross capacity of older HDDs is calculated as the product of the number of cylinders per recording zone, the number of bytes per sector (most commonly 512), and the count of zones of the drive.[citation needed] Some modern SATA drives also report cylinder-head-sector (CHS) capacities, but these are not physical parameters because the reported values are constrained by historic operating system interfaces. The C/H/S scheme has been replaced by logical block addressing (LBA), which is a simple linear addressing scheme that locates blocks by an integer index, starting at LBA 0 for the first block and incremented thereafter.[75] When using the C/H/S method to describe modern large drives, the number of heads is often set to 64, although as of 2013, a typical hard disk has only one to four platters.

In modern HDDs, spare capacity for defect management is not included in the published capacity; however, in many early HDDs a certain number of sectors were reserved as spares, thereby reducing the capacity available to the operating system.

For RAID subsystems, data integrity and fault-tolerance requirements also reduce the realized capacity. For example, a RAID1 subsystem has about half the total capacity as a result of data mirroring. RAID5 subsystems with x drives, lose 1/x of capacity to parity. RAID subsystems are multiple drives that appear to be one drive or more drives to the user, but provide fault-tolerance. Most RAID vendors use checksums to improve data integrity at the block level. Some vendors design systems using HDDs with sectors of 520 bytes to contain 512 bytes of user data and eight checksum bytes or using separate 512-byte sectors for the checksum data.[76]

Some systems may use hidden partitions for system recovery, reducing the capacity available to the end user.

System use[edit]
Main article: Disk formatting
The presentation of a hard disk drive to its host is determined by the disk controller. The actual presentation may differ substantially from the drive's native interface, particularly in mainframes or servers. Modern HDDs, such as SAS[73] and SATA[74] drives, appear at their interfaces as a contiguous set of logical blocks that are typically 512 bytes long, though the industry is in the process of changing to the 4,096-byte logical blocks layout, known as the Advanced Format (AF).[77]

The process of initializing these logical blocks on the physical disk platters is called low-level formatting, which is usually performed at the factory and is not normally changed in the field.[78][f] As a next step in preparing an HDD for use, high-level formatting writes partition and file system structures into selected logical blocks to make the remaining logical blocks available to the host's operating system and its applications.[79] The file system uses some of the disk space to structure the HDD and organize files, recording their file names and the sequence of disk areas that represent the file. Examples of data structures stored on disk to retrieve files include the File Allocation Table (FAT) in the DOS file system and inodes in many UNIX file systems, as well as other operating system data structures (also known as metadata). As a consequence, not all the space on an HDD is available for user files, but this system overhead is usually negligible.

Units[edit]
See also: Binary prefix  Disk drives
Decimal and binary unit prefixes interpretation[80][81]
Capacity advertised by manufacturers[g]	Capacity expected by some consumers[h]	Reported capacity
Windows, Linux[h]	OS X
10.6+[g]
With prefix	Bytes	Bytes	Diff.
100 GB	100,000,000,000	107,374,182,400	7.37%	93.1 GB, 95,367 MB	100 GB
1 TB	1,000,000,000,000	1,099,511,627,776	9.95%	931 GB, 953,674 MB	1,000 GB, 1,000,000 MB
The total capacity of HDDs is given by manufacturers in SI-based units[i] such as gigabytes (1 GB = 1,000,000,000 bytes) and terabytes (1 TB = 1,000,000,000,000 bytes).[80][82][83][84][85][86] The practice of using SI-based prefixes (denoting powers of 1,000) in the hard disk drive and computer industries dates back to the early days of computing;[87] by the 1970s, "million", "mega" and "M" were consistently used in the decimal sense for drive capacity.[88][89][90] However, capacities of memory (RAM, ROM) and CDs are traditionally quoted using a binary interpretation of the prefixes, i.e. using powers of 1024 instead of 1000.

Internally, computers do not represent either hard disk drive or memory capacity in powers of 1,024, but reporting it in this manner is a convention.[91] The Microsoft Windows family of operating systems uses the binary convention when reporting storage capacity, so an HDD offered by its manufacturer as a 1 TB drive is reported by these operating systems as a 931 GB HDD. OS X 10.6 ("Snow Leopard") uses decimal convention when reporting HDD capacity.[91] The default behavior of the df command-line utility on Linux is to report the HDD capacity as a number of 1024-byte units.[92]

The difference between the decimal and binary prefix interpretation caused some consumer confusion and led to class action suits against HDD manufacturers. The plaintiffs argued that the use of decimal prefixes effectively misled consumers while the defendants denied any wrongdoing or liability, asserting that their marketing and advertising complied in all respects with the law and that no class member sustained any damages or injuries.[93][94][95]

Price evolution[edit]
HDD price per byte improved at the rate of ?40% per year during 1988–1996, ?51% per year during 1996–2003, and ?34% per year during 2003–2010.[14][49] The price improvement decelerated to ?13% per year during 2011–2014, as areal density increase slowed and the 2011 Thailand floods damaged manufacturing facilities.[52]

Form factors[edit]
Past and present HDD form factors
Form factor	Status	Length	Width	Height	Largest capacity	Platters (max.)	Capacity
per platter
3.5-inch	Current	146 mm	101.6 mm	19 or 25.4 mm	10 TB[96] (October 2015)	5 or 7[97][j]	1149 GB[98]
2.5-inch	Current	100 mm	69.85 mm	5,[99] 7, 9.5,[k] 12.5, 15 or 19 mm[100]	4 TB[101] (2015)	5[102]	800 GB[102]
1.8-inch	Obsolete	78.5 mm[l]	54 mm	5 or 8 mm	320 GB[12] (2009)	2	220 GB [103]
8-inch	Obsolete	362 mm	241.3 mm	117.5 mm	?	?	?
5.25-inch FH	Obsolete	203 mm	146 mm	82.6 mm	47 GB[104] (1998)	14	3.36 GB
5.25-inch HH	Obsolete	203 mm	146 mm	41.4 mm	19.3 GB[105] (1998)	4[m]	4.83 GB
1.3-inch	Obsolete	?	43 mm	?	40 GB[106] (2007)	1	40 GB
1-inch (CFII/ZIF/IDE-Flex)	Obsolete	?	42 mm	?	20 GB (2006)	1	20 GB
0.85-inch	Obsolete	32 mm	24 mm	5 mm	8 GB[107][108] (2004)	1	8 GB

8-, 5.25-, 3.5-, 2.5-, 1.8- and 1-inch HDDs, together with a ruler to show the length of platters and read-write heads

A newer 2.5-inch (63.5 mm) 6,495 MB HDD compared to an older 5.25-inch full-height 110 MB HDD
IBM's first hard drive, the IBM 350, used a stack of fifty 24-inch platters and was of a size comparable to two large refrigerators. In 1962, IBM introduced its model 1311 disk, which used six 14-inch (nominal size) platters in a removable pack and was roughly the size of a washing machine. This became a standard platter size and drive form-factor for many years, used also by other manufacturers.[109] The IBM 2314 used platters of the same size in an eleven-high pack and introduced the "drive in a drawer" layout, although the "drawer" was not the complete drive.

Later drives were designed to fit entirely into a chassis that would mount in a 19-inch rack. Digital's RK05 and RL01 were early examples using single 14-inch platters in removable packs, the entire drive fitting in a 10.5-inch-high rack space (six rack units). In the mid-to-late 1980s the similarly sized Fujitsu Eagle, which used (coincidentally) 10.5-inch platters, was a popular product.

Such large platters were never used with microprocessor-based systems. With increasing sales of microcomputers having built in floppy-disk drives (FDDs), HDDs that would fit to the FDD mountings became desirable. Thus HDD Form factors, initially followed those of 8-inch, 5.25-inch, and 3.5-inch floppy disk drives. Because there were no smaller floppy disk drives, smaller HDD form factors developed from product offerings or industry standards.

8-inch
9.5 in ? 4.624 in ? 14.25 in (241.3 mm ? 117.5 mm ? 362 mm). In 1979, Shugart Associates' SA1000 was the first form factor compatible HDD, having the same dimensions and a compatible interface to the 8" FDD.
5.25-inch
5.75 in ? 3.25 in ? 8 in (146.1 mm ? 82.55 mm ? 203 mm). This smaller form factor, first used in an HDD by Seagate in 1980,[110] was the same size as full-height 5 1?4-inch-diameter (130 mm) FDD, 3.25-inches high. This is twice as high as "half height"; i.e., 1.63 in (41.4 mm). Most desktop models of drives for optical 120 mm disks (DVD, CD) use the half height 5?" dimension, but it fell out of fashion for HDDs. The format was standardized as EIA-741 and co-published as SFF-8501 for disk drives, with other SFF-85xx series standards covering related 5.25 inch devices (optical drives, etc.)[111] The Quantum Bigfoot HDD was the last to use it in the late 1990s, with "low-profile" (?25 mm) and "ultra-low-profile" (?20 mm) high versions.
3.5-inch
4 in ? 1 in ? 5.75 in (101.6 mm ? 25.4 mm ? 146 mm) = 376.77344 cm?. This smaller form factor is similar to that used in an HDD by Rodime in 1983,[112] which was the same size as the "half height" 3?" FDD, i.e., 1.63 inches high. Today, the 1-inch high ("slimline" or "low-profile") version of this form factor is the most popular form used in most desktops. The format was standardized in terms of dimensions and positions of mounting holes as EIA/ECA-740, co-published as SFF-8301.[113]
2.5-inch
2.75 in ? 0.275–0.75 in ? 3.945 in (69.85 mm ? 7–19 mm ? 100 mm) = 48.895–132.715 cm3. This smaller form factor was introduced by PrairieTek in 1988;[114] there is no corresponding FDD. The 2.5-inch drive format is standardized in the EIA/ECA-720 co-published as SFF-8201; when used with specific connectors, more detailed specifications are SFF-8212 for the 50-pin (ATA laptop) connector, SFF-8223 with the SATA, or SAS connector and SFF-8222 with the SCA-2 connector.[115]
It came to be widely used for HDDs in mobile devices (laptops, music players, etc.) and for solid-state drives (SSDs), by 2008 replacing some 3.5 inch enterprise-class drives.[116] It is also used in the PlayStation 3[117] and Xbox 360[118] video game consoles.
Drives 9.5 mm high became an unofficial standard for all except the largest-capacity laptop drives (usually having two platters inside); 12.5 mm-high drives, typically with three platters, are used for maximum capacity, but will not fit most laptop computers. Enterprise-class drives can have a height up to 15 mm.[119] Seagate released a 7 mm drive aimed at entry level laptops and high end netbooks in December 2009.[120] Western Digital released on April 23, 2013 a hard drive 5 mm in height specifically aimed at UltraBooks.[121]
1.8-inch
54 mm ? 8 mm ? 78.5 mm[l] = 33.912 cm?. This form factor, originally introduced by Integral Peripherals in 1993, evolved into the ATA-7 LIF with dimensions as stated. For a time it was increasingly used in digital audio players and subnotebooks, but its popularity decreased to the point where this form factor is increasingly rare and only a small percentage of the overall market.[122] There was an attempt to standardize this format as SFF-8123, but it was cancelled in 2005.[123] SATA revision 2.6 standardized the internal Micro SATA connector and device dimensions.
1-inch
42.8 mm ? 5 mm ? 36.4 mm. This form factor was introduced in 1999 as IBM's Microdrive to fit inside a CF Type II slot. Samsung calls the same form factor "1.3 inch" drive in its product literature.[124]
0.85-inch
24 mm ? 5 mm ? 32 mm. Toshiba announced this form factor in January 2004[125] for use in mobile phones and similar applications, including SD/MMC slot compatible HDDs optimized for video storage on 4G handsets. Toshiba manufactured a 4 GB (MK4001MTD) and an 8 GB (MK8003MTD) version and holds the Guinness World Record for the smallest HDD.[126][127]
As of 2012, 2.5-inch and 3.5-inch hard disks were the most popular sizes.

By 2009, all manufacturers had discontinued the development of new products for the 1.3-inch, 1-inch and 0.85-inch form factors due to falling prices of flash memory,[128][129] which has no moving parts.

While these sizes are customarily described by an approximately correct figure in inches, actual sizes have long been specified in millimeters.

Performance characteristics[edit]
Main article: Hard disk drive performance characteristics
Time to access data[edit]
The factors that limit the time to access the data on an HDD are mostly related to the mechanical nature of the rotating disks and moving heads. Seek time is a measure of how long it takes the head assembly to travel to the track of the disk that contains data. Rotational latency is incurred because the desired disk sector may not be directly under the head when data transfer is requested. These two delays are on the order of milliseconds each. The bit rate or data transfer rate (once the head is in the right position) creates delay which is a function of the number of blocks transferred; typically relatively small, but can be quite long with the transfer of large contiguous files. Delay may also occur if the drive disks are stopped to save energy.

An HDD's Average Access Time is its average seek time which technically is the time to do all possible seeks divided by the number of all possible seeks, but in practice is determined by statistical methods or simply approximated as the time of a seek over one-third of the number of tracks.[130]

Defragmentation is a procedure used to minimize delay in retrieving data by moving related items to physically proximate areas on the disk.[131] Some computer operating systems perform defragmentation automatically. Although automatic defragmentation is intended to reduce access delays, performance will be temporarily reduced while the procedure is in progress.[132]

Time to access data can be improved by increasing rotational speed (thus reducing latency) or by reducing the time spent seeking. Increasing areal density increases throughput by increasing data rate and by increasing the amount of data under a set of heads, thereby potentially reducing seek activity for a given amount of data. The time to access data has not kept up with throughput increases, which themselves have not kept up with growth in bit density and storage capacity.

Seek time[edit]
See also: Hard disk drive performance characteristics  Seek time
Average seek time ranges from under 4 ms for high-end server drives[133] to 15 ms for mobile drives, with the most common mobile drives at about 12 ms[134] and the most common desktop type typically being around 9 ms. The first HDD had an average seek time of about 600 ms;[3] by the middle of 1970s HDDs were available with seek times of about 25 ms.[135] Some early PC drives used a stepper motor to move the heads, and as a result had seek times as slow as 80–120 ms, but this was quickly improved by voice coil type actuation in the 1980s, reducing seek times to around 20 ms. Seek time has continued to improve slowly over time.

Some desktop and laptop computer systems allow the user to make a tradeoff between seek performance and drive noise. Faster seek rates typically require more energy usage to quickly move the heads across the platter, causing louder noises from the pivot bearing and greater device vibrations as the heads are rapidly accelerated during the start of the seek motion and decelerated at the end of the seek motion. Quiet operation reduces movement speed and acceleration rates, but at a cost of reduced seek performance.

Latency[edit]
Rotational speed
[rpm]	Average latency
[ms]
15,000	2
10,000	3
7,200	4.16
5,400	5.55
4,800	6.25
Latency is the delay for the rotation of the disk to bring the required disk sector under the read-write mechanism. It depends on rotational speed of a disk, measured in revolutions per minute (rpm). Average rotational latency is shown in the table on the right, based on the statistical relation that the average latency in milliseconds for such a drive is one-half the rotational period. Average latency (in miliseconds) is computed as 30,000 divided by rotational speed (in rpm).[n]

Data transfer rate[edit]
As of 2010, a typical 7,200-rpm desktop HDD has a sustained "disk-to-buffer" data transfer rate up to 1,030 Mbits/sec.[136] This rate depends on the track location; the rate is higher for data on the outer tracks (where there are more data sectors per rotation) and lower toward the inner tracks (where there are fewer data sectors per rotation); and is generally somewhat higher for 10,000-rpm drives. A current widely used standard for the "buffer-to-computer" interface is 3.0 Gbit/s SATA, which can send about 300 megabyte/s (10-bit encoding) from the buffer to the computer, and thus is still comfortably ahead of today's disk-to-buffer transfer rates. Data transfer rate (read/write) can be measured by writing a large file to disk using special file generator tools, then reading back the file. Transfer rate can be influenced by file system fragmentation and the layout of the files.[131]

HDD data transfer rate depends upon the rotational speed of the platters and the data recording density. Because heat and vibration limit rotational speed, advancing density becomes the main method to improve sequential transfer rates. Higher speeds require a more powerful spindle motor, which creates more heat. While areal density advances by increasing both the number of tracks across the disk and the number of sectors per track, only the latter increases the data transfer rate for a given rpm. Since data transfer rate performance only tracks one of the two components of areal density, its performance improves at a lower rate.[citation needed]

Other considerations[edit]
Other performance considerations include quality-adjusted price, power consumption, audible noise, and both operating and non-operating shock resistance.

The Federal Reserve Board has a quality-adjusted price index for large scale enterprise storage systems including three or more enterprise HDDs and associated controllers, racks and cables. Prices for these large scale storage systems improved at the rate of ?30% per year during 2004–2009 and ?22% per year during 2009–2014.[49]

Access and interfaces[edit]
Main article: Hard disk drive interface

Inner view of a 1998 Seagate HDD that used Parallel ATA interface

2.5-inch SATA drive on top of a 3.5-inch SATA drive, close-up of data and power connectors
HDDs are accessed over one of a number of bus types, including as of 2011 parallel ATA (PATA, also called IDE or EIDE; described before the introduction of SATA as ATA), Serial ATA (SATA), SCSI, Serial Attached SCSI (SAS), and Fibre Channel. Bridge circuitry is sometimes used to connect HDDs to buses with which they cannot communicate natively, such as IEEE 1394, USB and SCSI.

Modern HDDs present a consistent interface to the rest of the computer, no matter what data encoding scheme is used internally. Typically a DSP in the electronics inside the HDD takes the raw analog voltages from the read head and uses PRML and Reed–Solomon error correction[137] to decode the sector boundaries and sector data, then sends that data out the standard interface. That DSP also watches the error rate detected by error detection and correction, and performs bad sector remapping, data collection for Self-Monitoring, Analysis, and Reporting Technology, and other internal tasks.

Modern interfaces connect an HDD to a host bus interface adapter (today typically integrated into the "south bridge") with one data/control cable. Each drive also has an additional power cable, usually direct to the power supply unit.

Small Computer System Interface (SCSI), originally named SASI for Shugart Associates System Interface, was standard on servers, workstations, Commodore Amiga, Atari ST and Apple Macintosh computers through the mid-1990s, by which time most models had been transitioned to IDE (and later, SATA) family disks. The range limitations of the data cable allows for external SCSI devices.
Integrated Drive Electronics (IDE), later standardized under the name AT Attachment (ATA, with the alias P-ATA or PATA (Parallel ATA) retroactively added upon introduction of SATA) moved the HDD controller from the interface card to the disk drive. This helped to standardize the host/controller interface, reduce the programming complexity in the host device driver, and reduced system cost and complexity. The 40-pin IDE/ATA connection transfers 16 bits of data at a time on the data cable. The data cable was originally 40-conductor, but later higher speed requirements for data transfer to and from the HDD led to an "ultra DMA" mode, known as UDMA. Progressively swifter versions of this standard ultimately added the requirement for an 80-conductor variant of the same cable, where half of the conductors provides grounding necessary for enhanced high-speed signal quality by reducing cross talk.
EIDE was an unofficial update (by Western Digital) to the original IDE standard, with the key improvement being the use of direct memory access (DMA) to transfer data between the disk and the computer without the involvement of the CPU, an improvement later adopted by the official ATA standards. By directly transferring data between memory and disk, DMA eliminates the need for the CPU to copy byte per byte, therefore allowing it to process other tasks while the data transfer occurs.
Fibre Channel (FC) is a successor to parallel SCSI interface on enterprise market. It is a serial protocol. In disk drives usually the Fibre Channel Arbitrated Loop (FC-AL) connection topology is used. FC has much broader usage than mere disk interfaces, and it is the cornerstone of storage area networks (SANs). Recently other protocols for this field, like iSCSI and ATA over Ethernet have been developed as well. Confusingly, drives usually use copper twisted-pair cables for Fibre Channel, not fibre optics. The latter are traditionally reserved for larger devices, such as servers or disk array controllers.
Serial Attached SCSI (SAS). The SAS is a new generation serial communication protocol for devices designed to allow for much higher speed data transfers and is compatible with SATA. SAS uses a mechanically identical data and power connector to standard 3.5-inch SATA1/SATA2 HDDs, and many server-oriented SAS RAID controllers are also capable of addressing SATA HDDs. SAS uses serial communication instead of the parallel method found in traditional SCSI devices but still uses SCSI commands.
Serial ATA (SATA). The SATA data cable has one data pair for differential transmission of data to the device, and one pair for differential receiving from the device, just like EIA-422. That requires that data be transmitted serially. A similar differential signaling system is used in RS485, LocalTalk, USB, FireWire, and differential SCSI.
Integrity and failure[edit]

Close-up of an HDD head resting on a disk platter; its mirror reflection is visible on the platter surface.
Main articles: Hard disk drive failure and Data recovery
Due to the extremely close spacing between the heads and the disk surface, HDDs are vulnerable to being damaged by a head crash—a failure of the disk in which the head scrapes across the platter surface, often grinding away the thin magnetic film and causing data loss. Head crashes can be caused by electronic failure, a sudden power failure, physical shock, contamination of the drive's internal enclosure, wear and tear, corrosion, or poorly manufactured platters and heads.

The HDD's spindle system relies on air density inside the disk enclosure to support the heads at their proper flying height while the disk rotates. HDDs require a certain range of air densities in order to operate properly. The connection to the external environment and density occurs through a small hole in the enclosure (about 0.5 mm in breadth), usually with a filter on the inside (the breather filter).[138] If the air density is too low, then there is not enough lift for the flying head, so the head gets too close to the disk, and there is a risk of head crashes and data loss. Specially manufactured sealed and pressurized disks are needed for reliable high-altitude operation, above about 3,000 m (9,800 ft).[139] Modern disks include temperature sensors and adjust their operation to the operating environment. Breather holes can be seen on all disk drives—they usually have a sticker next to them, warning the user not to cover the holes. The air inside the operating drive is constantly moving too, being swept in motion by friction with the spinning platters. This air passes through an internal recirculation (or "recirc") filter to remove any leftover contaminants from manufacture, any particles or chemicals that may have somehow entered the enclosure, and any particles or outgassing generated internally in normal operation. Very high humidity present for extended periods of time can corrode the heads and platters.

For giant magnetoresistive (GMR) heads in particular, a minor head crash from contamination (that does not remove the magnetic surface of the disk) still results in the head temporarily overheating, due to friction with the disk surface, and can render the data unreadable for a short period until the head temperature stabilizes (so called "thermal asperity", a problem which can partially be dealt with by proper electronic filtering of the read signal).

When the logic board of a hard disk fails, the drive can often be restored to functioning order and the data recovered by replacing the circuit board with one of an identical hard disk. In the case of read-write head faults, they can be replaced using specialized tools in a dust-free environment. If the disk platters are undamaged, they can be transferred into an identical enclosure and the data can be copied or cloned onto a new drive. In the event of disk-platter failures, disassembly and imaging of the disk platters may be required.[140] For logical damage to file systems, a variety of tools, including fsck on UNIX-like systems and CHKDSK on Windows, can be used for data recovery. Recovery from logical damage can require file carving.

A common expectation is that hard disk drives designed and marketed for server use will fail less frequently than consumer-grade drives usually used in desktop computers. However, two independent studies by Carnegie Mellon University[141] and Google[142] found that the "grade" of a drive does not relate to the drive's failure rate.

A 2011 summary of research into SSD and magnetic disk failure patterns by Tom's Hardware summarized research findings as follows:[143]

Mean time between failures (MTBF) does not indicate reliability; the annualized failure rate is higher and usually more relevant.
Magnetic disks do not have a specific tendency to fail during early use, and temperature only has a minor effect; instead, failure rates steadily increase with age.
S.M.A.R.T. warns of mechanical issues but not other issues affecting reliability, and is therefore not a reliable indicator of condition.[144]
Failure rates of drives sold as "enterprise" and "consumer" are "very much similar", although these drive types are customized for their different operating environments.[145][146]
In drive arrays, one drive's failure significantly increases the short-term chance of a second drive failing.
Market segments[edit]
Desktop HDDs
They typically store between 60 GB and 4 TB and rotate at 5,400 to 10,000 rpm, and have a media transfer rate of 0.5 Gbit/s or higher (1 GB = 109 bytes; 1 Gbit/s = 109 bit/s). As of August 2014, the highest-capacity desktop HDDs store 8 TB.[147][148]
Mobile (laptop) HDDs

Two enterprise-grade SATA 2.5-inch 10,000 rpm HDDs, factory-mounted in 3.5-inch adapter frames
Smaller than their desktop and enterprise counterparts, they tend to be slower and have lower capacity. Mobile HDDs spin at 4,200 rpm, 5,200 rpm, 5,400 rpm, or 7,200 rpm, with 5,400 rpm being typical. 7,200 rpm drives tend to be more expensive and have smaller capacities, while 4,200 rpm models usually have very high storage capacities. Because of smaller platter(s), mobile HDDs generally have lower capacity than their greater desktop counterparts.
There are also 2.5-inch drives spinning at 10,000 rpm, which belong to the enterprise segment with no intention to be used in laptops.
Enterprise HDDs
Typically used with multiple-user computers running enterprise software. Examples are: transaction processing databases, internet infrastructure (email, webserver, e-commerce), scientific computing software, and nearline storage management software. Enterprise drives commonly operate continuously ("24/7") in demanding environments while delivering the highest possible performance without sacrificing reliability. Maximum capacity is not the primary goal, and as a result the drives are often offered in capacities that are relatively low in relation to their cost.[149]
The fastest enterprise HDDs spin at 10,000 or 15,000 rpm, and can achieve sequential media transfer speeds above 1.6 Gbit/s[150] and a sustained transfer rate up to 1 Gbit/s.[150] Drives running at 10,000 or 15,000 rpm use smaller platters to mitigate increased power requirements (as they have less air drag) and therefore generally have lower capacity than the highest capacity desktop drives. Enterprise HDDs are commonly connected through Serial Attached SCSI (SAS) or Fibre Channel (FC). Some support multiple ports, so they can be connected to a redundant host bus adapter.
Enterprise HDDs can have sector sizes larger than 512 bytes (often 520, 524, 528 or 536 bytes). The additional per-sector space can be used by hardware RAID controllers or applications for storing Data Integrity Field (DIF) or Data Integrity Extensions (DIX) data, resulting in higher reliability and prevention of silent data corruption.[151]
Consumer electronics HDDs
They include drives embedded into digital video recorders and automotive vehicles. The former are configured to provide a guaranteed streaming capacity, even in the face of read and write errors, while the latter are built to resist larger amounts of shock.
Manufacturers and sales[edit]

Diagram of HDD manufacturer consolidation
See also: History of hard disk drives and List of defunct hard disk manufacturers
More than 200 companies have manufactured HDDs over time. But consolidations have concentrated production into just three manufacturers today: Western Digital, Seagate, and Toshiba.

Worldwide revenues for disk storage were $32 billion in 2013, down about 3% from 2012.[152] Annualized shipments worldwide were 470 million units during the first three quarters of 2015, down 16% and 30% from the corresponding quarters of 2014 and 2011.[153] Another source reports 552 million units shipped in 2013, compared to 578 million in 2012, and 622 million in 2011.[152] The estimated 2015 market shares are about 40–45% each for Seagate and Western Digital and 13–16% for Toshiba. The two largest manufacturers report that the average sales price is $60 per HDD unit in 2015.

External hard disk drives[edit]
See also: USB mass storage device and Disk enclosure

Toshiba 1 TB 2.5" external USB 2.0 hard disk drive

3.0 TB 3.5" Seagate FreeAgent GoFlex plug and play external USB 3.0-compatible drive (left), 750 GB 3.5" Seagate Technology push-button external USB 2.0 drive (right), and a 500 GB 2.5" generic brand plug and play external USB 2.0 drive (front).
External hard disk drives[o] typically connect via USB; variants using USB 2.0 interface generally have slower data transfer rates when compared to internally mounted hard drives connected through SATA. Plug and play drive functionality offers system compatibility and features large storage options and portable design. As of March 2015, available capacities for external hard disk drives range from 500 GB to 8 TB.[154]

External hard disk drives are usually available as pre-assembled integrated products, but may be also assembled by combining an external enclosure (with USB or other interface) with a separately purchased drive. They are available in 2.5-inch and 3.5-inch sizes; 2.5-inch variants are typically called portable external drives, while 3.5-inch variants are referred to as desktop external drives. "Portable" drives are packaged in smaller and lighter enclosures than the "desktop" drives; additionally, "portable" drives use power provided by the USB connection, while "desktop" drives require external power bricks.

Features such as biometric security or multiple interfaces (for example, Firewire) are available at a higher cost.[155] There are pre-assembled external hard disk drives that, when taken out from their enclosures, cannot be used internally in a laptop or desktop computer due to embedded USB interface on their printed circuit boards, and lack of SATA (or Parallel ATA) interfaces.[156][157]

Visual representation[edit]
Hard disk drives are traditionally symbolized as a stylized stack of platters or as a cylinder, and are as such found in various diagrams; sometimes, they are depicted with small lights to indicate data access. In most modern graphical user environments (GUIs), hard disk drives are represented by an illustration or photograph of the drive enclosure.
File system permissions
From Wikipedia, the free encyclopedia
Most file systems have methods to assign permissions or access rights to specific users and groups of users. These systems control the ability of the users to view, change, navigate, and execute the contents of the filesystem.

Contents  [hide] 
1	Operating system variations
2	Traditional Unix permissions
2.1	Classes
2.2	Permissions
2.3	Changing permission behavior with setuid, setgid, and sticky bits
3	Notation of traditional Unix permissions
3.1	Symbolic notation
3.2	Numeric notation
4	User private group
5	See also
6	References
7	External links
Operating system variations[edit]
Unix-like and otherwise POSIX-compliant systems, including Linux-based systems and all Mac OS X versions, have a simple system for managing individual file permissions, which in this article are called "traditional Unix permissions". Most of these systems also support some kind of access control lists, either proprietary (old HP-UX ACLs, for example), or POSIX.1e ACLs, based on an early POSIX draft that was withdrawn in 1997, or NFSv4 ACLs, which are part of the NFSv4 standard.

Microsoft and IBM DOS variants (including MS-DOS, PC DOS, Windows 95, Windows 98, Windows 98 SE, and Windows Me) do not have permissions, only file attributes. There is a read-only attribute (R), which can be set or unset on a file by any user or program, and therefore does not prevent him/her/it from changing/deleting the file. There is no permission in these systems which would prevent a user from reading a file.

Other MS-DOS/PC DOS-compatible operating systems such as DR DOS 3.31 and higher, PalmDOS, Novell DOS and OpenDOS, FlexOS, 4680 OS, 4690 OS, Concurrent DOS, Multiuser DOS, Datapac System Manager and IMS REAL/32 support read/write/execute/delete file/directory access permissions on FAT volumes. With the exception of FlexOS, 4680 OS, and 4690 OS all these operating systems also support individual file/directory passwords. All operating systems except for DR DOS, PalmDOS, Novell DOS and OpenDOS also support three independent file/directory ownership classes world/group/owner, whereas the single-user operating systems DR DOS 6.0 and higher, PalmDOS, Novell DOS and OpenDOS only support them with an optional multi-user security module (SECURITY.BIN) loaded.

OpenVMS (a.k.a. VMS), as well as Microsoft Windows NT and its derivatives (including Windows 2000 and Windows XP), use access control lists (ACLs)[1] to administer a more complex and varied set of permissions. OpenVMS also uses a permission scheme similar to that of Unix, but more complex. There are four categories (System, Owner, Group, and World) and four types of access permissions (Read, Write, Execute and Delete). The categories are not mutually disjoint: World includes Group which in turn includes Owner. The System category independently includes system users (similar to superusers in Unix).[2]

Classic Mac Operating Systems' HFS do not support permissions, only file attributes: "Hidden" (does not show in directory listings); "Locked" (read-only); "Name locked" (cannot be renamed); and "Stationery" (copy-on-write).

The AmigaOS Filesystem, AmigaDOS supports a relatively advanced permissions system, for a single-user OS. In AmigaOS 1.x, files had Archive, Read, Write, Execute and Delete (collectively known as ARWED) permissions/flags. In AmigaOS 2.x and higher, additional Hold, Script, and Pure permissions/flags were added.

Mac OS X versions 10.3 ("Panther") and prior use POSIX-compliant permissions. Mac OS X, beginning with version 10.4 ("Tiger"), also support the use of NFSv4 ACLs. They still support "traditional Unix permissions" as used in previous versions of Mac OS X, and the Apple Mac OS X Server version 10.4+ File Services Administration Manual recommends using only traditional Unix permissions if possible. It also still supports the Mac OS Classic's "Protected" attribute.

Solaris ACL support depends on the filesystem being used; older UFS filesystem supports POSIX.1e ACLs, while ZFS supports only NFSv4 ACLs.[3]

Linux supports POSIX.1e ACLs. There is experimental support for NFSv4 ACLs for ext3 filesystem [4] and ext4 filesystem.

FreeBSD supports POSIX.1e ACLs on UFS, and NFSv4 ACLs on UFS and ZFS.[5][6]

IBM z/OS implements file security via RACF (Resource Access Control Facility)[7]

Traditional Unix permissions[edit]
Permissions on Unix-like systems are managed in three distinct scopes or classes. These scopes are known as user, group, and others.

When a file is created on a Unix-like system, its permissions are restricted by the umask of the process that created it.

Classes[edit]
Files and directories are owned by a user. The owner determines the file's user class. Distinct permissions apply to the owner.

Files and directories are assigned a group, which define the file's group class. Distinct permissions apply to members of the file's group. The owner may be a member of the file's group.

Users who are not the owner, nor a member of the group, comprise a file's others class. Distinct permissions apply to others.

The effective permissions are determined based on the first class the user falls within in the order of user, group then others. For example, the user who is the owner of the file will have the permissions given to the user class regardless of the permissions assigned to the group class or others class.

Permissions[edit]
Main article: Modes (Unix)
Unix-like systems implement three specific permissions that apply to each class:

The read permission grants the ability to read a file. When set for a directory, this permission grants the ability to read the names of files in the directory, but not to find out any further information about them such as contents, file type, size, ownership, permissions.
The write permission grants the ability to modify a file. When set for a directory, this permission grants the ability to modify entries in the directory. This includes creating files, deleting files, and renaming files.
The execute permission grants the ability to execute a file. This permission must be set for executable programs, including shell scripts, in order to allow the operating system to run them. When set for a directory, this permission grants the ability to access file contents and meta-information if its name is known, but not list files inside the directory, unless read is set also.
The effect of setting the permissions on a directory, rather than a file, is "one of the most frequently misunderstood file permission issues".[8]

When a permission is not set, the corresponding rights are denied. Unlike ACL-based systems, permissions on Unix-like systems are not inherited. Files created within a directory do not necessarily have the same permissions as that directory.

Changing permission behavior with setuid, setgid, and sticky bits[edit]
Unix-like systems typically employ three additional modes. These are actually attributes but are referred to as permissions or modes. These special modes are for a file or directory overall, not by a class, though in the symbolic notation (see below) the setuid bit is set in the triad for the user, the setgid bit is set in the triad for the group and the sticky bit is set in the triad for others.

The set user ID, setuid, or SUID mode. When a file with setuid is executed, the resulting process will assume the effective user ID given to the owner class. This enables users to be treated temporarily as root (or another user).
The set group ID, setgid, or SGID permission. When a file with setgid is executed, the resulting process will assume the group ID given to the group class. When setgid is applied to a directory, new files and directories created under that directory will inherit their group from that directory. (Default behaviour is to use the primary group of the effective user when setting the group of new files and directories, except on BSD-derived systems which behave as though the setgid bit is always set on all directories (See Setuid).)
The sticky mode. (Also known as the Text mode.) The classical behaviour of the sticky bit on executable files has been to encourage the kernel to retain the resulting process image in memory beyond termination; however such use of the sticky bit is now restricted to only a minority of unix-like operating systems (HP-UX and UnixWare). On a directory, the sticky permission prevents users from renaming, moving or deleting contained files owned by users other than themselves, even if they have write permission to the directory. Only the directory owner and superuser are exempt from this.
These additional modes are also referred to as setuid bit, setgid bit, and sticky bit, due to the fact that they each occupy only one bit.

Notation of traditional Unix permissions[edit]
Symbolic notation[edit]
Unix permissions are represented either in symbolic notation or in octal notation.

The most common form is symbolic notation as shown by ls -l.

Three permission triads
first triad	what the owner can do
second triad	what the group members can do
third triad	what other users can do
Each triad
first character	r: readable
second character	w: writable
third character	x: executable
s or t: setuid/setgid or sticky (also executable)
S or T: setuid/setgid or sticky (not executable)
The first character of the ls display indicates the file type and is not related to permissions. The remaining nine characters are in three sets, each representing a class of permissions as three characters. The first set represents the user class. The second set represents the group class. The third set represents the others class.

Each of the three characters represent the read, write, and execute permissions:

r if reading is permitted, - if it is not.
w if writing is permitted, - if it is not.
x if execution is permitted, - if it is not.
The following are some examples of symbolic notation:

-rwxr-xr-x: a regular file whose user class has full permissions and whose group and others classes have only the read and execute permissions.
crw-rw-r--: a character special file whose user and group classes have the read and write permissions and whose others class has only the read permission.
dr-x------: a directory whose user class has read and execute permissions and whose group and others classes have no permissions.
In some permission systems additional symbols in the ls -l display represent additional permission features:

+ (plus) suffix indicates an access control list that can control additional permissions.
. (dot) suffix indicates an SELinux context is present. Details may be listed with the command ls -Z.
@ suffix indicates extended file attributes are present.
To represent the setuid, setgid and sticky or text attributes, the executable character ('x' or '-') is modified. Though these attributes affect the overall file, not only users in one class, the setuid attribute modifies the executable character in the triad for the user, the setgid attribute modifies the executable character in the triad for the group and the sticky or text attribute modifies the executable character in the triad for others. For the setuid or setgid attributes, in the first or second triad, the 'x' becomes 's' and the '-' becomes 'S'. For the sticky or text attribute, in the third triad, the 'x' becomes 't' and the '-' becomes 'T'. Here is an example:

-rwsr-Sr-t: a file whose user class has read, write and execute permissions; whose group class has read permission; whose others class has read and execute permissions; and which has setuid, setgid and sticky attributes set.
Numeric notation[edit]
Another method for representing Unix permissions is an octal (base-8) notation as shown by stat -c %a. This notation consists of at least three digits. Each of the three rightmost digits represents a different component of the permissions: owner, group, and others. (If a fourth digit is present, the leftmost (high-order) digit addresses three additional attributes, the setuid bit, the setgid bit and the sticky bit.)

Each of these digits is the sum of its component bits in the binary numeral system. As a result, specific bits add to the sum as it is represented by a numeral:

The read bit adds 4 to its total (in binary 100),
The write bit adds 2 to its total (in binary 010), and
The execute bit adds 1 to its total (in binary 001).
These values never produce ambiguous combinations; each sum represents a specific set of permissions. More technically, this is an octal representation of a bit field – each bit references a separate permission, and grouping 3 bits at a time in octal corresponds to grouping these permissions by user, group, and others.

These are the examples from the symbolic notation section given in octal notation:

Symbolic Notation	Numeric Notation	English
----------	0000	no permissions
---x--x--x	0111	execute
--w--w--w-	0222	write
--wx-wx-wx	0333	write & execute
-r--r--r--	0444	read
-r-xr-xr-x	0555	read & execute
-rw-rw-rw-	0666	read & write
-rwxrwxrwx	0777	read, write, & execute
-rwxr-----	0740	user can read, write, & execute; group can only read; others have no permissions
User private group[edit]
Some systems diverge from the traditional POSIX-model of users and groups, by creating a new group – a "user private group" – for each user. Assuming each user is the only member of its user private group, this scheme allows an umask of 002 to be used without allowing other users to write to newly created files in normal directories because such files are assigned to the creating user's private group. However, when sharing files is desirable, the administrator can create a group containing the desired users, create a group-writable directory assigned to the new group, and, most importantly, make the directory setgid. Making it setgid will cause files created in it to be assigned to the same group as the directory and the 002 umask (enabled by using user private groups) will ensure that other members of the group will be able to write to those files. [9] [10]

See also[edit]
chattr or chflags which set attributes or flags including "immutable" flags which lock files, overriding and restricting their permissions
chmod, the command used to set permissions on Unix-like systems
Group identifier (Unix)
lsattr
POSIX
umask
User identifier (Unix)
chattr
From Wikipedia, the free encyclopedia
Not to be confused with chatr (HP-UX command).
chattr is the command in the Linux operating system that allows a user to set certain attributes of a file residing on a Linux file system. lsattr is the command that displays the attributes of a file.

Modern BSD-like systems, including OS X, have analogous chflags to set, but no command specifically meant to display them; specific options to the ls command are used instead.

The Solaris system has no commands specifically meant to manipulate them. chmod[1] and ls[2] are used instead.

Other Unices, in general, have no analogous commands. The similar-sounding commands chatr (from HP-UX) and lsattr (from AIX) exist but have unrelated functions.

Among other things, the chattr command is useful to make files immutable so that password files and certain system files cannot be erased during software upgrades.[3]

Contents  [hide] 
1	In Linux systems (chattr and lsattr)
1.1	File system support
1.2	chattr description
1.3	lsattr description
1.4	Attributes
1.4.1	Notes
1.4.2	Other attributes
2	In BSD-like systems (chflags)
2.1	File system support
2.2	chflags description
2.3	Displaying
2.4	Attributes
3	See also
4	Notes
5	References
In Linux systems (chattr and lsattr)[edit]
File system support[edit]
The attributes chattr and lsattr manipulate were originally specific to the Second Extended Filesystem family (ext2, ext3, ext4), and are available as part of the e2fsprogs package.

However, the functionality has since been extended, fully or partially, to many other systems, including XFS, ReiserFS, JFS and OCFS2.

Even the originally targeted file systems miss some features, as pointed further in this article.

chattr description[edit]
The form of the chattr command is:

 chattr [-RVf] [-+=AacDdijsTtSu] [-v version] files...
-R recursively changes attributes of directories and their contents
-V is to be verbose and print the program version
-f suppresses most error messages
lsattr description[edit]
The form of the lsattr command (gnu 1.41.3):

 lsattr [ -RVadv ] [ files...  ]
-R recursively lists attributes of directories and their contents
-V displays the program version
-a lists all files in directories, including dotfiles
-d lists directories like other files, rather than listing their contents
Attributes[edit]
Some attributes include:

File attributes on a Linux file system according to the chattr(1) Linux man page
Attribute	lsattr flag	chattr option	Semantics and rationale
No atime updates	A	+A to set
-A to clear	
When a file with the A attribute set is accessed, its atime record is not modified.
This avoids a certain amount of disk I/O for laptop systems.
Append only	a	+a to set
-a to clear[note 1]	
A file with the a attribute set can only be open in append mode for writing.
Compressed	c	+c to set
-c to clear[note 2]	
A file with the c attribute set is automatically compressed on the disk by the kernel.
A read from this file returns uncompressed data.
A write to this file compresses data before storing them on the disk.
Synchronous directory updates	D	+D to set
-D to clear	
When a directory with the D attribute set is modified, the changes are written synchronously on the disk
This is equivalent to the dirsync mount option, applied to a subset of the files.
No dump	d	+d to set
-d to clear	
A file with the d attribute set is not candidate for backup when the dump program is run.
Compression error	E	(unavailable)	
The E attribute is used by the experimental compression patches to indicate that a compressed file has a compression error.
Extent format	e	(unavailable)	
The e attribute indicates that the file is using extents for mapping the blocks on disk.
Huge file	h	(unavailable)	
The h attribute indicates the file is storing its blocks in units of the filesystem blocksize instead of in units of sectors.
It means that the file is, or at one time was, larger than 2TB.
Indexed directory	I	(unavailable)	
The I attribute is used by the htree program code to indicate that a directory is being indexed using hashed trees.
Immutable	i	+i to set
-i to clear[note 1]	
A file with the i attribute cannot be modified.
It cannot be deleted or renamed, no link can be created to this file and no data can be written to the file.
When set, prevents, even the superuser, from erasing or changing the contents of the file.
Data journaling	j	+j to set
-j to clear[note 3]	
A file with the j attribute has all of its data written to the ext3 journal before being written to the file itself, if the filesystem is mounted with the "data=ordered" or "data=writeback" options.
When the filesystem is mounted with the "data=journal" option all file data is already journaled, so this attribute has no effect.
Secure deletion	s	+s to set
-s to clear[note 2][note 4]	
When a file with the s attribute set is deleted, its blocks are zeroed and written back to the disk.
Synchronous updates	S	+S to set
-S to clear	
When a file with the S attribute set is modified, the changes are written synchronously on the disk; this is equivalent to the 'sync' mount option applied to a subset of the files.
This is equivalent to the sync mount option, applied to a subset of the files.
Top of directory hierarchy	T	+T to set
-T to clear	
A directory with the T attribute will be deemed to be the top of directory hierarchies for the purposes of the Orlov block allocator.
This is a hint to the block allocator used by ext3 and ext4 that the subdirectories under this directory are not related, and thus should be spread apart for allocation purposes.
For example: it is a very good idea to set the T attribute on the /home directory, so that /home/john and /home/mary are placed into separate block groups.
For directories where this attribute is not set, the Orlov block allocator will try to group subdirectories closer together where possible.
No tail-merging	t	+t to set
-t to clear	
For those filesystems which support tail-merging, a file with the t attribute will not have a partial block fragment at the end of the file merged with other files.
This is necessary for applications such as LILO which read the filesystem directly, and which don't understand tail-merged files.
Undeletable	u	+u to set
-u to clear[note 2]	
When a file with the u attribute set is deleted, its contents are saved.
This allows the user to ask for its undeletion.
Compression raw access	X	(unavailable)	
The X attribute is used by the experimental compression patches to indicate that a raw contents of a compressed file can be accessed directly.
Compressed dirty file	Z	(unavailable)	
The Z attribute is used by the experimental compression patches to indicate a compressed file is "dirty".
Version / generation number	-v	-v version	
File's version/generation number.
Notes[edit]
^ Jump up to: a b Only the superuser or a process possessing the CAP_LINUX_IMMUTABLE capability can set or clear these attributes.
^ Jump up to: a b c These attributes are not honored by the ext2 and ext3 filesystems as implemented in the current mainline Linux kernels.
Jump up ^ Only the superuser or a process possessing the CAP_SYS_RESOURCE capability can set or clear this attribute.
Jump up ^ This attribute is not honored by the ext4 filesystem as implemented in the current mainline Linux kernels as reported in Bug #17872.
Other attributes[edit]
Other attributes include:

no Copy-on-write (C) [4]
In BSD-like systems (chflags)[edit]
File system support[edit]
The chflags command is not specific to particular file systems. UFS on BSD systems, and HFS+, SMB, AFP, and FAT on OS X support least some flags.

chflags description[edit]
The form of the chflags command is:

 chflags [-R [-H | -L | -P]] flags file ...
-H If the -R option is specified, symbolic links on the command line are followed. (Symbolic links encountered in the tree traversal are not followed.)
-L If the -R option is specified, all symbolic links are followed.
-P If the -R option is specified, no symbolic links are followed. This is the default.
-R Change the file flags for the file hierarchies rooted in the files instead of just the files themselves.
Displaying[edit]
BSD-like systems, in general, have no default user-level command specifically meant to display the flags of a file. The ls command will do with either the -lo, or the -lO, depending on the system, flags passed.

Attributes[edit]
All attributes can be set or cleared by the super-user; some can also be set or cleared by the owner of the file. Some attributes include:

File attributes
Attribute	ls flag	chflags flag	Owner-settable	OS support	Semantics and rationale
Archived	arch	arch, archived	No	All	
Opaque	opaque	opaque	Yes	All	Directory is opaque when viewed through a union mount
No dump	nodump	nodump	Yes	All	
System append-only	sappnd	sappnd, sappend	No	All	Existing data in the file can't be overwritten and the file cannot be truncated
System immutable	schg	schg, schange, simmutable	No	All	File cannot be changed, renamed, moved, or removed
User append-only	uappnd	uappnd, uappend	Yes	All	Existing data in the file can't be overwritten and the file cannot be truncated
User immutable	uchg	uchg, uchange, uimmutable	Yes	All	Existing data in the file can't be overwritten
Hidden	hidden	hidden	Yes	OS X	File is hidden by default in the GUI (but not in ls)
See also[edit]
ATTRIB – analogous command in MS-DOS, OS/2 and Microsoft Windows
chown – change file/directory ownership in a Unix system
chmod – change file access control attributes in a Unix system
cacls – change file access control lists in Microsoft Windows NT
Notes[edit]
Jump up ^ chmod(1) – illumos and OpenSolaris User Commands Reference Manual from latest Sun based OpenSolaris
Jump up ^ ls(1) – illumos and OpenSolaris User Commands Reference Manual from latest Sun based OpenSolaris
Jump up ^ chflags(1) – OpenBSD General Commands Manual
Jump up ^ E2fsprogs: add compress and cow support in chattr, lsattr, retrieved April 9, 2012
References[edit]
chattr(1) – Linux User Commands Manual
lsattr(1) – Linux User Commands Manual
chflags(1) – OpenBSD General Commands Manual
chflags(1) – FreeBSD General Commands Manual
chflags(1) – NetBSD General Commands Manual
chflags(1) – Darwin and OS X General Commands Manual

chmod
From Wikipedia, the free encyclopedia
In Unix-like operating systems, chmod is the command and system call which may change the access permissions to file system objects (files and directories). It may also alter special mode flags. The request is filtered by the umask. The name is an abbreviation of change mode.[1]

Contents  [hide] 
1	History
2	Command syntax
2.1	Octal modes
2.1.1	Numeric example
2.2	Symbolic modes
2.2.1	Symbolic examples
2.3	Special modes
2.4	Command line examples
3	System call
4	See also
5	References
6	External links
History[edit]
A chmod command first appeared in AT&T Unix version 1.

As systems grew in number and types of users, access control lists [2] were added to many file systems in addition to these most basic modes to increase flexibility.

Command syntax[edit]
chmod [options] mode[,mode] file1 [file2 ...]
[3]

Usual implemented options include:

-R recursive, i.e. include objects in subdirectories
-f force, forge ahead with all objects even if errors occur
-v verbose, show objects processed
If a symbolic link is specified, the target object is affected. File modes directly associated with symbolic links themselves are typically never used.

To view the file mode, the ls or stat commands may be used:

$ ls -l findPhoneNumbers.sh
-rwxr-xr--  1 dgerman  staff  823 Dec 16 15:03 findPhoneNumbers.sh
$ stat -c %a findPhoneNumbers.sh
754
The r, w, and x specify the read, write, and execute access, respectively. The first character of the ls display denotes the object type; a hyphen represents a plain file. This script can be read, written to, and executed by the owner, read and executed by other members of the staff group and can also be read by others.

Octal modes[edit]
See also: Octal notation of file system permissions
The chmod numerical format accepts up to four octal digits. The three rightmost digits refer to permissions for the file owner, the group, and other users. The optional leading digit (when 4 digits are given) specifies the special setuid, setgid, and sticky flags.

Numerical permissions

#	Permission	rwx
7	read, write and execute	rwx
6	read and write	rw-
5	read and execute	r-x
4	read only	r--
3	write and execute	-wx
2	write only	-w-
1	execute only	--x
0	none	---
Numeric example[edit]
In order to permit all users who are members of the programmers group to update a file

$ ls -l sharedFile
-rw-r--r--  1 jsmith programmers 57 Jul  3 10:13  sharedFile
$ chmod 664 sharedFile
$ ls -l sharedFile
-rw-rw-r--  1 jsmith programmers 57 Jul  3 10:13  sharedFile
Since the setuid, setgid and sticky bits are not specified, this is equivalent to:

$ chmod 0664 sharedFile
Symbolic modes[edit]
See also: Symbolic notation of file system permissions
The chmod command also accepts a finer-grained symbolic notation,[4] which allows modifying specific modes while leaving other modes untouched. The symbolic mode is composed of three components, which are combined to form a single string of text:

$ chmod [references][operator][modes] file ...
The references (or classes) are used to distinguish the users to whom the permissions apply. If no references are specified it defaults to “all” but modifies only the permissions allowed by the umask. The references are represented by one or more of the following letters:

Reference	Class	Description
u	owner	file's owner
g	group	users who are members of the file's group
o	others	users who are neither the file's owner nor members of the file's group
a	all	all three of the above, same as ugo
The chmod program uses an operator to specify how the modes of a file should be adjusted. The following operators are accepted:

Operator	Description
+	adds the specified modes to the specified classes
-	removes the specified modes from the specified classes
=	the modes specified are to be made the exact modes for the specified classes
The modes indicate which permissions are to be granted or removed from the specified classes. There are three basic modes which correspond to the basic permissions:

Mode	Name	Description
r	read	read a file or list a directory's contents
w	write	write to a file or directory
x	execute	execute a file or recurse a directory tree
X	special execute	which is not a permission in itself but rather can be used instead of x. It applies execute permissions to directories regardless of their current permissions and applies execute permissions to a file which already has at least one execute permission bit already set (either owner, group or other). It is only really useful when used with '+' and usually in combination with the -R option for giving group or other access to a big directory tree without setting execute permission on normal files (such as text files), which would normally happen if you just used "chmod -R a+rx .", whereas with 'X' you can do "chmod -R a+rX ." instead
s	setuid/gid	details in Special modes section
t	sticky	details in Special modes section
Multiple changes can be specified by separating multiple symbolic modes with commas (without spaces).

Symbolic examples[edit]
Add write permission (w) to the group's(g) access modes of a directory,
allowing users in the same group to add files:

$ ls -ld shared_dir # show access modes before chmod
drwxr-xr-x   2 teamleader  usguys 96 Apr 8 12:53 shared_dir
$ chmod  g+w shared_dir
$ ls -ld shared_dir  # show access modes after chmod
drwxrwxr-x   2 teamleader  usguys 96 Apr 8 12:53 shared_dir
Remove write permissions (w) for all classes (a),
preventing anyone from writing to the file:

$ ls -l ourBestReferenceFile
-rw-rw-r--   2 teamleader  usguys 96 Apr 8 12:53 ourBestReferenceFile
$ chmod a-w ourBestReferenceFile
$ ls -l ourBestReferenceFile
-r--r--r--   2 teamleader  usguys 96 Apr 8 12:53 ourBestReferenceFile
Set the permissions for the owner and the group (ug) to read and execute (rx) only (no write permission) on referenceLib,
preventing anyone to add files.

$ ls -ld referenceLib
drwxr-----   2 teamleader  usguys 96 Apr 8 12:53 referenceLib
$ chmod ug=rx referenceLib
$ ls -ld referenceLib
dr-xr-x---   2 teamleader  usguys 96 Apr 8 12:53 referenceLib
Special modes[edit]
See also: File system permissions
The chmod command is also capable of changing the additional permissions or special modes of a file or directory. The symbolic modes use s to represent the setuid and setgid modes, and t to represent the sticky mode. The modes are only applied to the appropriate classes, regardless of whether or not other classes are specified.

Most operating systems support the specification of special modes using octal modes, but some do not. On these systems, only the symbolic modes can be used.

Command line examples[edit]
Command	Explanation
chmod a+r publicComments.txt	adds read permission for all classes (i.e. owner, group and others)
chmod +r publicComments.txt	adds read permission for all classes depending on umask
chmod a-x publicComments.txt	removes execute permission for all classes
chmod a+rx viewer.sh	adds read and execute permissions for all classes
chmod u=rw,g=r,o= internalPlan.txt	sets read and write permission for owner, sets read for group, and denies access for others
chmod -R u+w,go-w docs	adds write permission to the directory docs and all its contents (i.e. Recursively) for owner, and removes write permission for group and others
chmod ug=rw groupAgreements.txt	sets read and write permissions for owner and group
chmod 664 global.txt	sets read and write permissions for owner and group, and provides read to others.
chmod 0744 myCV.txt	sets read, write, and execute permissions for owner, and sets read permission for group and others (the 0 specifies no special modes)
chmod 1755 findReslts.sh	sets sticky bit, sets read, write, and execute permissions for owner, and sets read and execute permissions for group and others (this suggests that the script be retained in memory)
chmod 4755 setCtrls.sh	sets UID, sets read, write, and execute permissions for owner, and sets read and execute permissions for group and others
chmod 2755 setCtrls.sh	sets GID, sets read, write, and execute permissions for owner, and sets read and execute permissions for group and others
chmod -R u+rwX,g-rwx,o-rx personalStuff	Recursively (i.e. on all files and directories in personalStuff) adds read, write, and special execution permissions for owner, removes read, write, and execution permissions for group, and removes read and execution permissions for others
chmod -R a-x+X publicDocs	Recursively (i.e. on all files and directories in publicDocs) removes execute permission for all classes and adds special execution permission for all classes
System call[edit]
The POSIX standard defines the following function prototype:[5]

int chmod(const char *path, mode_t mode);
The mode parameter is a bitfield composed of various flags:

Flag	Octal value	Purpose
S_ISUID	04000	Set user ID on execution
S_ISGID	02000	Set group ID on execution
S_ISVTX	01000	Sticky bit
S_IRUSR, S_IREAD	00400	Read by owner
S_IWUSR, S_IWRITE	00200	Write by owner
S_IXUSR, S_IEXEC	00100	Execute/search by owner
S_IRGRP	00040	Read by group
S_IWGRP	00020	Write by group
S_IXGRP	00010	Execute/search by group
S_IROTH	00004	Read by others
S_IWOTH	00002	Write by others
S_IXOTH	00001	Execute/search by others
See also[edit]
File system permissions
Modes (Unix)
chown, the command used to change the owner of a file or directory on Unix-like systems
chgrp, the command used to change the group of a file or directory on Unix-like systems
cacls, a command used on Windows NT and its derivatives to modify the access control lists associated with a file or directory
attrib
umask, restricts mode (permissions) at file or directory creation on Unix-like systems
User identifier
Group identifier
List of Unix programs
References[edit]
Jump up ^ Tutorial for chmod
Jump up ^ "AIX 5.3 System managment". IBM knowledge Center. IBM. Retrieved 30 August 2015.
Jump up ^ chmod
Jump up ^ "AIX 5.5 Commands Reference". IBM Knowledge Center. IBM. Retrieved 30 August 2015.
Jump up ^ "chmod function". The Open Group Base Specifications Issue 7, 2013 Edition. The Open Group. Retrieved 30 August 2015.
External links[edit]
chmod(1): change file modes – FreeBSD General Commands Manual
chmod — manual page from GNU coreutils.
GNU "Setting Permissions" manual
CHMOD-Win 3.0 — Freeware Windows' ACL  CHMOD converter.
Beginners tutorial with on-line "live" example
Online Chmod Calculator

Group identifier
From Wikipedia, the free encyclopedia
  (Redirected from Group identifier (Unix))
"Egid" redirects here. For the Egyptian General Intelligence Directorate, see Egyptian General Intelligence Directorate.

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2014) (Learn how and when to remove this template message)
In Unix-like systems, multiple users can be categorized into groups. POSIX and conventional Unix file system permissions are organized into three classes, user, group, and others. The use of groups allows additional abilities to be delegated in an organized fashion, such as access to disks, printers, and other peripherals. This method, among others, also enables the superuser to delegate some administrative tasks to normal users, similar to the Administrators group on Microsoft Windows NT and its derivatives.

A group identifier, often abbreviated to GID, is a numeric value used to represent a specific group. The range of values for a GID varies amongst different systems; at the very least, a GID can be between 0 and 32,767, with one restriction: the login group for the superuser must have GID 0. This numeric value is used to refer to groups in the /etc/passwd and /etc/group files or their equivalents. Shadow password files and Network Information Service also refer to numeric GIDs. The group identifier is a necessary component of Unix file systems and processes.

Contents  [hide] 
1	Supplementary groups
2	Effective vs. real
3	Conventions
3.1	Type
3.2	Reserved ranges
3.3	Special values
4	Personal groups
5	See also
6	References
Supplementary groups[edit]
In Unix systems, every user must be a member of at least one group, the primary group which is identified by the numeric GID of the user's entry in the group database, which can be viewed with the command getent passwd (usually stored in /etc/passwd or LDAP). This group is referred to as the primary group ID. A user may be listed as member of additional groups in the relevant entries in the group database, which can be viewed with getent group (usually stored in /etc/group or LDAP); the IDs of these groups are referred to as supplementary group IDs.

Effective vs. real[edit]
Unix processes have an effective (EUID, EGID), a real (UID, GID) and a saved (SUID, SGID) ID. Normally these are identical, but in setgid processes they are different.

Conventions[edit]
Type[edit]
Originally, a signed 16-bit integer was used. Since the sign was not necessary – negative numbers do not make valid group IDs – an unsigned integer is now used instead, allowing group IDs between 0 and 65,535. Modern operating systems usually use unsigned 32-bit integers, which allow for group IDs between 0 and 4,294,967,295.

Reserved ranges[edit]
Many Linux systems reserve the GID number range 0 to 99 for statically allocated groups, and either 100?499 or 100?999 for groups dynamically allocated by the system in post-installation scripts. These ranges are often specified in /etc/login.defs, for useradd, groupadd and similar tools.

On FreeBSD, porters who need a GID for their package can pick a free one from the range 50 to 999 and then register this static allocation in ports/GIDs.[1]

Special values[edit]
0: The superuser normally has a GID of zero (0).
?1: The value (gid_t) -1 is reserved by POSIX to identify an omitted argument.[2]
65,534: The Linux kernel defaults to 216?2 = 65,534 (which many Linux distributions map to the group name "nogroup") when a 32-bit GID does not fit into the return value of a 16-bit system call.[3] The value is also returned by idmapd if a group name in an incoming NFSv4 packet does not match any known group on the system.
Personal groups[edit]
Many system administrators allocate for each user also a personal primary group that has the same name as the user's login name, and often also has the same numeric GID as the user's UID. Such personal groups have no other members and make collaboration with other users in shared directories easier, by allowing users to habitually work with umask 0002. This way, newly created files can have by default write permissions enabled for group members, because this will normally only enable write access for members of the personal group, that is only for the file's owner. However, if a file is created in a shared directory that belongs to another group and has the setgid bit set, then the created file will automatically become writable to members of that directory's group as well.

On many Linux systems, the USERGROUPS_ENAB variable in /etc/login.defs controls whether commands like useradd or userdel automatically add or delete an associated personal group.

See also[edit]
setuid
User identifier
Process identifier
Inode
FAT access rights

POSIX
From Wikipedia, the free encyclopedia
Not to be confused with Unix, Unix-like, or Linux.
The Portable Operating System Interface (POSIX)[1] is a family of standards specified by the IEEE Computer Society for maintaining compatibility between operating systems. POSIX defines the application programming interface (API), along with command line shells and utility interfaces, for software compatibility with variants of Unix and other operating systems.[2][3]

Contents  [hide] 
1	Name
2	Overview
3	Versions
3.1	Parts before 1997
3.1.1	POSIX.1
3.1.2	POSIX.1b
3.1.3	POSIX.1c
3.1.4	POSIX.2
3.2	Versions after 1997
3.2.1	POSIX.1-2001
3.2.2	POSIX.1-2004 (with two TCs)
3.2.3	POSIX.1-2008 (with one TC)
4	Controversies
4.1	512- vs 1024-byte blocks
5	POSIX-oriented operating systems
5.1	POSIX-certified
5.2	Mostly POSIX-compliant
5.2.1	POSIX for Windows
5.2.2	POSIX for OS/2
5.2.3	POSIX for DOS
5.2.4	Compliant via compatibility feature
6	See also
7	References
8	External links
Name[edit]
Originally, the name "POSIX" referred to IEEE Std 1003.1-1988, released in 1988. The family of POSIX standards is formally designated as IEEE 1003 and the international standard name is ISO/IEC 9945.

The standards emerged from a project that began circa 1985. Richard Stallman suggested the name POSIX to the IEEE instead of former IEEE-IX. The committee found it more easily pronounceable and memorable, and thus adopted it.[2][4][better source needed]

Overview[edit]
Unix was selected as the basis for a standard system interface partly because it was "manufacturer-neutral." However, several major versions of Unix existed so there was a need to develop a common denominator system. The POSIX specifications for Unix-like operating systems originally consisted of a single document for the core programming interface, but eventually grew to 19 separate documents (POSIX.1, POSIX.2, etc.).[5] The standardized user command line and scripting interface were based on the UNIX System V shell.[6] Many user-level programs, services, and utilities including awk, echo, ed were also standardized, along with required program-level services including basic I/O (file, terminal, and network) services. POSIX also defines a standard threading library API which is supported by most modern operating systems. Nowadays, most of POSIX parts are combined into a single standard, IEEE Std 1003.1-2008, also known as POSIX.1-2008.

As of 2014, POSIX documentation is divided in two parts:

POSIX.1, 2013 Edition: POSIX Base Definitions, System Interfaces, and Commands and Utilities (which include POSIX.1, extensions for POSIX.1, Real-time Services, Threads Interface, Real-time Extensions, Security Interface, Network File Access and Network Process-to-Process Communications, User Portability Extensions, Corrections and Extensions, Protection and Control Utilities and Batch System Utilities. This is POSIX 1003.1-2008 with Technical Corrigendum 1.)
POSIX Conformance Testing: A test suite for POSIX accompanies the standard: VSX-PCTS or the VSX POSIX Conformance Test Suite.[7]
The development of the POSIX standard takes place in the Austin Group, a joint working group linking the IEEE, The Open Group and the ISO/IEC JTC 1 organizations.

Versions[edit]
Parts before 1997[edit]
Before 1997, POSIX comprised several standards:

POSIX.1[edit]
POSIX.1, Core Services (incorporates Standard ANSI C) (IEEE Std 1003.1-1988)
Process Creation and Control
Signals
Floating Point Exceptions
Segmentation / Memory Violations
Illegal Instructions
Bus Errors
Timers
File and Directory Operations
Pipes
C Library (Standard C)
I/O Port Interface and Control
Process Triggers
POSIX.1b[edit]
POSIX.1b, Real-time extensions (IEEE Std 1003.1b-1993, later appearing as librt - the Realtime Extensions library)[8])
Priority Scheduling
Real-Time Signals
Clocks and Timers
Semaphores
Message Passing
Shared Memory
Asynch and Synch I/O
Memory Locking Interface
POSIX.1c[edit]
POSIX.1c, Threads extensions (IEEE Std 1003.1c-1995)
Thread Creation, Control, and Cleanup
Thread Scheduling
Thread Synchronization
Signal Handling
POSIX.2[edit]
POSIX.2, Shell and Utilities (IEEE Std 1003.2-1992)
Command Interpreter
Utility Programs
Versions after 1997[edit]
After 1997, the Austin Group developed the POSIX revisions. The specifications are known under the name Single UNIX Specification, before they become a POSIX standard when formally approved by the ISO.

POSIX.1-2001[edit]
POSIX.1-2001 or IEEE Std 1003.1-2001 equates to the Single UNIX Specification version 3[9]

This standard consisted of:

the Base Definitions, Issue 6,
the System Interfaces and Headers, Issue 6,
the Commands and Utilities, Issue 6.
POSIX.1-2004 (with two TCs)[edit]
IEEE Std 1003.1-2004 involved a minor update of POSIX.1-2001. It incorporated two minor updates or errata referred to as Technical Corrigenda.[10] Its contents are available on the web.[11]

POSIX.1-2008 (with one TC)[edit]
As of 2016, Base Specifications, Issue 7 or IEEE Std 1003.1, 2013 edition represents the current version.[12][13] A free online copy is available.

This standard consists of:

the Base Definitions, Issue 7,
the System Interfaces and Headers, Issue 7,
the Commands and Utilities, Issue 7,
the Rationale volume.
Controversies[edit]
512- vs 1024-byte blocks[edit]
POSIX mandates 512-byte block sizes for the df and du utilities, reflecting the default size of blocks on disks. When Richard Stallman and the GNU team were implementing POSIX for the GNU operating system, they objected to this on the grounds that most people think in terms of 1024 byte (or 1 KiB) blocks. The environment variable POSIXLY_CORRECT was introduced to allow the user to force the standards-compliant behaviour.[14] The variable name POSIX_ME_HARDER was also discussed.[15] The variable POSIXLY_CORRECT is now also used for a number of other behaviour quirks, where "POSIX and common sense disagree".[citation needed]

POSIX-oriented operating systems[edit]
Depending upon the degree of compliance with the standards, one can classify operating systems as fully or partly POSIX compatible. Certified products can be found at the IEEE's website.[16]

POSIX-certified[edit]
Some versions of the following operating systems have been certified to conform to one or more of the various POSIX standards. This means that they passed the automated conformance tests.[17]

AIX[18]
HP-UX[19]
IRIX[20]
OS X (since 10.5 Leopard)[21][22][discuss]
Solaris[23]
Tru64[19]
UnixWare[24]
QNX Neutrino[25]
Inspur K-UX[26]
Integrity[27]
Mostly POSIX-compliant[edit]
[icon]	This section requires expansion. (January 2007)
The following, while not officially certified as POSIX compatible, comply in large part:

BeOS (and subsequently Haiku)
Contiki
Darwin (core of OS X and iOS)
FreeBSD[28]
illumos
Linux (most distributions — see Linux Standard Base)
MINIX (now MINIX3)
NetBSD
Nucleus RTOS
NuttX
OpenBSD
OpenSolaris[29]
PikeOS RTOS for embedded systems with optional PSE51 and PSE52 partitions; see partition (mainframe)
RTEMS – POSIX API support designed to IEEE Std. 1003.13-2003 PSE52
Sanos
SkyOS
Syllable
VSTa
VxWorks (VxWorks is often used as a shell around non-posix Kernels i.e. TiMOS/SROS )
Android (Available through Android NDK)[citation needed]
MPE/iX[30]
POSIX for Windows[edit]
Cygwin provides a largely POSIX-compliant development and run-time environment for Microsoft Windows.
MinGW, a fork of Cygwin, provides a less POSIX-compliant development environment and supports compatible C-programmed applications via Msvcrt, Microsoft's old Visual C runtime library.
Microsoft POSIX subsystem, an optional Windows subsystem included in Windows NT-based operating systems up to Windows 2000. POSIX-1 as it stood in 1990 revision, without threads or sockets.
Interix, originally OpenNT by Softway Systems, Inc., is an upgrade and replacement for Microsoft POSIX subsystem that was purchased by Microsoft in 1999. It was initially marketed as a stand-alone add-on product and then later included it as a component in Windows Services for UNIX (SFU) and finally incorporated it as a component in Windows Server 2003 R2 and later Windows OS releases under the name "Subsystem for UNIX-based Applications" (SUA); later made deprecated in 2012 (Windows 8)[31] and dropped in 2013 (2012 R2, 8.1). It enables full POSIX compliance for certain Microsoft Windows products[citation needed].
UWIN from AT&T Research implements a POSIX layer on top of the Win32 APIs.
MKS Toolkit, originally created for MS-DOS, is a software package produced and maintained by MKS Inc. that provides a Unix-like environment for scripting, connectivity and porting Unix and Linux software to both 32- and 64-bit Microsoft Windows systems. A subset of it was included in the first release of Windows Services for UNIX (SFU) in 1998.[32]
Windows C Runtime Library and Windows Sockets API implement commonly used POSIX API functions for file, time, environment, and socket access,[33] although the support remains largely incomplete and not fully interoperable with POSIX-compliant implementations.[34][35][discuss]
POSIX for OS/2[edit]
Mostly POSIX compliant environments for OS/2:

emx+gcc – largely POSIX compliant
POSIX for DOS[edit]
Partially POSIX compliant environments for DOS include:

emx+gcc – largely POSIX compliant
DJGPP – partially POSIX compliant
DR-DOS multitasking core via EMM386 /MULTI - a POSIX threads frontend API extension is available
Compliant via compatibility feature[edit]
The following are not officially certified as POSIX compatible, but they conform in large part to the standards by implementing POSIX support via some sort of compatibility feature, usually translation libraries, or a layer atop the kernel. Without these features, they are usually noncompliant.

eCos – POSIX is part of standard distribution, and used by many applications. 'external links' section below has more information.
MorphOS (through the built-in ixemul library)
OpenVMS (through optional POSIX package)
Plan 9 from Bell Labs APE - ANSI/POSIX Environment[36]
RIOT (through optional POSIX module)
Symbian OS with PIPS (PIPS Is POSIX on Symbian)
Windows NT kernel when using Microsoft SFU 3.5 or SUA
Windows 2000 Server or Professional with Service Pack 3 or later. To be POSIX compliant, one must activate optional features of Windows NT and Windows 2000 Server.[37]
Windows XP Professional with Service Pack 1 or later
Windows Server 2003
Windows Server 2008 and Ultimate and Enterprise versions of Windows Vista
Windows Server 2008 R2 and Ultimate and Enterprise versions of Windows 7
albeit deprecated, still available for Windows Server 2012 and Enterprise version of Windows 8
UNIX System Services that runs on z/OS (certified as compliant)
See also[edit]
POSIX signal
POSIX Threads
POSIX sockets are basically Berkeley sockets[citation needed]
TRON project – alternative OS standards to POSIX
Common User Access – User interface standard
Interix – a full-featured POSIX and Unix environment subsystem for Microsoft's Windows NT-based operating systems
C POSIX library
Real-time operating system
Portable character set

umask
From Wikipedia, the free encyclopedia
In computing, umask is a command that determines the settings of a mask that controls how file permissions are set for newly created files. It also may refer to a function that sets the mask, or it may refer to the mask itself, which is formally known as the file mode creation mask. The mask is a grouping of bits, each of which restricts how its corresponding permission is set for newly created files. The bits in the mask may be changed by invoking the umask command.

In UNIX, each file has a set of attributes which control who can read, write or execute it. When a program creates a file, UNIX requires that the file permissions be set to an initial setting. The mask restricts permission settings. If the mask has a bit set to "1", it means the corresponding initial file permission will be disabled. A bit set to "0" in the mask means that the corresponding permission will be determined by the program and the system. In other words, the mask acts as a last-stage filter that strips away permissions as a file is created; each bit that is set to a "1" strips away its corresponding permission. Permissions may be changed later by users and programs using chmod.

Each program (technically called a process) has its own mask. Each process is able to change the settings of its own mask using a function call. When the process is a shell, the mask is set with the umask command. When a shell or process launches a new process, the child process inherits the mask from its parent process. Generally, the mask only affects file permissions during the creation of new files and has no effect when file permissions are changed in existing files, however, in some specific cases it can help determine permissions when file permissions are changed in existing files using the chmod command.

The mask is stored as a group of bits. It may be represented as binary, octal, or symbolic notation. The umask command allows the mask to be set using as octal (e.g., 0754) or symbolic (e.g., u=rwx,g=rx,o=r) notation. (see octal and symbolic notation).

The umask command is used with Unix-like operating systems and the umask function is defined in the POSIX.1 specification.

Contents  [hide] 
1	History
2	Shell command
2.1	Displaying the current mask
2.2	Setting the mask using octal notation
2.2.1	Octal codes
2.3	Setting the mask using symbolic notation
2.4	Command line examples
3	Function call
4	Mask effect
4.1	Truth table
4.2	How the mask is applied
4.3	Exceptions
5	Processes
6	Mount option
7	See also
8	References
History[edit]
The mask, the umask command, and the umask function were not part of the original implementation of UNIX. The operating system evolved in a relatively small computer center environment where security was not an issue. It eventually grew to serve hundreds of users from different organizations. At first, developers made creation modes for key files more restrictive, especially for cases of actual security breaches, but this was not a general solution. The mask and the umask command were introduced around 1978 between the sixth edition and the eighth edition of the operating system, so it could allow sites, groups, and individuals to choose their own defaults.[1] The mask has since been implemented in most, if not all, of the contemporary implementations of UNIX-like operating systems.

Shell command[edit]
In a shell, the mask is set by using the umask command. The syntax of the command is:[2]

umask [-S ] [maskExpression]
(the items within the [brackets] are optional)

Displaying the current mask[edit]
If the umask command is invoked without any arguments, it will display the current mask. The output will be in either octal or symbolic notation depending on the operating system,[3] Invoking umask with the -S argument (i.e., umask -S) will force it to display using symbolic notation. For example:

$ umask         # display current value (as octal)
0022
$ umask -S      # display current value symbolically
u=rwx,g=rx,o=rx
Setting the mask using octal notation[edit]
If the umask command is invoked with an octal argument, it will directly set the bits of the mask to that argument:

$ umask 0077        # set the mask to 0077
$ umask             # display the mask (in octal)
0077
$ umask -S          # display the mask symbolically
u=rwx,g=,o=
If fewer than 4 digits are entered, leading zeros are assumed. An error will result if the argument is not a valid octal number or if it has more than 4 digits.[4] The three rightmost octal digits address the 'owner', 'group' and 'other' user classes, respectively. If a fourth digit is present, the leftmost (high-order) digit addresses three additional attributes, the setuid bit, the setgid bit and the sticky bit.

Octal codes[edit]
Octal digit in
umask command	Permissions the mask will
prohibit from being set during file creation
0	any permission may be set
1	setting of execute permission is prohibited
2	setting of write permission is prohibited
3	setting of write and execute permission is prohibited
4	setting of read permission is prohibited
5	setting of read and execute permission is prohibited
6	setting of read and write permission is prohibited
7	all permissions are prohibited from being set
Setting the mask using symbolic notation[edit]
When umask is invoked using symbolic notation, it will modify or set the flags as specified by the maskExpression with the syntax :

[user-class-letters] operator permission-symbols
Multiple maskExpressions are separated by commas.

A space terminates the maskExpression (s).

The permissions are applied to different user classes:
Letter	Class	Description
u	user	the owner
g	group	users who are members of the file's group
o	others	users who are not the owner of the file or members of the group
a	all	all three of the above, the same as ugo. (The default if no user-class-letters are specified in the maskExpression.)
The operator specifies how the permission modes of the mask should be adjusted.
Operator	Effect on the mask
+	permissions specified are enabled, permissions that are not specified are unchanged.
-	permissions specified are prohibited from being enabled, permissions that are not specified are unchanged.
=	permissions specified are enabled, permissions that are not specified are prohibited from being enabled.
The permission-symbols indicate which file permission settings are to be allowed or prohibited by the mask
Symbol	Name	Description
r	read	read a file or list a directory's contents
w	write	write to a file or directory
x	execute	execute a file or recurse a directory tree
X	special execute	See File permissions.
s	setuid/gid	See File permissions.
t	sticky	See File permissions.
For example:

umask u-w
Prohibit write permission to be enabled for the user. The rest of the flags in the mask are unchanged.

Example of multiple changes:

umask u-w,g=r,o+r
This would set the mask so that it would:

prohibit the write permission from being set for the user, while leaving the rest of the flags unchanged;
allow the read permission to be enabled for the group, while prohibiting write and execute permission for the group;
allow the read permission to be enabled for others, while leaving the rest of the other flags unchanged.
Command line examples[edit]
Here are more examples of using the umask command to change the mask.

umask command issued	How the mask will affect permissions of subsequently created files/directories
umask a+r	allows read permission to be enabled for all user classes; the rest of the mask bits are unchanged
umask a-x	prohibits enabling execute permission for all user classes; the rest of the mask bits are unchanged
umask a+rw	allows read or write permission to be enabled for all user classes; the rest of the mask bits are unchanged
umask +rwx	allows read, write or execute permission to be enabled for all user classes; (Note: On some UNIX platforms, this will restore the mask to a default.)
umask u=rw,go=	allow read and write permission to be enabled for the owner, while prohibiting execute permission from being enabled for the owner; prohibit enabling any permissions for the group and others
umask u+w,go-w	allow write permission to be enabled for the owner; prohibit write permission from being enabled for the group and others;
umask -S	display the current umask in symbolic notation
umask 777	disallow read, write, and execute permission for all (probably not useful because even owner cannot read files created with this mask!)
umask 000	allow read, write, and execute permission for all (potential security risk)
umask 077	allow read, write, and execute permission for the file's owner, but prohibit read, write, and execute permission for everyone else
umask 113	allow read or write permission to be enabled for the owner and the group, but not execute permission; allow read permission to be enabled for others, but not write or execute permission
umask 0755	equivalent to u-rwx (4+2+1),go=w (4+1 & 4+1). (The 0 specifies that special modes[clarify] may be enabled if allowed by the OS.)
Example showing effect of umask:

$ umask -S       # Show the (frequently initial) setting 
u=rwx,g=rx,o=rx
$ gcc hello.c      # compile and create executable file a.out
$ ls -l a.out 
-rwxr-xr-x  1 me  developer  6010 Jul 10 17:10 a.out 
$ # the umask prohibited Write permission for Group and Others
$ ls  > listOfMyFiles    # output file created by redirection does not attempt to set eXecute
$ ls -l listOfMyFiles
-rw-r--r--  1 me  developer  6010 Jul 10 17:14 listOfMyFiles 
$ # the umask prohibited Write permission for Group and Others
$ ############################################################
$ umask u-w  # remove user write permission from umask
$ umask -S
u=rx,g=rx,o=rx
$ ls > protectedListOfFiles
$ ls -l protectedListOfFiles
-r--r--r--  1 me  developer  6010 Jul 10 17:15 protectedListOfFiles 
rm listOfMyFiles
override r--r--r-- me/developer for protectedListOfFiles? 
$ # warning that protectedListOfFiles is not writable, answering Y will remove the file
$ #####################################################################################
$ umask g-r,o-r  # removed group read and other read from mask
$ umask -S
u=rx,g=x,o=x
$ ls > secretListOfFiles
$ ls -l secretListOfFiles
-r--------  1 me  developer  6010 Jul 10 17:16 secretListOfFiles
Function call[edit]
The mask may be set using a function call. The GNU functions are declared in sys/stat.h. The function is:[5]

mode_t umask (mode_t mask)
The umask function will set the mask of the current process to mask, and return the previous value of the mask.

Here is an example from GNU.org which shows how to read the mask without changing it permanently:

mode_t
read_umask (void) {
    mode_t mask = umask (0);
    umask (mask);
    return mask;
}
However, on GNU/Hurd systems it is better to use the getumask function if reading the mask value is the only requirement, because it is reentrant.

mode_t getumask (void)
Returns the current value of the mask for the current process. (Note: The getumask function is a GNU extension and is only available on GNU/Hurd systems.

Mask effect[edit]
The mask is applied whenever a file is created. If the mask has a bit set to "1", that means the corresponding file permission will always be disabled when files are subsequently created. A bit set to "0" in the mask means that the corresponding permission will be determined by the requesting process and the OS when files are subsequently created. In other words, the mask acts as a last-stage filter that strips away permissions as a file is created; each bit that is set to a "1" strips away that corresponding permission for the file.

Truth table[edit]
Here is the truth table for the masking logic. Each bit in the requesting process' file permission mode is operated on by the mask using this logic to yield the permission mode that is applied to the file as it is created. (p is a bit in the requested file permission mode of a process that is creating a file; q is a bit in the mask; r is the resulting bit in the created file's permission mode)

p	q	r
T	T	F
T	F	T
F	T	F
F	F	F
How the mask is applied[edit]
This shows how octal in the umask command appears in the mask and eventually affects a program's request for creating a file with, e.g., full (r, w, x) permissions.
Octal digit in
 umask command 	 Binary in 
the mask	 Negation 
of mask	Logical AND
 with "rwx" request[6] 
0	000	111	rwx
1	001	110	rw-
2	010	101	r-x
3	011	100	r--
4	100	011	-wx
5	101	010	-w-
6	110	001	--x
7	111	000	---
Programmatically, the mask is applied by the OS by first negating (complementing) the mask, and then performing a logical AND with the requested file mode. In the [probably] first UNIX manual to describe its function,[1] the manual says,

"the actual mode... of the newly-created file is the logical and of the given mode and the complement of the argument. Only the low-order 9 bits of the mask (the protection bits) participate. In other words, the mask shows [indicates] the bits to be turned off when files are created."

—?UNIX Eighth Edition Manual, Bell Labs UNIX (manual), AT&T Laboratories
In boolean logic the application of the mask can be represented as

C: (P&(~Q))

This says that the file's permission mode (C) is a result of a logical AND operation between the negation of the mask (Q), and the process' requested permission mode setting (P).

Exceptions[edit]
Note: Many operating systems do not allow a file to be created with execute permissions. In these environments, newly created files will always have execute permission disabled for all users.

The mask is generally only applied to functions that create a new file, however, there are exceptions. For example, when using UNIX and GNU versions of chmod to set the permissions of a file, and symbolic notation is used, and no user is specified, then the mask is applied to the requested permissions before they are applied to the file. For example:

$ umask 0000
$ chmod +rwx filename
$ ls -l filename
-rwxrwxrwx filename
$ umask 0022
$ chmod +rwx filename
$ ls -l filename
-rwxr-xr-x filename
Processes[edit]
Each process has its own mask, which is applied whenever the process creates a new file. When a shell, or any other process, spawns a new process, the child process inherits the mask from its parent process.[7] When the process is a shell, the mask is changed by the umask command. As with other processes, any process launched from the shell inherits that shell's mask.

Mount option[edit]
In the Linux kernel, the fat, hfs, hpfs, ntfs, and udf file system drivers support a umask mount option, which controls how the disk information is mapped to permissions. This is not the same as the per-process umask described above, although the permissions are calculated in a similar way. Some of these file system drivers also support separate umasks for files and directories, using mount options such as fmask.

User identifier
From Wikipedia, the free encyclopedia
  (Redirected from User identifier (Unix))
Unix-like operating systems identify a user within the kernel by a value called a user identifier, often abbreviated to user ID or UID. The UID, along with the group identifier (GID) and other access control criteria, is used to determine which system resources a user can access. The password file maps textual user names to UIDs, but in the kernel, only UIDs are used. UIDs are stored in the inodes of the Unix file system, running processes, tar archives, and the now-obsolete Network Information Service. In POSIX-compliant environments, the command-line command id gives the current user's UID, as well as more information such as the user name, primary user group and group identifier (GID).

Contents  [hide] 
1	Process attributes
1.1	Effective user ID
1.1.1	File system user ID
1.2	Saved user ID
1.3	Real user ID
2	Conventions
2.1	Type
2.2	Reserved ranges
2.3	Special values
3	See also
4	References
Process attributes[edit]
The POSIX standard introduced three different UID fields into the process descriptor table, to allow privileged processes to take on different roles dynamically:

Effective user ID[edit]
The effective UID (euid) of a process is used for most access checks. It is also used as the owner for files created by that process. The effective GID (egid) of a process also affects access control and may also affect file creation, depending on the semantics of the specific kernel implementation in use and possibly the mount options used. According to BSD Unix semantics, the group ownership given a newly created file is unconditionally inherited from the group ownership of the directory in which it is created. According to AT&T UNIX System V semantics (also adopted by Linux variants) newly created files will normally be given the group ownership of the egid of the process that creates them. Most filesystems implement a method to select whether BSD or AT&T semantics should be used regarding group ownership of newly created files, BSD semantics is selected for specific directories in case that the S_ISGID (s-gid) permission is set.[1]

File system user ID[edit]
Linux also has a file system user ID (fsuid) which is used explicitly for access control to the file system. It matches the euid unless explicitly set otherwise. It may be root's user ID only if ruid, suid, or euid is root. Whenever the euid is changed, the change is propagated to the fsuid.

The intent of fsuid is to permit programs (e.g., the NFS server) to limit themselves to the file system rights of some given uid without giving that uid permission to send them signals. Since kernel 2.0, the existence of fsuid is no longer necessary because Linux adheres to SUSv3 rules for sending signals, but fsuid remains for compatibility reasons.[2]

Saved user ID[edit]
The saved user ID (suid) is used when a program running with elevated privileges needs to temporarily do some unprivileged work: it changes its effective user ID from a privileged value (typically root) to some unprivileged one, and this triggers a copy of the privileged user ID to the saved user ID slot.[3] Later, it can set its effective user ID back to the saved user ID (an unprivileged process can only set its effective user ID to three values: its real user ID, its saved user ID, and its effective user ID—i.e., unchanged) to resume its privileges.

Real user ID[edit]
The real UID (ruid) and real GID (rgid) identify the real owner of the process and affect the permissions for sending signals. A process without superuser privilege can signal another process only if the sender’s real or effective UID matches the real or saved UID of the receiver.[2] Since child processes inherit the credentials from the parent, they can signal each other.

Conventions[edit]
Type[edit]
POSIX requires the UID to be an integer type. Most Unix-like operating systems represent the UID as an unsigned integer. The size of UID values varies amongst different systems; some UNIX OS's[which?] used 15-bit values, allowing values up to 32767[citation needed], while others such as Linux (before version 2.4) supported 16-bit UIDs, making 65536 unique IDs possible. The majority of modern Unix-like systems (e.g., Solaris-2.0 in 1990, Linux 2.4 in 2001) have switched to 32-bit UIDs, allowing 4,294,967,296 (232) unique IDs.

Reserved ranges[edit]
The Linux Standard Base Core Specification specifies that UID values in the range 0 to 99 should be statically allocated by the system, and shall not be created by applications, while UIDs from 100 to 499 should be reserved for dynamic allocation by system administrators and post install scripts.[4]

On FreeBSD, porters who need a UID for their package can pick a free one from the range 50 to 999 and then register this static allocation in ports/UIDs.[5]

Some POSIX systems allocate UIDs for new users starting from 500 (OS X, Red Hat Enterprise Linux), others start at 1000 (openSUSE, Debian[6]). On many Linux systems, these ranges are specified in /etc/login.defs, for useradd and similar tools.

Central UID allocations in enterprise networks (e.g., via LDAP and NFS servers) may limit themselves to using only UID numbers well above 1000, to avoid potential conflicts with UIDs locally allocated on client computers. NFSv4 can help avoid numeric identifier collisions, by identifying users (and groups) in protocol packets using "user@domain" names rather than integer numbers, at the expense of additional translation steps.

Special values[edit]
0: The superuser normally has a UID of zero (0).[7]
?1: The value (uid_t) -1 is reserved by POSIX to identify an omitted argument.[8]
Nobody: Historically, the user “nobody” was assigned UID -2 by several operating systems, although other values such as 215?1 = 32,767 are also in use, such as by OpenBSD.[9] For compatibility between 16-bit and 32-bit UIDs, many Linux distributions now set it to be 216?2 = 65,534; the Linux kernel defaults to returning this value when a 32-bit UID does not fit into the return value of the 16-bit system calls.[10] An alternative convention assigns the last UID of the range statically allocated for system use (0-99) to nobody: 99.
See also[edit]
setuid
Sticky bit
Group identifier
Process identifier
File system permissions
Open (system call)
Mount (Unix)
FAT access rights
Security Identifier (SID) – the Windows NT equivalent

Data remanence is the residual representation of digital data that remains even after attempts have been made to remove or erase the data. This residue may result from data being left intact by a nominal file deletion operation, by reformatting of storage media that does not remove data previously written to the media, or through physical properties of the storage media that allow previously written data to be recovered. Data remanence may make inadvertent disclosure of sensitive information possible should the storage media be released into an uncontrolled environment (e.g., thrown in the trash, or lost).

Various techniques have been developed to counter data remanence. These techniques are classified as clearing, purging/sanitizing or destruction. Specific methods include overwriting, degaussing, encryption, and media destruction.

Effective application of countermeasures can be complicated by several factors, including media that are inaccessible, media that cannot effectively be erased, advanced storage systems that maintain histories of data throughout the data's life cycle, and persistence of data in memory that is typically considered volatile.

Several standards exist for the secure removal of data and the elimination of data remanence.

Contents  [hide] 
1	Causes
2	Countermeasures
2.1	Clearing
2.2	Purging
2.3	Destruction
3	Specific methods
3.1	Overwriting
3.1.1	Feasibility of recovering overwritten data
3.2	Degaussing
3.3	Encryption
3.4	Media destruction
4	Complications
4.1	Inaccessible media areas
4.2	Advanced storage systems
4.3	Optical media
4.4	Data on solid-state drives
4.5	Data in RAM
5	Standards
6	See also
7	References
8	Further reading
Causes[edit]
Many operating systems, file managers, and other software provide a facility where a file is not immediately deleted when the user requests that action. Instead, the file is moved to a holding area, to allow the user to easily revert a mistake. Similarly, many software products automatically create backup copies of files that are being edited, to allow the user to restore the original version, or to recover from a possible crash (autosave feature).

Even when an explicit deleted file retention facility is not provided or when the user does not use it, operating systems do not actually remove the contents of a file when it is deleted unless they are aware that explicit erasure commands are required, like on a solid-state drive. (In such cases, the operating system will issue the Serial ATA TRIM command or the SCSI UNMAP command to let the drive know to no longer maintain the deleted data.) Instead, they simply remove the file's entry from the file system directory, because this requires less work and is therefore faster, and the contents of the file—the actual data—remain on the storage medium. The data will remain there until the operating system reuses the space for new data. In some systems, enough filesystem metadata are also left behind to enable easy undeletion by commonly available utility software. Even when undelete has become impossible, the data, until it has been overwritten, can be read by software that reads disk sectors directly. Computer forensics often employs such software.

Likewise, reformatting, repartitioning, or reimaging a system is unlikely to write to every area of the disk, though all will cause the disk to appear empty or, in the case of reimaging, empty except for the files present in the image, to most software.

Finally, even when the storage media is overwritten, physical properties of the media may permit recovery of the previous contents. In most cases however, this recovery is not possible by just reading from the storage device in the usual way, but requires using laboratory techniques such as disassembling the device and directly accessing/reading from its components.[citation needed]

The section on complications gives further explanations for causes of data remanence.

Countermeasures[edit]
Main article: Data erasure
There are three levels commonly recognized for eliminating remnant data:

Clearing[edit]
Clearing is the removal of sensitive data from storage devices in such a way that there is assurance that the data may not be reconstructed using normal system functions or software file/data recovery utilities.[citation needed] The data may still be recoverable, but not without special laboratory techniques.[1]

Clearing is typically an administrative protection against accidental disclosure within an organization. For example, before a hard drive is re-used within an organization, its contents may be cleared to prevent their accidental disclosure to the next user.

Purging[edit]
Purging or sanitizing is the removal of sensitive data from a system or storage device with the intent that the data can not be reconstructed by any known technique.[citation needed] Purging, proportional to the sensitivity of the data, is generally done before releasing media beyond control, such as before discarding old media, or moving media to a computer with different security requirements.

Destruction[edit]
The storage media is made unusable for conventional equipment. Effectiveness of destroying the media varies by medium and method. Depending on recording density of the media, and/or the destruction technique, this may leave data recoverable by laboratory methods. Conversely, destruction using appropriate techniques is the most secure method of preventing retrieval.

Specific methods[edit]
Overwriting[edit]
A common method used to counter data remanence is to overwrite the storage media with new data. This is often called wiping or shredding a file or disk, by analogy to common methods of destroying print media, although the mechanism bears no similarity to these. Because such a method can often be implemented in software alone, and may be able to selectively target only part of the media, it is a popular, low-cost option for some applications. Overwriting is generally an acceptable method of clearing, as long as the media is writable and not damaged.

The simplest overwrite technique writes the same data everywhere—often just a pattern of all zeros. At a minimum, this will prevent the data from being retrieved simply by reading from the media again using standard system functions.

In an attempt to counter more advanced data recovery techniques, specific overwrite patterns and multiple passes have often been prescribed. These may be generic patterns intended to eradicate any trace signatures, for example, the seven-pass pattern: 0xF6, 0x00, 0xFF, random, 0x00, 0xFF, random; sometimes erroneously[clarification needed] attributed to the US standard DOD 5220.22-M.

One challenge with an overwrite is that some areas of the disk may be inaccessible, due to media degradation or other errors. Software overwrite may also be problematic in high-security environments which require stronger controls on data commingling than can be provided by the software in use. The use of advanced storage technologies may also make file-based overwrite ineffective (see the discussion below under Complications).

There are specialized machines and software that are capable of doing overwriting. The software can sometimes be a standalone operating system specifically designed for data destruction. There are also machines specifically designed to wipe hard drives to the department of defense specifications DOD 5220.22-M as well.[citation needed]

Feasibility of recovering overwritten data[edit]
Peter Gutmann investigated data recovery from nominally overwritten media in the mid-1990s. He suggested magnetic force microscopy may be able to recover such data, and developed specific patterns, for specific drive technologies, designed to counter such.[2] These patterns have come to be known as the Gutmann method.

Daniel Feenberg, an economist at the private National Bureau of Economic Research, claims that the chances of overwritten data being recovered from a modern hard drive amount to "urban legend".[3] He also points to the "18? minute gap" Rose Mary Woods created on a tape of Richard Nixon discussing the Watergate break-in. Erased information in the gap has not been recovered, and Feenberg claims doing so would be an easy task compared to recovery of a modern high density digital signal.

As of November 2007, the United States Department of Defense considers overwriting acceptable for clearing magnetic media within the same security area/zone, but not as a sanitization method. Only degaussing or physical destruction is acceptable for the latter.[4]

On the other hand, according to the 2006 NIST Special Publication 800-88 (p. 7): "Studies have shown that most of today’s media can be effectively cleared by one overwrite" and "for ATA disk drives manufactured after 2001 (over 15 GB) the terms clearing and purging have converged."[5] An analysis by Wright et al. of recovery techniques, including magnetic force microscopy, also concludes that a single wipe is all that is required for modern drives. They point out that the long time required for multiple wipes "has created a situation where many organisations ignore the issue all together – resulting in data leaks and loss."[6]

Degaussing[edit]
Degaussing is the removal or reduction of a magnetic field of a disk or drive, using a device called a degausser that has been designed for the media being erased. Applied to magnetic media, degaussing may purge an entire media element quickly and effectively.

Degaussing often renders hard disks inoperable, as it erases low-level formatting that is only done at the factory during manufacturing. In some cases, it is possible to return the drive to a functional state by having it serviced at the manufacturer. However, some modern degaussers use such a strong magnetic pulse that the motor that spins the platters may be destroyed in the degaussing process, and servicing may not be cost-effective. Degaussed computer tape such as DLT can generally be reformatted and reused with standard consumer hardware.

In some high-security environments, one may be required to use a degausser that has been approved for the task. For example, in US government and military jurisdictions, one may be required to use a degausser from the NSA's "Evaluated Products List".[7]

Encryption[edit]
Encrypting data before it is stored on the media may mitigate concerns about data remanence. If the decryption key is strong and carefully controlled, it may effectively make any data on the media unrecoverable. Even if the key is stored on the media, it may prove easier or quicker to overwrite just the key, vs the entire disk. This process is called crypto erase in the security industry.[8]

Encryption may be done on a file-by-file basis, or on the whole disk. Cold boot attacks are one of the few possible methods for subverting a full-disk encryption method, as there is no possibility of storing the plain text key in an unencrypted section of the medium. See the section Complications: Data in RAM for further discussion.

Other side-channel attacks (such as keyloggers, acquisition of a written note containing the decryption key, or rubber hose cryptography) may offer a greater chance to success, but do not rely on weaknesses in the cryptographic method employed. As such, their relevance for this article is minor.

Media destruction[edit]

The pieces of a physically destroyed hard disk drive.
Thorough destruction of the underlying storage media is the most certain way to counter data remanence. However, the process is generally time-consuming, cumbersome, and may require extremely thorough methods, as even a small fragment of the media may contain large amounts of data.

Specific destruction techniques include:

Physically breaking the media apart (e.g., by grinding or shredding)
Chemically altering the media into a non-readable, non-reverse-constructible state (e.g., through incineration or exposure to caustic/corrosive chemicals)
Phase transition (e.g., liquefaction or vaporization of a solid disk)
For magnetic media, raising its temperature above the Curie point
For many electric/electronic volatile and non-volatile storage mediums, exposure to electromagnetic fields greatly exceeding safe operational specifications (e.g., high-voltage electric current or high-amplitude microwave radiation)[citation needed]
Complications[edit]
Inaccessible media areas[edit]
Storage media may have areas which become inaccessible by normal means. For example, magnetic disks may develop new bad sectors after data has been written, and tapes require inter-record gaps. Modern hard disks often feature reallocation of marginal sectors or tracks, automated in a way that the OS would not need to work with it. This problem is especially significant in solid state drives (SSDs) that rely on relatively large relocated bad block tables. Attempts to counter data remanence by overwriting may not be successful in such situations, as data remnants may persist in such nominally inaccessible areas.

Advanced storage systems[edit]
Data storage systems with more sophisticated features may make overwrite ineffective, especially on a per-file basis. For example, journaling file systems increase the integrity of data by recording write operations in multiple locations, and applying transaction-like semantics; on such systems, data remnants may exist in locations "outside" the nominal file storage location. Some file systems also implement copy-on-write or built-in revision control, with the intent that writing to a file never overwrites data in-place. Furthermore, technologies such as RAID and anti-fragmentation techniques may result in file data being written to multiple locations, either by design (for fault tolerance), or as data remnants.

Wear leveling can also defeat data erasure, by relocating blocks between the time when they are originally written and the time when they are overwritten. For this reason, some security protocols tailored to operating systems or other software featuring automatic wear leveling recommend conducting a free-space wipe of a given drive and then copying many small, easily identifiable "junk" files or files containing other nonsensitive data to fill as much of that drive as possible, leaving only the amount of free space necessary for satisfactory operation of system hardware and software. As storage and/or system demands grow, the "junk data" files can be deleted as necessary to free up space; even if the deletion of "junk data" files is not secure, their initial nonsensitivity reduces to near zero the consequences of recovery of data remanent from them.[citation needed]

Optical media[edit]
As optical media are not magnetic, they are not erased by conventional degaussing. Write-once optical media (CD-R, DVD-R, etc.) also cannot be purged by overwriting. Read/write optical media, such as CD-RW and DVD-RW, may be receptive to overwriting. Methods for successfully sanitizing optical discs include delaminating or abrading the metallic data layer, shredding, incinerating, destructive electrical arcing (as by exposure to microwave energy), and submersion in a polycarbonate solvent (e.g., acetone).

Data on solid-state drives[edit]
Research[9] from the Center for Magnetic Recording and Research, University of California, San Diego has uncovered problems inherent in erasing data stored on solid-state drives (SSDs). Researchers discovered three problems with file storage on SSDs:

First, built-in commands are effective, but manufacturers sometimes implement them incorrectly. Second, overwriting the entire visible address space of an SSD twice is usually, but not always, sufficient to sanitize the drive. Third, none of the existing hard drive-oriented techniques for individual file sanitization are effective on SSDs.[9](p1)

Solid-state drives, which are flash-based, differ from hard-disk drives in two ways: first, in the way data is stored; and second, in the way the algorithms are used to manage and access that data. These differences can be exploited to recover previously erased data. SSDs maintain a layer of indirection between the logical addresses used by computer systems to access data and the internal addresses that identify physical storage. This layer of indirection hides idiosyncratic media interfaces and enhances SSD performance, reliability, and lifespan (see wear leveling); but it can also produce copies of the data that are invisible to the user and that a sophisticated attacker could recover. For sanitizing entire disks, sanitize commands built into the SSD hardware have been found to be effective when implemented correctly, and software-only techniques for sanitizing entire disks have been found to work most, but not all, of the time.[9]:section 5 In testing, none of the software techniques were effective for sanitizing individual files. These included well-known algorithms such as the Gutmann method, US DoD 5220.22-M, RCMP TSSIT OPS-II, Schneier 7 Pass, and Mac OS X Secure Erase Trash.[9]:section 5

The TRIM feature in many SSD devices, if properly implemented, will eventually erase data after it is deleted, but the process can take some time, typically several minutes. Many older operating systems do not support this feature, and not all combinations of drives and operating systems work.[10]

Data in RAM[edit]
Data remanence has been observed in static random-access memory (SRAM), which is typically considered volatile (i.e., the contents degrade with loss of external power). In one study, data retention was observed even at room temperature.[11]

Data remanence has also been observed in dynamic random-access memory (DRAM). Modern DRAM chips have a built-in self-refresh module, as they not only require a power supply to retain data, but must also be periodically refreshed to prevent their data contents from fading away from the capacitors in their integrated circuits. A study found data remanence in DRAM with data retention of seconds to minutes at room temperature and "a full week without refresh when cooled with liquid nitrogen."[12] The study authors were able to use a cold boot attack to recover cryptographic keys for several popular full disk encryption systems, including Microsoft BitLocker, Apple FileVault, dm-crypt for Linux, and TrueCrypt.[12](p12)

Despite some memory degradation, authors of the above described study were able to take advantage of redundancy in the way keys are stored after they have been expanded for efficient use, such as in key scheduling. The authors recommend that computers be powered down, rather than be left in a "sleep" state, when not in physical control of the owner. In some cases, such as certain modes of the software program BitLocker, the authors recommend that a boot password or a key on a removable USB device be used.[12](p12) TRESOR is a kernel patch for Linux specifically intended to prevent cold boot attacks on RAM by ensuring encryption keys are neither user accessible nor stored in RAM.

Standards[edit]
Australia
ASD ISM 2014, Australian Government Information Security Manual, 2014 [13]
Canada
RCMP B2-002, IT Media Overwrite and Secure Erase Products, May 2009 [14]
Communications Security Establishment Clearing and Declassifying Electronic Data Storage Devices, July 2006 [15]
New Zealand
GCSB NZISM 2010, New Zealand Information Security Manual, Dec 2010 [16]
NZSIS PSM 2009, Protective Security Manual
United Kingdom
Asset Disposal and Information Security Alliance (ADISA), ADISA IT Asset Disposal Security Standard[17]
United States
NIST Special Publication 800-88, Guidelines for Media Sanitization, September 2006 [1]
DoD 5220.22-M, National Industrial Security Program Operating Manual (NISPOM), February 2006 [18]
Current editions no longer contain any references to specific sanitization methods. Standards for sanitization are left up to the Cognizant Security Authority.[18]
Although the NISPOM text itself never described any specific methods for sanitization, past editions (1995 and 1997)[19] did contain explicit sanitization methods within the Defense Security Service (DSS) Clearing and Sanitization Matrix inserted after Section 8-306. The DSS still provides this matrix and it continues to specify methods.[4] As of the Nov 2007 edition of the matrix, overwriting is no longer acceptable for sanitization of magnetic media. Only degaussing (with an NSA approved degausser) or physical destruction is acceptable.
Army AR380-19, Information Systems Security, February 1998 [20] replaced by AR 25-2 http://www.apd.army.mil/pdffiles/r25_2.pdf (Army Publishing Directorate, 2009)
Air Force AFSSI 8580, Remanence Security, 17 November 2008[21]
Navy NAVSO P5239-26, Remanence Security, September 1993 [22]
Computer forensics
From Wikipedia, the free encyclopedia
Forensic science

Physiological sciences
Forensic anthropology
Forensic dentistry
Forensic entomology
Forensic pathology
Forensic botany
Forensic biology
DNA profiling
DNA phenotyping
Bloodstain pattern analysis
Forensic chemistry
Social sciences
Forensic psychology
Forensic psychiatry
Forensic criminalistics
Ballistics
Ballistic fingerprinting
Body identification
Fingerprint analysis
Forensic accounting
Forensic arts
Forensic footwear evidence
Forensic toxicology
Gloveprint analysis
Palmprint analysis
Questioned document examination
Vein matching
Digital forensics
Computer forensics
Forensic data analysis
Database forensics
Mobile device forensics
Network forensics
Forensic video
Forensic audio
Related disciplines
Fire investigation
Fire accelerant detection
Forensic engineering
Forensic linguistics
Forensic materials engineering
Forensic polymer engineering
Forensic statistics
Vehicular accident reconstruction
Related articles
Crime scene
CSI effect
Perry Mason syndrome
Pollen calendar
Skid mark
Trace evidence
Use of DNA in
forensic entomology
v t e

Computer forensics analysis is not limited only to computer media
Computer forensics (sometimes known as computer forensic science[1]) is a branch of digital forensic science pertaining to evidence found in computers and digital storage media. The goal of computer forensics is to examine digital media in a forensically sound manner with the aim of identifying, preserving, recovering, analyzing and presenting facts and opinions about the digital information.

Although it is most often associated with the investigation of a wide variety of computer crime, computer forensics may also be used in civil proceedings. The discipline involves similar techniques and principles to data recovery, but with additional guidelines and practices designed to create a legal audit trail.

Evidence from computer forensics investigations is usually subjected to the same guidelines and practices of other digital evidence. It has been used in a number of high-profile cases and is becoming widely accepted as reliable within U.S. and European court systems.

Contents  [hide] 
1	Overview
2	Use as evidence
3	Forensic process
3.1	Techniques
3.2	Volatile data
3.3	Analysis tools
4	Certifications
5	See also
6	References
7	Further reading
7.1	Related journals
8	External links
Overview[edit]
In the early 1980s personal computers became more accessible to consumers, leading to their increased use in criminal activity (for example, to help commit fraud). At the same time, several new "computer crimes" were recognized (such as hacking). The discipline of computer forensics emerged during this time as a method to recover and investigate digital evidence for use in court. Since then computer crime and computer related crime has grown, and has jumped 67% between 2002 and 2003.[2] Today it is used to investigate a wide variety of crime, including child pornography, fraud, espionage, cyberstalking, murder and rape. The discipline also features in civil proceedings as a form of information gathering (for example, Electronic discovery)

Forensic techniques and expert knowledge are used to explain the current state of a digital artifact; such as a computer system, storage medium (e.g. hard disk or CD-ROM), an electronic document (e.g. an email message or JPEG image).[3] The scope of a forensic analysis can vary from simple information retrieval to reconstructing a series of events. In a 2002 book Computer Forensics authors Kruse and Heiser define computer forensics as involving "the preservation, identification, extraction, documentation and interpretation of computer data".[4] They go on to describe the discipline as "more of an art than a science", indicating that forensic methodology is backed by flexibility and extensive domain knowledge. However, while several methods can be used to extract evidence from a given computer the strategies used by law enforcement are fairly rigid and lacking the flexibility found in the civilian world.[5]

Use as evidence[edit]
In court, computer forensic evidence is subject to the usual requirements for digital evidence. This requires that information be authentic, reliably obtained, and admissible.[6] Different countries have specific guidelines and practices for evidence recovery. In the United Kingdom, examiners often follow Association of Chief Police Officers guidelines that help ensure the authenticity and integrity of evidence. While voluntary, the guidelines are widely accepted in British courts.

Computer forensics has been used as evidence in criminal law since the mid-1980s, some notable examples include:[7]

BTK Killer: Dennis Rader was convicted of a string of serial killings that occurred over a period of sixteen years. Towards the end of this period, Rader sent letters to the police on a floppy disk. Metadata within the documents implicated an author named "Dennis" at "Christ Lutheran Church"; this evidence helped lead to Rader's arrest.
Joseph E. Duncan III: A spreadsheet recovered from Duncan's computer contained evidence that showed him planning his crimes. Prosecutors used this to show premeditation and secure the death penalty.[8]
Sharon Lopatka: Hundreds of emails on Lopatka's computer lead investigators to her killer, Robert Glass.[7]
Corcoran Group: This case confirmed parties' duties to preserve digital evidence when litigation has commenced or is reasonably anticipated. Hard drives were analyzed by a computer forensics expert who could not find relevant emails the Defendants should have had. Though the expert found no evidence of deletion on the hard drives, evidence came out that the defendants were found to have intentionally destroyed emails, and misled and failed to disclose material facts to the plaintiffs and the court.
Dr. Conrad Murray: Dr. Conrad Murray, the doctor of the deceased Michael Jackson, was convicted partially by digital evidence on his computer. This evidence included medical documentation showing lethal amounts of propofol.
Forensic process[edit]
Main article: Digital forensic process

A portable Tableau write blocker attached to a Hard Drive
Computer forensic investigations usually follow the standard digital forensic process or phases: acquisition, examination, analysis and reporting. Investigations are performed on static data (i.e. acquired images) rather than "live" systems. This is a change from early forensic practices where a lack of specialist tools led to investigators commonly working on live data.

Techniques[edit]
A number of techniques are used during computer forensics investigations and much has been written on the many techniques used by law enforcement in particular.See, e.g., "Defending Child Pornography Cases".

Cross-drive analysis
A forensic technique that correlates information found on multiple hard drives. The process, still being researched, can be used to identify social networks and to perform anomaly detection.[9][10]
Live analysis
The examination of computers from within the operating system using custom forensics or existing sysadmin tools to extract evidence. The practice is useful when dealing with Encrypting File Systems, for example, where the encryption keys may be collected and, in some instances, the logical hard drive volume may be imaged (known as a live acquisition) before the computer is shut down.
Deleted files
A common technique used in computer forensics is the recovery of deleted files. Modern forensic software have their own tools for recovering or carving out deleted data.[11] Most operating systems and file systems do not always erase physical file data, allowing investigators to reconstruct it from the physical disk sectors. File carving involves searching for known file headers within the disk image and reconstructing deleted materials.
Stochastic forensics
A method which uses stochastic properties of the computer system to investigate activities lacking digital artifacts. Its chief use is to investigate data theft.
Steganography
One of the techniques used to hide data is via steganography, the process of hiding data inside of a picture or digital image. An example would be to hide pornographic images of children or other information that a given criminal does not want to have discovered. Computer forensics professionals can fight this by looking at the hash of the file and comparing it to the original image (if available.) While the image appears exactly the same, the hash changes as the data changes.[12]
Volatile data[edit]
When seizing evidence, if the machine is still active, any information stored solely in RAM that is not recovered before powering down may be lost.[8] One application of "live analysis" is to recover RAM data (for example, using Microsoft's COFEE tool, windd, WindowsSCOPE) prior to removing an exhibit. CaptureGUARD Gateway bypasses Windows login for locked computers, allowing for the analysis and acquisition of physical memory on a locked computer.

RAM can be analyzed for prior content after power loss, because the electrical charge stored in the memory cells takes time to dissipate, an effect exploited by the cold boot attack. The length of time that data is recoverable is increased by low temperatures and higher cell voltages. Holding unpowered RAM below ?60 ?C helps preserve residual data by an order of magnitude, improving the chances of successful recovery. However, it can be impractical to do this during a field examination.[13]

Some of the tools needed to extract volatile data, however, require that a computer be in a forensic lab, both to maintain a legitimate chain of evidence, and to facilitate work on the machine. If necessary, law enforcement applies techniques to move a live, running desktop computer. These include a mouse jiggler, which moves the mouse rapidly in small movements and prevents the computer from going to sleep accidentally. Usually, an uninterruptible power supply (UPS) provides power during transit.

However, one of the easiest ways to capture data is by actually saving the RAM data to disk. Various file systems that have journaling features such as NTFS and ReiserFS keep a large portion of the RAM data on the main storage media during operation, and these page files can be reassembled to reconstruct what was in RAM at that time.[14]

Analysis tools[edit]
See also: List of digital forensics tools
A number of open source and commercial tools exist for computer forensics investigation. Typical forensic analysis includes a manual review of material on the media, reviewing the Windows registry for suspect information, discovering and cracking passwords, keyword searches for topics related to the crime, and extracting e-mail and pictures for review.[7]

Certifications[edit]
There are several computer forensics certifications available, such as the ISFCE Certified Computer Examiner, Digital Forensics Investigation Professional (DFIP) and IACRB Certified Computer Forensics Examiner.

IACIS (the International Association of Computer Investigative Specialists) offers the Certified Computer Forensic Examiner (CFCE) program.

Asian School of Cyber Laws offers international level certifications in Digital Evidence Analysis and in Digital Forensic Investigation. These Courses are available in online and class room mode.

Many commercial based forensic software companies are now also offering proprietary certifications on their products. For example, Guidance Software offering the (EnCE) certification on their tool EnCase, AccessData offering (ACE) certification on their tool FTK, PassMark Software offering (OCE) certification on their tool OSForensics, and X-Ways Software Technology offering (X-PERT) certification for their software, X-Ways Forensics.[15]

See also[edit]
Counter forensics
Certified Computer Examiner
Certified Forensic Computer Examiner
Cryptanalysis
Data remanence
Disk encryption
Encryption
Hidden file and hidden directory
Information technology audit
MAC times
Steganalysis
United States v. Arnold
Cryptography
From Wikipedia, the free encyclopedia
"Secret code" redirects here. For the Aya Kamiki album, see Secret Code.
"Cryptology" redirects here. For the David S. Ware album, see Cryptology (album).
Lorenz cipher machine twelve rotors with mechanism
German Lorenz cipher machine, used in World War II to encrypt very-high-level general staff messages
Cryptography or cryptology (from Greek ??????? krypt?s, "hidden, secret"; and ??????? graphein, "writing", or -????? -logia, "study", respectively[1]) is the practice and study of techniques for secure communication in the presence of third parties called adversaries.[2] More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages;[3] various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation[4] are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, and electrical engineering. Applications of cryptography include ATM cards, computer passwords, and electronic commerce.

Cryptography prior to the modern age was effectively synonymous with encryption, the conversion of information from a readable state to apparent nonsense. The originator of an encrypted message (Alice) shared the decoding technique needed to recover the original information only with intended recipients (Bob), thereby precluding unwanted persons (Eve) from doing the same. The cryptography literature often uses Alice ("A") for the sender, Bob ("B") for the intended recipient, and Eve ("eavesdropper") for the adversary.[5] Since the development of rotor cipher machines in World War I and the advent of computers in World War II, the methods used to carry out cryptology have become increasingly complex and its application more widespread.

Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.

The growth of cryptographic technology has raised a number of legal issues in the information age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export.[6] In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation.[7] Cryptography also plays a major role in digital rights management and copyright infringement of digital media.[8]

Contents  [hide] 
1	Terminology
2	History of cryptography and cryptanalysis
2.1	Classic cryptography
2.2	Computer era
3	Modern cryptography
3.1	Symmetric-key cryptography
3.2	Public-key cryptography
3.3	Cryptanalysis
3.4	Cryptographic primitives
3.5	Cryptosystems
4	Legal issues
4.1	Prohibitions
4.2	Export controls
4.3	NSA involvement
4.4	Digital rights management
4.5	Forced disclosure of encryption keys
5	See also
6	References
7	Further reading
8	External links
Terminology[edit]
diagram showing shift three alphabetic cypher D becomes A and E becomes B
Alphabet shift ciphers are believed to have been used by Julius Caesar over 2,000 years ago.[5] This is an example with k=3. In other words, the letters in the alphabet are shifted three in one direction to encrypt and three in the other direction to decrypt.
Until modern times, cryptography referred almost exclusively to encryption, which is the process of converting ordinary information (called plaintext) into unintelligible text (called ciphertext).[9] Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext. A cipher (or cypher) is a pair of algorithms that create the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and in each instance by a "key". The key is a secret (ideally known only to the communicants), usually a short string of characters, which is needed to decrypt the ciphertext. Formally, a "cryptosystem" is the ordered list of elements of finite possible plaintexts, finite possible cyphertexts, finite possible keys, and the encryption and decryption algorithms which correspond to each key. Keys are important both formally and in actual practice, as ciphers without variable keys can be trivially broken with only the knowledge of the cipher used and are therefore useless (or even counter-productive) for most purposes. Historically, ciphers were often used directly for encryption or decryption without additional procedures such as authentication or integrity checks. There are two kinds of cryptosystems: symmetric and asymmetric. In symmetric systems the same key (the secret key) is used to encrypt and decrypt a message. Data manipulation in symmetric systems is faster than asymmetric systems as they generally use shorter key lengths. Asymmetric systems use a public key to encrypt a message and a private key to decrypt it. Use of asymmetric systems enhances the security of communication.[10] Examples of asymmetric systems include RSA (Rivest-Shamir-Adleman), and ECC (Elliptic Curve Cryptography). Symmetric models include the commonly used AES (Advanced Encryption Standard) which replaced the older DES (Data Encryption Standard).[11]

In colloquial use, the term "code" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, code has a more specific meaning. It means the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, "wallaby" replaces "attack at dawn").

Cryptanalysis is the term used for the study of methods for obtaining the meaning of encrypted information without access to the key normally required to do so; i.e., it is the study of how to crack encryption algorithms or their implementations.

Some use the terms cryptography and cryptology interchangeably in English, while others (including US military practice generally) use cryptography to refer specifically to the use and practice of cryptographic techniques and cryptology to refer to the combined study of cryptography and cryptanalysis.[12][13] English is more flexible than several other languages in which cryptology (done by cryptologists) is always used in the second sense above. RFC 2828 advises that steganography is sometimes included in cryptology.[14]

The study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.) is called cryptolinguistics.

History of cryptography and cryptanalysis[edit]
Main article: History of cryptography
Before the modern era, cryptography was concerned solely with message confidentiality (i.e., encryption)—conversion of messages from a comprehensible form into an incomprehensible one and back again at the other end, rendering it unreadable by interceptors or eavesdroppers without secret knowledge (namely the key needed for decryption of that message). Encryption attempted to ensure secrecy in communications, such as those of spies, military leaders, and diplomats. In recent decades, the field has expanded beyond confidentiality concerns to include techniques for message integrity checking, sender/receiver identity authentication, digital signatures, interactive proofs and secure computation, among others.

Classic cryptography[edit]
Skytala stick with strip of paper wound around in spiral
Reconstructed ancient Greek scytale, an early cipher device
The earliest forms of secret writing required little more than writing implements since most people could not read. More literacy, or literate opponents, required actual cryptography. The main classical cipher types are transposition ciphers, which rearrange the order of letters in a message (e.g., 'hello world' becomes 'ehlol owrdl' in a trivially simple rearrangement scheme), and substitution ciphers, which systematically replace letters or groups of letters with other letters or groups of letters (e.g., 'fly at once' becomes 'gmz bu podf' by replacing each letter with the one following it in the Latin alphabet). Simple versions of either have never offered much confidentiality from enterprising opponents. An early substitution cipher was the Caesar cipher, in which each letter in the plaintext was replaced by a letter some fixed number of positions further down the alphabet. Suetonius reports that Julius Caesar used it with a shift of three to communicate with his generals. Atbash is an example of an early Hebrew cipher. The earliest known use of cryptography is some carved ciphertext on stone in Egypt (ca 1900 BCE), but this may have been done for the amusement of literate observers rather than as a way of concealing information.

The Greeks of Classical times are said to have known of ciphers (e.g., the scytale transposition cipher claimed to have been used by the Spartan military).[15] Steganography (i.e., hiding even the existence of a message so as to keep it confidential) was also first developed in ancient times. An early example, from Herodotus, was a message tattooed on a slave's shaved head and concealed under the regrown hair.[9] More modern examples of steganography include the use of invisible ink, microdots, and digital watermarks to conceal information.

In India, the 2000-year-old Kamasutra of V?tsy?yana speaks of two different kinds of ciphers called Kautiliyam and Mulavediya. In the Kautiliyam, the cipher letter substitutions are based on phonetic relations, such as vowels becoming consonants. In the Mulavediya, the cipher alphabet consists of pairing letters and using the reciprocal ones.[9]

Arabic text of a book by Al-Kindi
First page of a book by Al-Kindi which discusses encryption of messages
Ciphertexts produced by a classical cipher (and some modern ciphers) will reveal statistical information about the plaintext, and that information can often be used to break the cipher. After the discovery of frequency analysis, perhaps by the Arab mathematician and polymath Al-Kindi (also known as Alkindus) in the 9th century,[16] nearly all such ciphers could be broken by an informed attacker. Such classical ciphers still enjoy popularity today, though mostly as puzzles (see cryptogram). Al-Kindi wrote a book on cryptography entitled Risalah fi Istikhraj al-Mu'amma (Manuscript for the Deciphering Cryptographic Messages), which described the first known use of frequency analysis cryptanalysis techniques.[16][17]

book sized metal machine with large dial left page and nineteen small dials right page
16th-century book-shaped French cipher machine, with arms of Henri II of France
manuscript from Gabriel de Luetz d'Aramon in bound volume
Enciphered letter from Gabriel de Luetz d'Aramon, French Ambassador to the Ottoman Empire, after 1546, with partial decipherment
Language letter frequencies may offer little help for some extended historical encryption techniques such as homophonic cipher that tend to flatten the frequency distribution. For those ciphers, language letter group (or n-gram) frequencies may provide an attack.

Essentially all ciphers remained vulnerable to cryptanalysis using the frequency analysis technique until the development of the polyalphabetic cipher, most clearly by Leon Battista Alberti around the year 1467, though there is some indication that it was already known to Al-Kindi.[17] Alberti's innovation was to use different ciphers (i.e., substitution alphabets) for various parts of a message (perhaps for each successive plaintext letter at the limit). He also invented what was probably the first automatic cipher device, a wheel which implemented a partial realization of his invention. In the polyalphabetic Vigen?re cipher, encryption uses a key word, which controls letter substitution depending on which letter of the key word is used. In the mid-19th century Charles Babbage showed that the Vigen?re cipher was vulnerable to Kasiski examination, but this was first published about ten years later by Friedrich Kasiski.[18]

Although frequency analysis can be a powerful and general technique against many ciphers, encryption has still often been effective in practice, as many a would-be cryptanalyst was unaware of the technique. Breaking a message without using frequency analysis essentially required knowledge of the cipher used and perhaps of the key involved, thus making espionage, bribery, burglary, defection, etc., more attractive approaches to the cryptanalytically uninformed. It was finally explicitly recognized in the 19th century that secrecy of a cipher's algorithm is not a sensible nor practical safeguard of message security; in fact, it was further realized that any adequate cryptographic scheme (including ciphers) should remain secure even if the adversary fully understands the cipher algorithm itself. Security of the key used should alone be sufficient for a good cipher to maintain confidentiality under an attack. This fundamental principle was first explicitly stated in 1883 by Auguste Kerckhoffs and is generally called Kerckhoffs's Principle; alternatively and more bluntly, it was restated by Claude Shannon, the inventor of information theory and the fundamentals of theoretical cryptography, as Shannon's Maxim—'the enemy knows the system'.

Different physical devices and aids have been used to assist with ciphers. One of the earliest may have been the scytale of ancient Greece, a rod supposedly used by the Spartans as an aid for a transposition cipher (see image above). In medieval times, other aids were invented such as the cipher grille, which was also used for a kind of steganography. With the invention of polyalphabetic ciphers came more sophisticated aids such as Alberti's own cipher disk, Johannes Trithemius' tabula recta scheme, and Thomas Jefferson's multi cylinder (not publicly known, and reinvented independently by Bazeries around 1900). Many mechanical encryption/decryption devices were invented early in the 20th century, and several patented, among them rotor machines—famously including the Enigma machine used by the German government and military from the late 1920s and during World War II.[19] The ciphers implemented by better quality examples of these machine designs brought about a substantial increase in cryptanalytic difficulty after WWI.[20]

Computer era[edit]
Cryptanalysis of the new mechanical devices proved to be both difficult and laborious. In the United Kingdom, cryptanalytic efforts at Bletchley Park during WWII spurred the development of more efficient means for carrying out repetitious tasks. This culminated in the development of the Colossus, the world's first fully electronic, digital, programmable computer, which assisted in the decryption of ciphers generated by the German Army's Lorenz SZ40/42 machine.

Just as the development of digital computers and electronics helped in cryptanalysis, it made possible much more complex ciphers. Furthermore, computers allowed for the encryption of any kind of data representable in any binary format, unlike classical ciphers which only encrypted written language texts; this was new and significant. Computer use has thus supplanted linguistic cryptography, both for cipher design and cryptanalysis. Many computer ciphers can be characterized by their operation on binary bit sequences (sometimes in groups or blocks), unlike classical and mechanical schemes, which generally manipulate traditional characters (i.e., letters and digits) directly. However, computers have also assisted cryptanalysis, which has compensated to some extent for increased cipher complexity. Nonetheless, good modern ciphers have stayed ahead of cryptanalysis; it is typically the case that use of a quality cipher is very efficient (i.e., fast and requiring few resources, such as memory or CPU capability), while breaking it requires an effort many orders of magnitude larger, and vastly larger than that required for any classical cipher, making cryptanalysis so inefficient and impractical as to be effectively impossible.

Extensive open academic research into cryptography is relatively recent; it began only in the mid-1970s. In recent times, IBM personnel designed the algorithm that became the Federal (i.e., US) Data Encryption Standard; Whitfield Diffie and Martin Hellman published their key agreement algorithm;[21] and the RSA algorithm was published in Martin Gardner's Scientific American column. Since then, cryptography has become a widely used tool in communications, computer networks, and computer security generally. Some modern cryptographic techniques can only keep their keys secret if certain mathematical problems are intractable, such as the integer factorization or the discrete logarithm problems, so there are deep connections with abstract mathematics. There are very few cryptosystems that are proven to be unconditionally secure. The one-time pad is one. There are a few important ones that are proven secure under certain unproven assumptions. For example, the infeasibility of factoring extremely large integers is the basis for believing that RSA is secure, and some other systems, but even there, the proof is usually lost due to practical considerations. There are systems similar to RSA, such as one by Michael O. Rabin that is provably secure provided factoring n = pq is impossible, but the more practical system RSA has never been proved secure in this sense. The discrete logarithm problem is the basis for believing some other cryptosystems are secure, and again, there are related, less practical systems that are provably secure relative to the discrete log problem.[22]

As well as being aware of cryptographic history, cryptographic algorithm and system designers must also sensibly consider probable future developments while working on their designs. For instance, continuous improvements in computer processing power have increased the scope of brute-force attacks, so when specifying key lengths, the required key lengths are similarly advancing.[23] The potential effects of quantum computing are already being considered by some cryptographic system designers; the announced imminence of small implementations of these machines may be making the need for this preemptive caution rather more than merely speculative.[4]

Essentially, prior to the early 20th century, cryptography was chiefly concerned with linguistic and lexicographic patterns. Since then the emphasis has shifted, and cryptography now makes extensive use of mathematics, including aspects of information theory, computational complexity, statistics, combinatorics, abstract algebra, number theory, and finite mathematics generally. Cryptography is also a branch of engineering, but an unusual one since it deals with active, intelligent, and malevolent opposition (see cryptographic engineering and security engineering); other kinds of engineering (e.g., civil or chemical engineering) need deal only with neutral natural forces. There is also active research examining the relationship between cryptographic problems and quantum physics (see quantum cryptography and quantum computer).

Modern cryptography[edit]
The modern field of cryptography can be divided into several areas of study. The chief ones are discussed here; see Topics in Cryptography for more.

Symmetric-key cryptography[edit]
Main article: Symmetric-key algorithm
diagram showing encrypt with a key and decrypt process
Symmetric-key cryptography, where a single key is used for encryption and decryption
Symmetric-key cryptography refers to encryption methods in which both the sender and receiver share the same key (or, less commonly, in which their keys are different, but related in an easily computable way). This was the only kind of encryption publicly known until June 1976.[21]

logic diagram showing International Data Encryption Algorithm cypher process
One round (out of 8.5) of the IDEA cipher, used in some versions of PGP for high-speed encryption of, for instance, e-mail
Symmetric key ciphers are implemented as either block ciphers or stream ciphers. A block cipher enciphers input in blocks of plaintext as opposed to individual characters, the input form used by a stream cipher.

The Data Encryption Standard (DES) and the Advanced Encryption Standard (AES) are block cipher designs that have been designated cryptography standards by the US government (though DES's designation was finally withdrawn after the AES was adopted).[24] Despite its deprecation as an official standard, DES (especially its still-approved and much more secure triple-DES variant) remains quite popular; it is used across a wide range of applications, from ATM encryption[25] to e-mail privacy[26] and secure remote access.[27] Many other block ciphers have been designed and released, with considerable variation in quality. Many have been thoroughly broken, such as FEAL.[4][28]

Stream ciphers, in contrast to the 'block' type, create an arbitrarily long stream of key material, which is combined with the plaintext bit-by-bit or character-by-character, somewhat like the one-time pad. In a stream cipher, the output stream is created based on a hidden internal state that changes as the cipher operates. That internal state is initially set up using the secret key material. RC4 is a widely used stream cipher; see Category:Stream ciphers.[4] Block ciphers can be used as stream ciphers; see Block cipher modes of operation.

Cryptographic hash functions are a third type of cryptographic algorithm. They take a message of any length as input, and output a short, fixed length hash, which can be used in (for example) a digital signature. For good hash functions, an attacker cannot find two messages that produce the same hash. MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but it isn't yet widely deployed; and the US standards authority thought it "prudent" from a security perspective to develop a new standard to "significantly improve the robustness of NIST's overall hash algorithm toolkit."[29] Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012 when the NIST announced that Keccak would be the new SHA-3 hash algorithm.[30]

Message authentication codes (MACs) are much like cryptographic hash functions, except that a secret key can be used to authenticate the hash value upon receipt;[4] this additional complication blocks an attack scheme against bare digest algorithms, and so has been thought worth the effort.

Public-key cryptography[edit]
Main article: Public-key cryptography
diagram of Public-key cryptography showing public key and private key
Public-key cryptography, where different keys are used for encryption and decryption
Symmetric-key cryptosystems use the same key for encryption and decryption of a message, though a message or group of messages may have a different key than others. A significant disadvantage of symmetric ciphers is the key management necessary to use them securely. Each distinct pair of communicating parties must, ideally, share a different key, and perhaps each ciphertext exchanged as well. The number of keys required increases as the square of the number of network members, which very quickly requires complex key management schemes to keep them all consistent and secret. The difficulty of securely establishing a secret key between two communicating parties, when a secure channel does not already exist between them, also presents a chicken-and-egg problem which is a considerable practical obstacle for cryptography users in the real world.

headshots of Whitfield Diffie and Martin Hellman
Whitfield Diffie and Martin Hellman, authors of the first published paper on public-key cryptography
In a groundbreaking 1976 paper, Whitfield Diffie and Martin Hellman proposed the notion of public-key (also, more generally, called asymmetric key) cryptography in which two different but mathematically related keys are used—a public key and a private key.[31] A public key system is so constructed that calculation of one key (the 'private key') is computationally infeasible from the other (the 'public key'), even though they are necessarily related. Instead, both keys are generated secretly, as an interrelated pair.[32] The historian David Kahn described public-key cryptography as "the most revolutionary new concept in the field since polyalphabetic substitution emerged in the Renaissance".[33]

In public-key cryptosystems, the public key may be freely distributed, while its paired private key must remain secret. In a public-key encryption system, the public key is used for encryption, while the private or secret key is used for decryption. While Diffie and Hellman could not find such a system, they showed that public-key cryptography was indeed possible by presenting the Diffie–Hellman key exchange protocol, a solution that is now widely used in secure communications to allow two parties to secretly agree on a shared encryption key.[21]

Diffie and Hellman's publication sparked widespread academic efforts in finding a practical public-key encryption system. This race was finally won in 1978 by Ronald Rivest, Adi Shamir, and Len Adleman, whose solution has since become known as the RSA algorithm.[34]

The Diffie–Hellman and RSA algorithms, in addition to being the first publicly known examples of high quality public-key algorithms, have been among the most widely used. Others include the Cramer–Shoup cryptosystem, ElGamal encryption, and various elliptic curve techniques. See Category:Asymmetric-key cryptosystems.

To much surprise, a document published in 1997 by the Government Communications Headquarters (GCHQ), a British intelligence organization, revealed that cryptographers at GCHQ had anticipated several academic developments.[35] Reportedly, around 1970, James H. Ellis had conceived the principles of asymmetric key cryptography. In 1973, Clifford Cocks invented a solution that essentially resembles the RSA algorithm.[35][36] And in 1974, Malcolm J. Williamson is claimed to have developed the Diffie–Hellman key exchange.[37]

padlock icon in the internet browser line next to the url
Padlock icon from the Firefox Web browser, which indicates that TLS, a public-key cryptography system, is in use.
Public-key cryptography can also be used for implementing digital signature schemes. A digital signature is reminiscent of an ordinary signature; they both have the characteristic of being easy for a user to produce, but difficult for anyone else to forge. Digital signatures can also be permanently tied to the content of the message being signed; they cannot then be 'moved' from one document to another, for any attempt will be detectable. In digital signature schemes, there are two algorithms: one for signing, in which a secret key is used to process the message (or a hash of the message, or both), and one for verification, in which the matching public key is used with the message to check the validity of the signature. RSA and DSA are two of the most popular digital signature schemes. Digital signatures are central to the operation of public key infrastructures and many network security schemes (e.g., SSL/TLS, many VPNs, etc.).[28]

Public-key algorithms are most often based on the computational complexity of "hard" problems, often from number theory. For example, the hardness of RSA is related to the integer factorization problem, while Diffie–Hellman and DSA are related to the discrete logarithm problem. More recently, elliptic curve cryptography has developed, a system in which security is based on number theoretic problems involving elliptic curves. Because of the difficulty of the underlying problems, most public-key algorithms involve operations such as modular multiplication and exponentiation, which are much more computationally expensive than the techniques used in most block ciphers, especially with typical key sizes. As a result, public-key cryptosystems are commonly hybrid cryptosystems, in which a fast high-quality symmetric-key encryption algorithm is used for the message itself, while the relevant symmetric key is sent with the message, but encrypted using a public-key algorithm. Similarly, hybrid signature schemes are often used, in which a cryptographic hash function is computed, and only the resulting hash is digitally signed.[4]

Cryptanalysis[edit]
Main article: Cryptanalysis
Enigma machine typewriter keypad over many rotors in a wood box
Variants of the Enigma machine, used by Germany's military and civil authorities from the late 1920s through World War II, implemented a complex electro-mechanical polyalphabetic cipher. Breaking and reading of the Enigma cipher at Poland's Cipher Bureau, for 7 years before the war, and subsequent decryption at Bletchley Park, was important to Allied victory.[9]
The goal of cryptanalysis is to find some weakness or insecurity in a cryptographic scheme, thus permitting its subversion or evasion.

It is a common misconception that every encryption method can be broken. In connection with his WWII work at Bell Labs, Claude Shannon proved that the one-time pad cipher is unbreakable, provided the key material is truly random, never reused, kept secret from all possible attackers, and of equal or greater length than the message.[38] Most ciphers, apart from the one-time pad, can be broken with enough computational effort by brute force attack, but the amount of effort needed may be exponentially dependent on the key size, as compared to the effort needed to make use of the cipher. In such cases, effective security could be achieved if it is proven that the effort required (i.e., "work factor", in Shannon's terms) is beyond the ability of any adversary. This means it must be shown that no efficient method (as opposed to the time-consuming brute force method) can be found to break the cipher. Since no such proof has been found to date, the one-time-pad remains the only theoretically unbreakable cipher.

There are a wide variety of cryptanalytic attacks, and they can be classified in any of several ways. A common distinction turns on what Eve (an attacker) knows and what capabilities are available. In a ciphertext-only attack, Eve has access only to the ciphertext (good modern cryptosystems are usually effectively immune to ciphertext-only attacks). In a known-plaintext attack, Eve has access to a ciphertext and its corresponding plaintext (or to many such pairs). In a chosen-plaintext attack, Eve may choose a plaintext and learn its corresponding ciphertext (perhaps many times); an example is gardening, used by the British during WWII. In a chosen-ciphertext attack, Eve may be able to choose ciphertexts and learn their corresponding plaintexts.[4] Finally in a man-in-the-middle attack Eve gets in between Alice (the sender) and Bob (the recipient), accesses and modifies the traffic and then forwards it to the recipient.[39] Also important, often overwhelmingly so, are mistakes (generally in the design or use of one of the protocols involved; see Cryptanalysis of the Enigma for some historical examples of this).

Kaiserschloss Kryptologen monument numbers on stele
Pozna? monument (center) to Polish cryptologists whose breaking of Germany's Enigma machine ciphers, beginning in 1932, altered the course of World War II
Cryptanalysis of symmetric-key ciphers typically involves looking for attacks against the block ciphers or stream ciphers that are more efficient than any attack that could be against a perfect cipher. For example, a simple brute force attack against DES requires one known plaintext and 255 decryptions, trying approximately half of the possible keys, to reach a point at which chances are better than even that the key sought will have been found. But this may not be enough assurance; a linear cryptanalysis attack against DES requires 243 known plaintexts and approximately 243 DES operations.[40] This is a considerable improvement on brute force attacks.

Public-key algorithms are based on the computational difficulty of various problems. The most famous of these is integer factorization (e.g., the RSA algorithm is based on a problem related to integer factoring), but the discrete logarithm problem is also important. Much public-key cryptanalysis concerns numerical algorithms for solving these computational problems, or some of them, efficiently (i.e., in a practical time). For instance, the best known algorithms for solving the elliptic curve-based version of discrete logarithm are much more time-consuming than the best known algorithms for factoring, at least for problems of more or less equivalent size. Thus, other things being equal, to achieve an equivalent strength of attack resistance, factoring-based encryption techniques must use larger keys than elliptic curve techniques. For this reason, public-key cryptosystems based on elliptic curves have become popular since their invention in the mid-1990s.

While pure cryptanalysis uses weaknesses in the algorithms themselves, other attacks on cryptosystems are based on actual use of the algorithms in real devices, and are called side-channel attacks. If a cryptanalyst has access to, for example, the amount of time the device took to encrypt a number of plaintexts or report an error in a password or PIN character, he may be able to use a timing attack to break a cipher that is otherwise resistant to analysis. An attacker might also study the pattern and length of messages to derive valuable information; this is known as traffic analysis[41] and can be quite useful to an alert adversary. Poor administration of a cryptosystem, such as permitting too short keys, will make any system vulnerable, regardless of other virtues. And, of course, social engineering, and other attacks against the personnel who work with cryptosystems or the messages they handle (e.g., bribery, extortion, blackmail, espionage, torture, ...) may be the most productive attacks of all.

Cryptographic primitives[edit]
Much of the theoretical work in cryptography concerns cryptographic primitives—algorithms with basic cryptographic properties—and their relationship to other cryptographic problems. More complicated cryptographic tools are then built from these basic primitives. These primitives provide fundamental properties, which are used to develop more complex tools called cryptosystems or cryptographic protocols, which guarantee one or more high-level security properties. Note however, that the distinction between cryptographic primitives and cryptosystems, is quite arbitrary; for example, the RSA algorithm is sometimes considered a cryptosystem, and sometimes a primitive. Typical examples of cryptographic primitives include pseudorandom functions, one-way functions, etc.

Cryptosystems[edit]
One or more cryptographic primitives are often used to develop a more complex algorithm, called a cryptographic system, or cryptosystem. Cryptosystems (e.g., El-Gamal encryption) are designed to provide particular functionality (e.g., public key encryption) while guaranteeing certain security properties (e.g., chosen-plaintext attack (CPA) security in the random oracle model). Cryptosystems use the properties of the underlying cryptographic primitives to support the system's security properties. Of course, as the distinction between primitives and cryptosystems is somewhat arbitrary, a sophisticated cryptosystem can be derived from a combination of several more primitive cryptosystems. In many cases, the cryptosystem's structure involves back and forth communication among two or more parties in space (e.g., between the sender of a secure message and its receiver) or across time (e.g., cryptographically protected backup data). Such cryptosystems are sometimes called cryptographic protocols.

Some widely known cryptosystems include RSA encryption, Schnorr signature, El-Gamal encryption, PGP, etc. More complex cryptosystems include electronic cash[42] systems, signcryption systems, etc. Some more 'theoretical' cryptosystems include interactive proof systems,[43] (like zero-knowledge proofs),[44] systems for secret sharing,[45][46] etc.

Until recently[timeframe?], most security properties of most cryptosystems were demonstrated using empirical techniques or using ad hoc reasoning. Recently[timeframe?], there has been considerable effort to develop formal techniques for establishing the security of cryptosystems; this has been generally called provable security. The general idea of provable security is to give arguments about the computational difficulty needed to compromise some security aspect of the cryptosystem (i.e., to any adversary).

The study of how best to implement and integrate cryptography in software applications is itself a distinct field (see Cryptographic engineering and Security engineering).

Legal issues[edit]
See also: Cryptography laws in different nations
Prohibitions[edit]
Cryptography has long been of interest to intelligence gathering and law enforcement agencies. Secret communications may be criminal or even treasonous[citation needed]. Because of its facilitation of privacy, and the diminution of privacy attendant on its prohibition, cryptography is also of considerable interest to civil rights supporters. Accordingly, there has been a history of controversial legal issues surrounding cryptography, especially since the advent of inexpensive computers has made widespread access to high quality cryptography possible.

In some countries, even the domestic use of cryptography is, or has been, restricted. Until 1999, France significantly restricted the use of cryptography domestically, though it has since relaxed many of these rules. In China and Iran, a license is still required to use cryptography.[6] Many countries have tight restrictions on the use of cryptography. Among the more restrictive are laws in Belarus, Kazakhstan, Mongolia, Pakistan, Singapore, Tunisia, and Vietnam.[47]

In the United States, cryptography is legal for domestic use, but there has been much conflict over legal issues related to cryptography. One particularly important issue has been the export of cryptography and cryptographic software and hardware. Probably because of the importance of cryptanalysis in World War II and an expectation that cryptography would continue to be important for national security, many Western governments have, at some point, strictly regulated export of cryptography. After World War II, it was illegal in the US to sell or distribute encryption technology overseas; in fact, encryption was designated as auxiliary military equipment and put on the United States Munitions List.[48] Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe.

Export controls[edit]
Main article: Export of cryptography
In the 1990s, there were several challenges to US export regulation of cryptography. After the source code for Philip Zimmermann's Pretty Good Privacy (PGP) encryption program found its way onto the Internet in June 1991, a complaint by RSA Security (then called RSA Data Security, Inc.) resulted in a lengthy criminal investigation of Zimmermann by the US Customs Service and the FBI, though no charges were ever filed.[49][50] Daniel J. Bernstein, then a graduate student at UC Berkeley, brought a lawsuit against the US government challenging some aspects of the restrictions based on free speech grounds. The 1995 case Bernstein v. United States ultimately resulted in a 1999 decision that printed source code for cryptographic algorithms and systems was protected as free speech by the United States Constitution.[51]

In 1996, thirty-nine countries signed the Wassenaar Arrangement, an arms control treaty that deals with the export of arms and "dual-use" technologies such as cryptography. The treaty stipulated that the use of cryptography with short key-lengths (56-bit for symmetric encryption, 512-bit for RSA) would no longer be export-controlled.[52] Cryptography exports from the US became less strictly regulated as a consequence of a major relaxation in 2000;[53] there are no longer very many restrictions on key sizes in US-exported mass-market software. Since this relaxation in US export restrictions, and because most personal computers connected to the Internet include US-sourced web browsers such as Firefox or Internet Explorer, almost every Internet user worldwide has potential access to quality cryptography via their browsers (e.g., via Transport Layer Security). The Mozilla Thunderbird and Microsoft Outlook E-mail client programs similarly can transmit and receive emails via TLS, and can send and receive email encrypted with S/MIME. Many Internet users don't realize that their basic application software contains such extensive cryptosystems. These browsers and email programs are so ubiquitous that even governments whose intent is to regulate civilian use of cryptography generally don't find it practical to do much to control distribution or use of cryptography of this quality, so even when such laws are in force, actual enforcement is often effectively impossible.[citation needed]

NSA involvement[edit]

NSA headquarters in Fort Meade, Maryland
See also: Clipper chip
Another contentious issue connected to cryptography in the United States is the influence of the National Security Agency on cipher development and policy. The NSA was involved with the design of DES during its development at IBM and its consideration by the National Bureau of Standards as a possible Federal Standard for cryptography.[54] DES was designed to be resistant to differential cryptanalysis,[55] a powerful and general cryptanalytic technique known to the NSA and IBM, that became publicly known only when it was rediscovered in the late 1980s.[56] According to Steven Levy, IBM discovered differential cryptanalysis,[50] but kept the technique secret at the NSA's request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have.

Another instance of the NSA's involvement was the 1993 Clipper chip affair, an encryption microchip intended to be part of the Capstone cryptography-control initiative. Clipper was widely criticized by cryptographers for two reasons. The cipher algorithm (called Skipjack) was then classified (declassified in 1998, long after the Clipper initiative lapsed). The classified cipher caused concerns that the NSA had deliberately made the cipher weak in order to assist its intelligence efforts. The whole initiative was also criticized based on its violation of Kerckhoffs's Principle, as the scheme included a special escrow key held by the government for use by law enforcement, for example in wiretaps.[50]

Digital rights management[edit]
Main article: Digital rights management
Cryptography is central to digital rights management (DRM), a group of techniques for technologically controlling use of copyrighted material, being widely implemented and deployed at the behest of some copyright holders. In 1998, U.S. President Bill Clinton signed the Digital Millennium Copyright Act (DMCA), which criminalized all production, dissemination, and use of certain cryptanalytic techniques and technology (now known or later discovered); specifically, those that could be used to circumvent DRM technological schemes.[57] This had a noticeable impact on the cryptography research community since an argument can be made that any cryptanalytic research violated, or might violate, the DMCA. Similar statutes have since been enacted in several countries and regions, including the implementation in the EU Copyright Directive. Similar restrictions are called for by treaties signed by World Intellectual Property Organization member-states.

The United States Department of Justice and FBI have not enforced the DMCA as rigorously as had been feared by some, but the law, nonetheless, remains a controversial one. Niels Ferguson, a well-respected cryptography researcher, has publicly stated that he will not release some of his research into an Intel security design for fear of prosecution under the DMCA.[58] Both Alan Cox (longtime number 2 in Linux kernel development) and Edward Felten (and some of his students at Princeton) have encountered problems related to the Act. Dmitry Sklyarov was arrested during a visit to the US from Russia, and jailed for five months pending trial for alleged violations of the DMCA arising from work he had done in Russia, where the work was legal. In 2007, the cryptographic keys responsible for Blu-ray and HD DVD content scrambling were discovered and released onto the Internet. In both cases, the MPAA sent out numerous DMCA takedown notices, and there was a massive Internet backlash[8] triggered by the perceived impact of such notices on fair use and free speech.

Forced disclosure of encryption keys[edit]
Main article: Key disclosure law
In the United Kingdom, the Regulation of Investigatory Powers Act gives UK police the powers to force suspects to decrypt files or hand over passwords that protect encryption keys. Failure to comply is an offense in its own right, punishable on conviction by a two-year jail sentence or up to five years in cases involving national security.[7] Successful prosecutions have occurred under the Act; the first, in 2009,[59] resulted in a term of 13 months' imprisonment.[60] Similar forced disclosure laws in Australia, Finland, France, and India compel individual suspects under investigation to hand over encryption keys or passwords during a criminal investigation.

In the United States, the federal criminal case of United States v. Fricosu addressed whether a search warrant can compel a person to reveal an encryption passphrase or password.[61] The Electronic Frontier Foundation (EFF) argued that this is a violation of the protection from self-incrimination given by the Fifth Amendment.[62] In 2012, the court ruled that under the All Writs Act, the defendant was required to produce an unencrypted hard drive for the court.[63]

In many jurisdictions, the legal status of forced disclosure remains unclear.

The 2016 FBI–Apple encryption dispute concerns the ability of courts in the United States to compel manufacturers' assistance in unlocking cell phones whose contents are cryptographically protected.

As a potential counter-measure to forced disclosure some cryptographic software supports plausible deniability, where the encrypted data is indistinguishable from unused random data (for example such as that of a drive which has been securely wiped).
Data erasure
From Wikipedia, the free encyclopedia
Data erasure (also called data clearing or data wiping) is a software-based method of overwriting the data that aims to completely destroy all electronic data residing on a hard disk drive or other digital media. Permanent data erasure goes beyond basic file deletion commands, which only remove direct pointers to the data disk sectors and make the data recovery possible with common software tools. Unlike degaussing and physical destruction, which render the storage media unusable, data erasure removes all information while leaving the disk operable, preserving IT assets and the environment. New flash memory–based media implementations, such as solid-state drives or USB flash drives can cause data erasure techniques to fail allowing remnant data to be recoverable.[1]

Software-based overwriting uses a software application to write a stream of zeros, ones or meaningless pseudorandom data onto all sectors of a hard disk drive. There are key differentiators between data erasure and other overwriting methods, which can leave data intact and raise the risk of data breach, identity theft or failure to achieve regulatory compliance. Many data eradication programs also provide multiple overwrites so that they support recognized government and industry standards, though a single-pass overwrite is widely considered to be sufficient for modern hard disk drives. Good software should provide verification of data removal, which is necessary for meeting certain standards.

To protect the data on lost or stolen media, some data erasure applications remotely destroy the data if the password is incorrectly entered. Data erasure tools can also target specific data on a disk for routine erasure, providing a hacking protection method that is less time-consuming than software encryption. Hardware/firmware encryption built into the drive itself or integrated controllers is a popular solution with no degradation in performance at all.

Presently, dedicated hardware/firmware encryption solutions can perform a 256-bit full AES encryption faster than the drive electronics can write the data. Drives with this capability are known as self-encrypting drives (SEDs); they are present on most modern enterprise-level laptops and are increasingly used in the enterprise to protect the data. Changing the encryption key renders inaccessible all data stored on a SED, which is an easy and very fast method for achieving a 100% data erasure. Theft of an SED results in a physical asset loss, but the stored data is inaccessible without the decryption key that is not stored on a SED, assuming there are no effective attacks against AES or its implementation in the drive hardware.

Contents  [hide] 
1	Importance
1.1	Data breach
1.2	Regulatory compliance
1.3	Preserving assets and the environment
1.3.1	Limitations
2	Differentiators
2.1	Full disk overwriting
2.2	Hardware support
2.3	Standards
2.4	Number of overwrites needed
3	E-waste and information security
4	See also
5	References
Importance[edit]
Information technology (IT) assets commonly hold large volumes of confidential data. Social security numbers, credit card numbers, bank details, medical history and classified information are often stored on computer hard drives or servers. These can inadvertently or intentionally make their way onto other media such as printer, USB, flash, Zip, Jaz, and REV drives.

Data breach[edit]
	This section may require cleanup to meet Wikipedia's quality standards. (January 2012)
Increased storage of sensitive data, combined with rapid technological change and the shorter lifespan of IT assets, has driven the need for permanent data erasure of electronic devices as they are retired or refurbished. Also, compromised networks and laptop theft and loss, as well as that of other portable media, are increasingly common sources of data breaches.

If data erasure does not occur when a disk is retired or lost, an organization or user faces a possibility that the data will be stolen and compromised, leading to identity theft, loss of corporate reputation, threats to regulatory compliance and financial impacts. Companies have spent nearly $5 million on average to recover when corporate data were lost or stolen.[2][dubious – discuss] High profile incidents of data theft include:

CardSystems Solutions (2005-06-19): Credit card breach exposes 40 million accounts.[3]
Lifeblood (2008-02-13): Missing laptops contain personal information including dates of birth and some Social Security numbers of 321,000.[4]
Hannaford (2008-03-17): Breach exposes 4.2 million credit, debit cards.[5]
Compass Bank (2008-03-21): Stolen hard drive contains 1,000,000 customer records.[6]
University of Florida College of Medicine, Jacksonville (2008-05-20): Photographs and identifying information of 1,900 on improperly disposed computer.[7]
Oklahoma Corporation Commission (2008-05-21): Server sold at auction compromises more than 5,000 Social Security numbers.[8]
Regulatory compliance[edit]
Strict industry standards and government regulations are in place that force organizations to mitigate the risk of unauthorized exposure of confidential corporate and government data. Regulations in the United States include HIPAA (Health Insurance Portability and Accountability Act); FACTA (The Fair and Accurate Credit Transactions Act of 2003); GLB (Gramm-Leach Bliley); Sarbanes-Oxley Act (SOx); and Payment Card Industry Data Security Standards (PCI DSS) and the Data Protection Act in the United Kingdom. Failure to comply can result in fines and damage to company reputation, as well as civil and criminal liability.

Preserving assets and the environment[edit]
Data erasure offers an alternative to physical destruction and degaussing for secure removal of all the disk data. Physical destruction and degaussing destroy the digital media, requiring disposal and contributing to electronic waste while negatively impacting the carbon footprint of individuals and companies.[9] Hard drives are nearly 100% recyclable and can be collected at no charge from a variety of hard drive recyclers after they have been sanitized.

Limitations[edit]
Data erasure may not work completely on flash based media, such as Solid State Drives and USB Flash Drives, as these devices can store remnant data which is inaccessible to the erasure technique, and data can be retrieved from the individual flash memory chips inside the device.[1] Data erasure through overwriting only works on hard drives that are functioning and writing to all sectors. Bad sectors cannot usually be overwritten, but may contain recoverable information. Bad sectors, however, may be invisible to the host system and thus to the erasing software. Disk encryption before use prevents this problem. Software-driven data erasure could also be compromised by malicious code.[10]

Differentiators[edit]
Software-based data erasure uses a special application to write a combination of ones and zeroes onto each hard disk drive sector. The level of security depends on the number of times the entire drive is written over.

Full disk overwriting[edit]
While there are many overwriting programs, only those capable of complete data erasure offer full security by destroying the data on all areas of a hard drive. Disk overwriting programs that cannot access the entire hard drive, including hidden/locked areas like the host protected area (HPA), device configuration overlay (DCO), and remapped sectors, perform an incomplete erasure, leaving some of the data intact. By accessing the entire hard drive, data erasure eliminates the risk of data remanence.

Data erasure can also bypass the BIOS and OS.[citation needed] Overwriting programs that operate through the BIOS and OS will not always perform a complete erasure due to altered or corrupted BIOS data and may report back a complete and successful erasure even if they do not access the entire hard disk, leaving the data accessible.

Hardware support[edit]
Data erasure can be deployed over a network to target multiple PCs rather than having to erase each one sequentially. In contrast with DOS-based overwriting programs that may not detect all network hardware, Linux-based data erasure software supports high-end server and storage area network (SAN) environments with hardware support for Serial ATA, Serial Attached SCSI (SAS) and Fibre Channel disks and remapped sectors. It operates directly with sector sizes such as 520, 524, and 528, removing the need to first reformat back to 512 sector size.

Standards[edit]
Many government and industry standards exist for software-based overwriting that removes the data. A key factor in meeting these standards is the number of times the data are overwritten. Also, some standards require a method to verify that all the data have been removed from the entire hard drive and to view the overwrite pattern. Complete data erasure should account for hidden areas, typically DCO, HPA and remapped sectors.

The 1995 edition of the National Industrial Security Program Operating Manual (DoD 5220.22-M) permitted the use of overwriting techniques to sanitize some types of media by writing all addressable locations with a character, its complement, and then a random character. This provision was removed in a 2001 change to the manual and was never permitted for Top Secret media, but it is still listed as a technique by many providers of the data erasure software.[11]

Data erasure software should provide the user with a validation certificate indicating that the overwriting procedure was completed properly. Data erasure software should[citation needed] also comply with requirements to erase hidden areas, provide a defects log list and list bad sectors that could not be overwritten.

Overwriting Standard	Date	Overwriting Rounds	Pattern	Notes
U.S. Navy Staff Office Publication NAVSO P-5239-26[12]	1993	3	A character, its complement, random	Verification is mandatory
U.S. Air Force System Security Instruction 5020[13]	1996	4	All zeros, all ones, any character	Verification is mandatory
Peter Gutmann's Algorithm	1996	1 to 35	Various, including all of the other listed methods	Originally intended for MFM and RLL disks, which are now obsolete
Bruce Schneier's Algorithm[14]	1996	7	All ones, all zeros, pseudo-random sequence five times	
U.S. DoD Unclassified Computer Hard Drive Disposition[15]	2001	3	A character, its complement, another pattern	
German Federal Office for Information Security[16]	2004	2-3	Non-uniform pattern, its complement	
Communications Security Establishment Canada ITSG-06[17]	2006	3	All ones or zeros, its complement, a pseudo-random pattern	For unclassified media
NIST SP-800-88[18]	2006	1	?	
U.S. National Industrial Security Program Operating Manual (DoD 5220.22-M)[11]	2006	?	?	No longer specifies any method.
NSA/CSS Storage Device Declassification Manual (SDDM)[19]	2007	0	?	Degauss or destroy only
Australian Government ICT Security Manual 2014 - Controls[20]	2014	1	Random pattern (only for disks bigger than 15 GB)	Degauss magnetic media or destroy Top Secret media
New Zealand Government Communications Security Bureau NZSIT 402[21]	2008	1	?	For data up to Confidential
British HMG Infosec Standard 5, Baseline Standard[22]	?	1	Random Pattern	Verification is mandatory
British HMG Infosec Standard 5, Enhanced Standard	?	3	All ones, all zeros, random	Verification is mandatory
Data can sometimes be recovered from a broken hard drive. However, if the platters on a hard drive are damaged, such as by drilling a hole through the drive (and the platters inside), then the data can only theoretically be recovered by bit-by-bit analysis of each platter with advanced forensic technology.

Number of overwrites needed[edit]
Data on floppy disks can sometimes be recovered by forensic analysis even after the disks have been overwritten once with zeros (or random zeros and ones).[23] This is not the case with modern hard drives:

According to the 2006 NIST Special Publication 800-88 Section 2.3 (p. 6): "Basically the change in track density and the related changes in the storage medium have created a situation where the acts of clearing and purging the media have converged. That is, for ATA disk drives manufactured after 2001 (over 15 GB) clearing by overwriting the media once is adequate to protect the media from both keyboard and laboratory attack."[18]
According to the 2006 Center for Magnetic Recording Research Tutorial on Disk Drive Data Sanitization Document (p. 8): "Secure erase does a single on-track erasure of the data on the disk drive. The U.S. National Security Agency published an Information Assurance Approval[citation needed] of single-pass overwrite, after technical testing at CMRR showed that multiple on-track overwrite passes gave no additional erasure."[24] "Secure erase" is a utility built into modern ATA hard drives that overwrites all data on a disk, including remapped (error) sectors.[25]
Further analysis by Wright et al. seems to also indicate that one overwrite is all that is generally required.[26]
E-waste and information security[edit]

The e-waste centre of Agbogbloshie, Ghana. Multimillion-dollar agreements from United States security institutions such as the Defense Intelligence Agency (DIA), the Transportation Security Administration and Homeland Security have all resurfaced in Agbogbloshie.[27][28]
E-waste presents a potential security threat to individuals and exporting countries. Hard drives that are not properly erased before the computer is disposed of can be reopened, exposing sensitive information. Credit card numbers, private financial data, account information and records of online transactions can be accessed by most willing individuals. Organized criminals in Ghana commonly search the drives for information to use in local scams.[27]

Government contracts have been discovered on hard drives found in Agbogbloshie, the unregulated e-waste centre in Ghana. Multimillion-dollar agreements from United States security institutions such as the Defense Intelligence Agency (DIA), the Transportation Security Administration and Homeland Security have all resurfaced in Agbogbloshie.[27][28]
Data recovery
From Wikipedia, the free encyclopedia

[hide]This article has multiple issues. Please help improve it (see how) or discuss these issues on the talk page.
This article needs additional citations for verification. (February 2012)
This article is written like a manual or guidebook. (April 2016)
In computing, data recovery is a process of salvaging inaccessible data from corrupted or damaged secondary storage, removable media or files, when the data they store cannot be accessed in a normal way. The data is most often salvaged from storage media such as internal or external hard disk drives (HDDs), solid-state drives (SSDs), USB flash drives, magnetic tapes, CDs, DVDs, RAID subsystems, and other electronic devices. Recovery may be required due to physical damage to the storage device or logical damage to the file system that prevents it from being mounted by the host operating system (OS).

The most common data recovery scenario involves an operating system failure, malfunction of a storage device, accidental damage or deletion, etc. (typically, on a single-drive, single-partition, single-OS system), in which case the goal is simply to copy all wanted files to another drive. This can be easily accomplished using a Live CD, many of which provide a means to mount the system drive and backup drives or removable media, and to move the files from the system drive to the backup media with a file manager or optical disc authoring software. Such cases can often be mitigated by disk partitioning and consistently storing valuable data files (or copies of them) on a different partition from the replaceable OS system files.

Another scenario involves a drive-level failure, such as a compromised file system or drive partition, or a hard disk drive failure. In any of these cases, the data cannot be easily read. Depending on the situation, solutions involve repairing the file system, partition table or master boot record, or drive recovery techniques ranging from software-based recovery of corrupted data, hardware- and software-based recovery of damaged service areas (also known as the hard disk drive's "firmware"), to hardware replacement on a physically damaged drive. If a drive recovery is necessary, the drive itself has typically failed permanently, and the focus is rather on a one-time recovery, salvaging whatever data can be read.

In a third scenario, files have been "deleted" from a storage medium. Typically, the contents of deleted files are not removed immediately from the drive; instead, references to them in the directory structure are removed, and the space they occupy is made available for later overwriting. For the end users, deleted files are not discoverable through a standard file manager, but that data still technically exists on the drive. In the meantime, the original file contents remain, often in a number of disconnected fragments, and may be recoverable.

The term "data recovery" is also used in the context of forensic applications or espionage, where data which have been encrypted or hidden, rather than damaged, are recovered.

Contents  [hide] 
1	Physical damage
1.1	Recovery techniques
1.1.1	Hardware repair
2	Logical damage
2.1	Corrupt partitions and file systems, media errors
2.2	Overwritten data
3	Remote data recovery
4	Four phases of data recovery
5	See also
6	References
7	Further reading
Physical damage[edit]
See also: Data recovery hardware
A wide variety of failures can cause physical damage to storage media, which may result from human errors and natural disasters. CD-ROMs can have their metallic substrate or dye layer scratched off; hard disks can suffer any of several mechanical failures, such as head crashes and failed motors; tapes can simply break. Physical damage always causes at least some data loss, and in many cases the logical structures of the file system are damaged as well. Any logical damage must be dealt with before files can be salvaged from the failed media.

Most physical damage cannot be repaired by end users. For example, opening a hard disk drive in a normal environment can allow airborne dust to settle on the platter and become caught between the platter and the read/write head, causing new head crashes that further damage the platter and thus compromise the recovery process. Furthermore, end users generally do not have the hardware or technical expertise required to make these repairs. Consequently, data recovery companies are often employed to salvage important data with the more reputable ones using class 100 dust- and static-free cleanrooms.[1]

Recovery techniques[edit]
Recovering data from physically damaged hardware can involve multiple techniques. Some damage can be repaired by replacing parts in the hard disk. This alone may make the disk usable, but there may still be logical damage. A specialized disk-imaging procedure is used to recover every readable bit from the surface. Once this image is acquired and saved on a reliable medium, the image can be safely analyzed for logical damage and will possibly allow much of the original file system to be reconstructed.

Hardware repair[edit]

Media that has suffered a catastrophic electronic failure requires data recovery in order to salvage its contents.
A common misconception is that a damaged printed circuit board (PCB) may be simply replaced during recovery procedures by an identical PCB from a healthy drive. While this may work in rare circumstances on hard disk drives manufactured before 2003, it will not work on newer drives. Electronics boards of modern drives usually contain drive-specific adaptation data required for accessing their system areas, so the related componentry needs to be either reprogrammed (if possible) or unsoldered and transferred between two electronics boards.[2][3]

Each hard disk drive has what is called a system area or service area; this portion of the drive, which is not directly accessible to the end user, usually contains drive's firmware and adaptive data that helps the drive operate within normal parameters.[4] One function of the system area is to log defective sectors within the drive; essentially telling the drive where it can and cannot write data.

The sector lists are also stored on various chips attached to the PCB, and they are unique to each hard disk drive. If the data on the PCB do not match what is stored on the platter, then the drive will not calibrate properly.[5] In most cases the drive heads will click because they are unable to find the data matching what is stored on the PCB.

Logical damage[edit]
See also: List of data recovery software

Result of a failed data recovery from a hard disk drive.
The term "logical damage" refers to situations in which the error is not a problem in the hardware and requires software-level solutions.

Corrupt partitions and file systems, media errors[edit]
In some cases, data on a hard disk drive can be unreadable due to damage to the partition table or file system, or to (intermittent) media errors. In the majority of these cases, at least a portion of the original data can be recovered by repairing the damaged partition table or file system using specialized data recovery software such as Testdisk or M3 RAW Drive Recovery;[6] software like dd rescue can image media despite intermittent errors, and image raw data when there is partition table or file system damage. This type of data recovery can be performed by people without expertise in drive hardware, as it requires no special physical equipment or access to platters.

Sometimes data can be recovered using relatively simple methods and tools; more serious cases can require expert intervention, particularly if parts of files are irrecoverable. Data carving is the recovery of parts of damaged files using knowledge of their structure.

Overwritten data[edit]
See also: Data erasure
After data has been physically overwritten on a hard disk drive, it is generally assumed that the previous data are no longer possible to recover. In 1996, Peter Gutmann, a computer scientist, presented a paper that suggested overwritten data could be recovered through the use of magnetic force microscope.[7] In 2001, he presented another paper on a similar topic.[8] To guard against this type of data recovery, Gutmann and Colin Plumb designed a method of irreversibly scrubbing data, known as the Gutmann method and used by several disk-scrubbing software packages.

Substantial criticism has followed, primarily dealing with the lack of any concrete examples of significant amounts of overwritten data being recovered.[9] Although Gutmann's theory may be correct, there is no practical evidence that overwritten data can be recovered, while research has shown to support that overwritten data cannot be recovered.[specify][10][11][12]

Solid-state drives (SSD) overwrite data differently from hard disk drives (HDD) which makes at least some of their data easier to recover. Most SSDs use flash memory to store data in pages and blocks, referenced by logical block addresses (LBA) which are managed by the flash translation layer (FTL). When the FTL modifies a sector it writes the new data to another location and updates the map so the new data appear at the target LBA. This leaves the pre-modification data in place, with possibly many generations, and recoverable by data recovery software.

Remote data recovery[edit]
Recovery experts do not always need to have physical access to the damaged hardware. When the lost data can be recovered by software techniques, they can often perform the recovery using remote access software over the Internet, LAN or other connection to the physical location of the damaged media. The process is essentially no different from what the end user could perform by themselves.[13]

Remote recovery requires a stable connection with an adequate bandwidth. However, it is not applicable where access to the hardware is required, as in cases of physical damage.

Four phases of data recovery[edit]
Usually, there are four phases when it comes to successful data recovery, though that can vary depending on the type of data corruption and recovery required.[14]

Phase 1: Repair the hard disk drive
Repair the hard disk drive so it is running in some form, or at least in a state suitable for reading the data from it. For example, if heads are bad they need to be changed; if the PCB is faulty then it needs to be fixed or replaced; if the spindle motor is bad the platters and heads should be moved to a new drive.
Phase 2: Image the drive to a new drive or a disk image file
When a hard disk drive fails, the importance of getting the data off the drive is the top priority. The longer a faulty drive is used, the more likely further data loss is to occur. Creating an image of the drive will ensure that there is a secondary copy of the data on another device, on which it is safe to perform testing and recovery procedures without harming the source.
Phase 3: Logical recovery of files, partition, MBR and MFT
After the drive has been cloned to a new drive, it is suitable to attempt the retrieval of lost data. If the drive has failed logically, there are a number of reasons for that. Using the clone it may be possible to repair the partition table, MBR and MFT in order to read the file system's data structure and retrieve stored data.
Phase 4: Repair damaged files that were retrieved
Data damage can be caused when, for example, a file is written to a sector on the drive that has been damaged. This is the most common cause in a failing drive, meaning that data needs to be reconstructed to become readable. Corrupted documents can be recovered by several software methods or by manually reconstructing the document using a hex editor.
Electronic waste
From Wikipedia, the free encyclopedia
For the EC directive, see Waste Electrical and Electronic Equipment Directive.

[hide]This article has multiple issues. Please help improve it (see how) or discuss these issues on the talk page.
This article may require cleanup to meet Wikipedia's quality standards. The specific problem is: fix References (May 2013)
This article needs additional citations for verification. (May 2013)

Defective and obsolete electronic equipment
Electronic waste or e-waste describes discarded electrical or electronic devices. Used electronics which are destined for reuse, resale, salvage, recycling or disposal are also considered e-waste. Informal processing of e-waste in developing countries can lead to adverse human health effects and environmental pollution.

Electronic scrap components, such as CPUs, contain potentially harmful components such as lead, cadmium, beryllium, or brominated flame retardants. Recycling and disposal of e-waste may involve significant risk to workers and communities in developed countries[1] and great care must be taken to avoid unsafe exposure in recycling operations and leaking of materials such as heavy metals from landfills and incinerator ashes.[2]

Contents  [hide] 
1	Definition
2	Amount of electronic waste world-wide
3	Global trade issues
3.1	Trade
3.2	Guiyu
3.3	Other informal e-waste recycling sites
4	Environmental impact
5	Information security
6	E-waste management
6.1	Recycling
6.2	Consumer awareness efforts
6.3	Processing techniques
6.4	Benefits of recycling
7	Electronic waste substances
7.1	Hazardous
7.2	Generally non-hazardous
8	See also
9	References
10	Further reading
11	External links
Definition[edit]



Hoarding (left), disassembling (center) and collecting (right) electronic waste in Bengaluru, India
"Electronic waste" may be defined as discarded computers, office electronic equipment, entertainment device electronics, mobile phones, television sets, and refrigerators. This includes used electronics which are destined for reuse, resale, salvage, recycling, or disposal. Others are re-usables (working and repairable electronics) and secondary scrap (copper, steel, plastic, etc.) to be "commodities", and reserve the term "waste" for residue or material which is dumped by the buyer rather than recycled, including residue from reuse and recycling operations. Because loads of surplus electronics are frequently commingled (good, recyclable, and non-recyclable), several public policy advocates apply the term "e-waste" broadly to all surplus electronics. Cathode ray tubes (CRTs) are considered one of the hardest types to recycle.[3]

CRTs have relatively high concentration of lead and phosphors (not to be confused with phosphorus), both of which are necessary for the display. The United States Environmental Protection Agency (EPA) includes discarded CRT monitors in its category of "hazardous household waste"[4] but considers CRTs that have been set aside for testing to be commodities if they are not discarded, speculatively accumulated, or left unprotected from weather and other damage.

The EU and its member states operate a system via the European Waste Catalogue (EWC)- a European Council Directive, which is interpreted into "member state law". In the UK (an EU member state). This is in the form of the List of Wastes Directive. However, the list (and EWC) gives broad definition (EWC Code 16 02 13*) of Hazardous Electronic wastes, requiring "waste operators" to employ the Hazardous Waste Regulations (Annex 1A, Annex 1B) for refined definition. Constituent materials in the waste also require assessment via the combination of Annex II and Annex III, again allowing operators to further determine whether a waste is hazardous.[5]

Debate continues over the distinction between "commodity" and "waste" electronics definitions. Some exporters are accused of deliberately leaving difficult-to-recycle, obsolete, or non-repairable equipment mixed in loads of working equipment (though this may also come through ignorance, or to avoid more costly treatment processes). Protectionists may broaden the definition of "waste" electronics in order to protect domestic markets from working secondary equipment.

The high value of the computer recycling subset of electronic waste (working and reusable laptops, desktops, and components like RAM) can help pay the cost of transportation for a larger number of worthless pieces than can be achieved with display devices, which have less (or negative) scrap value. In A 2011 report, "Ghana E-Waste Country Assessment",[6] found that of 215,000 tons of electronics imported to Ghana, 30% were brand new and 70% were used. Of the used product, the study concluded that 15% was not reused and was scrapped or discarded. This contrasts with published but uncredited claims that 80% of the imports into Ghana were being burned in primitive conditions.

Amount of electronic waste world-wide[edit]

A fragment of discarded circuit board.
Rapid changes in technology, changes in media (tapes, software, MP3), falling prices, and planned obsolescence have resulted in a fast-growing surplus of electronic waste around the globe. Technical solutions are available, but in most cases a legal framework, a collection, logistics, and other services need to be implemented before a technical solution can be applied.

Display units (CRT, LCD, LED monitors), processors (CPU, GPU, or APU chips), memory (DRAM or SRAM), and audio components have different useful lives. Processors are most frequently out-dated (by software no longer being optimized) and are more likely to become "e-waste", while display units are most often replaced while working without repair attempts, due to changes in wealthy nation appetites for new display technology. This problem could potentially be solved with Modular Smartphones or Phonebloks. These types of phones are more durable and have the technology to change certain parts of the phone making them more environmentally friendly. Being able to simply replace the part of the phone that is broken will reduce e-waste.[7] An estimated 50 million tons of E-waste are produced each year.[2] The USA discards 30 million computers each year and 100 million phones are disposed of in Europe each year. The Environmental Protection Agency estimates that only 15–20% of e-waste is recycled, the rest of these electronics go directly into landfills and incinerators.[8][9]

According to a report by UNEP titled, "Recycling – from E-Waste to Resources," the amount of e-waste being produced – including mobile phones and computers – could rise by as much as 500 percent over the next decade in some countries, such as India.[10] The United States is the world leader in producing electronic waste, tossing away about 3 million tons each year.[11] China already produces about 2.3 million tons (2010 estimate) domestically, second only to the United States. And, despite having banned e-waste imports, China remains a major e-waste dumping ground for developed countries.[11]

Society today revolves around technology and by the constant need for the newest and most high tech products we are contributing to mass amount of e-waste.[12] Since the invention of the iPhone, cell phones have become the top source of e-waste products because they are not made to last more than two years. Electrical waste contains hazardous but also valuable and scarce materials. Up to 60 elements can be found in complex electronics.[13] As of 2013, Apple has sold over 796 million iDevices (iPod, iPhone, iPad). Cell phone companies make cell phones that are not made to last so that the consumer will purchase new phones. Companies give these products such short life spans because they know that the consumer will want a new product and will buy it if they make it.[14] In the United States, an estimated 70% of heavy metals in landfills comes from discarded electronics.[15][16]

While there is agreement that the number of discarded electronic devices is increasing, there is considerable disagreement about the relative risk (compared to automobile scrap, for example), and strong disagreement whether curtailing trade in used electronics will improve conditions, or make them worse. According to an article in Motherboard, attempts to restrict the trade have driven reputable companies out of the supply chain, with unintended consequences.[17]

Global trade issues[edit]
See also: Global Waste Trade and Electronic waste by country

Electronic waste is often exported to developing countries.

4.5-volt, D, C, AA, AAA, AAAA, A23, 9-volt, CR2032, and LR44 cells are all recyclable in most countries.

The E-waste centre of Agbogbloshie, Ghana, where electronic waste is burnt and disassembled with no safety or environmental considerations.
One theory is that increased regulation of electronic waste and concern over the environmental harm in mature economies creates an economic disincentive to remove residues prior to export. Critics of trade in used electronics maintain that it is still too easy for brokers calling themselves recyclers to export unscreened electronic waste to developing countries, such as China,[18] India and parts of Africa, thus avoiding the expense of removing items like bad cathode ray tubes (the processing of which is expensive and difficult). The developing countries have become toxic dump yards of e-waste. Proponents of international trade point to the success of fair trade programs in other industries, where cooperation has led to creation of sustainable jobs, and can bring affordable technology in countries where repair and reuse rates are higher.

Defenders of the trade[who?] in used electronics say that extraction of metals from virgin mining has been shifted to developing countries. Recycling of copper, silver, gold, and other materials from discarded electronic devices is considered better for the environment than mining. They also state that repair and reuse of computers and televisions has become a "lost art" in wealthier nations, and that refurbishing has traditionally been a path to development.

South Korea, Taiwan, and southern China all excelled in finding "retained value" in used goods, and in some cases have set up billion-dollar industries in refurbishing used ink cartridges, single-use cameras, and working CRTs. Refurbishing has traditionally been a threat to established manufacturing, and simple protectionism explains some criticism of the trade. Works like "The Waste Makers" by Vance Packard explain some of the criticism of exports of working product, for example the ban on import of tested working Pentium 4 laptops to China, or the bans on export of used surplus working electronics by Japan.

Opponents of surplus electronics exports argue that lower environmental and labor standards, cheap labor, and the relatively high value of recovered raw materials leads to a transfer of pollution-generating activities, such as smelting of copper wire. In China, Malaysia, India, Kenya, and various African countries, electronic waste is being sent to these countries for processing, sometimes illegally. Many surplus laptops are routed to developing nations as "dumping grounds for e-waste".[19]

Because the United States has not ratified the Basel Convention or its Ban Amendment, and has few domestic federal laws forbidding the export of toxic waste, the Basel Action Network estimates that about 80% of the electronic waste directed to recycling in the U.S. does not get recycled there at all, but is put on container ships and sent to countries such as China.[20][21][22][23] This figure is disputed as an exaggeration by the EPA, the Institute of Scrap Recycling Industries, and the World Reuse, Repair and Recycling Association.

Independent research by Arizona State University showed that 87–88% of imported used computers did not have a higher value than the best value of the constituent materials they contained, and that "the official trade in end-of-life computers is thus driven by reuse as opposed to recycling".[24]

Trade[edit]
Proponents of the trade say growth of internet access is a stronger correlation to trade than poverty. Haiti is poor and closer to the port of New York than southeast Asia, but far more electronic waste is exported from New York to Asia than to Haiti. Thousands of men, women, and children are employed in reuse, refurbishing, repair, and re-manufacturing, unsustainable industries in decline in developed countries. Denying developing nations access to used electronics may deny them sustainable employment, affordable products, and internet access, or force them to deal with even less scrupulous suppliers. In a series of seven articles for The Atlantic, Shanghai-based reporter Adam Minter describes many of these computer repair and scrap separation activities as objectively sustainable.[25]

Opponents of the trade argue that developing countries utilize methods that are more harmful and more wasteful. An expedient and prevalent method is simply to toss equipment onto an open fire, in order to melt plastics and to burn away non-valuable metals. This releases carcinogens and neurotoxins into the air, contributing to an acrid, lingering smog. These noxious fumes include dioxins and furans. Bonfire refuse can be disposed of quickly into drainage ditches or waterways feeding the ocean or local water supplies.[23]

In June 2008, a container of electronic waste, destined from the Port of Oakland in the U.S. to Sanshui District in mainland China, was intercepted in Hong Kong by Greenpeace.[26] Concern over exports of electronic waste were raised in press reports in India,[27][28] Ghana,[29][30][31] C?te d'Ivoire,[32] and Nigeria.[33]

Guiyu[edit]
Main article: Electronic waste in China
Guiyu in the Shantou region of China is a massive electronic waste processing community.[20][34][35] It is often referred to as the "e-waste capital of the world." Traditionally, Guiyu was an agricultural community; however, in the mid-1990s it transformed into an e-waste recycling center involving over 75% of the local households and an additional 100,000 migrant workers.[36] Thousands of individual workshops employ laborers to snip cables, pry chips from circuit boards, grind plastic computer cases into particles, and dip circuit boards in acid baths to dissolve the precious metals. Others work to strip insulation from all wiring in an attempt to salvage tiny amounts of copper wire.[37] Uncontrolled burning, disassembly, and disposal has led to a number of environmental problems such as groundwater contamination, atmospheric pollution, and water pollution either by immediate discharge or from surface runoff (especially near coastal areas), as well as health problems including occupational safety and health effects among those directly and indirectly involved, due to the methods of processing the waste.

A number of studies have been conducted to measure a number of chemicals associated with informal e-waste recycling in the populations. One study enrolled children from Guiyu and a control site 50 km away to measure blood lead levels (BLLs).[38] The average BLL in Guiyu was 15.3 ug/dL compared to 9.9 ug/dL in the control site. In the United States, the CDC has set a reference level for blood lead at 5 ug/dL.[39] High levels of lead in young children can impact IQ and the development of the central nervous system. The highest concentrations of lead were found in the children of parents whose workshop dealt with circuit boards and the lowest was among those who recycled plastic.[40]

Six of the many villages in Guiyu specialize in circuit-board disassembly, seven in plastics and metals reprocessing, and two in wire and cable disassembly. Greenpeace, an environmental group, sampled dust, soil, river sediment and groundwater in Guiyu. They found very high levels of toxic heavy metals and organic contaminants in both places.[41] Lai Yun, a campaigner for the group found "over 10 poisonous metals, such as lead, mercury and cadmium."

Guiyu is only one example of digital dumps but similar places can be found across the world in Nigeria, Ghana, and India.[42] With amounts of e-waste growing rapidly each year urgent solutions are required. While the waste continues to flow into digital dumps like Guiyu there are measures that can help reduce the flow of e-waste.[43]

A suggested preventative step involves the major electronics firms removing the worst chemicals in their products in order to make them safer and easier to recycle.

Other informal e-waste recycling sites[edit]
Guiyu is likely one of the oldest and largest informal e-waste recycling sites in the world, however, there are many sites worldwide, including India, Ghana, Nigeria, and the Philippines. Most research involving informal e-waste recycling has been done in Guiyu, but there are a handful of studies that describe exposure levels in e-waste workers, the community, and the environment.

Bangalore, located in southern India, is often referred as the "Silicon Valley of India" and has a growing informal e-waste recycling sector.[44][45] Hair samples were collected from workers at an e-waste recycling facility and a e-waste recycling slum community in Bangalore.[46] Levels of V, Cr, Mn, Mo, Sn, Tl, and Pb were significantly higher in the workers at the e-waste recycling facility compared to the e-waste workers in the slum community. However, Co, Ag, Cd, and Hg levels were significantly higher in the slum community workers compared to the facility workers.

A study in Ghana found higher levels of urinary PAH-metabolites in e-waste workers compared to unexposed controls.[47] They also found a greater frequency of complaints of cough, chest pain, and vertigo from those exposed to emissions from the e-waste recycling processes.

Environmental impact[edit]

Old keyboards and one mouse.
The processes of dismantling and disposing of electronic waste in developing countries lead to a number of environmental impacts as illustrated in the graphic. Liquid and atmospheric releases end up in bodies of water, groundwater, soil, and air and therefore in land and sea animals – both domesticated and wild, in crops eaten by both animals and human, and in drinking water.[48]

One study of environmental effects in Guiyu, China found the following:[49]

Airborne dioxins – one type found at 100 times levels previously measured
Levels of carcinogens in duck ponds and rice paddies exceeded international standards for agricultural areas and cadmium, copper, nickel, and lead levels in rice paddies were above international standards
Heavy metals found in road dust – lead over 300 times that of a control village's road dust and copper over 100 times
The environmental impact of the processing of different electronic waste components

E-Waste Component	Process Used	Potential Environmental Hazard
Cathode ray tubes (used in TVs, computer monitors, ATM, video cameras, and more)	Breaking and removal of yoke, then dumping	Lead, barium and other heavy metals leaching into the ground water and release of toxic phosphor
Printed circuit board (image behind table – a thin plate on which chips and other electronic components are placed)	De-soldering and removal of computer chips; open burning and acid baths to remove metals after chips are removed.	Air emissions and discharge into rivers of glass dust, tin, lead, brominated dioxin, beryllium cadmium, and mercury
Chips and other gold plated components	Chemical stripping using nitric and hydrochloric acid and burning of chips	PAHs, heavy metals, brominated flame retardants discharged directly into rivers acidifying fish and flora. Tin and lead contamination of surface and groundwater. Air emissions of brominated dioxins, heavy metals, and PAHs
Plastics from printers, keyboards, monitors, etc.	Shredding and low temp melting to be reused	Emissions of brominated dioxins, heavy metals and hydrocarbons
Computer wires	Open burning and stripping to remove copper	PAHs released into air, water and soil.
[50]

Information security[edit]
E-waste presents a potential security threat to individuals and exporting countries. Hard drives that are not properly erased before the computer is disposed of can be reopened, exposing sensitive information. Credit card numbers, private financial data, account information, and records of online transactions can be accessed by most willing individuals. Organized criminals in Ghana commonly search the drives for information to use in local scams.[51]

Government contracts have been discovered on hard drives found in Agbogbloshie. Multimillion-dollar agreements from United States security institutions such as the Defense Intelligence Agency (DIA), the Transportation Security Administration and Homeland Security have all resurfaced in Agbogbloshie.[51][52]

E-waste management[edit]
Recycling[edit]

Computer monitors are typically packed into low stacks on wooden pallets for recycling and then shrink-wrapped.
See also: Computer recycling
Audiovisual components, televisions, VCRs, stereo equipment, mobile phones, other handheld devices, and computer components contain valuable elements and substances suitable for reclamation, including lead, copper, and gold.

One of the major challenges is recycling the printed circuit boards from the electronic wastes. The circuit boards contain such precious metals as gold, silver, platinum, etc. and such base metals as copper, iron, aluminum, etc. One way e-waste is processed is by melting circuit boards, burning cable sheathing to recover copper wire and open- pit acid leaching for separating metals of value.[53] Conventional method employed is mechanical shredding and separation but the recycling efficiency is low. Alternative methods such as cryogenic decomposition have been studied for printed circuit board recycling,[54] and some other methods are still under investigation.

As properly disposing of or reusing electronics can help prevent health problems, reduce greenhouse-gas emissions and create jobs,[55] there have been calls to reform "the methodology for e-waste disposal and re-use in developing countries"[56] with reuse and refurbishing offering a more environmentally friendly and socially conscious alternative to downcycling processes.

Consumer awareness efforts[edit]
Globe icon.
The examples and perspective in this section may not represent a worldwide view of the subject. Please improve this article and discuss the issue on the talk page. (December 2011)
The U.S. Environmental Protection Agency encourages electronic recyclers to become certified by demonstrating to an accredited, independent third party auditor that they meet specific standards to safely recycle and manage electronics. This works to ensure the highest environmental standards are being maintained. Two certifications for electronic recyclers currently exist and are endorsed by the EPA. Customers are encouraged to choose certified electronics recyclers. Responsible electronics recycling reduces environmental and human health impacts, increases the use of reusable and refurbished equipment and reduces energy use while conserving limited resources. The two EPA-endorsed certification programs are: Responsible Recyclers Practices (R2) and E-Stewards. Certified companies ensure they are meeting strict environmental standards which maximize reuse and recycling, minimize exposure to human health or the environment, ensure safe management of materials and require destruction of all data used on electronics.[57] Certified electronics recyclers have demonstrated through audits and other means that they continually meet specific high environmental standards and safely manage used electronics. Once certified, the recycler is held to the particular standard by continual oversight by the independent accredited certifying body. A certification board accredits and oversees certifying bodies to ensure that they meet specific responsibilities and are competent to audit and provide certification.[58]

Some U.S. retailers offer opportunities for consumer recycling of discarded electronic devices.[59][60]

In the US, the Consumer Electronics Association (CEA) urges consumers to dispose properly of end-of-life electronics through its recycling locator at www.GreenerGadgets.org. This list only includes manufacturer and retailer programs that use the strictest standards and third-party certified recycling locations, to provide consumers assurance that their products will be recycled safely and responsibly. CEA research has found that 58 percent of consumers know where to take their end-of-life electronics, and the electronics industry would very much like to see that level of awareness increase. Consumer electronics manufacturers and retailers sponsor or operate more than 5,000 recycling locations nationwide and have vowed to recycle one billion pounds annually by 2016,[61] a sharp increase from 300 million pounds industry recycled in 2010.

The Sustainable Materials Management Electronic Challenge was created by the United States Environmental Protection Agency (EPA). Participants of the Challenge are manufacturers of electronics and electronic retailers. These companies collect end-of-life (EOL) electronics at various locations and send them to a certified, third-party recycler. Program participants are then able publicly promote and report 100% responsible recycling for their companies.[62]

The Electronics TakeBack Coalition[63] is a campaign aimed at protecting human health and limiting environmental effects where electronics are being produced, used, and discarded. The ETBC aims to place responsibility for disposal of technology products on electronic manufacturers and brand owners, primarily through community promotions and legal enforcement initiatives. It provides recommendations for consumer recycling and a list of recyclers judged environmentally responsible.[64]

The Certified Electronics Recycler program[65] for electronic recyclers is a comprehensive, integrated management system standard that incorporates key operational and continual improvement elements for quality, environmental and health and safety (QEH&S) performance.

The grassroots Silicon Valley Toxics Coalition focuses on promoting human health and addresses environmental justice problems resulting from toxins in technologies.

The World Reuse, Repair, and Recycling Association (wr3a.org) is an organization dedicated to improving the quality of exported electronics, encouraging better recycling standards in importing countries, and improving practices through "Fair Trade" principles.

Take Back My TV[66] is a project of The Electronics TakeBack Coalition and grades television manufacturers to find out which are responsible and which are not.

The e-Waste Association of South Africa (eWASA)[67] has been instrumental in building a network of e-waste recyclers and refurbishers in the country. It continues to drive the sustainable, environmentally sound management of all e-waste in South Africa.

E-Cycling Central is a website from the Electronic Industry Alliance which allows you to search for electronic recycling programs in your state. It lists different recyclers by state to find reuse, recycle, or find donation programs across the country.[68]

Ewasteguide.info is a Switzerland-based website dedicated to improving the e-waste situation in developing and transitioning countries. The site contains news, events, case studies, and more.[69]

StEP: Solving the E-Waste Problem This website of StEP, an initiative founded by various UN organizations to develop strategies to solve the e-waste problem, follows its activities and programs.[70][71]

Processing techniques[edit]

Recycling the lead from batteries.
In many developed countries, electronic waste processing usually first involves dismantling the equipment into various parts (metal frames, power supplies, circuit boards, plastics), often by hand, but increasingly by automated shredding equipment. A typical example is the NADIN electronic waste processing plant in Novi Iskar, Bulgaria—the largest facility of its kind in Eastern Europe.[72][73] The advantages of this process are the human's ability to recognize and save working and repairable parts, including chips, transistors, RAM, etc. The disadvantage is that the labor is cheapest in countries with the lowest health and safety standards.

In an alternative bulk system,[74] a hopper conveys material for shredding into an unsophisticated mechanical separator, with screening and granulating machines to separate constituent metal and plastic fractions, which are sold to smelters or plastics recyclers. Such recycling machinery is enclosed and employs a dust collection system. Some of the emissions are caught by scrubbers and screens. Magnets, eddy currents, and Trommel screens are employed to separate glass, plastic, and ferrous and nonferrous metals, which can then be further separated at a smelter.

Leaded glass from CRTs is reused in car batteries, ammunition, and lead wheel weights, or sold to foundries as a fluxing agent in processing raw lead ore. Copper, gold, palladium, silver and tin are valuable metals sold to smelters for recycling. Hazardous smoke and gases are captured, contained and treated to mitigate environmental threat. These methods allow for safe reclamation of all valuable computer construction materials. Hewlett-Packard product recycling solutions manager Renee St. Denis describes its process as: "We move them through giant shredders about 30 feet tall and it shreds everything into pieces about the size of a quarter. Once your disk drive is shredded into pieces about this big, it's hard to get the data off".[75]

An ideal electronic waste recycling plant combines dismantling for component recovery with increased cost-effective processing of bulk electronic waste.

Reuse is an alternative option to recycling because it extends the lifespan of a device. Devices still need eventual recycling, but by allowing others to purchase used electronics, recycling can be postponed and value gained from device use.

Benefits of recycling[edit]
Recycling raw materials from end-of-life electronics is the most effective solution to the growing e-waste problem. Most electronic devices contain a variety of materials, including metals that can be recovered for future uses. By dismantling and providing reuse possibilities, intact natural resources are conserved and air and water pollution caused by hazardous disposal is avoided. Additionally, recycling reduces the amount of greenhouse gas emissions caused by the manufacturing of new products.[76] Another benefit of recycling e-waste is that many of the materials can be recycled and re-used again. Materials that can be recycled include "ferrous (iron-based) and non-ferrous metals, glass, and various types of plastic." “Non-ferrous metals, mainly aluminum and copper can all be re-smelted and re-manufactured. Ferrous metals such as steel and iron can be also be re-used."[77] Due to the recent surge in popularity in 3D printing, certain 3D printers have been designed (FDM variety) to produce waste that can be easily recycled which decreases the amount of harmful pollutants in the atmosphere.[78] The excess plastic from these printers that comes out as a byproduct can also be reused to create new 3D printed creations.[79]

Benefits of recycling are extended when responsible recycling methods are used. In the U.S., responsible recycling aims to minimize the dangers to human health and the environment that disposed and dismantled electronics can create. Responsible recycling ensures best management practices of the electronics being recycled, worker health and safety, and consideration for the environment locally and abroad.[80] In Europe, metals that are recycled are returned to companies of origin at a reduced cost.[81] Through an extremely committed recycling system, manufacturers in Japan have been pushed to make their products more sustainable. Since many companies were responsible for the recycling of their own products, this imposed responsibility on manufacturers requiring many to redesign their infrastructure. As a result, manufacturers in Japan have the added option to sell the recycled metals.[82]

Electronic waste substances[edit]

Several sizes of button and coin cell with 2 9v batteries as a size comparison. They are all recycled in many countries since they contain lead, mercury and cadmium.
Some computer components can be reused in assembling new computer products, while others are reduced to metals that can be reused in applications as varied as construction, flatware, and jewelry.

Substances found in large quantities include epoxy resins, fiberglass, PCBs, PVC (polyvinyl chlorides), thermosetting plastics, lead, tin, copper, silicon, beryllium, carbon, iron and aluminium.

Elements found in small amounts include cadmium, mercury, and thallium.[83]

Elements found in trace amounts include americium, antimony, arsenic, barium, bismuth, boron, cobalt, europium, gallium, germanium, gold, indium, lithium, manganese, nickel, niobium, palladium, platinum, rhodium, ruthenium, selenium, silver, tantalum, terbium, thorium, titanium, vanadium, and yttrium.

Almost all electronics contain lead and tin (as solder) and copper (as wire and printed circuit board tracks), though the use of lead-free solder is now spreading rapidly. The following are ordinary applications:

Hazardous[edit]

Recyclers in the street in S?o Paulo, Brazil with old computers
E-Waste Component	Processed Used	Adverse Health Effects
Americium	The radioactive source in smoke alarms.	It is known to be carcinogenic.[84]
Lead	Solder, CRT monitor glass, lead-acid batteries, some formulations of PVC. A typical 15-inch cathode ray tube may contain 1.5 pounds of lead,[4] but other CRTs have been estimated as having up to 8 pounds of lead.	Adverse effects of lead exposure include impaired cognitive function, behavioral disturbances, attention deficits, hyperactivity, conduct problems and lower IQ.[85] These effects are most damaging to children whose developing nervous systems are very susceptible to damage caused by lead, cadmium, and mercury.[86]
Mercury	Found in fluorescent tubes (numerous applications), tilt switches (mechanical doorbells, thermostats),[87] and flat screen monitors.	Health effects include sensory impairment, dermatitis, memory loss, and muscle weakness. Exposure in-utero causes fetal deficits in motor function, attention and verbal domains.[85] Environmental effects in animals include death, reduced fertility, and slower growth and development.
Cadmium	Found in light-sensitive resistors, corrosion-resistant alloys for marine and aviation environments, and nickel-cadmium batteries. The most common form of cadmium is found in Nickel-cadmium rechargeable batteries. These batteries tend to contain between 6 and 18% cadmium. The sale of Nickel-Cadmium batteries has been banned in the European Union except for medical use. When not properly recycled it can leach into the soil, harming microorganisms and disrupting the soil ecosystem. Exposure is caused by proximity to hazardous waste sites and factories and workers in the metal refining industry.	The inhalation of cadmium can cause severe damage to the lungs and is also known to cause kidney damage.[88] Cadmium is also associated with deficits in cognition, learning, behavior, and neuromotor skills in children.[85]
Hexavalent chromium	Used in metal coatings to protect from corrosion.	A known carcinogen after occupational inhalation exposure.[85]
There is also evidence of cytotoxic and genotoxic effects of some chemicals, which have been shown to inhibit cell proliferation, cause cell membrane lesion, cause DNA single-strand breaks, and elevate Reactive Oxygen Species (ROS) levels.[89]

Sulphur	Found in lead-acid batteries.	Health effects include liver damage, kidney damage, heart damage, eye and throat irritation. When released into the environment, it can create sulphuric acid.
Brominated Flame Retardants (BFRs)	Used as flame retardants in plastics in most electronics. Includes PBBs, PBDE, DecaBDE, OctaBDE, PentaBDE.	Health effects include impaired development of the nervous system, thyroid problems, liver problems.[90] Environmental effects: similar effects as in animals as humans. PBBs were banned from 1973 to 1977 on. PCBs were banned during the 1980s.
Perfluorooctanoic acid (PFOA)	Used as an antistatic additive in industrial applications and found in electronics, also found in non-stick cookware (PTFE). PFOAs are formed synthetically through environmental degradation.	Studies in mice have found the following health effects: Hepatotoxicity, developmental toxicity, immunotoxicity, hormonal effects and carcinogenic effects. Studies have found increased maternal PFOA levels to be associated with an increased risk of spontaneous abortion (miscarriage) and stillbirth. Increased maternal levels of PFOA are also associated with decreases in mean gestational age (preterm birth), mean birth weight (low birth weight), mean birth length (small for gestational age), and mean APGAR score.[91]
Beryllium oxide	Filler in some thermal interface materials such as thermal grease used on heatsinks for CPUs and power transistors,[92] magnetrons, X-ray-transparent ceramic windows, heat transfer fins in vacuum tubes, and gas lasers.	Occupational exposures associated with lung cancer, other common adverse health effects are beryllium sensitization, chronic beryllium disease, and acute beryllium disease.[93]
Other health effects

DNA breaks can increase the likelihood of developing cancer (if the damage is to a tumor suppressor gene)
DNA damages are a special problem in non-dividing or slowly dividing cells, where unrepaired damages will tend to accumulate over time. On the other hand, in rapidly dividing cells, unrepaired DNA damages that do not kill the cell by blocking replication will tend to cause replication errors and thus mutation
Elevated Reactive Oxygen Species (ROS) levels can cause damage to cell structures (oxidative stress)[89]
Generally non-hazardous[edit]

An iMac G4 that has been repurposed into a lamp (photographed next to a Mac Classic and a flip phone).
E-Waste Component	Process Used
Aluminium	nearly all electronic goods using more than a few watts of power (heatsinks), electrolytic capacitors.
Copper	copper wire, printed circuit board tracks, component leads.
Germanium	1950s–1960s transistorized electronics (bipolar junction transistors).
Gold	connector plating, primarily in computer equipment.
Iron	steel chassis, cases, and fixings.
Lithium	lithium-ion batteries.
Nickel	nickel-cadmium batteries.
Silicon	glass, transistors, ICs, printed circuit boards.
Tin	solder, coatings on component leads.
Zinc	plating for steel parts.
See also[edit]
Portal icon	Environment portal
Portal icon	Electronics portal
2000s commodities boom
Computer Recycling
Digger gold
eDay
Electronic waste in Japan
Green computing
Mobile phone recycling
Material safety data sheet
Polychlorinated biphenyls
Retrocomputing
Radio Row
Policy and conventions:

Basel Action Network (BAN)
Basel Convention
China RoHS
e-Stewards
Restriction of Hazardous Substances Directive (RoHS)
Soesterberg Principles
Sustainable Electronics Initiative (SEI)
Waste Electrical and Electronic Equipment Directive
Organizations
Asset Disposal and Information Security Alliance (ADISA)[94]
Empa
IFixit
International Network for Environmental Compliance and Enforcement
Institute of Scrap Recycling Industries (ISRI)
Solving the E-waste Problem
World Reuse, Repair and Recycling Association
General:

Retail hazardous waste
Waste
Environmental racism in Europe#Romani settlements and e-waste (France)
Encryption
From Wikipedia, the free encyclopedia
"Encrypt" redirects here. For the film, see Encrypt (film).
This article is about algorithms for encryption and decryption. For an overview of cryptographic technology in general, see Cryptography.
In cryptography, encryption is the process of encoding messages or information in such a way that only authorized parties can read it. Encryption does not of itself prevent interception, but denies the message content to the interceptor. In an encryption scheme, the intended communication information or message, referred to as plaintext, is encrypted using an encryption algorithm, generating ciphertext that can only be read if decrypted. For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is in principle possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, large computational resources and skill are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients, but not to unauthorized interceptors.

Contents  [hide] 
1	Purpose of encryption
2	Types of encryption
2.1	Symmetric key encryption
2.2	Public key encryption
3	Uses of encryption
3.1	Message verification
4	See also
5	References
6	Further reading
Purpose of encryption[edit]
The purpose of encryption is to ensure that only somebody who is authorized to access data (e.g. a text message or a file), will be able to read it, using the decryption key. Somebody who is not authorized can be excluded, because he or she does not have the required key, without which it is impossible to read the encrypted information.

Types of encryption[edit]
Symmetric key encryption[edit]
In symmetric-key schemes,[1] the encryption and decryption keys are the same. Communicating parties must have the same key before they can achieve secure communication.

Public key encryption[edit]

Illustration of how encryption is used within servers Public key encryption.
In public-key encryption schemes, the encryption key is published for anyone to use and encrypt messages. However, only the receiving party has access to the decryption key that enables messages to be read.[2] Public-key encryption was first described in a secret document in 1973;[3] before then all encryption schemes were symmetric-key (also called private-key).[4]:478

A publicly available public key encryption application called Pretty Good Privacy (PGP) was written in 1991 by Phil Zimmermann, and distributed free of charge with source code; it was purchased by Symantec in 2010 and is regularly updated.[5]

Uses of encryption[edit]
Encryption has long been used by military and governments to facilitate secret communication. It is now commonly used in protecting information within many kinds of civilian systems. For example, the Computer Security Institute reported that in 2007, 71% of companies surveyed utilized encryption for some of their data in transit, and 53% utilized encryption for some of their data in storage.[6] Encryption can be used to protect data "at rest", such as information stored on computers and storage devices (e.g. USB flash drives). In recent years there have been numerous reports of confidential data such as customers' personal records being exposed through loss or theft of laptops or backup drives. Encrypting such files at rest helps protect them should physical security measures fail. Digital rights management systems, which prevent unauthorized use or reproduction of copyrighted material and protect software against reverse engineering (see also copy protection), is another somewhat different example of using encryption on data at rest.[7]

Encryption is also used to protect data in transit, for example data being transferred via networks (e.g. the Internet, e-commerce), mobile telephones, wireless microphones, wireless intercom systems, Bluetooth devices and bank automatic teller machines. There have been numerous reports of data in transit being intercepted in recent years.[8] Data should also be encrypted when transmitted across networks in order to protect against eavesdropping of network traffic by unauthorized users.[9]

Message verification[edit]
Encryption, by itself, can protect the confidentiality of messages, but other techniques are still needed to protect the integrity and authenticity of a message; for example, verification of a message authentication code (MAC) or a digital signature. Standards for cryptographic software and hardware to perform encryption are widely available, but successfully using encryption to ensure security may be a challenging problem. A single error in system design or execution can allow successful attacks. Sometimes an adversary can obtain unencrypted information without directly undoing the encryption. See, e.g., traffic analysis, TEMPEST, or Trojan horse.[10]

Digital signature and encryption must be applied to the ciphertext when it is created (typically on the same device used to compose the message) to avoid tampering; otherwise any node between the sender and the encryption agent could potentially tamper with it. Encrypting at the time of creation is only secure if the encryption device itself has not been tampered with.

See also[edit]
Portal icon	Cryptography portal
Substitution cipher
Rotor cipher machines
Brute-force attack
Cold boot attack
Export of cryptography
Cyberspace Electronic Security Act (in the US)
Disk encryption
Key management
Physical Layer Encryption
Geo-Blocking
References[edit]
Jump up ^ Symmetric-key encryption software
Jump up ^ Bellare, Mihir. "Public-Key Encryption in a Multi-user Setting: Security Proofs and Improvements." Springer Berlin Heidelberg, 2000. Page 1.
Jump up ^ "Public-Key Encryption - how GCHQ got there first!". gchq.gov.uk. Archived from the original on May 19, 2010.
Jump up ^ Goldreich, Oded. Foundations of Cryptography: Volume 2, Basic Applications. Vol. 2. Cambridge university press, 2004.
Jump up ^ "Symantec buys encryption specialist PGP for $300M". Computerworld. 2010-04-29. Retrieved 2010-04-29.
Jump up ^ Robert Richardson, 2008 CSI Computer Crime and Security Survey at 19.i.cmpnet.com
Jump up ^ "DRM". Electronic Frontier Foundation.
Jump up ^ Fiber Optic Networks Vulnerable to Attack, Information Security Magazine, November 15, 2006, Sandra Kay Miller
Jump up ^ "Data Encryption in Transit Guideline".
Jump up ^ "What is a Trojan Virus - Malware Protection - Kaspersky Lab US".
Forensic identification
From Wikipedia, the free encyclopedia
Forensic identification is the application of forensic science, or "forensics", and technology to identify specific objects from the trace evidence they leave, often at a crime scene or the scene of an accident. Forensic means "for the courts".

Contents  [hide] 
1	Human identification
1.1	Foot creases
1.2	Downfalls
2	Animal identification
2.1	Wildlife forensics
2.2	Domestic animal forensics
3	Product identification
4	Networks
5	Applications
6	Organizations
7	See also
8	References
9	External links
Human identification[edit]

Droplets of human blood. In addition to analyzing for DNA, the droplets are round and show no spattering, indicating they impacted at a relatively slow velocity, in this case from a height of two feet.
People can be identified by their fingerprints. This assertion is supported by the philosophy of friction ridge identification, which states that friction ridge identification is established through the agreement of friction ridge formations, in sequence, having sufficient uniqueness to individualize.

Friction ridge identification is also governed by four premises or statements of fact:

Friction ridges develop on the fetus in their definitive form prior to birth.
Friction ridges are persistent throughout life except for permanent scarring, disease, or decomposition after death.
Friction ridge paths and the details in small areas of friction ridges are unique and never repeated.
Overall, friction ridge patterns vary within limits which allow for classification.
People can also be identified from traces of their DNA from blood, skin, hair, saliva, and semen[1] by DNA fingerprinting, from their ear print, from their teeth or bite by forensic odontology, from a photograph or a video recording by facial recognition systems, from the video recording of their walk by gait analysis, from an audio recording by voice analysis, from their handwriting by handwriting analysis, from the content of their writings by their writing style (e.g. typical phrases, factual bias, and/or misspellings of words), or from other traces using other biometric techniques.

Since forensic identification has been first introduced to the courts in 1980, the first exoneration due to DNA evidence was in 1989 and there have been 336 additional exonerations since then.[2][3] Those who specialize in forensic identification continue to make headway with new discoveries and technological advances to make convictions more accurate.[4][5]

Body identification is a subfield of forensics concerned with identifying someone from their remains.

Foot creases[edit]
Feet also have friction ridges like fingerprints do. Friction ridges have been widely accepted as a form of identification with fingerprints but not entirely with feet. Feet have creases which remain over time due to the depth it reaches in the dermal layer of the skin, making them permanent.[6] These creases are valuable when individualizing the owner. The concept of no two fingerprints are alike is also applied to foot creases.[7] Foot creases can grow as early as 13 weeks after conception when the volar pads begin to grow and when the pads regress, the creases remain.[8][9] When foot crease identification is used in a criminal case, it should be used in conjunction with morphology and friction ridges to ensure precise identification. There is record of foot crease identification used in a criminal case to solve a murder.[6][10] Sometimes with marks left by the foot with ink, blood, mud, or other substances, the appearance of creases or ridges become muddled or extra creases may appear due to cracked skin, folded skin, or fissures. In order to truly compare morphological feature, the prints of feet must be clear enough to distinguish between individuals.

Downfalls[edit]
The two basic conceptual foundations of forensic identification is that everyone is individualized and unique.[2] This individualization belief was invented by a police records clerk, Alphonse Bertillon, based off of the idea that "nature never repeats," originating from the father of social statistics, Lambert Adolphe Jacques Quetelet. The belief was passed down through generations being generally accepted, but it was never scientifically proven.[11] There was a study done intending to show that no two fingerprints were the same, but the results were inconclusive.[12] Many modern forensic and evidentiary scholars collectively agree that individualization to one object, such as a fingerprint, bite mark, handwriting, or ear mark is not possible. In court cases, forensic scientists can fall victim to observer bias when not sufficiently blinded to the case or results of other pertinent tests. This has happened in cases like United States v. Green and State v. Langill. Also, the proficiency tests that forensic analysts must do are often not as demanding to be considered admissible in court.

Animal identification[edit]
Wildlife forensics[edit]
There are many different applications for wildlife forensics and below are only some of the procedures and processes used to distinguish species.

Species Identification: The important of species identification is most prominent in animal population that are illegally hunted, harvested, and traded,[13] such as rhinoceroses, lions, and African elephants. In order to distinguish which species is which, mtDNA, or mitochondrial DNA, is the most used genetic marker because it's easier to type from highly decomposed and processed tissue compared to nuclear DNA.[14] Additionally, the mitochondrial DNA has multiple copies per cell,[14] which is another reason it's frequently used. When nuclear DNA is used, certain segments of the strands are amplified in order to compare those to segments of mitochondrial DNA. This comparison is used to figure out related genes and species proximity since distant relatives of animals are closer in proximity in the gene tree.[15] That being said, the comparison process demands precision because mistakes can easily be made due to genes evolving and mutating in the evolution of species.[16]

Determination of geographic origin: Determining the origin of a certain species aids research in population numbers and lineage data.[13] Phylogenetic studies are most often used to find the broad geographic area of which a species reside.[17] For example, in California seahorses were being sold for traditional medicinal purposes and the phylogenetic data of those seahorses led researchers to find their origin and from which population they came from and what species they were.[18] In addition to phylogenetic data, assignment tests are used to find the probability of a species belonging to or originating from a specific population and genetic markers of a specimen are utilized.[19][20][21][22] These types of tests are most accurate when all potential population's data have been gathered. Statistical analyses are used in assignment tests based on an individual's microsatellites or Amplified Fragment Length Polymorphisms (AFLPs).[19][22][23][24] Using microsatellites in these studies is more favorable than AFLPs because the AFLPs required non-degraded tissue samples and higher errors have been reported when using AFLPs. [23][25]

Domestic animal forensics[edit]
Domestic animals such as dogs and cats can be utilized to help solve criminal cases. These can include homicides, sexual assaults, or robberies. DNA evidence from dogs alone have helped over 20 criminal cases in Great Britain and the U.S. since 1996.[26] There are only a few laboratories though that are able to process and analyze evidence or data from domestic animals.[27] Forensics can be used in animal attacks as well. In cases like dog attacks, the hair blood, and saliva surrounding the wounds a victim has can be analyzed to find a match for the attacker.[28] In the competitive realm, DNA analysis is used in many cases to find illegal substances in racehorses by urine samples and comparisons of STRs.[29][30][31]

Product identification[edit]
Color copiers and maybe some color computer printers steganographically embed their identification number to some printouts as a countermeasure against currency forgeries.
Copiers and computer printers can be potentially identified by the minor variants of the way they feed the paper through the printing mechanism, leaving banding artifacts.[32][33] Analysis of the toners is also used.[34]
Documents are characterized by the composition of their paper and ink.
Firearms can be identified by the striations on the bullets they fired and imprints on the cartridge casings.
Paper shredders can be potentially identified in a similar way, by spacing and wear of their blades.
Photo identification is used to detect and identify forged digital photos.[35]
Typewriters can be identified by minor variations of positioning and wear of their letters.
Illegal drugs can be identified by which color it turns when a reagent is added during a color test. Gas Chromatography, Infrared Spectrometry or Mass Spectrometry is used in combination with the color test to identify the type of drug.[36]
Networks[edit]
Cars can be automatically found on CCTV records by automatic number plate recognition.
Computers connected to the Internet can often be identified by their IP address or MAC address.
Radio transceivers can be potentially identified by minute variations of their output signal.
Social networks can be discovered by network analysis of banking, telecommunication and postal records.
Applications[edit]
Sometimes, manufacturers and film distributors may intentionally leave subtle forensic markings on their products to identify them in case of piracy or involvement in a crime. (Cf. watermark, digital watermark, steganography. DNA marking.)

Organizations[edit]
Association of Firearm and Tool Mark Examiners
Canadian Identification Society
International Association for Identification
Gutmann method
From Wikipedia, the free encyclopedia
The Gutmann method is an algorithm for securely erasing the contents of computer hard disk drives, such as files. Devised by Peter Gutmann and Colin Plumb and presented in the paper Secure Deletion of Data from Magnetic and Solid-State Memory in July 1996, it involved writing a series of 35 patterns over the region to be erased.

The selection of patterns assumes that the user does not know the encoding mechanism used by the drive, so it includes patterns designed specifically for three types of drives. A user who knows which type of encoding the drive uses can choose only those patterns intended for their drive. A drive with a different encoding mechanism would need different patterns.

Most of the patterns in the Gutmann method were designed for older MFM/RLL encoded disks. Gutmann himself has noted that more modern drives no longer use these older encoding techniques, making parts of the method irrelevant. He said "In the time since this paper was published, some people have treated the 35-pass overwrite technique described in it more as a kind of voodoo incantation to banish evil spirits than the result of a technical analysis of drive encoding techniques".[1][2]

Since about 2001, some ATA IDE and SATA hard drive manufacturer designs include support for the ATA Secure Erase standard, obviating the need to apply the Gutmann method when erasing an entire drive.[3] However, a 2011 research found that 4 out of 8 manufacturers did not implement ATA Secure Erase correctly.[4]

Contents  [hide] 
1	Technical overview
2	Method
3	Criticism
4	Software implementations
5	See also
6	Notes
7	External links
Technical overview[edit]
One standard way to recover data that have been overwritten on a hard drive is to capture and process the analog signal obtained from the drive's read/write head prior to this analog signal being digitized. This analog signal will be close to an ideal digital signal, but the differences will reveal important information. By calculating the ideal digital signal and then subtracting it from the actual analog signal, it is possible to amplify the signal remaining after subtraction and use it to determine what had previously been written on the disk.

For example:

Analog signal:        +11.1  -8.9  +9.1 -11.1 +10.9  -9.1
Ideal digital signal: +10.0 -10.0 +10.0 -10.0 +10.0 -10.0
Difference:            +1.1  +1.1  -0.9  -1.1  +0.9  +0.9
Previous signal:      +11    +11   -9   -11    +9    +9
This can then be done again to see the previous data written:

Recovered signal:     +11    +11   -9   -11    +9    +9
Ideal digital signal: +10.0 +10.0 -10.0 -10.0 +10.0 +10.0
Difference:            +1    +1    +1    -1    -1    -1
Previous signal:      +10   +10   -10   -10   +10   +10
However, even when overwriting the disk repeatedly with random data it is theoretically possible to recover the previous signal. The permittivity of a medium changes with the frequency of the magnetic field[citation needed]. This means that a lower frequency field will penetrate deeper into the magnetic material on the drive than a high frequency one[citation needed]. So a low frequency signal will, in theory, still be detectable even after it has been overwritten hundreds of times by a high frequency signal.

The patterns used are designed to apply alternating magnetic fields of various frequencies and various phases to the drive surface and thereby approximate degaussing the material below the surface of the drive[citation needed].

Method[edit]
An overwrite session consists of a lead-in of four random write patterns, followed by patterns 5 to 31 (see rows of table below), executed in a random order, and a lead-out of four more random patterns.

Each of patterns 5 to 31 was designed with a specific magnetic media encoding scheme in mind, which each pattern targets. The drive is written to for all the passes even though the table below only shows the bit patterns for the passes that are specifically targeted at each encoding scheme. The end result should obscure any data on the drive so that only the most advanced physical scanning (e.g., using a magnetic force microscope) of the drive is likely to be able to recover any data.[citation needed]

The series of patterns is as follows:

Gutmann overwrite method
Pass	Data written	Pattern written to disk for targeted encoding scheme
In binary notation	In hex notation	(1,7) RLL	(2,7) RLL	MFM
1	(Random)	(Random)			
2	(Random)	(Random)			
3	(Random)	(Random)			
4	(Random)	(Random)			
5	01010101 01010101 01010101	55 55 55	100...		000 1000...
6	10101010 10101010 10101010	AA AA AA	00 100...		0 1000...
7	10010010 01001001 00100100	92 49 24		00 100000...	0 100...
8	01001001 00100100 10010010	49 24 92		0000 100000...	100 100...
9	00100100 10010010 01001001	24 92 49		100000...	00 100...
10	00000000 00000000 00000000	00 00 00	101000...	1000...	
11	00010001 00010001 00010001	11 11 11	0 100000...		
12	00100010 00100010 00100010	22 22 22	00000 100000...		
13	00110011 00110011 00110011	33 33 33	10...	1000000...	
14	01000100 01000100 01000100	44 44 44	000 100000...		
15	01010101 01010101 01010101	55 55 55	100...		000 1000...
16	01100110 01100110 01100110	66 66 66	0000 100000...	000000 10000000...	
17	01110111 01110111 01110111	77 77 77	100010...		
18	10001000 10001000 10001000	88 88 88	00 100000...		
19	10011001 10011001 10011001	99 99 99	0 100000...	00 10000000...	
20	10101010 10101010 10101010	AA AA AA	00 100...		0 1000...
21	10111011 10111011 10111011	BB BB BB	00 101000...		
22	11001100 11001100 11001100	CC CC CC	0 10...	0000 10000000...	
23	11011101 11011101 11011101	DD DD DD	0 101000...		
24	11101110 11101110 11101110	EE EE EE	0 100010...		
25	11111111 11111111 11111111	FF FF FF	0 100...	000 100000...	
26	10010010 01001001 00100100	92 49 24		00 100000...	0 100...
27	01001001 00100100 10010010	49 24 92		0000 100000...	100 100...
28	00100100 10010010 01001001	24 92 49		100000...	00 100...
29	01101101 10110110 11011011	6D B6 DB		0 100...	
30	10110110 11011011 01101101	B6 DB 6D		100...	
31	11011011 01101101 10110110	DB 6D B6		00 100...	
32	(Random)	(Random)			
33	(Random)	(Random)			
34	(Random)	(Random)			
35	(Random)	(Random)			
Encoded bits shown in bold are what should be present in the ideal pattern, although due to the encoding the complementary bit is actually present at the start of the track.

Criticism[edit]
The delete function in most operating systems simply marks the space occupied by the file as reusable (removes the pointer to the file) without immediately removing any of its contents. At this point the file can be fairly easily recovered by numerous recovery applications. However, once the space is overwritten with other data, there is no known way to use software to recover it. It cannot be done with software alone since the storage device only returns its current contents via its normal interface. Gutmann claims that intelligence agencies have sophisticated tools, including magnetic force microscopes, which together with image analysis, can detect the previous values of bits on the affected area of the media (for example hard disk).

Daniel Feenberg of the National Bureau of Economic Research, an American private nonprofit research organization, criticized Gutmann's claim that intelligence agencies are likely to be able to read overwritten data, citing a lack of evidence for such claims.[5] Nevertheless, some published government security procedures consider a disk overwritten once to still be sensitive.[6]

Gutmann himself has responded to some of these criticisms and also criticized how his algorithm has been abused in an epilogue to his original paper, in which he states:[1][2]

In the time since this paper was published, some people have treated the 35-pass overwrite technique described in it more as a kind of voodoo incantation to banish evil spirits than the result of a technical analysis of drive encoding techniques. As a result, they advocate applying the voodoo to PRML and EPRML drives even though it will have no more effect than a simple scrubbing with random data. In fact performing the full 35-pass overwrite is pointless for any drive since it targets a blend of scenarios involving all types of (normally-used) encoding technology, which covers everything back to 30+-year-old MFM methods (if you don't understand that statement, re-read the paper). If you're using a drive which uses encoding technology X, you only need to perform the passes specific to X, and you never need to perform all 35 passes. For any modern PRML/EPRML drive, a few passes of random scrubbing is the best you can do. As the paper says, "A good scrubbing with random data will do about as well as can be expected". This was true in 1996, and is still true now.

—?Peter Gutmann, Secure Deletion of Data from Magnetic and Solid-State Memory, University of Auckland Department of Computer Science.
Software implementations[edit]
CCleaner and Recuva, utilities developed by Piriform
Darik's Boot and Nuke (DBAN) (whole disk only)
Disk Utility a program provided with Mac OS X (whole disk or free space only)
FreeOTFE and FreeOTFE Explorer (disk encryption system)
Lavasoft Privacy Toolbox
PeaZip Secure delete function (files/directories only)
shred program of the GNU Core Utilities[7]
srm, also used by Mac OS X
TrueCrypt (disk encryption system) (free space only)
Shredit secure erase utility.

Memory controller
From Wikipedia, the free encyclopedia
  (Redirected from Memory scrambling)

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2006) (Learn how and when to remove this template message)
The memory controller is a digital circuit that manages the flow of data going to and from the computer's main memory. A memory controller can be a separate chip or integrated into another chip, such as being placed on the same die or as an integral part of a microprocessor; in the latter case, it is usually called an integrated memory controller (IMC). A memory controller is sometimes also called a memory chip controller (MCC)[1] or a memory controller unit (MCU).[2]

Contents  [hide] 
1	History
2	Purpose
3	Variants
3.1	Double data rate memory
3.2	Dual-channel memory
3.3	Fully buffered memory
3.4	Flash memory controller
4	See also
5	References
6	External links
History[edit]
Computers using Intel microprocessors have traditionally had a memory controller implemented on their motherboard's northbridge, but many modern microprocessors, such as DEC/Compaq's Alpha 21364, AMD's Athlon 64 and Opteron processors, IBM's POWER5, Sun Microsystems's UltraSPARC T1, and more recently Intel's Core i7 and Core i5 CPUs have an integrated memory controller (IMC) on the microprocessor in order to reduce memory latency.

While an integrated memory controller has the potential to increase the system's performance, it locks the microprocessor to a specific type (or types) of memory, forcing a redesign in order to support newer memory technologies. When DDR2 SDRAM was introduced, AMD released new Athlon 64 CPUs. These new models, with a DDR2 controller, use a different physical socket (known as Socket AM2), so that they will only fit in motherboards designed for the new type of RAM. When the memory controller is not on-die, the same CPU may be installed on a new motherboard, with an updated northbridge.

The integration of the memory controller onto the die of the microprocessor is not a new concept. Some microprocessors in the 1990s, such as the DEC Alpha 21066 and HP PA-7300LC, had integrated memory controllers; however, rather than for performance gains, this was implemented to reduce the cost of systems by eliminating the need for an external memory controller.

Some CPUs are designed to have their memory controllers as dedicated external components that are not part of the chipset. An example is IBM POWER8, which uses external Centaur chips that are mounted onto DIMM modules and act as memory buffers, L4 cache chips, and as the actual memory controllers. The current Centaur chip is using DDR3 memory but a future version can use DDR4 or some other memory technology without need to swap the POWER8 chip itself.[3]

Purpose[edit]
Memory controllers contain the logic necessary to read and write to DRAM, and to "refresh" the DRAM. Without constant refreshes, DRAM will lose the data written to it as the capacitors leak their charge within a fraction of a second (not less than 64 milliseconds according to JEDEC standards).

Reading and writing to DRAM is performed by selecting the row and column data addresses of the DRAM as the inputs to the multiplexer circuit, where the demultiplexer on the DRAM uses the converted inputs to select the correct memory location and return the data, which is then passed back through a multiplexer to consolidate the data in order to reduce the required bus width for the operation.

Bus width is the number of parallel lines available to communicate with the memory cell. Memory controllers' bus widths range from 8-bit in earlier systems, to 512-bit in more complicated systems and video cards (typically implemented as four 64-bit simultaneous memory controllers operating in parallel, though some are designed to operate in "gang mode" where two 64-bit memory controllers can be used to access a 128-bit memory device).

Some memory controllers, such as the one integrated into PowerQUICC II processors, can be connected to different kinds of devices at the same time, including SDRAM, SRAM, ROM, and memory-mapped I/O; each kind of these devices requires a slightly different control bus, while the memory controller presents a common system bus / front-side bus to the processor. Some memory controllers, such as the one integrated into PowerQUICC II processors, include error detection and correction hardware.[4]

A few experimental memory controllers contain a second level of address translation, in addition to the first level of address translation performed by the memory management unit.[5]

Memory controllers integrated into certain Intel Core processors also provide memory scrambling as a feature that turns user data written to the main memory into pseudo-random patterns.[6][7] As such, memory scrambling prevents forensic and reverse-engineering analysis based on DRAM data remanence, by effectively rendering various types of cold boot attacks ineffective. However, this feature has been designed to address DRAM-related electrical problems, not to prevent security issues, so it may not be rigorously cryptographically secure.[8]

Variants[edit]
Double data rate memory[edit]
Double data rate (DDR) memory controllers are used to drive DDR SDRAM, where data is transferred on both rising and falling edges of the system's memory clock. DDR memory controllers are significantly more complicated when compared to single data rate controllers, but they allow for twice the data to be transferred without increasing the memory cell's clock rate or bus width.

Dual-channel memory[edit]
Dual Channel memory controllers are memory controllers where the DRAM devices are separated on to two different buses to allow two memory controllers to access them in parallel. This doubles the theoretical amount of bandwidth of the bus. In theory, more channels can be built (a channel for every DRAM cell would be the ideal solution), but due to wire count, line capacitance, and the need for parallel access lines to have identical lengths, more channels are very difficult to add.

Fully buffered memory[edit]
Main article: Fully Buffered DIMM
Fully buffered memory systems place a memory buffer device on every memory module (called an FB-DIMM when Fully Buffered RAM is used), which unlike traditional memory controller devices, use a serial data link to the memory controller instead of the parallel link used in previous RAM designs. This decreases the number of the wires necessary to place the memory devices on a motherboard (allowing for a smaller number of layers to be used, meaning more memory devices can be placed on a single board), at the expense of increasing latency (the time necessary to access a memory location). This increase is due to the time required to convert the parallel information read from the DRAM cell to the serial format used by the FB-DIMM controller, and back to a parallel form in the memory controller on the motherboard.

In theory, the FB-DIMM's memory buffer device could be built to access any DRAM cells, allowing for memory cell agnostic memory controller design, but this has not been demonstrated, as the technology is in its infancy.

Flash memory controller[edit]
Main article: Flash memory controller
Many flash memory devices, such as USB memory sticks, include a flash memory controller on chip. This is essentially the same as a RAM controller, except that flash memory doesn't need to be constantly refreshed and retains its memory state if power is removed. Flash memory is inherently slower to access than RAM and often becomes unusable after a few million write cycles, which generally makes it unsuitable for RAM applications.
Paper shredder
From Wikipedia, the free encyclopedia

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2009) (Learn how and when to remove this template message)

Paper shredder with built-in wastebasket
A paper shredder is a mechanical device used to cut paper into chad, typically either strips or fine particles. Government organizations, businesses, and private individuals use shredders to destroy private, confidential, or otherwise sensitive documents. Privacy experts often recommend that individuals shred bills, tax documents, credit card and bank account statements, and other items which could be used by thieves to commit fraud or identity theft.

Contents  [hide] 
1	History
2	Types
2.1	Trucks
2.2	Kiosks
3	Services
4	Innovation
5	Applications
6	Unshredding
7	Forensic identification
8	Injury risk with residential use
9	See also
10	References
History[edit]
The first paper shredder is credited to prolific inventor Abbot Augustus Low (May 12, 1844 – 1912) of Horseshoe, located on the Western shore of Horseshoe Lake, in Piercefield, New York.[1] His patent for a "waste paper receptacle" to offer an improved method of disposing of waste paper was filed on February 2, 1909, and received the U.S. patent number 929,960 on August 3, 1909. Low’s invention was never manufactured, however.

Adolf Ehinger's paper shredder, based on a hand-crank pasta maker, was manufactured in 1935 in Germany. Supposedly he needed to shred his anti-Nazi propaganda to avoid the inquiries of the authorities. Ehinger later marketed his shredders to government agencies and financial institutions converting from hand-crank to electric motor. Ehinger's company, EBA Maschinenfabrik, manufactured the first cross-cut paper shredders in 1959 and continues to do so to this day as EBA Krug & Priester GmbH & Co. in Balingen.

The U.S. embassy in Iran used strip-cut paper shredders to reduce paper pages to strips before the embassy was taken over in 1979 (though not entirely successfully: some documents have been reconstructed from the strips, see below). After Colonel Oliver North told Congress that he used a Schleicher Intimus 007 S cross-cut model to shred Iran-Contra documents, sales for that company increased nearly 20 percent in 1987.[2]

Until the mid-1980s, it was rare for paper shredders to be used by non-government entities. After the 1988 Supreme Court decision in California v. Greenwood, in which the Supreme Court of the United States held that the Fourth Amendment does not prohibit the warrantless search and seizure of garbage left for collection outside of a home, paper shredders became more popular among U.S. citizens with privacy concerns. Anti-burning laws, concern over landfills, industrial espionage, and identity theft concerns resulted in increased demand for paper shredding.

The US Federal Trade Commission estimates that 9 million cases of identity theft take place per year in the USA alone[3] and recommends that individuals defend themselves against identity theft by shredding financial documents before disposal.[4]

News agencies have driven awareness of information theft to the extent that most consumers, healthcare organizations and businesses understand the importance of destroying confidential information. Also, information privacy laws like FACTA, HIPAA and the Gramm–Leach–Bliley Act are driving shredder sales, as businesses and individuals take steps to comply to avoid legal complications.

Types[edit]

Multi-cut scissors used to shred paper
Shredders range in size and price from small and inexpensive units designed for a certain amount of pages, to large units used by commercial shredding services that cost hundreds of thousands of dollars and can shred millions of documents per hour. Some shredders used by a commercial shredding service are built into a shredding truck.

The general small shredder is an electrically powered device, but there some that are manually powered, such as special scissors with multiple blade pairs and hand-cranked rotary shredders.

These machines are classified according to the size and shape of the shreds they produce. (As a practical matter, this is also a measure of the degree of randomness or entropy they conduct.) All types of shredders can range in size from standard scissors and other hand-operated devices all the way up to truck-sized shredders. There are also shredder selector sites that can help consumers choose a shredder that is appropriate for their needs.

Strip-cut shredders, the least secure, use rotating knives to cut narrow strips as long as the original sheet of paper. Such strips can be reassembled by a determined and patient investigator or adversary, as the product (the destroyed information) of this type of shredder is the least randomized. It also creates the highest volume of waste inasmuch as the strips are not compressed.
Cross-cut or confetti-cut shredders use two contra-rotating drums to cut rectangular, parallelogram, or lozenge (diamond-shaped) shreds.
Particle-cut shredders create tiny square or circular pieces.
Cardboard shredders are designed specifically to shred corrugated material into either strips or a mesh pallet.
Disintegrators and granulators repeatedly cut the paper at random until the particles are small enough to pass through a mesh.
Hammermills pound the paper through a screen.
Pierce-and-tear shredders have rotating blades that pierce the paper and then tear it apart.
Grinders have a rotating shaft with cutting blades that grind the paper until it is small enough to fall through a screen.

The shredded remains of a National Lottery play slip.
There are numerous standards for the security levels of paper shredders, including:

DIN 66399
Level P-1 = ?12 mm wide strips of any length
Level P-2 = ?6 mm wide strips of any length
Level P-3 = ?2 mm wide strips of any length or ?320 mm? particles (of any width)
Level P-4 = ?160 mm? particles with width ? 6 mm
Level P-5 = ?30 mm? particles with width ? 2 mm
Level P-6 = ?10 mm? particles with width ? 1 mm
Level P-7 = ?5 mm? particles with width ? 1 mm
United States Department of Defense (DoD)
Top Secret = 0.8 x 11.1 mm (1/32" ? 7/16") no longer approved after 1 October 2008 for U.S. government classified documents.[dubious – discuss]
United States National Security Agency/CSS 02-01 = 1 ? 5 mm required for all U.S. government classified document destruction starting 1 October 2008.[dubious – discuss]
Historically, the General Services Administration (GSA) set paper shredder guidance in the Interim Federal Specification FF-S-001169 dated July 1971, superseded by standard A-A-2599 for classified material, which was canceled in February 2000. GSA has not published a new standard since.

There are alternative systems that use combustion, chemical decomposition, or composting for disposing of the shreds.

Trucks[edit]
A mobile shredding truck is a box truck with an industrial-size paper shredder mounted inside the box, typically in the front section of the box, closest to the cab. The box is divided into two sections: the shredding equipment area, and the payload area for storage of the shredded materials. These trucks have been designed to shred up to 8,000 lb (3,600 kg) of paper an hour. Mobile shredding trucks can have a shredded material storage capacity of 6,000 to 15,000 lb (2,700 to 6,800 kg) of shredded paper. Office paper is the typical material being shredded, but with increasing security concerns customers also request shredding of CDs, DVDs, hard drives, credit cards, and uniforms, among other things. There are four major types of mobile shredding trucks — pierce-and-tear (most popular), grinder, hammermill, and generator-powered with cross cut or strip cut options.

Kiosks[edit]
A shredding kiosk is an automated retail machine (or kiosk) that allows public access to a commercial or industrial-capacity paper shredder. This is an alternative solution to the use of a personal or business paper shredder, where the public can use a faster and more powerful shredder, paying for each shredding event rather than purchasing shredding equipment. A shredding kiosk with industrial shredder hardware can have a feed capacity many times greater than personal shredders. Variations of payment systems depend on if the kiosk is truly standalone; they can include paying by time of use or by pre-weighing material, and may include a payment system as part of the machine, or use may be controlled by the kiosk's host location.

Services[edit]
Some companies outsource their shredding to shredding services. These companies either shred on-site, with mobile shredder trucks or have off-site shredding facilities. Documents that need to be destroyed are often placed in locked bins that are emptied periodically.

In Canada, there are no provincial or federal regulations for the operations of the industry. Companies can voluntarily join National Association for Information Destruction (NAID) where they get bi-yearly announced and unannounced audits.


Shredding console
Innovation[edit]
As the demand for commercial and personal shredders grows, shredder manufacturers continue to develop new features that improve the shredder user's experience with efficiency, convenience and safety.

Jam proof shredders – many new shredders have means to detect paper thickness to avoid paper jams by rejecting paper that is fed over capacity, and have more powerful motors to handle jumbled or misfed paper.
Safety sensor – The shredder automatically shuts off when hands are too close to the paper entry.
Silent operation – Shredders designed for noise reduction in shared workspaces or department copy rooms.
Energy savings – Shredders that enter a power-saving sleep mode when not in use.
Mess reduction – the shredder features an automatic cleaning cycle that prevents paper build-up on cutters. To eliminate overflow, a sensor lets the user know when to empty the bin while a sliding flap contains dangling shreds.
Applications[edit]
Animal bedding — Animal bedding is one of the most economical and ecological methods of recycling. Waste paper or card, be it news print, off cuts, or cardboard boxes, can be shredded and bagged to produce a warm and comfortable bed for animals. This then decomposes very quickly on a muck heap.[5]
Security shredding — Document destruction, to prevent identity theft was one of the earliest uses of shredders. Shredding into strips, or dicing documents, makes it nearly impossible for the documents to be read after shredding.
Void fill and packaging — Shredded material can be used both as an aesthetic and functional product. Shredded cellophane and shredded cardboard in elastic mat form can be used to package high street goods. Shredded cardboard in mat and strip form can be used as a void fill for the transportation of goods.
Cardboard briquettes — Briquettes are quickly becoming a viable alternative to coal and other non renewable fuels.
Children's playgrounds — Once tires have been shredded and granulated, they are often combined with a strong resin to create soft rubber playground surfaces.
Waste reduction — Shredding waste material generally reduces waste volume by up to 75%, which, for the remaining material that reaches landfill, is much less robust and takes up less room in fill.
Insulation — In Japan, finely shredded newsprint is often mixed with flame-retardant chemicals and glue to create a spray-able insulation material for the construction of buildings to be applied on wall interiors and on the underside of roofing.
Shredding at high levels continues in government agencies, too. According to the report of the Paul Volcker Committee, between April and December 2004, Kofi Annan's Chef de Cabinet, Iqbal Riza, authorized thousands of United Nations documents shredded, including the entire chronological files of the Oil-for-Food Programme during the years 1997 through 1999.[6]

The Union Bank of Switzerland used paper shredders to destroy evidence that their company owned property stolen from Jews during the Holocaust by the Nazi government. The shredding was disclosed to the public through the work of Christoph Meili, a security guard working at the bank who happened to wander by a room where the shredding was taking place. Also in the shredding room were books from the German Reichsbank.[7] They listed stock accounts for companies involved in the holocaust, including BASF, Degussa, and Degesch.[8] They also listed real-estate records for Berlin properties that had been forcibly taken by the Nazis, placed in Swiss accounts, and then claimed to be owned by UBS.[9] Destruction of such documents was a violation of Swiss laws.[10]

Unshredding[edit]

An example of shredded and reassembled degrees during Iran hostage crisis
In some cases it is technically possible to reassemble the pieces of shredded documents; the feasibility of such a project is a cost-to-benefit calculation. If the chad is not further randomized, the strips that belonged to the same document tend to come out of the shredder close to each other and remain roughly in that configuration. Furthermore, when the documents are fed to the shredder in a way that the lines of text are not perpendicular to the shredder blades, portions of text may remain legible on the strips.

If the shredder doesn't cut paper small enough, confidential documents could be removed from the trash, reassembled and read. This can lead to corporate espionage as dumpster diving is the easiest way for professional thieves to steal sensitive information from businesses. For this reason, a host of new shredders provide protection from information theft by cutting paper into pieces significantly smaller than the length of a staple.

These micro-cut shredders make it difficult for criminals to assemble a document by cutting a piece of paper into about 3,770 bits versus the average confetti-cut shredder, which cuts a piece of paper into 300 pieces or the average strip-cut shredder, which cuts a piece of paper into 34 strips.

	Wikisource has original text related to this article:
Documents Seized from the US Embassy in Tehran
After the Iranian Revolution and the takeover of the U.S. embassy in Tehran in 1979, Iranians enlisted local carpet weavers who reconstructed the pieces by hand. The recovered documents would be later released by the Iranian regime in a series of books called "Documents from the US espionage Den".[11] The US government subsequently improved its shredding techniques by adding pulverizing, pulping, and chemical decomposition protocols.

Modern computer technology considerably speeds up the process of reassembling shredded documents. The strips are scanned on both sides, and then a computer determines how the strips should be put together. Robert Johnson of the National Association for Information Destruction[12] has stated that there is a huge demand for document reconstruction. Several companies offer commercial document reconstruction services. For maximum security, documents should be shredded so that the words of the document go through the shredder horizontally (i.e. perpendicular to the blades). Many of the documents in the Enron Accounting scandals were fed through the shredder the wrong way, making them easier to reassemble.

In 2003, there was an effort underway to recover the shredded archives of the Stasi, the East German secret police.[13] There are "millions of shreds of paper that panicked Stasi officials threw into garbage bags during the regime's final days in the fall of 1989". As it took three dozen people six years to reconstruct 300 of the 16,000 bags, the Fraunhofer-IPK institute has developed the "Stasi-Schnipselmaschine" (Stasi snippet machine) for computerized reconstruction and is testing it in a pilot project.

In 2011, DARPA’s Shredder Challenge called upon computer scientists, puzzle enthusiasts, and anyone else with an interest in solving complex problems, to compete for up to $50,000 by piecing together a series of shredded documents. The Shredder Challenge consisted of five separate puzzles in which the number of documents, the document subject matter and the method of shredding were varied to present challenges of increasing difficulty. To complete each problem, participants were required to provide the answer to a puzzle embedded in the content of the reconstructed document. The overall prizewinner and prize awarded was dependent on the number and difficulty of the problems solved. DARPA declared a winner on December 2, 2011 (the winning entry was submitted 33 days after the challenge began) - the winner was "All Your Shreds Are Belong To U.S." using a combination system that used automated sorting to pick the best fragment combinations to be reviewed by humans.[14]

Forensic identification[edit]
Document shredders display certain device-specific characteristics, "fingerprints", like the exact spacing of the blades, the degree and pattern of their wear. These can be reconstructed from the minute variations of size of the paper strips and the microscopic marks on their edges, and by comparison with the strips produced by known shredders, the individual shredder that was used to destroy a given document may be determined. Jack Brassil, a researcher for Hewlett-Packard, works on a project for making shredders more easily traceable.[15] (Cf. the forensic identification of typewriters.)

Chemical decomposition or vermicomposting can subvert forensic identification, but has higher cost or time requirements and creates other forensic evidence (such as purchase of chemicals).

Injury risk with residential use[edit]

The cutting head of a small paper shredder.
As with any motorized cutting equipment, there is a risk of injury. As shredders migrated to home environments with children and pets, shredder safety became an important issue. As early as 1985, personal injuries have occurred as a result of consumers using shredders. As the number of paper shredders per household increased, so did the number of accidents. From January 2000 through September 2005, the Consumer Product Safety Commission (CPSC) received 50 reports of incidents involving finger amputations, lacerations, and other severe injuries from paper shredders. The majority of injuries happened to children under the age of five.

To increase consumer awareness of these potential safety hazards, two important safety alerts were issued:

Fall 2005 – Consumer Product Safety Council Shredder Safety Alert
February 2006 – American Academy of Pediatrics Shredder Safety Alert
Although designed with a narrow opening to the cutting wheels, shredders still pose a danger to pets and small children. Children as young as four months of age have the ability to imitate adults. As children grow older, this tendency to mimic adult behavior increases and parents might not anticipate the dangers of children accidentally activating a shredder. This puts children at risk for serious injury, even with adult supervision. Children’s fingers can easily be pulled into the paper entry through the force of the shredding mechanism.

Many new shredders on the market now feature improved safety features:

Safety sensor — this technology ensures the shredder automatically shuts off when hands are too close to the paper entry. The sensor at the paper entry senses the electric field around humans and larger pets. When this electric field is detected, the shredder instantly shuts down. As soon as the electric field is removed, the shredder starts up again.
Slimmer paper entry – this reduces the chance of fingers getting pulled into the shredding mechanism. The entry can also be made of an inflexible material to prevent the opening from widening under pressure.
Safety lock – this safety feature puts the shredder into a safe, inactive mode to prevent children from activating the shredder mechanism. The adult user simply locks the activation switch in the "off" position.
Safety flap – this flap covers cutters when the shredder head is removed from the wastebasket.
Safety interlock switch – this switch ensures the shredder will not activate when the shredder head is removed from the wastebasket.
Many home shredders can be left in a "stand-by" mode that will start the cutting process when anything is inserted into the feed slot. In homes with small children or pets, simply keeping the shredder unplugged while not in use can also reduce any risk.[16][17]
Physical information security
From Wikipedia, the free encyclopedia

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2010) (Learn how and when to remove this template message)
Physical information security is the intersection, the common ground between physical security and information security. It primarily concerns the protection of tangible information-related assets such as computer systems and storage media against physical, real-world threats such as unauthorized physical access, theft, fire and flood. It typically involves physical controls such as protective barriers and locks, uninterruptible power supplies, and shredders. Information security controls in the physical domain complement those in the logical domain (such as encryption), and procedural or administrative controls (such as information security awareness and compliance with policies and laws).

Contents  [hide] 
1	Background
2	Examples of physical attacks to obtain information
2.1	Dumpster diving
2.2	Overt access
3	Examples of Physical Information Security Controls
4	See also
5	References
6	External links
Background[edit]
Asset are inherently valuable and yet vulnerable to a wide variety of threats, both malicious (e.g. theft, arson) and accidental/natural (e.g. lost property, bush fire). If threats materialize and exploit those vulnerabilities causing incidents, there are likely to be adverse impacts on the organizations or individuals who legitimately own and utilize the assets, varying from trivial to devastating in effect. Security controls are intended to reduce the probability or frequency of occurrence and/or the severity of the impacts arising from incidents, thus protecting the value of the assets.

Physical security involves the use of controls such as smoke detectors, fire alarms and extinguishers, along with related laws, regulations, policies and procedures concerning their use. Barriers such as fences, walls and doors are obvious physical security controls, designed to deter or prevent unauthorized physical access to a controlled area, such as a home or office. The moats and battlements of Mediaeval castles are classic examples of physical access controls, as are bank vaults and safes.

Information security controls protect the value of information assets, particularly the information itself (i.e. the intangible information content, data, intellectual property, knowledge etc.) but also computer and telecommunications equipment, storage media (including papers and digital media), cables and other tangible information-related assets (such as computer power supplies). The corporate mantra "Our people are our greatest assets" is literally true in the sense that so-called knowledge workers qualify as extremely valuable, perhaps irreplaceable information assets. Health and safety measures and even medical practice could therefore also be classed as physical information security controls since they protect humans against injuries, diseases and death. This perspective exemplifies the ubiquity and value of information. Modern human society is heavily reliant on information, and information has importance and value at a deeper, more fundamental level. In principle, the subcellular biochemical mechanisms that maintain the accuracy of DNA replication could even be classed as vital information security controls, given that genes are 'the information of life'.

Malicious actors who may benefit from physical access to information assets include computer crackers, corporate spies, and fraudsters. The value of information assets is self-evident in the case of, say, stolen laptops or servers that can be sold-on for cash, but the information content is often far more valuable, for example encryption keys or passwords (used to gain access to further systems and information), trade secrets and other intellectual property (inherently valuable or valuable because of the commercial advantages they confer), and credit card numbers (used to commit identity fraud and further theft). Furthermore, the loss, theft or damage of computer systems, plus power interruptions, mechanical/electronic failures and other physical incidents prevent them being used, typically causing disruption and consequential costs or losses. Unauthorized disclosure of confidential information, and even the coercive threat of such disclosure, can be damaging as we saw in the Sony Pictures Entertainment hack at the end of 2004 and in numerous privacy breach incidents. Even in the absence of evidence that disclosed personal information has actually been exploited, the very fact that it is no longer secured and under the control of its rightful owners is itself a potentially harmful privacy impact. Substantial fines, adverse publicity/reputational damage and other noncompliance penalties and impacts that flow from serious privacy breaches are best avoided, regardless of cause!

Examples of physical attacks to obtain information[edit]
There are several ways to obtain information through physical attacks or exploitations. A few examples are described below.

Dumpster diving[edit]
Dumpster diving is the practice of searching through trash in the hope of obtaining something valuable such as information carelessly discarded on paper, computer disks or other hardware.

Overt access[edit]
Sometimes attackers will simply go into a building and take the information they need. [1] Frequently when using this strategy, an attacker will masquerade as someone who belongs in the situation. They may pose as a copy room employee, remove a document from someone's desk, copy the document, replace the original, and leave with the copied document. Individuals pretending to building maintenance may gain access to otherwise restricted spaces. [2] [3] They might walk right out of the building with a trash bag containing sensitive documents, carrying portable devices or storage media that were left out on desks, or perhaps just the memory of a password on a sticky note stuck to someone's computer screen or called out to a colleague across an open office.

Examples of Physical Information Security Controls[edit]
Literally shredding paper documents prior to their disposal is a commonplace physical information security control, intended to prevent the information content - if not the media - from falling into the wrong hands. Digital data can also be shredded in a figurative sense, either by being strongly encrypted or by being repeatedly overwritten until there is no realistic probability of the information ever being retrieved, even using sophisticated forensic analysis: this too constitutes a physical information security control since the purged computer storage media can be freely discarded or sold without compromising the original information content. The two techniques may be combined in high-security situations, where digital shredding of the data content is followed by physical shredding and incineration to destroy the storage media.

Many organizations restrict physical access to controlled areas such as their offices by requiring that people present valid identification cards, proximity passes or physical keys. Provided the access tokens or devices are themselves strictly controlled and secure (making it hard for unauthorized people to obtain or fabricate and use them), and the associated electronic or mechanical locks, doors, walls, barriers etc. are sufficiently strong and complete, unauthorized physical entry to the controlled areas is prevented, protecting the information and other assets within. Likewise, office workers are generally encouraged or required to obey "clear desk" policies, protecting documents and other storage media (including portable IT devices) by tidying them away out of sight, perhaps in locked drawers, filing cabinets, safes or vaults according to the risks. Requiring workers to memorize their passwords rather than writing them down in a place that might be observed by an onlooker (maybe a colleague, visitor or intruder) is an example of risk avoidance.

Computers plainly need electrical power, hence they are vulnerable to issues such as power cuts, accidental disconnection, flat batteries, brown-outs, surges, spikes, electrical interference and electronic failures. Physical information security controls to address the associated risks include: fuses, no-break battery-backed power supplies, electrical generators, redundant power sources and cabling, "Do not remove" warning signs on plugs, surge protectors, power quality monitoring, spare batteries, professional design and installation of power circuits plus regular inspections/tests and preventive maintenance. It is ironic that so-called uninterruptible power supplies often lead to power interruptions if they are inadequately specified, designed, manufactured, used, managed or maintained - an illustration of the failure of a critical (physical) control.
USB flash drive security
From Wikipedia, the free encyclopedia
  (Redirected from Secure USB drive)

This article includes a list of references, but its sources remain unclear because it has insufficient inline citations. Please help to improve this article by introducing more precise citations. (September 2009)
Secure USB flash drives protect the data stored on them from access by unauthorized users. USB flash drive products have been on the market since 2000, and their use is increasing exponentially.[1][2] As both consumers and businesses have increased demand for these drives, manufacturers are producing faster devices with greater data storage capacities.

An increasing number of portable devices are used in business, such as laptops, notebooks, universal serial bus (USB) flash drives, personal digital assistants (PDAs), advanced mobile phones and other mobile devices.

Companies in particular are at risk when sensitive data are stored on unsecured USB flash drives by employees who use the devices to transport data outside the office. The consequences of losing drives loaded with such information can be significant, and include the loss of customer data, financial information, business plans and other confidential information, with the associated risk of reputation damage.

Contents  [hide] 
1	Major dangers of USB drives
1.1	Data leakage
1.2	Malware infections
2	Solutions
2.1	Software encryption
2.2	Hardware encryption
2.2.1	Compromised systems
3	Remote management
4	See also
5	References
6	External links
Major dangers of USB drives[edit]
USB flash drives pose two major challenges to information system security: data leakage owing to their small size and ubiquity, and system compromise through infections from computer viruses and other malicious software.

Data leakage[edit]
The large storage capacity of USB flash drives relative to their small size and low cost means that using them for data storage without adequate operational and logical controls can pose a serious threat to information confidentiality, integrity, and availability. The following factors should be taken into consideration for securing USB drives assets:

Storage: USB flash drives are hard to track physically, being stored in bags, backpacks, laptop cases, jackets, trouser pockets, or left at unattended workstations.
Usage: tracking corporate data stored on personal flash drives is a significant challenge; the drives are small, common, and constantly moving. While many enterprises have strict management policies toward USB drives, and some companies ban them outright to minimize risk, others seem unaware of the risks these devices pose to system security.
The average cost of a data breach from any source (not necessarily a flash drive) ranges from less than $100,000 to about $2.5 million.[1]

A SanDisk survey [3] characterized the data corporate end users most frequently copy:

customer data (25%)
financial information (17%)
business plans (15%)
employee data (13%)
marketing plans (13%)
intellectual property (6%)
source code (6%)
Examples of security breaches resulting from USB drives include:

In the UK:
HM Revenue & Customs lost personal details of 6,500 private pension holders
In the United States:
a USB drive was stolen with names, grades, and social security numbers of 6,500 former students [4]
USB flash drives with US Army classified military information were up for sale at a bazaar outside Bagram, Afghanistan.[5]
Malware infections[edit]
See also: BadUSB
In the early days of computer viruses and malware the primary means of transmission and infection was the floppy disk. Today, USB flash drives perform the same data and software storage and transfer role as the floppy disk, often used for transferring files between computers which may be on different networks or in different offices, owned by different people; this has made USB flash drives a leading form of information system infection. When a piece of malware gets onto a USB flash drive it may infect the devices into which that drive is subsequently plugged.

The prevalence of malware infection by means of USB flash drive was documented in a 2011 Microsoft study [6] analyzing data from more than 600 million systems worldwide in the first half of 2011. The study found that 26 percent of all malware infections of Windows system were due to USB flash drives exploiting the AutoRun feature in Microsoft Windows. That finding was in line with other statistics, such as the monthly reporting of most commonly detected malware by antivirus company ESET, which lists abuse of autorun.inf as first among the top ten threats in 2011.[7]

The Windows autorun.inf file contains information on programs meant to run automatically when removable media (often USB flash drives and similar devices) are accessed by a Windows PC user. The default Autorun setting in Windows versions prior to Windows 7 will automatically run a program listed in the autorun.inf file when you access many kinds of removable media. Many types of malware copy themselves to removable storage devices: while this is not always the program’s primary distribution mechanism, malware authors often build in additional infection techniques.

Examples of malware spread by USB flash drives include:

The Stuxnet worm.[8]
Flame modular computer malware.[9]
Solutions[edit]
Since the security of the physical drive cannot be guaranteed without compromising the benefits of portability, security measures are primarily devoted to making the data on a compromised drive inaccessible to unauthorized users and unauthorized processes, such as may be executed by malware. One common approach is to encrypt the data for storage, and routinely scan drives for malware with an antivirus program, although other methods are possible.

Software encryption[edit]
Software solutions such as dm-crypt, FreeOTFE, Data Protecto and TrueCrypt allow the contents of a USB drive to be encrypted automatically and transparently. Also, Windows 7 Enterprise and Ultimate Editions and Windows Server 2008 R2 provide USB drive encryption using BitLocker to Go. The Apple Computer Mac OS X operating system has provided software for disc data encryption since Mac OS X Panther was issued in 2003 (see also: Disk Utility).[10]

Additional software can be installed on an external USB drive to prevent access to files in case the drive becomes lost or stolen. Installing software on company computers may help track and minimize risk by recording the interactions between any USB drive and the computer and storing them in a centralized database.

Hardware encryption[edit]
Some USB drives utilize hardware encryption in which microchips within the USB drive provide automatic and transparent encryption.[11] Some manufacturers offer drives that require a pin code to be entered into a physical keypad on the device before allowing access to the drive. The cost of these USB drives can be significant but is starting to fall due to this type of USB drive gaining popularity.

Hardware systems may offer additional features, such as the ability to automatically overwrite the contents of the drive if the wrong password is entered more than a certain number of times. This type of functionality cannot be provided by a software system since the encrypted data can simply be copied from the drive. However, this form of hardware security can result in data loss if activated accidentally by legitimate users, and strong encryption algorithms essentially make such functionality redundant.

As the encryption keys used in hardware encryption are typically never stored in the computer's memory, technically hardware solutions are less subject to "cold boot" attacks than software-based systems.[12] In reality however, "cold boot" attacks pose little (if any) threat, assuming basic, rudimentary, security precautions are taken with software-based systems.[13]

Compromised systems[edit]
The security of encrypted flash drives is constantly tested by individual hackers as well as professional security firms. At times (as in January 2010) data on flash drives that have been positioned as secure were found[14] to have a bug that potentially could give access to data without knowledge of the correct password.

Flash drives that have been compromised (and fixed) include:

SanDisk Cruzer Enterprise[15]
Kingston DataTraveler BlackBox[16]
Verbatim Corporate Secure USB Flash Drive[17]
Trek Technology ThumbDrive CRYPTO[18]
All of the above companies reacted immediately. Kingston offered replacement drives with a different security architecture. SanDisk, Verbatim, and Trek released patches.

Remote management[edit]
In commercial environments, where most secure USB drives are used,[1] a central/remote management system may provide organizations with an additional level of IT asset control, significantly reducing the risks of a harmful data breach. This can include initial user deployment and ongoing management, password recovery, data backup, remote tracking of sensitive data, and termination of any issued secure USB drives. Such management systems are available as software as a service (SaaS), where Internet connectivity is allowed, or as behind-the-firewall solutions.
Zeroisation
From Wikipedia, the free encyclopedia
In cryptography, zeroisation (also spelled zeroization) is the practice of erasing sensitive parameters (electronically stored data, cryptographic keys, and CSPs) from a cryptographic module to prevent their disclosure if the equipment is captured. This is generally accomplished by altering or deleting the contents to prevent recovery of the data.[1] When encryption was performed by mechanical devices, this would often mean changing all the machine's settings to some fixed, meaningless value, such as zero. On machines with letter settings rather than numerals, the letter 'O' was often used instead. Some machines had a button or lever for performing this process in a single step. Zeroisation would typically be performed at the end of an encryption session to prevent accidental disclosure of the keys, or immediately when there was a risk of capture by an adversary.[2]

In modern software based cryptographic modules, zeroisation is made considerably more complex by issues such as virtual memory, compiler optimisations[3] and use of flash memory.[4] Also, zeroisation may need to be applied not only to the key, but also to a plaintext and some intermediate values. A cryptographic software developer must have an intimate understanding of memory management in a machine, and be prepared to zeroise data whenever a sensitive location might move outside the security boundary. Typically this will involve overwriting the data with zeroes, but in the case of some types of non-volatile storage the process is much more complex; see data remanence.

As well as zeroising data due to memory management, software designers consider performing zeroisation:

When an application changes mode (e.g. to a test mode) or user;
When a computer process changes privileges;
On termination (including abnormal termination);
On any error condition which may indicate instability or tampering;
Upon user request;
Immediately, the last time the parameter is required; and
Possibly if a parameter has not been required for some time.
Informally, software developers may also use zeroise to mean any overwriting of sensitive data, not necessarily of a cryptographic nature.

In tamper resistant hardware, automatic zeroisation may be initiated when tampering is detected. Such hardware may be rated for cold zeroisation, the ability to zeroise itself without its normal power supply enabled.

Standards for zeroisation are specified in ANSI X9.17 and FIPS 140-2.

Digital data
From Wikipedia, the free encyclopedia
This article is about broad technical and mathematical information regarding digital data. For alternate or more specific uses, see Digital (disambiguation).
Digital data, in information theory and information systems, are discrete, discontinuous representations of information or works, as contrasted with continuous, or analog signals which behave in a continuous manner, or represent information using a continuous function.

Although digital representations are the subject matter of discrete mathematics, the information represented can be either discrete, such as numbers and letters, or it can be continuous, such as sounds, images, and other measurements.

The word digital comes from the same source as the words digit and digitus (the Latin word for finger), as fingers are often used for discrete counting. Mathematician George Stibitz of Bell Telephone Laboratories used the word digital in reference to the fast electric pulses emitted by a device designed to aim and fire anti-aircraft guns in 1942.[1] The term is most commonly used in computing and electronics, especially where real-world information is converted to binary numeric form as in digital audio and digital photography.

Contents  [hide] 
1	Symbol to digital conversion
2	Properties of digital information
3	Historical digital systems
4	See also
5	References
6	Further reading
Symbol to digital conversion[edit]
Since symbols (for example, alphanumeric characters) are not continuous, representing symbols digitally is rather simpler than conversion of continuous or analog information to digital. Instead of sampling and quantization as in analog-to-digital conversion, such techniques as polling and encoding are used.

A symbol input device usually consists of a group of switches that are polled at regular intervals to see which switches are switched. Data will be lost if, within a single polling interval, two switches are pressed, or a switch is pressed, released, and pressed again. This polling can be done by a specialized processor in the device to prevent burdening the main CPU. When a new symbol has been entered, the device typically sends an interrupt, in a specialized format, so that the CPU can read it.

For devices with only a few switches (such as the buttons on a joystick), the status of each can be encoded as bits (usually 0 for released and 1 for pressed) in a single word. This is useful when combinations of key presses are meaningful, and is sometimes used for passing the status of modifier keys on a keyboard (such as shift and control). But it does not scale to support more keys than the number of bits in a single byte or word.

Devices with many switches (such as a computer keyboard) usually arrange these switches in a scan matrix, with the individual switches on the intersections of x and y lines. When a switch is pressed, it connects the corresponding x and y lines together. Polling (often called scanning in this case) is done by activating each x line in sequence and detecting which y lines then have a signal, thus which keys are pressed. When the keyboard processor detects that a key has changed state, it sends a signal to the CPU indicating the scan code of the key and its new state. The symbol is then encoded, or converted into a number, based on the status of modifier keys and the desired character encoding.

A custom encoding can be used for a specific application with no loss of data. However, using a standard encoding such as ASCII is problematic if a symbol such as '?' needs to be converted but is not in the standard.

It is estimated that in the year 1986 less than 1% of the world's technological capacity to store information was digital and in 2007 it was already 94%.[2] The year 2002 is assumed to be the year when human kind was able to store more information in digital than in analog format (the "beginning of the digital age").[3][4]

Properties of digital information[edit]
All digital information possesses common properties that distinguish it from analog data with respect to communications:

Synchronization: Since digital information is conveyed by the sequence in which symbols are ordered, all digital schemes have some method for determining the beginning of a sequence. In written or spoken human languages synchronization is typically provided by pauses (spaces), capitalization, and punctuation. Machine communications typically use special synchronization sequences.
Language: All digital communications require a language, which in this context consists of all the information that the sender and receiver of the digital communication must both possess, in advance, in order for the communication to be successful. Languages are generally arbitrary and specify the meaning to be assigned to particular symbol sequences, the allowed range of values, methods to be used for synchronization, etc.
Errors: Disturbances (noise) in analog communications invariably introduce some, generally small deviation or error between the intended and actual communication. Disturbances in a digital communication do not result in errors unless the disturbance is so large as to result in a symbol being misinterpreted as another symbol or disturb the sequence of symbols. It is therefore generally possible to have an entirely error-free digital communication. Further, techniques such as check codes may be used to detect errors and guarantee error-free communications through redundancy or retransmission. Errors in digital communications can take the form of substitution errors in which a symbol is replaced by another symbol, or insertion/deletion errors in which an extra incorrect symbol is inserted into or deleted from a digital message. Uncorrected errors in digital communications have unpredictable and generally large impact on the information content of the communication.
Copying: Because of the inevitable presence of noise, making many successive copies of an analog communication is infeasible because each generation increases the noise. Because digital communications are generally error-free, copies of copies can be made indefinitely.
Granularity: The digital representation of a continuously variable analog value typically involves a selection of the number of symbols to be assigned to that value. The number of symbols determines the precision or resolution of the resulting datum. The difference between the actual analog value and the digital representation is known as quantization error. For example, if the actual temperature is 23.234456544453 degrees, but if only two digits (23) are assigned to this parameter in a particular digital representation, the quantizing error is: 0.234456544453. This property of digital communication is known as granularity.
Compressible: According to Miller, "Uncompressed digital data is very large, and in its raw form would actually produce a larger signal (therefore be more difficult to transfer) than analog data. However, digital data can be compressed. Compression reduces the amount of bandwidth space needed to send information. Data can be compressed, sent and then decompressed at the site of consumption. This makes it possible to send much more information and result in, for example, digital television signals offering more room on the airwave spectrum for more television channels."[4]
Historical digital systems[edit]
Even though digital signals are generally associated with the binary electronic digital systems used in modern electronics and computing, digital systems are actually ancient, and need not be binary or electronic.[citation needed]

Written text (due to the limited character set and the use of discrete symbols - the alphabet in most cases)
The abacus was created sometime between 1000 BC and 500 BC, it later became a form of calculation frequency. Nowadays it can be used as a very advanced, yet basic digital calculator that uses beads on rows to represent numbers. Beads only have meaning in discrete up and down states, not in analog in-between states.
A beacon is perhaps the simplest non-electronic digital signal, with just two states (on and off). In particular, smoke signals are one of the oldest examples of a digital signal, where an analog "carrier" (smoke) is modulated with a blanket to generate a digital signal (puffs) that conveys information.
Morse code uses six digital states—dot, dash, intra-character gap (between each dot or dash), short gap (between each letter), medium gap (between words), and long gap (between sentences)—to send messages via a variety of potential carriers such as electricity or light, for example using an electrical telegraph or a flashing light.
The Braille system was the first binary format for character encoding, using a six-bit code rendered as dot patterns.
Flag semaphore uses rods or flags held in particular positions to send messages to the receiver watching them some distance away.
International maritime signal flags have distinctive markings that represent letters of the alphabet to allow ships to send messages to each other.
More recently invented, a modem modulates an analog "carrier" signal (such as sound) to encode binary electrical digital information, as a series of binary digital sound pulses. A slightly earlier, surprisingly reliable version of the same concept was to bundle a sequence of audio digital "signal" and "no signal" information (i.e. "sound" and "silence") on magnetic cassette tape for use with early home computers.
Information sensitivity
From Wikipedia, the free encyclopedia

[hide]This article has multiple issues. Please help improve it (see how) or discuss these issues on the talk page.
This article needs additional citations for verification. (July 2007)
The examples and perspective in this article deal primarily with the United States and do not represent a worldwide view of the subject. (December 2010)
Information sensitivity is the control of access to information or knowledge that might result in loss of an advantage or level of security if disclosed to others.

Loss, misuse, modification, or unauthorized access to sensitive information can adversely affect the privacy or welfare of an individual, trade secrets of a business or even the security and international relations of a nation depending on the level of sensitivity and nature of the information.[1]

Contents  [hide] 
1	Non-sensitive information
1.1	Public information
1.2	Routine business information
2	Types of sensitive information
2.1	Confidential business information
2.2	Classified
3	Legal protection from unauthorised disclosure
3.1	Personal and private information
3.2	Confidential business information
3.3	Classified information
4	Digital sensitive information
5	See also
6	External links
7	Notes
Non-sensitive information[edit]
Public information[edit]
This refers to information that is already a matter of public record or knowledge. With regard to government and private organizations, access to or release of such information may be requested by any member of the public, and there are often formal processes laid out for how to do so.[2] The accessibility of government-held public records is an important part of government transparency, accountability to its citizens, and the values of democracy.[3] Public records may furthermore refer to information about identifiable individuals that is not considered confidential, including but not limited to: census records, criminal records, sex offender registry files, and voter registration.

Routine business information[edit]
This includes business information that is not subjected to special protection and may be routinely shared with anyone inside or outside of the business.

Types of sensitive information[edit]
Confidential information is used in a general sense to mean sensitive information whose access is subject to restriction, and may refer to information about an individual as well as that which pertains to a business.

However, there are situations in which the release of personal information could have a negative effect on its owner. For example, a person trying to avoid a stalker will be inclined to further restrict access to such personal information. Furthermore, a person's SSN or SIN, credit card numbers, and other financial information may be considered private if their disclosure might lead to crimes such as identity theft or fraud.

Some types of private information, including records of a person's health care, education, and employment may be protected by privacy laws. Unauthorized disclosure of private information can make the perpetrator liable for civil remedies and may in some cases be subject to criminal penalties.

Even though they are often used interchangeably, personal information is sometimes distinguished from private information, or personally identifiable information. The latter is distinct from the former in that private information can be used to identify a unique individual. Personal information, on the other hand, is information belonging to the private life of an individual that cannot be used to uniquely identify that individual. This can range from an individual’s favourite colour, to the details of their domestic life.[4] The latter is a common example of personal information that is also regarded as sensitive, where the individual sharing these details with a trusted listener would prefer for it not to be shared with anyone else, and the sharing of which may result in unwanted consequences.

Confidential business information[edit]
Confidential business information refers to information whose disclosure may harm the business. Such information may include trade secrets, sales and marketing plans, new product plans, notes associated with patentable inventions, customer and supplier information, financial data, and more.[5]

Classified[edit]
Classified information generally refers to information that is subject to special security classification regulations imposed by many national governments, the disclosure of which may cause harm to national interests and security. The protocol of restriction imposed upon such information is categorized into a hierarchy of classification levels in almost every national government worldwide, with the most restricted levels containing information that may cause the greatest danger to national security if leaked. Authorized access is granted to individuals on a need to know basis who have also passed the appropriate level of security clearance. Classified information can be reclassified to a different level or declassified (made available to the public) depending on changes of situation or new intelligence.

Legal protection from unauthorised disclosure[edit]
Personal and private information[edit]
Data privacy concerns exist in various aspects of daily life wherever personal data is stored and collected, such as on the internet, in medical records, financial records, and expression of political opinions. In over 80 countries in the world, personally identifiable information is protected by information privacy laws, which outline limits to the collection and use of personally identifiable information by public and private entities. Such laws usually require entities to give clear and unambiguous notice to the individual of the types of data being collected, its reason for collection, and planned uses of the data. In consent-based legal frameworks, explicit consent of the individual is required as well.[6]

In the European Union, the Data Protection Directive provides a rigorous standard for privacy protection legislation across all member states. Although the Directive is not legally binding in itself, all member states are expected to enact their own national privacy legislation within three years of the Directive’s adoption that conforms to all of its standards.[7] Since adoption, the Directive has demonstrated significant influence on the privacy legislation of non-EU nations, through its requirements on the privacy laws of non-member nations engaging in transborder flows of private data with EU member nations.[8]

Presently, the EU is drafting the General Data Protection Regulation (GDPR), which will replace the Directive and account for the privacy implications of recent changes in technology such as social networks and cloud computing. The GDPR is expected to be adopted in 2014, and be implemented by member states by 2016.[9][10]

In Canada, the Personal Information Protection and Electronic Documents Act (PIPEDA) regulates the collection and use of personal data and electronic documents by public and private organizations. PIPEDA is in effect in all federal and provincial jurisdictions, except provinces where existing privacy laws are determined to be “substantially similar”.[11]

Rather than a comprehensive framework, the United States has in contrast a patchwork of privacy legislation pertaining to different specific aspects of data privacy, relying on a mix of legislation, regulation, and self-regulation.

Additionally, many other countries have enacted their own legislature regarding data privacy protection, and more are still in the process of doing so.[12]

Confidential business information[edit]
The confidentiality of sensitive business information is established through non-disclosure agreements, a legally binding contract between two parties in a professional relationship. NDAs may be one-way, such as in the case of an employee receiving confidential information about the employing organization, or two-way between businesses needing to share information with one another to accomplish a business goal. Depending on the severity of consequences, a violation of non-disclosure may result in employment loss, loss of business and client contacts, criminal charges or a civil lawsuit, and a hefty sum in damages.[13] When NDAs are signed between employer and employee at the initiation of employment, a non-compete clause may be a part of the agreement as an added protection of sensitive business information, where the employee agrees not to work for competitors or start their own competing business within a certain time or geographical limit.

Unlike personal and private information, there is no internationally recognized framework protecting trade secrets, or even an agreed-upon definition of the term “trade secret”.[14] However, many countries and political jurisdictions have taken the initiative to account for the violation of commercial confidentiality in their criminal or civil laws. For example, under the US Economic Espionage Act of 1996, it is a federal crime in the United States to misappropriate trade secrets with the knowledge that it will benefit a foreign power, or will injure the owner of the trade secret.[15] More commonly, breach of commercial confidentiality falls under civil law, such as in the United Kingdom.[16] In some developing countries, trade secret laws are either non-existent or poorly developed and offer little substantial protection.[17]

Classified information[edit]
In many countries, unauthorized disclosure of classified information is a criminal offence, and may be punishable by fines, prison sentence, or even the death penalty, depending on the severity of the violation.[18][19] For less severe violations, civil sanctions may be imposed, ranging from reprimand to revoking of security clearance and subsequent termination of employment.[20]

Whistleblowing is the intentional disclosure of sensitive information to a third-party with the intention of revealing alleged illegal, immoral, or otherwise harmful actions.[21] There are many examples of present and former government employees disclosing classified information regarding national government misconduct to the public and media, in spite of the criminal consequences that await them.

Espionage, or spying, involves obtaining sensitive information without the permission or knowledge of its holder. The use of spies is a part of national intelligence gathering in most countries, and has been used as a political strategy by nation-states since ancient times. It is unspoken knowledge in international politics that countries are spying on one another all the time, even their allies.[22]

Digital sensitive information[edit]
Computer security is information security applied to computing and network technology, and is a significant and ever-growing field in computer science. The term computer insecurity, on the other hand, refers to the concept that computer systems are inherently vulnerable to attack, and therefore an evolving arms race between those who exploit existing vulnerabilities in security systems and those who must then engineer new mechanisms of security.

A number of security concerns have arisen in the recent years as increasing amounts of sensitive information at every level have found their primary existence in digital form. At the personal level, credit card fraud, internet fraud, and other forms of identity theft have become widespread concerns that individuals need to be aware of on a day-to-day basis. The existence of large databases of classified information on computer networks is also changing the face of domestic and international politics. Cyber-warfare and cyber espionage is becoming of increasing importance to the national security and strategy of nations around the world, and it is estimated that 120 nations around the world are currently actively engaged in developing and deploying technology for these purposes.[23]

Philosophies and internet cultures such as open-source governance, hacktivism, and the popular hacktivist slogan “information wants to be free” reflects some of the cultural shifts in perception towards political and government secrecy. The popular, controversial Wikileaks is just one of many manifestations of a growing cultural sentiment that is becoming an additional challenge to the security and integrity of classified information.[24]

See also[edit]
Espionage
Federal Standard 1037C and the National Information Systems Security Glossary
Mandatory Access Control
Privacy protocol
Data remanence
From Wikipedia, the free encyclopedia
Data remanence is the residual representation of digital data that remains even after attempts have been made to remove or erase the data. This residue may result from data being left intact by a nominal file deletion operation, by reformatting of storage media that does not remove data previously written to the media, or through physical properties of the storage media that allow previously written data to be recovered. Data remanence may make inadvertent disclosure of sensitive information possible should the storage media be released into an uncontrolled environment (e.g., thrown in the trash, or lost).

Various techniques have been developed to counter data remanence. These techniques are classified as clearing, purging/sanitizing or destruction. Specific methods include overwriting, degaussing, encryption, and media destruction.

Effective application of countermeasures can be complicated by several factors, including media that are inaccessible, media that cannot effectively be erased, advanced storage systems that maintain histories of data throughout the data's life cycle, and persistence of data in memory that is typically considered volatile.

Several standards exist for the secure removal of data and the elimination of data remanence.

Contents  [hide] 
1	Causes
2	Countermeasures
2.1	Clearing
2.2	Purging
2.3	Destruction
3	Specific methods
3.1	Overwriting
3.1.1	Feasibility of recovering overwritten data
3.2	Degaussing
3.3	Encryption
3.4	Media destruction
4	Complications
4.1	Inaccessible media areas
4.2	Advanced storage systems
4.3	Optical media
4.4	Data on solid-state drives
4.5	Data in RAM
5	Standards
6	See also
7	References
8	Further reading
Causes[edit]
Many operating systems, file managers, and other software provide a facility where a file is not immediately deleted when the user requests that action. Instead, the file is moved to a holding area, to allow the user to easily revert a mistake. Similarly, many software products automatically create backup copies of files that are being edited, to allow the user to restore the original version, or to recover from a possible crash (autosave feature).

Even when an explicit deleted file retention facility is not provided or when the user does not use it, operating systems do not actually remove the contents of a file when it is deleted unless they are aware that explicit erasure commands are required, like on a solid-state drive. (In such cases, the operating system will issue the Serial ATA TRIM command or the SCSI UNMAP command to let the drive know to no longer maintain the deleted data.) Instead, they simply remove the file's entry from the file system directory, because this requires less work and is therefore faster, and the contents of the file—the actual data—remain on the storage medium. The data will remain there until the operating system reuses the space for new data. In some systems, enough filesystem metadata are also left behind to enable easy undeletion by commonly available utility software. Even when undelete has become impossible, the data, until it has been overwritten, can be read by software that reads disk sectors directly. Computer forensics often employs such software.

Likewise, reformatting, repartitioning, or reimaging a system is unlikely to write to every area of the disk, though all will cause the disk to appear empty or, in the case of reimaging, empty except for the files present in the image, to most software.

Finally, even when the storage media is overwritten, physical properties of the media may permit recovery of the previous contents. In most cases however, this recovery is not possible by just reading from the storage device in the usual way, but requires using laboratory techniques such as disassembling the device and directly accessing/reading from its components.[citation needed]

The section on complications gives further explanations for causes of data remanence.

Countermeasures[edit]
Main article: Data erasure
There are three levels commonly recognized for eliminating remnant data:

Clearing[edit]
Clearing is the removal of sensitive data from storage devices in such a way that there is assurance that the data may not be reconstructed using normal system functions or software file/data recovery utilities.[citation needed] The data may still be recoverable, but not without special laboratory techniques.[1]

Clearing is typically an administrative protection against accidental disclosure within an organization. For example, before a hard drive is re-used within an organization, its contents may be cleared to prevent their accidental disclosure to the next user.

Purging[edit]
Purging or sanitizing is the removal of sensitive data from a system or storage device with the intent that the data can not be reconstructed by any known technique.[citation needed] Purging, proportional to the sensitivity of the data, is generally done before releasing media beyond control, such as before discarding old media, or moving media to a computer with different security requirements.

Destruction[edit]
The storage media is made unusable for conventional equipment. Effectiveness of destroying the media varies by medium and method. Depending on recording density of the media, and/or the destruction technique, this may leave data recoverable by laboratory methods. Conversely, destruction using appropriate techniques is the most secure method of preventing retrieval.

Specific methods[edit]
Overwriting[edit]
A common method used to counter data remanence is to overwrite the storage media with new data. This is often called wiping or shredding a file or disk, by analogy to common methods of destroying print media, although the mechanism bears no similarity to these. Because such a method can often be implemented in software alone, and may be able to selectively target only part of the media, it is a popular, low-cost option for some applications. Overwriting is generally an acceptable method of clearing, as long as the media is writable and not damaged.

The simplest overwrite technique writes the same data everywhere—often just a pattern of all zeros. At a minimum, this will prevent the data from being retrieved simply by reading from the media again using standard system functions.

In an attempt to counter more advanced data recovery techniques, specific overwrite patterns and multiple passes have often been prescribed. These may be generic patterns intended to eradicate any trace signatures, for example, the seven-pass pattern: 0xF6, 0x00, 0xFF, random, 0x00, 0xFF, random; sometimes erroneously[clarification needed] attributed to the US standard DOD 5220.22-M.

One challenge with an overwrite is that some areas of the disk may be inaccessible, due to media degradation or other errors. Software overwrite may also be problematic in high-security environments which require stronger controls on data commingling than can be provided by the software in use. The use of advanced storage technologies may also make file-based overwrite ineffective (see the discussion below under Complications).

There are specialized machines and software that are capable of doing overwriting. The software can sometimes be a standalone operating system specifically designed for data destruction. There are also machines specifically designed to wipe hard drives to the department of defense specifications DOD 5220.22-M as well.[citation needed]

Feasibility of recovering overwritten data[edit]
Peter Gutmann investigated data recovery from nominally overwritten media in the mid-1990s. He suggested magnetic force microscopy may be able to recover such data, and developed specific patterns, for specific drive technologies, designed to counter such.[2] These patterns have come to be known as the Gutmann method.

Daniel Feenberg, an economist at the private National Bureau of Economic Research, claims that the chances of overwritten data being recovered from a modern hard drive amount to "urban legend".[3] He also points to the "18? minute gap" Rose Mary Woods created on a tape of Richard Nixon discussing the Watergate break-in. Erased information in the gap has not been recovered, and Feenberg claims doing so would be an easy task compared to recovery of a modern high density digital signal.

As of November 2007, the United States Department of Defense considers overwriting acceptable for clearing magnetic media within the same security area/zone, but not as a sanitization method. Only degaussing or physical destruction is acceptable for the latter.[4]

On the other hand, according to the 2006 NIST Special Publication 800-88 (p. 7): "Studies have shown that most of today’s media can be effectively cleared by one overwrite" and "for ATA disk drives manufactured after 2001 (over 15 GB) the terms clearing and purging have converged."[5] An analysis by Wright et al. of recovery techniques, including magnetic force microscopy, also concludes that a single wipe is all that is required for modern drives. They point out that the long time required for multiple wipes "has created a situation where many organisations ignore the issue all together – resulting in data leaks and loss."[6]

Degaussing[edit]
Degaussing is the removal or reduction of a magnetic field of a disk or drive, using a device called a degausser that has been designed for the media being erased. Applied to magnetic media, degaussing may purge an entire media element quickly and effectively.

Degaussing often renders hard disks inoperable, as it erases low-level formatting that is only done at the factory during manufacturing. In some cases, it is possible to return the drive to a functional state by having it serviced at the manufacturer. However, some modern degaussers use such a strong magnetic pulse that the motor that spins the platters may be destroyed in the degaussing process, and servicing may not be cost-effective. Degaussed computer tape such as DLT can generally be reformatted and reused with standard consumer hardware.

In some high-security environments, one may be required to use a degausser that has been approved for the task. For example, in US government and military jurisdictions, one may be required to use a degausser from the NSA's "Evaluated Products List".[7]

Encryption[edit]
Encrypting data before it is stored on the media may mitigate concerns about data remanence. If the decryption key is strong and carefully controlled, it may effectively make any data on the media unrecoverable. Even if the key is stored on the media, it may prove easier or quicker to overwrite just the key, vs the entire disk. This process is called crypto erase in the security industry.[8]

Encryption may be done on a file-by-file basis, or on the whole disk. Cold boot attacks are one of the few possible methods for subverting a full-disk encryption method, as there is no possibility of storing the plain text key in an unencrypted section of the medium. See the section Complications: Data in RAM for further discussion.

Other side-channel attacks (such as keyloggers, acquisition of a written note containing the decryption key, or rubber hose cryptography) may offer a greater chance to success, but do not rely on weaknesses in the cryptographic method employed. As such, their relevance for this article is minor.

Media destruction[edit]

The pieces of a physically destroyed hard disk drive.
Thorough destruction of the underlying storage media is the most certain way to counter data remanence. However, the process is generally time-consuming, cumbersome, and may require extremely thorough methods, as even a small fragment of the media may contain large amounts of data.

Specific destruction techniques include:

Physically breaking the media apart (e.g., by grinding or shredding)
Chemically altering the media into a non-readable, non-reverse-constructible state (e.g., through incineration or exposure to caustic/corrosive chemicals)
Phase transition (e.g., liquefaction or vaporization of a solid disk)
For magnetic media, raising its temperature above the Curie point
For many electric/electronic volatile and non-volatile storage mediums, exposure to electromagnetic fields greatly exceeding safe operational specifications (e.g., high-voltage electric current or high-amplitude microwave radiation)[citation needed]
Complications[edit]
Inaccessible media areas[edit]
Storage media may have areas which become inaccessible by normal means. For example, magnetic disks may develop new bad sectors after data has been written, and tapes require inter-record gaps. Modern hard disks often feature reallocation of marginal sectors or tracks, automated in a way that the OS would not need to work with it. This problem is especially significant in solid state drives (SSDs) that rely on relatively large relocated bad block tables. Attempts to counter data remanence by overwriting may not be successful in such situations, as data remnants may persist in such nominally inaccessible areas.

Advanced storage systems[edit]
Data storage systems with more sophisticated features may make overwrite ineffective, especially on a per-file basis. For example, journaling file systems increase the integrity of data by recording write operations in multiple locations, and applying transaction-like semantics; on such systems, data remnants may exist in locations "outside" the nominal file storage location. Some file systems also implement copy-on-write or built-in revision control, with the intent that writing to a file never overwrites data in-place. Furthermore, technologies such as RAID and anti-fragmentation techniques may result in file data being written to multiple locations, either by design (for fault tolerance), or as data remnants.

Wear leveling can also defeat data erasure, by relocating blocks between the time when they are originally written and the time when they are overwritten. For this reason, some security protocols tailored to operating systems or other software featuring automatic wear leveling recommend conducting a free-space wipe of a given drive and then copying many small, easily identifiable "junk" files or files containing other nonsensitive data to fill as much of that drive as possible, leaving only the amount of free space necessary for satisfactory operation of system hardware and software. As storage and/or system demands grow, the "junk data" files can be deleted as necessary to free up space; even if the deletion of "junk data" files is not secure, their initial nonsensitivity reduces to near zero the consequences of recovery of data remanent from them.[citation needed]

Optical media[edit]
As optical media are not magnetic, they are not erased by conventional degaussing. Write-once optical media (CD-R, DVD-R, etc.) also cannot be purged by overwriting. Read/write optical media, such as CD-RW and DVD-RW, may be receptive to overwriting. Methods for successfully sanitizing optical discs include delaminating or abrading the metallic data layer, shredding, incinerating, destructive electrical arcing (as by exposure to microwave energy), and submersion in a polycarbonate solvent (e.g., acetone).

Data on solid-state drives[edit]
Research[9] from the Center for Magnetic Recording and Research, University of California, San Diego has uncovered problems inherent in erasing data stored on solid-state drives (SSDs). Researchers discovered three problems with file storage on SSDs:

First, built-in commands are effective, but manufacturers sometimes implement them incorrectly. Second, overwriting the entire visible address space of an SSD twice is usually, but not always, sufficient to sanitize the drive. Third, none of the existing hard drive-oriented techniques for individual file sanitization are effective on SSDs.[9](p1)

Solid-state drives, which are flash-based, differ from hard-disk drives in two ways: first, in the way data is stored; and second, in the way the algorithms are used to manage and access that data. These differences can be exploited to recover previously erased data. SSDs maintain a layer of indirection between the logical addresses used by computer systems to access data and the internal addresses that identify physical storage. This layer of indirection hides idiosyncratic media interfaces and enhances SSD performance, reliability, and lifespan (see wear leveling); but it can also produce copies of the data that are invisible to the user and that a sophisticated attacker could recover. For sanitizing entire disks, sanitize commands built into the SSD hardware have been found to be effective when implemented correctly, and software-only techniques for sanitizing entire disks have been found to work most, but not all, of the time.[9]:section 5 In testing, none of the software techniques were effective for sanitizing individual files. These included well-known algorithms such as the Gutmann method, US DoD 5220.22-M, RCMP TSSIT OPS-II, Schneier 7 Pass, and Mac OS X Secure Erase Trash.[9]:section 5

The TRIM feature in many SSD devices, if properly implemented, will eventually erase data after it is deleted, but the process can take some time, typically several minutes. Many older operating systems do not support this feature, and not all combinations of drives and operating systems work.[10]

Data in RAM[edit]
Data remanence has been observed in static random-access memory (SRAM), which is typically considered volatile (i.e., the contents degrade with loss of external power). In one study, data retention was observed even at room temperature.[11]

Data remanence has also been observed in dynamic random-access memory (DRAM). Modern DRAM chips have a built-in self-refresh module, as they not only require a power supply to retain data, but must also be periodically refreshed to prevent their data contents from fading away from the capacitors in their integrated circuits. A study found data remanence in DRAM with data retention of seconds to minutes at room temperature and "a full week without refresh when cooled with liquid nitrogen."[12] The study authors were able to use a cold boot attack to recover cryptographic keys for several popular full disk encryption systems, including Microsoft BitLocker, Apple FileVault, dm-crypt for Linux, and TrueCrypt.[12](p12)

Despite some memory degradation, authors of the above described study were able to take advantage of redundancy in the way keys are stored after they have been expanded for efficient use, such as in key scheduling. The authors recommend that computers be powered down, rather than be left in a "sleep" state, when not in physical control of the owner. In some cases, such as certain modes of the software program BitLocker, the authors recommend that a boot password or a key on a removable USB device be used.[12](p12) TRESOR is a kernel patch for Linux specifically intended to prevent cold boot attacks on RAM by ensuring encryption keys are neither user accessible nor stored in RAM.
Data storage device
From Wikipedia, the free encyclopedia

[hide]This article has multiple issues. Please help improve it (see how) or discuss these issues on the talk page.
This article possibly contains original research. (April 2012)
This article needs additional citations for verification. (April 2012)

Many different consumer electronic devices can store data.

Edison cylinder phonograph ca. 1899. The phonograph cylinder is a storage medium. The phonograph may be considered a storage device.

A reel-to-reel tape recorder (Sony TC-630). The magnetic tape is a data storage medium. The recorder is data storage equipment using a portable medium (tape reel) to store the data.

Crafting tools such as paint brushes can be used as data storage equipment. The paint and canvas can be used as data storage media.

RNA might be the oldest data storage medium.[1]
A data storage device is a device for recording (storing) information (data). Recording can be done using virtually any form of energy, spanning from manual muscle power in handwriting, to acoustic vibrations in phonographic recording, to electromagnetic energy modulating magnetic tape and optical discs.

A storage device may hold information, process information, or both. A device that only holds information is a recording medium. Devices that process information (data storage equipment) may either access a separate portable (removable) recording medium or a permanent component to store and retrieve data.

Electronic data storage requires electrical power to store and retrieve that data. Most storage devices that do not require vision and a brain to read data fall into this category. Electromagnetic data may be stored in either an analog data or digital data format on a variety of media. This type of data is considered to be electronically encoded data, whether it is electronically stored in a semiconductor device, for it is certain that a semiconductor device was used to record it on its medium. Most electronically processed data storage media (including some forms of computer data storage) are considered permanent (non-volatile) storage, that is, the data will remain stored when power is removed from the device. In contrast, most electronically stored information within most types of semiconductor (computer chips) microcircuits are volatile memory, for it vanishes if power is removed.

Except for barcodes and OCR data, electronic data storage is easier to revise and may be more cost effective than alternative methods due to smaller physical space requirements and the ease of replacing (rewriting) data on the same medium.[2] However, the durability of methods such as printed data is still superior to that of most electronic storage media. The durability limitations may be overcome with the ease of duplicating (backing-up) electronic data.

Contents  [hide] 
1	Terminology
2	Global capacity, digitization, and trends
3	Data storage equipment
3.1	Portable methods
3.2	Semi-portable methods
3.3	Volatile methods
4	Recording medium
4.1	Ancient examples
4.2	Modern examples by energy used
4.3	Modern examples by shape
4.4	Weight and volume
5	See also
6	References
7	Further reading
8	External links
Terminology[edit]
Devices that are not used exclusively for recording such as hands, mouths, musical instruments, and devices that are intermediate in the storing/retrieving process, like eyes, ears, cameras, scanners, microphones, speakers, monitors, or video projectors, are generally not considered storage devices. Devices that are exclusively for recording such as printers, exclusively for reading, like barcode readers, or devices that process only one form of information, like phonographs may or may not be considered storage devices. In computing these are known as input/output devices.

All information is data. However, not all data is information.

Many data storage devices are also media players. Any device that can store and playback multimedia may also be considered a media player such as in the case with the HD media player. Designated hard drives are used to play saved or streaming media on home cinemas or home theater PCs.

Global capacity, digitization, and trends[edit]
In a recent study in Science it was estimated that the world's technological capacity to store information in analog and digital devices grew from less than 3 (optimally compressed) exabytes in 1986, to 295 (optimally compressed) exabytes in 2007,[3] and doubles roughly every 3 years.[4]

It is estimated that the year 2002 marked the beginning of the digital age for information storage, the year that marked the date when human kind started to store more information on digital than on analog storage devices.[3] In 1986 only 1% of the world's capacity to store information was in digital format, which grew to 3% by 1993, 25% in the year 2000, and exploded to 97% of the world's storage capacity by 2007.

Data storage equipment[edit]
Any input/output equipment may be considered data storage equipment if it writes to and reads from a data storage medium.

Data storage equipment uses either:

portable methods (easily replaced),
semi-portable methods, requiring mechanical disassembly tools and/or opening a chassis, or
Volatile methods, meaning loss of memory if disconnected from the unit.
The following are examples of those methods:

Portable methods[edit]
Hand crafting
Flat surface
Printmaking
Photographic
Fabrication
Automated assembly
Textile
Molding
Solid freeform fabrication
Cylindrical accessing
Memory card reader/drive
Tape drive
Mono reel or reel-to-reel, see also Open reels
Compact Cassette player/recorder
Data cartridge (tape)
Disk accessing
Disk drive
Disk pack
Disk enclosure
ROM cartridge
Peripheral networking
Flash memory devices
Semi-portable methods[edit]
Hard disk drive
non-volatile RAM
Volatile methods[edit]
Volatile RAM
Neurons[5]
Recording medium[edit]
A recording medium is a physical material that holds data expressed in any of the existing recording formats. With electronic media, the data and the recording medium is sometimes\s part of the surface of the medium.

Some recording media may be temporary either by design or by nature. Volatile organic compounds may be used to preserve the environment or to purposely make data expire over time. Data such as smoke signals or skywriting are temporary by nature.

Ancient examples[edit]

The Gutenberg Bible displayed by the United States Library of Congress, demonstrating printed pages as a storage medium

A set of index cards in a file box are a nonlinear storage medium.
Optical
Any object visible to the eye, used to mark a location such as a stone, flag, or skull.
Any crafting material used to form shapes such as clay, wood, metal, glass, wax, or quipu.
Any hard surface that could hold carvings.
Any branding surface that would scar under intense heat (chiefly for livestock or humans).
Any marking substance such as paint, ink, or chalk.
Any surface that would hold a marking substance such as, papyrus, paper, or skin.
Chemical
RNA
DNA – for uses of DNA by people for storing information, see DNA digital data storage
Pheromone
Modern examples by energy used[edit]

Graffiti on a public wall. Public surfaces are being used as unconventional data storage media, often without permission.

Photographic film is a photochemical data storage medium

A floppy disk is a magnetic data storage medium

A 3.5" PATA hard drive is both storage equipment and a storage medium.
Chemical
Dipstick
Thermodynamic
Thermometer
Photochemical
Photographic film
Mechanical
Pins and holes
Paper
Punched card
Paper tape
Music roll
Music box cylinder or disk
Grooves (See also Audio Data)
Phonograph cylinder
Gramophone record
Dictabelt (groove on plastic belt)
Capacitance Electronic Disc
Magnetic storage
Wire recording (stainless steel wire)
Magnetic tape
Drum memory (magnetic drum)
Floppy disk
Optical storage
Optical jukebox
Photographic paper
X-ray
Microform
Hologram
Projected foil
Optical disc
Magneto-optical drive
Holographic data storage
3D optical data storage
Electrical
Semiconductor used in volatile random-access memory
Floating-gate transistor used in non-volatile memory cards
Modern examples by shape[edit]
A typical way to classify data storage media is to consider its shape and type of movement (or non-movement) relative to the read/write device(s) of the storage apparatus as listed:

Paper card storage
Punched card (mechanical)
Cams and tracers (pipe organ combination-action memory memorizing stop selections)
Tape storage (long, thin, flexible, linearly moving bands)
Paper tape (mechanical)
Magnetic tape (a tape passing one or more read/write/erase heads)
Disk storage (flat, round, rotating object)
Gramophone record (used for distributing some 1980s home computer programs) (mechanical)
Carousel memory (magnetic rolls)
Floppy disk, ZIP disk (removable) (magnetic)
Holographic
Optical disc such as CD, DVD, Blu-ray Disc
Minidisc
Hard disk drive (magnetic)
Magnetic bubble memory
Flash memory/memory card (solid state semiconductor memory)
xD-Picture Card
MultiMediaCard
USB flash drive (also known as a "thumb drive" or "keydrive")
SmartMedia
CompactFlash I and II
Secure Digital
Sony Memory Stick (Std/Duo/PRO/MagicGate versions)
Solid-state drive
Bekenstein (2003) foresees that miniaturization might lead to the invention of devices that store bits on a single atom.[6]

Manufacturing
From Wikipedia, the free encyclopedia
Robocrane Project.jpg

Textile factory (Germany, circa 1975).
Manufacturing is the production of merchandise for use or sale using labour and machines, tools, chemical and biological processing, or formulation. The term may refer to a range of human activity, from handicraft to high tech, but is most commonly applied to industrial production, in which raw materials are transformed into finished goods on a large scale. Such finished goods may be used for manufacturing other, more complex products, such as aircraft, household appliances or automobiles, or sold to wholesalers, who in turn sell them to retailers, who then sell them to end users and consumers.

Manufacturing takes turns under all types of economic systems. In a free market economy, manufacturing is usually directed toward the mass production of products for sale to consumers at a profit. In a collectivist economy, manufacturing is more frequently directed by the state to supply a centrally planned economy. In mixed market economies, manufacturing occurs under some degree of government regulation.

Modern manufacturing includes all intermediate processes required for the production and integration of a product's components. Some industries, such as semiconductor and steel manufacturers use the term fabrication instead.

The manufacturing sector is closely connected with engineering and industrial design. Examples of major manufacturers in North America include General Motors Corporation, General Electric, Procter & Gamble, General Dynamics, Boeing, Pfizer, and Precision Castparts. Examples in Europe include Volkswagen Group, Siemens, and Michelin. Examples in Asia include Sony, Huawei, Lenovo, Toyota, Samsung, and Bridgestone.

Contents  [hide] 
1	History and development
1.1	Manufacturing systems: changes in methods of manufacturing
2	Industrial policy
2.1	Economics of manufacturing
2.2	Manufacturing and investment
3	Countries by manufacturing output using the most recent known data
4	Manufacturing processes
5	Theories
6	Control
7	See also
8	References
9	Sources
10	External links
History and development[edit]

Assembly of Section 41 of a Boeing 787 Dreamliner

A female industrial worker amidst heavy steel semi-products (KINEX BEARINGS, Byt?a, Slovakia, c. 1995–2000)

A modern automobile assembly line.
In its earliest form, manufacturing was usually carried out by a single skilled artisan with assistants. Training was by apprenticeship. In much of the pre-industrial world, the guild system protected the privileges and trade secrets of urban artisans.
Before the Industrial Revolution, most manufacturing occurred in rural areas, where household-based manufacturing served as a supplemental subsistence strategy to agriculture (and continues to do so in places). Entrepreneurs organized a number of manufacturing households into a single enterprise through the putting-out system.
Toll manufacturing is an arrangement whereby a first firm with specialized equipment processes raw materials or semi-finished goods for a second firm.
Manufacturing systems: changes in methods of manufacturing[edit]
Craft or guild system
Agile manufacturing
American system of manufacturing
English system of manufacturing
Fabrication
Flexible manufacturing
Just-in-time manufacturing
Lean manufacturing
Mass customization (2000s) - 3D printing, design-your-own web sites for sneakers, fast fashion
Mass production
Ownership
Packaging and labeling
Prefabrication
Putting-out system
Rapid manufacturing
Reconfigurable manufacturing system
Soviet collectivism in manufacturing
Industrial policy[edit]
Main article: Industrial policy
Economics of manufacturing[edit]
According to some economists, manufacturing is a wealth-producing sector of an economy, whereas a service sector tends to be wealth-consuming.[1][2] Emerging technologies have provided some new growth in advanced manufacturing employment opportunities in the Manufacturing Belt in the United States. Manufacturing provides important material support for national infrastructure and for national defense.

On the other hand, most manufacturing may involve significant social and environmental costs. The clean-up costs of hazardous waste, for example, may outweigh the benefits of a product that creates it. Hazardous materials may expose workers to health risks. These costs are now well known and there is effort to address them by improving efficiency, reducing waste, using industrial symbiosis, and eliminating harmful chemicals.[3] The increased use of technologies such as 3D printing also offer the potential to reduce the environmental impact of producing finished goods through distributed manufacturing.[4]

The negative costs of manufacturing can also be addressed legally. Developed countries regulate manufacturing activity with labor laws and environmental laws. Across the globe, manufacturers can be subject to regulations and pollution taxes to offset the environmental costs of manufacturing activities. Labor unions and craft guilds have played a historic role in the negotiation of worker rights and wages. Environment laws and labor protections that are available in developed nations may not be available in the third world. Tort law and product liability impose additional costs on manufacturing. These are significant dynamics in the ongoing process, occurring over the last few decades, of manufacture-based industries relocating operations to "developing-world" economies where the costs of production are significantly lower than in "developed-world" economies.

Manufacturing and investment[edit]

Capacity utilization in manufacturing in the FRG and in the USA
Surveys and analyses of trends and issues in manufacturing and investment around the world focus on such things as:

The nature and sources of the considerable variations that occur cross-nationally in levels of manufacturing and wider industrial-economic growth;
Competitiveness; and
Attractiveness to foreign direct investors.
In addition to general overviews, researchers have examined the features and factors affecting particular key aspects of manufacturing development. They have compared production and investment in a range of Western and non-Western countries and presented case studies of growth and performance in important individual industries and market-economic sectors.[5][6]

On June 26, 2009, Jeff Immelt, the CEO of General Electric, called for the United States to increase its manufacturing base employment to 20% of the workforce, commenting that the U.S. has outsourced too much in some areas and can no longer rely on the financial sector and consumer spending to drive demand.[7] Further, while U.S. manufacturing performs well compared to the rest of the U.S. economy, research shows that it performs poorly compared to manufacturing in other high-wage countries.[8] A total of 3.2 million – one in six U.S. manufacturing jobs – have disappeared between 2000 and 2007.[9] In the UK, EEF the manufacturers organisation has led calls for the UK economy to be rebalanced to rely less on financial services and has actively promoted the manufacturing agenda.
Textile
From Wikipedia, the free encyclopedia
"Fabric" redirects here. For other uses, see Fabric (disambiguation) and Textile (disambiguation).

Sunday textile market on the sidewalks of Karachi, Pakistan

Simple textile (magnified)

A small fabric shop in canal town Al-Mukalla, Yemen

Late antique textile, Egyptian, now in the Dumbarton Oaks collection

Mrs. Cond? Nast wearing one of the famous Fortuny tea gowns. This one has no tunic but is finely pleated, in the Fortuny manner, and falls in long lines, closely following the figure, to the floor.

Traditional Romanian table cloth, Maramure?
A textile[1] or cloth[2] is a flexible material consisting of a network of natural or artificial fibres (yarn or thread). Yarn is produced by spinning raw fibres of wool, flax, cotton, or other material to produce long strands.[3] Textiles are formed by weaving, knitting, crocheting, knotting, or felting.

The words fabric and cloth are used in textile assembly trades (such as tailoring and dressmaking) as synonyms for textile. However, there are subtle differences in these terms in specialized usage. Textile refers to any material made of interlacing fibres. Fabric refers to any material made through weaving, knitting, spreading, crocheting, or bonding that may be used in production of further goods (garments, etc.). Cloth may be used synonymously with fabric but often refers to a finished piece of fabric used for a specific purpose (e.g., table cloth).


Alpaca textile at the Otavalo Artisan Market in the Andes Mountains, Ecuador
Contents  [hide] 
1	Etymology
2	History
3	Uses
4	Fashion and textile designers
5	Sources and types
5.1	Animal textiles
5.2	Plant textiles
5.3	Mineral textiles
5.4	Synthetic textiles
6	Production methods
7	Treatments
8	See also
9	References
10	Further reading
Etymology[edit]
The word 'textile' is from Latin, from the adjective textilis, meaning 'woven', from textus, the past participle of the verb texere, 'to weave'.[4]

The word 'fabric' also derives from Latin, most recently from the Middle French fabrique, or 'building, thing made', and earlier as the Latin fabrica 'workshop; an art, trade; a skillful production, structure, fabric', which is from the Latin faber, or 'artisan who works in hard materials', from PIE dhabh-, meaning 'to fit together'.[5]

The word 'cloth' derives from the Old English cla?, meaning a cloth, woven or felted material to wrap around one, from Proto-Germanic kalithaz (compare O.Frisian 'klath', Middle Dutch 'cleet', Dutch 'kleed', Middle High German 'kleit', and German 'kleid', all meaning "garment").[6]

History[edit]
Main article: History of clothing and textiles
The discovery of dyed flax fibres in a cave in the Republic of Georgia dated to 34,000 BCE suggests textile-like materials were made even in prehistoric times.[7][8]


Textile machinery at the Cambrian Factory, Llanwrtyd, Wales in the 1940s.
The production of textiles is a craft whose speed and scale of production has been altered almost beyond recognition by industrialization and the introduction of modern manufacturing techniques. However, for the main types of textiles, plain weave, twill, or satin weave, there is little difference between the ancient and modern methods.

Uses[edit]
Textiles have an assortment of uses, the most common of which are for clothing and for containers such as bags and baskets. In the household they are used in carpeting, upholstered furnishings, window shades, towels, coverings for tables, beds, and other flat surfaces, and in art. In the workplace they are used in industrial and scientific processes such as filtering. Miscellaneous uses include flags, backpacks, tents, nets, handkerchiefs, cleaning rags, transportation devices such as balloons, kites, sails, and parachutes; textiles are also used to provide strengthening in composite materials such as fibreglass and industrial geotextiles. Textiles are used in many traditional crafts such as sewing, quilting and embroidery.

Textiles for industrial purposes, and chosen for characteristics other than their appearance, are commonly referred to as technical textiles. Technical textiles include textile structures for automotive applications, medical textiles (e.g. implants), geotextiles (reinforcement of embankments), agrotextiles (textiles for crop protection), protective clothing (e.g. against heat and radiation for fire fighter clothing, against molten metals for welders, stab protection, and bullet proof vests). In all these applications stringent performance requirements must be met. Woven of threads coated with zinc oxide nanowires, laboratory fabric has been shown capable of "self-powering nanosystems" using vibrations created by everyday actions like wind or body movements.[9][10]

Fashion and textile designers[edit]
Fashion designers commonly rely on textile designs to set their fashion collections apart from others. Armani, the late Gianni Versace, and Emilio Pucci can be easily recognized by their signature print driven designs.

Sources and types[edit]
Textiles can be made from many materials. These materials come from four main sources: animal (wool, silk), plant (cotton, flax, jute), mineral (asbestos, glass fibre), and synthetic (nylon, polyester, acrylic). In the past, all textiles were made from natural fibres, including plant, animal, and mineral sources. In the 20th century, these were supplemented by artificial fibres made from petroleum.

Textiles are made in various strengths and degrees of durability, from the finest gossamer to the sturdiest canvas. Microfibre refers to fibres made of strands thinner than one denier.

Animal textiles[edit]
Animal textiles are commonly made from hair, fur, skin or silk (in the silkworms case).

Wool refers to the hair of the domestic goat or sheep, which is distinguished from other types of animal hair in that the individual strands are coated with scales and tightly crimped, and the wool as a whole is coated with a wax mixture known as lanolin (sometimes called wool grease), which is waterproof and dirtproof[citation needed]. Woollen refers to a bulkier yarn produced from carded, non-parallel fibre, while worsted refers to a finer yarn spun from longer fibres which have been combed to be parallel. Wool is commonly used for warm clothing. Cashmere, the hair of the Indian cashmere goat, and mohair, the hair of the North African angora goat, are types of wool known for their softness.

Other animal textiles which are made from hair or fur are alpaca wool, vicu?a wool, llama wool, and camel hair, generally used in the production of coats, jackets, ponchos, blankets, and other warm coverings. Angora refers to the long, thick, soft hair of the angora rabbit. Qiviut is the fine inner wool of the muskox.

Wadmal is a coarse cloth made of wool, produced in Scandinavia, mostly 1000~1500 CE.

Silk is an animal textile made from the fibres of the cocoon of the Chinese silkworm which is spun into a smooth fabric prized for its softness. There are two main types of the silk: 'mulberry silk' produced by the Bombyx Mori, and 'wild silk' such as Tussah silk. Silkworm larvae produce the first type if cultivated in habitats with fresh mulberry leaves for consumption, while Tussah silk is produced by silkworms feeding purely on oak leaves. Around four-fifths of the world's silk production consists of cultivated silk.[11]

Plant textiles[edit]
Grass, rush, hemp, and sisal are all used in making rope. In the first two, the entire plant is used for this purpose, while in the last two, only fibres from the plant are utilized. Coir (coconut fibre) is used in making twine, and also in floormats, doormats, brushes, mattresses, floor tiles, and sacking.

Straw and bamboo are both used to make hats. Straw, a dried form of grass, is also used for stuffing, as is kapok.

Fibres from pulpwood trees, cotton, rice, hemp, and nettle are used in making paper.

Cotton, flax, jute, hemp, modal and even bamboo fibre are all used in clothing. Pi?a (pineapple fibre) and ramie are also fibres used in clothing, generally with a blend of other fibres such as cotton. Nettles have also been used to make a fibre and fabric very similar to hemp or flax. The use of milkweed stalk fibre has also been reported, but it tends to be somewhat weaker than other fibres like hemp or flax.

The inner bark of the lacebark tree is a fine netting that has been used to make clothing and accessories as well as utilitarian articles such as rope.

Acetate is used to increase the shininess of certain fabrics such as silks, velvets, and taffetas.

Seaweed is used in the production of textiles: a water-soluble fibre known as alginate is produced and is used as a holding fibre; when the cloth is finished, the alginate is dissolved, leaving an open area.

Lyocell is a synthetic fabric derived from wood pulp. It is often described as a synthetic silk equivalent; it is a tough fabric that is often blended with other fabrics – cotton, for example.

Fibres from the stalks of plants, such as hemp, flax, and nettles, are also known as 'bast' fibres.

Mineral textiles[edit]
Asbestos and basalt fibre are used for vinyl tiles, sheeting, and adhesives, "transite" panels and siding, acoustical ceilings, stage curtains, and fire blankets.

Glass fibre is used in the production of ironing board and mattress covers, ropes and cables, reinforcement fibre for composite materials, insect netting, flame-retardant and protective fabric, soundproof, fireproof, and insulating fibres. Glass fibres are woven and coated with Teflon to produce beta cloth, a virtually fireproof fabric which replaced nylon in the outer layer of United States space suits since 1968.

Metal fibre, metal foil, and metal wire have a variety of uses, including the production of cloth-of-gold and jewellery. Hardware cloth (US term only) is a coarse woven mesh of steel wire, used in construction. It is much like standard window screening, but heavier and with a more open weave. It is sometimes used together with screening on the lower part of screen doors, to resist scratching by dogs. It serves similar purposes as chicken wire, such as fences for poultry and traps for animal control.

Synthetic textiles[edit]

A variety of contemporary fabrics. From the left: evenweave cotton, velvet, printed cotton, calico, felt, satin, silk, hessian, polycotton.

Woven tartan of Clan Campbell, Scotland.

Embroidered skirts by the Alfaro-N??ez family of Cochas, Peru, using traditional Peruvian embroidery methods.[12]
All synthetic textiles are used primarily in the production of clothing.

Polyester fibre is used in all types of clothing, either alone or blended with fibres such as cotton.

Aramid fibre (e.g. Twaron) is used for flame-retardant clothing, cut-protection, and armor.

Acrylic is a fibre used to imitate wools, including cashmere, and is often used in replacement of them.

Nylon is a fibre used to imitate silk; it is used in the production of pantyhose. Thicker nylon fibres are used in rope and outdoor clothing.

Spandex (trade name Lycra) is a polyurethane product that can be made tight-fitting without impeding movement. It is used to make activewear, bras, and swimsuits.

Olefin fibre is a fibre used in activewear, linings, and warm clothing. Olefins are hydrophobic, allowing them to dry quickly. A sintered felt of olefin fibres is sold under the trade name Tyvek.

Ingeo is a polylactide fibre blended with other fibres such as cotton and used in clothing. It is more hydrophilic than most other synthetics, allowing it to wick away perspiration.

Lurex is a metallic fibre used in clothing embellishment.

Milk proteins have also been used to create synthetic fabric. Milk or casein fibre cloth was developed during World War I in Germany, and further developed in Italy and America during the 1930s.[13] Milk fibre fabric is not very durable and wrinkles easily, but has a pH similar to human skin and possesses anti-bacterial properties. It is marketed as a biodegradable, renewable synthetic fibre.[14]

Carbon fibre is mostly used in composite materials, together with resin, such as carbon fibre reinforced plastic. The fibres are made from polymer fibres through carbonization.


A. C. Lawrence Leather Co. c. 1910 Peabody, Massachusetts, US
Production methods[edit]
Main article: Textile manufacturing
Top five exporters of textiles—2013
($ billion)
China	274
India	40
Italy	36
Germany	35
Bangladesh	28
Source:[15]
Weaving is a textile production method which involves interlacing a set of longer threads (called the warp) with a set of crossing threads (called the weft). This is done on a frame or machine known as a loom, of which there are a number of types. Some weaving is still done by hand, but the vast majority is mechanised.

Knitting and crocheting involve interlacing loops of yarn, which are formed either on a knitting needle or on a crochet hook, together in a line. The two processes are different in that knitting has several active loops at one time, on the knitting needle waiting to interlock with another loop, while crocheting never has more than one active loop on the needle. Knitting can be performed by machine, but crochet can only be performed by hand.

Spread Tow is a production method where the yarn are spread into thin tapes, and then the tapes are woven as warp and weft. This method is mostly used for composite materials; Spread Tow Fabrics can be made in carbon, aramide, etc.

Braiding or plaiting involves twisting threads together into cloth. Knotting involves tying threads together and is used in making macrame.

Lace is made by interlocking threads together independently, using a backing and any of the methods described above, to create a fine fabric with open holes in the work. Lace can be made by either hand or machine.

Carpets, rugs, velvet, velour, and velveteen are made by interlacing a secondary yarn through woven cloth, creating a tufted layer known as a nap or pile.

Felting involves pressing a mat of fibres together, and working them together until they become tangled. A liquid, such as soapy water, is usually added to lubricate the fibres, and to open up the microscopic scales on strands of wool.

Nonwoven textiles are manufactured by the bonding of fibres to make fabric. Bonding may be thermal or mechanical, or adhesives can be used.

Bark cloth is made by pounding bark until it is soft and flat.

Treatments[edit]
Textiles are often dyed, with fabrics available in almost every colour. The dying process often requires several dozen gallons of water for each pound of clothing.[16] Coloured designs in textiles can be created by weaving together fibres of different colours (tartan or Uzbek Ikat), adding coloured stitches to finished fabric (embroidery), creating patterns by resist dyeing methods, tying off areas of cloth and dyeing the rest (tie-dyeing), or drawing wax designs on cloth and dyeing in between them (batik), or using various printing processes on finished fabric. Woodblock printing, still used in India and elsewhere today, is the oldest of these dating back to at least 220 CE in China. Textiles are also sometimes bleached, making the textile pale or white.


Brilliantly dyed traditional woven textiles of Guatemala, and woman weaving on a backstrap loom.
Textiles are sometimes finished by chemical processes to change their characteristics. In the 19th century and early 20th century starching was commonly used to make clothing more resistant to stains and wrinkles. Since the 1990s, with advances in technologies such as permanent press process, finishing agents have been used to strengthen fabrics and make them wrinkle free.[17] More recently, nanomaterials research has led to additional advancements, with companies such as Nano-Tex and NanoHorizons developing permanent treatments based on metallic nanoparticles for making textiles more resistant to things such as water, stains, wrinkles, and pathogens such as bacteria and fungi.[18]

More so today than ever before, textiles receive a range of treatments before they reach the end-user. From formaldehyde finishes (to improve crease-resistance) to biocidic finishes and from flame retardants to dyeing of many types of fabric, the possibilities are almost endless. However, many of these finishes may also have detrimental effects on the end user. A number of disperse, acid and reactive dyes (for example) have been shown to be allergenic to sensitive individuals.[19] Further to this, specific dyes within this group have also been shown to induce purpuric contact dermatitis.[20]

Although formaldehyde levels in clothing are unlikely to be at levels high enough to cause an allergic reaction,[21] due to the presence of such a chemical, quality control and testing are of utmost importance. Flame retardants (mainly in the brominated form) are also of concern where the environment, and their potential toxicity, are concerned.[22] Testing for these additives is possible at a number of commercial laboratories, it is also possible to have textiles tested for according to the Oeko-tex certification standard which contains limits levels for the use of certain chemicals in textiles products.

Molding (process)
From Wikipedia, the free encyclopedia
For other uses, see Mold (cooking implement).

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2015) (Learn how and when to remove this template message)

One half of a bronze mold for casting a socketed spear head dated to the period 1400-1000 BC. There are no known parallels for this mold.

Stone mold of the Bronze Age used to produce spear tips.

Ancient Greek molds, used to mass-produce clay figurines, 5th/4th century BC. Beside them, the modern casts taken from them. On display in the Ancient Agora Museum in Athens, housed in the Stoa of Attalus.

Ancient wooden molds used for jaggery & sweets, archaeological museum in Jaffna, Sri Lanka.
Molding or moulding (see spelling differences) is the process of manufacturing by shaping liquid or pliable raw material using a rigid frame called a mold or matrix.[1] This itself may have been made using a pattern or model of the final object.

A mold or mould is a hollowed-out block that is filled with a liquid or pliable material like plastic, glass, metal, or ceramic raw materials.[2] The liquid hardens or sets inside the mold, adopting its shape. A mold is the counterpart to a cast. The very common bi-valve molding process uses two molds, one for each half of the object. Piece-molding uses a number of different molds, each creating a section of a complicated object. This is generally only used for larger and more valuable objects.

The manufacturer who makes the molds is called the moldmaker. A release agent is typically used to make removal of the hardened/set substance from the mold easier. Typical uses for molded plastics include molded furniture, molded household goods, molded cases, and structural materials.

Types of molding include:

Blow molding
Powder metallurgy plus sintering
Compression molding
Extrusion molding
Injection molding
Laminating
Reaction injection molding
Matrix molding
Rotational molding (or Rotomolding)
Spin casting
Transfer molding
Thermoforming
Vacuum forming, a simplified version of thermoformingA data storage device is a device for recording (storing) information (data). Recording can be done using virtually any form of energy, spanning from manual muscle power in handwriting, to acoustic vibrations in phonographic recording, to electromagnetic energy modulating magnetic tape and optical discs.

A storage device may hold information, process information, or both. A device that only holds information is a recording medium. Devices that process information (data storage equipment) may either access a separate portable (removable) recording medium or a permanent component to store and retrieve data.

Electronic data storage requires electrical power to store and retrieve that data. Most storage devices that do not require vision and a brain to read data fall into this category. Electromagnetic data may be stored in either an analog data or digital data format on a variety of media. This type of data is considered to be electronically encoded data, whether it is electronically stored in a semiconductor device, for it is certain that a semiconductor device was used to record it on its medium. Most electronically processed data storage media (including some forms of computer data storage) are considered permanent (non-volatile) storage, that is, the data will remain stored when power is removed from the device. In contrast, most electronically stored information within most types of semiconductor (computer chips) microcircuits are volatile memory, for it vanishes if power is removed.

Except for barcodes and OCR data, electronic data storage is easier to revise and may be more cost effective than alternative methods due to smaller physical space requirements and the ease of replacing (rewriting) data on the same medium.[2] However, the durability of methods such as printed data is still superior to that of most electronic storage media. The durability limitations may be overcome with the ease of duplicating (backing-up) electronic data.

Contents  [hide] 
1	Terminology
2	Global capacity, digitization, and trends
3	Data storage equipment
3.1	Portable methods
3.2	Semi-portable methods
3.3	Volatile methods
4	Recording medium
4.1	Ancient examples
4.2	Modern examples by energy used
4.3	Modern examples by shape
4.4	Weight and volume
5	See also
6	References
7	Further reading
8	External links
Terminology[edit]
Devices that are not used exclusively for recording such as hands, mouths, musical instruments, and devices that are intermediate in the storing/retrieving process, like eyes, ears, cameras, scanners, microphones, speakers, monitors, or video projectors, are generally not considered storage devices. Devices that are exclusively for recording such as printers, exclusively for reading, like barcode readers, or devices that process only one form of information, like phonographs may or may not be considered storage devices. In computing these are known as input/output devices.

All information is data. However, not all data is information.

Many data storage devices are also media players. Any device that can store and playback multimedia may also be considered a media player such as in the case with the HD media player. Designated hard drives are used to play saved or streaming media on home cinemas or home theater PCs.

Global capacity, digitization, and trends[edit]
In a recent study in Science it was estimated that the world's technological capacity to store information in analog and digital devices grew from less than 3 (optimally compressed) exabytes in 1986, to 295 (optimally compressed) exabytes in 2007,[3] and doubles roughly every 3 years.[4]

It is estimated that the year 2002 marked the beginning of the digital age for information storage, the year that marked the date when human kind started to store more information on digital than on analog storage devices.[3] In 1986 only 1% of the world's capacity to store information was in digital format, which grew to 3% by 1993, 25% in the year 2000, and exploded to 97% of the world's storage capacity by 2007.

Data storage equipment[edit]
Any input/output equipment may be considered data storage equipment if it writes to and reads from a data storage medium.

Data storage equipment uses either:

portable methods (easily replaced),
semi-portable methods, requiring mechanical disassembly tools and/or opening a chassis, or
Volatile methods, meaning loss of memory if disconnected from the unit.
The following are examples of those methods:

Portable methods[edit]
Hand crafting
Flat surface
Printmaking
Photographic
Fabrication
Automated assembly
Textile
Molding
Solid freeform fabrication
Cylindrical accessing
Memory card reader/drive
Tape drive
Mono reel or reel-to-reel, see also Open reels
Compact Cassette player/recorder
Data cartridge (tape)
Disk accessing
Disk drive
Disk pack
Disk enclosure
ROM cartridge
Peripheral networking
Flash memory devices
Semi-portable methods[edit]
Hard disk drive
non-volatile RAM
Volatile methods[edit]
Volatile RAM
Neurons[5]
Recording medium[edit]
A recording medium is a physical material that holds data expressed in any of the existing recording formats. With electronic media, the data and the recording medium is sometimes\s part of the surface of the medium.

Some recording media may be temporary either by design or by nature. Volatile organic compounds may be used to preserve the environment or to purposely make data expire over time. Data such as smoke signals or skywriting are temporary by nature.

Ancient examples[edit]

The Gutenberg Bible displayed by the United States Library of Congress, demonstrating printed pages as a storage medium

A set of index cards in a file box are a nonlinear storage medium.
Optical
Any object visible to the eye, used to mark a location such as a stone, flag, or skull.
Any crafting material used to form shapes such as clay, wood, metal, glass, wax, or quipu.
Any hard surface that could hold carvings.
Any branding surface that would scar under intense heat (chiefly for livestock or humans).
Any marking substance such as paint, ink, or chalk.
Any surface that would hold a marking substance such as, papyrus, paper, or skin.
Chemical
RNA
DNA – for uses of DNA by people for storing information, see DNA digital data storage
Pheromone
Modern examples by energy used[edit]

Graffiti on a public wall. Public surfaces are being used as unconventional data storage media, often without permission.

Photographic film is a photochemical data storage medium

A floppy disk is a magnetic data storage medium

A 3.5" PATA hard drive is both storage equipment and a storage medium.
Chemical
Dipstick
Thermodynamic
Thermometer
Photochemical
Photographic film
Mechanical
Pins and holes
Paper
Punched card
Paper tape
Music roll
Music box cylinder or disk
Grooves (See also Audio Data)
Phonograph cylinder
Gramophone record
Dictabelt (groove on plastic belt)
Capacitance Electronic Disc
Magnetic storage
Wire recording (stainless steel wire)
Magnetic tape
Drum memory (magnetic drum)
Floppy disk
Optical storage
Optical jukebox
Photographic paper
X-ray
Microform
Hologram
Projected foil
Optical disc
Magneto-optical drive
Holographic data storage
3D optical data storage
Electrical
Semiconductor used in volatile random-access memory
Floating-gate transistor used in non-volatile memory cards
Modern examples by shape[edit]
A typical way to classify data storage media is to consider its shape and type of movement (or non-movement) relative to the read/write device(s) of the storage apparatus as listed:

Paper card storage
Punched card (mechanical)
Cams and tracers (pipe organ combination-action memory memorizing stop selections)
Tape storage (long, thin, flexible, linearly moving bands)
Paper tape (mechanical)
Magnetic tape (a tape passing one or more read/write/erase heads)
Disk storage (flat, round, rotating object)
Gramophone record (used for distributing some 1980s home computer programs) (mechanical)
Carousel memory (magnetic rolls)
Floppy disk, ZIP disk (removable) (magnetic)
Holographic
Optical disc such as CD, DVD, Blu-ray Disc
Minidisc
Hard disk drive (magnetic)
Magnetic bubble memory
Flash memory/memory card (solid state semiconductor memory)
xD-Picture Card
MultiMediaCard
USB flash drive (also known as a "thumb drive" or "keydrive")
SmartMedia
CompactFlash I and II
Secure Digital
Sony Memory Stick (Std/Duo/PRO/MagicGate versions)
Solid-state drive

Arduino
From Wikipedia, the free encyclopedia
For other uses, see Arduino (disambiguation).
Arduino
Arduino Logo.svg
Arduino Uno - R3.jpg
"Arduino Uno" SMD Revision 3
Type	Single-board microcontroller
Operating system	None
CPU	AVR, ARM Cortex, Intel Quark
Memory	SRAM
Storage	Flash Memory, EEPROM
Graphics	None
Website	www.arduino.cc
Arduino is a software company, project, and user community that designs and manufactures computer open-source hardware, open-source software, and microcontroller-based kits for building digital devices and interactive objects that can sense and control physical devices.[1]

The project is based on microcontroller board designs, produced by several vendors, using various microcontrollers. These systems provide sets of digital and analog I/O pins that can interface to various expansion boards (termed shields) and other circuits. The boards feature serial communication interfaces, including Universal Serial Bus (USB) on some models, for loading programs from personal computers. For programming the microcontrollers, the Arduino project provides an integrated development environment (IDE) based on a programming language named Processing, which also supports the languages C and C++.

The first Arduino was introduced in 2005, aiming to provide a low cost, easy way for novices and professionals to create devices that interact with their environment using sensors and actuators. Common examples of such devices intended for beginner hobbyists include simple robots, thermostats, and motion detectors.

Arduino boards are available commercially in preassembled form, or as do-it-yourself kits. The hardware design specifications are openly available, allowing the Arduino boards to be produced by anyone. Adafruit Industries estimated in mid-2011 that over 300,000 official Arduinos had been commercially produced,[2] and in 2013 that 700,000 official boards were in users' hands.[3]

Contents  [hide] 
1	History
2	Hardware
2.1	Official boards
2.2	Shields
3	Software
3.1	Sample program
4	Development
5	Applications
6	Recognitions
7	Trademark dispute
8	See also
9	Notes
10	References
11	Further reading
12	External links
History[edit]
Colombian student Hernando Barrag?n created the development platform Wiring as his Master's thesis project in 2004 at the Interaction Design Institute Ivrea in Ivrea, Italy. Massimo Banzi and Casey Reas (known for his work on Processing) were supervisors for his thesis. The goal was to create low cost, simple tools for non-engineers to create digital projects. The Wiring platform consisted of a hardware PCB with an ATmega128 microcontroller, an integrated development environment (IDE) based on Processing and library functions to easily program the microcontroller.[4]

In 2005, Massimo Banzi, with David Mellis (then an IDII student) and David Cuartielles, added support for the cheaper ATmega8 microcontroller to Wiring. But instead of continuing the work on Wiring, they forked (or copied) the Wiring source code and started running it as a separate project, called Arduino.[4]

The Arduino's initial core team consisted of Massimo Banzi, David Cuartielles, Tom Igoe, Gianluca Martino, and David Mellis.[5]

The name Arduino comes from a bar in Ivrea, where some of the founders of the project used to meet. The bar was named after Arduin of Ivrea, who was the margrave of the March of Ivrea and King of Italy from 1002 to 1014.[6]

Following the completion of the Wiring platform, its lighter, lower cost versions[7] were created and made available to the open-source community. Associated researchers, including David Cuartielles, promoted the idea.[5]

Hardware[edit]
Ambox current red.svg
This section's factual accuracy may be compromised due to out-of-date information. Please update this article to reflect recent events or newly available information. (October 2015)

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2013) (Learn how and when to remove this template message)

An early Arduino board[8] with an RS-232 serial communication interface (upper left) and an Atmel ATmega8 microcontroller chip (black, lower right); the 14 digital I/O pins are located at the top and the six analog input pins at the lower right.
An Arduino board historically consists of an Atmel 8-, 16- or 32-bit AVR microcontroller (although since 2015 other makers' microcontrollers have been used) with complementary components that facilitate programming and incorporation into other circuits. An important aspect of the Arduino is its standard connectors, which let users connect the CPU board to a variety of interchangeable add-on modules termed shields. Some shields communicate with the Arduino board directly over various pins, but many shields are individually addressable via an I?C serial bus—so many shields can be stacked and used in parallel. Before 2015, Official Arduinos had used the Atmel megaAVR series of chips, specifically the ATmega8, ATmega168, ATmega328, ATmega1280, and ATmega2560. In 2015, units by other producers were added. A handful of other processors have also been used by Arduino compatible devices. Most boards include a 5 V linear regulator and a 16 MHz crystal oscillator (or ceramic resonator in some variants), although some designs such as the LilyPad run at 8 MHz and dispense with the onboard voltage regulator due to specific form-factor restrictions. An Arduino's microcontroller is also pre-programmed with a boot loader that simplifies uploading of programs to the on-chip flash memory, compared with other devices that typically need an external programmer. This makes using an Arduino more straightforward by allowing the use of an ordinary computer as the programmer. Currently, optiboot bootloader is the default bootloader installed on Arduino UNO.[9]

At a conceptual level, when using the Arduino integrated development environment, all boards are programmed over a serial connection. Its implementation varies with the hardware version. Some serial Arduino boards contain a level shifter circuit to convert between RS-232 logic levels and transistor–transistor logic (TTL) level signals. Current Arduino boards are programmed via Universal Serial Bus (USB), implemented using USB-to-serial adapter chips such as the FTDI FT232. Some boards, such as later-model Uno boards, substitute the FTDI chip with a separate AVR chip containing USB-to-serial firmware, which is reprogrammable via its own ICSP header. Other variants, such as the Arduino Mini and the unofficial Boarduino, use a detachable USB-to-serial adapter board or cable, Bluetooth or other methods, when used with traditional microcontroller tools instead of the Arduino IDE, standard AVR in-system programming (ISP) programming is used.


An official Arduino Uno Revision 2 with descriptions of the I/O locations
The Arduino board exposes most of the microcontroller's I/O pins for use by other circuits. The Diecimila[a], Duemilanove[b], and current Uno[c] provide 14 digital I/O pins, six of which can produce pulse-width modulated signals, and six analog inputs, which can also be used as six digital I/O pins. These pins are on the top of the board, via female 0.1-inch (2.54 mm) headers. Several plug-in application shields are also commercially available. The Arduino Nano, and Arduino-compatible Bare Bones Board[10] and Boarduino[11] boards may provide male header pins on the underside of the board that can plug into solderless breadboards.

Many Arduino-compatible and Arduino-derived boards exist. Some are functionally equivalent to an Arduino and can be used interchangeably. Many enhance the basic Arduino by adding output drivers, often for use in school-level education, to simplify making buggies and small robots. Others are electrically equivalent but change the form factor, sometimes retaining compatibility with shields, sometimes not. Some variants use different processors, of varying compatibility.

Official boards[edit]
Further information: List of Arduino boards and compatible systems
The original Arduino hardware was produced by the Italian company Smart Projects.[12] Some Arduino-branded boards have been designed by the American companies SparkFun Electronics and Adafruit Industries.[13] As of 2016, 17 versions of the Arduino hardware had been commercially produced.

Example Arduino boards

Arduino Diecimila in Stoicheia
 

Arduino Duemilanove (rev 2009b)
 

Arduino UNO
 

Arduino Leonardo
 

Arduino Mega
 

Arduino MEGA 2560 R3 (front side)[d]
 

Arduino MEGA 2560 R3 (back side)[d]
 

Arduino Nano
 

Arduino Due
(ARM Cortex-M3 core)
 

LilyPad Arduino (rev 2007)
 

Arduino Yun
Shields[edit]
Arduino and Arduino-compatible boards use printed circuit expansion boards called shields, which plug into the normally supplied Arduino pin headers. Shields can provide motor controls, Global Positioning System (GPS), Ethernet, liquid crystal display (LCD), or breadboarding (prototyping). Several shields can also be made do it yourself (DIY).[15][16][17]

Example Arduino shields

Multiple shields can be stacked. In this example the top shield contains a solderless breadboard.
 

Screw-terminal breakout shield in a wing-type format
 

Adafruit Motor Shield with screw terminals for connection to motors
 

Adafruit Datalogging Shield with a Secure Digital (SD) card slot and real-time clock (RTC) chip
 

HackARobot Fabric Shield – designed for Arduino Nano to hook up motors and sensors such as gyroscope or GPS, and other breakout boards such as WiFi, Bluetooth, RF, etc.
Software[edit]
Arduino Software IDE
Arduino 1.0 IDE, Ubuntu 11.10.png
Screenshot of the Arduino IDE showing the Blink simple beginner program
Developer(s)	Arduino Software
Stable release	1.6.8 / 9 March 2016; 58 days ago[18]
Written in	Java, C and C++
Operating system	Cross-platform
Type	Integrated development environment
License	LGPL or GPL license
Website	arduino.cc
Arduino programs may be written in any programming language with a compiler that produces binary machine code. Atmel provides a development environment for their microcontrollers, AVR Studio and the newer Atmel Studio.[19][20]

The Arduino project provides the Arduino integrated development environment (IDE), which is a cross-platform application written in the programming language Java. It originated from the IDE for the languages Processing and Wiring. It is designed to introduce programming to artists and other newcomers unfamiliar with software development. It includes a code editor with features such as syntax highlighting, brace matching, and automatic indentation, and provides simple one-click mechanism to compile and load programs to an Arduino board. A program written with the IDE for Arduino is called a "sketch".[21]

The Arduino IDE supports the languages C and C++ using special rules to organize code. The Arduino IDE supplies a software library called Wiring from the Wiring project, which provides many common input and output procedures. A typical Arduino C/C++ sketch consist of two functions that are compiled and linked with a program stub main() into an executable cyclic executive program:

setup(): a function that runs once at the start of a program and that can initialize settings.
loop(): a function called repeatedly until the board powers off.
After compiling and linking with the GNU toolchain, also included with the IDE distribution, the Arduino IDE employs the program avrdude to convert the executable code into a text file in hexadecimal coding that is loaded into the Arduino board by a loader program in the board's firmware.

Sample program[edit]
A typical program for a beginning Arduino programmer blinks a light-emitting diode (LED) on and off. This program is usually loaded in the Arduino board by the manufacturer. In the Arduino environment, a user might write such a program as shown:[22]

#define LED_PIN 13

void setup() {
    pinMode(LED_PIN, OUTPUT);       // Enable pin 13 for digital output
}

void loop() {
    digitalWrite(LED_PIN, HIGH);    // Turn on the LED
    delay(1000);                    // Wait one second (1000 milliseconds)
    digitalWrite(LED_PIN, LOW);     // Turn off the LED
    delay(1000);                    // Wait one second
}
Power LED and Integrated LED on Arduino Compatible Board
Power LED (red) and integrated LED on Line 13 (green) on Arduino compatible board, made in China
Most Arduino boards contain an LED and a load resistor connected between pin 13 and ground which is a convenient feature for many tests.[22]

Development[edit]

Arduino-compatible R3 UNO board made in China with no Arduino logo, but with identical markings, including "Made in Italy" text
Arduino is an open-source hardware. The hardware reference designs are distributed under a Creative Commons Attribution Share-Alike 2.5 license and are available on the Arduino website. Layout and production files for some versions of the hardware are also available. The source code for the IDE is released under the GNU General Public License, version 2.[23]

Although the hardware and software designs are freely available under copyleft licenses, the developers have requested that the name "Arduino" be exclusive to the official product and not be used for derived works without permission. The official policy document on use of the Arduino name emphasizes that the project is open to incorporating work by others into the official product.[24] Several Arduino-compatible products commercially released have avoided the Arduino name by using -duino name variants.[25]

Applications[edit]
See also: List of open-source hardware projects
Xoscillo, an open-source oscilloscope[26]
Scientific equipment[27] such as the Chemduino[28]
Arduinome, a MIDI controller device that mimics the Monome
OBDuino, a trip computer that uses the on-board diagnostics interface found in most modern cars
Ardupilot, drone software and hardware
ArduinoPhone, a do-it-yourself cellphone[29][30]
GertDuino, an Arduino mate for the Raspberry Pi[31]
Water quality testing platform[32]
Homemade CNC using Arduino and DC motors with close loop control by Homofaciens[33]
DC motor control using Arduino and H-Bridge[34]
Recognitions[edit]
The Arduino project received an honorary mention in the Digital Communities category at the 2006 Prix Ars Electronica.[35]

Trademark dispute[edit]
In early 2008, the five cofounders of the Arduino project created a company, Arduino LLC,[36] to hold the trademarks associated with Arduino. The manufacture and sale of the boards was to be done by external companies, and Arduino LLC would get a royalty from them. The founding bylaws of Arduino LLC specified that each of the five founders transfer ownership of the Arduino brand to the newly formed company.[citation needed]

At the end of 2008, Gianluca Martino's company, Smart Projects, registered the Arduino trademark in Italy and kept this a secret from the other cofounders for about two years. This was revealed when the Arduino company tried to register the trademark in other areas of the world (they originally registered only in the US), and discovered that it was already registered in Italy. Negotiations with Gianluca and his firm to bring the trademark under control of the original Arduino company failed. In 2014, Smart Projects began refusing to pay royalties. They then appointed a new CEO, Mr. Musto, who renamed the company to Arduino SRL and created a website named arduino.org, copying the graphics and layout of the original Arduino.cc. This resulted in a rift in the Arduino development team. All Arduino boards are still available to consumers, and the designs are open source, so the implications of this are uncertain.[37][38][39]

In May 2015, "Genuino" was created around the world as another trademark, held by Arduino LLC, and is currently being used as Arduino LLC's brand name outside of the US.[40]

Robot
From Wikipedia, the free encyclopedia
This article is about mechanical robots. For software agents, see Bot. For other uses of the term, see Robot (disambiguation).

ASIMO (2000) at the Expo 2005, a bipedal humanoid robot.

Articulated welding robots used in a factory, a type of industrial robot.

The quadrupedal military robot Cheetah, an evolution of BigDog (pictured), was clocked as the world's fastest legged robot in 2012, beating the record set by an MIT bipedal robot in 1989.[1]
A robot is a mechanical or virtual artificial agent, usually an electro-mechanical machine that is guided by a computer program or electronic circuitry. Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY's TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patent assist robots, dog therapy robots, collectively programmed swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own.

The branch of technology that deals with the design, construction, operation, and application of robots,[2] as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, and/or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.

From the time of ancient civilization there have been many accounts of user-configurable automated devices and even automata resembling animals and humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.

The word 'robot' was first used to denote a fictional humanoid in a 1921 play R.U.R. by the Czech writer, Karel ?apek but it was Karel's brother Josef ?apek who was the word's true inventor.[3][4] Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948. The first digital and programmable robot was invented by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.[5]

Robots have replaced humans[6] in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea.

There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising unemployment as they replace workers in increasing numbers of functions.[7] The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.

Contents  [hide] 
1	Summary
2	History
2.1	Early beginnings
2.2	Remote-controlled systems
2.3	Humanoid robots
2.4	Modern autonomous robots
3	Future development and trends
3.1	New functionalities and prototypes
4	Etymology
5	Modern robots
5.1	Mobile robot
5.2	Industrial robots (manipulating)
5.3	Service robot
5.4	Educational robot
5.5	Modular robot
5.6	Collaborative robots
6	Robots in society
6.1	Autonomy and ethical questions
6.2	Military robots
6.3	Relationship to unemployment
7	Contemporary uses
7.1	General-purpose autonomous robots
7.2	Factory robots
7.3	Dirty, dangerous, dull or inaccessible tasks
7.4	Military robots
7.5	Mining robots
7.6	Healthcare
7.7	Research robots
8	Robots in popular culture
8.1	Literature
8.2	Cinema
8.3	Problems depicted in popular culture
9	See also
9.1	Specific robotics concepts
9.2	Robotics methods and categories
9.3	Specific robots and devices
10	References
11	Further reading
12	External links
Summary

KITT (a fictitious robot) is mentally anthropomorphic

ASIMO is physically anthropomorphic
The word robot can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots.[8] There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to do some or all of the following: accept electronic programming, process data or physical perceptions electronically, operate autonomously to some degree, move around, operate physical parts of itself or physical processes, sense and manipulate their environment, and exhibit intelligent behavior — especially behavior which mimics humans or other animals.[9][10] Closely related to the concept of a robot is the field of Synthetic Biology, which studies entities whose nature is more comparable to beings than to machines.

History
Main article: History of robots
The idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China,[11] Ancient Greece, and Ptolemaic Egypt,[12] attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas,[13] the artificial birds of Mozi and Lu Ban,[14] a "speaking" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the Lie Zi.[11]

Early beginnings
Many ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus[15] (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the Cretan island of Europa from pirates.


Washstand automaton reconstruction, as described by Philo of Byzantium (Greece, 3rd century BC).
In ancient Greece, the Greek engineer Ctesibius (c. 270 BC) "applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures."[16][17] In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called "The Pigeon". Hero of Alexandria (10–70 AD), a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.[18]


Al-Jazari's toy boat, musical automata
The 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka. [19] [20]

In ancient China, the 3rd century text of the Lie Zi describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs.[11] There are also accounts of flying automata in the Han Fei Zi and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds (ma yuan) that could successfully fly.[14] In 1066, the Chinese inventor Su Song built a water clock in the form of a tower which featured mechanical figurines which chimed the hours.


Su Song's astronomical clock tower showing the mechanical figurines which chimed the hours.
The beginning of automata is associated with the invention of early Su Song's astronomical clock tower featured mechanical figurines that chimed the hours.[21][22][23] His mechanism had a programmable drum machine with pegs (cams) that bumped into little levers that operated percussion instruments. The drummer could be made to play different rhythms and different drum patterns by moving the pegs to different locations.[23]

In Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw.[24] The design was probably based on anatomical research recorded in his Vitruvian Man. It is not known whether he attempted to build it.

In Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century Karakuri zui (Illustrated Machinery, 1796). One such automaton was the karakuri ningy?, a mechanized puppet.[25] Different variations of the karakuri existed: the Butai karakuri, which were used in theatre, the Zashiki karakuri, which were small and used in homes, and the Dashi karakuri which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.

In France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.[26]

Remote-controlled systems

The Brennan torpedo, one of the earliest 'guided missiles'
Remotely operated vehicles were demonstrated in the late 19th Century in the form of several types of remotely controlled torpedos. The early 1870s saw remotely controlled torpedos by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).[27]

The Brennan torpedo, invented by Louis Brennan in 1877 was powered by two contra-rotating propellors that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it "the world's first practical guided missile".[28] In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by "Hertzian" (radio) waves[29][30] and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.[31][32]

Archibald Low, known as the "father of radio guidance systems" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.

Humanoid robots

A Czech writer Karel ?apek — first user of the term 'robot'
The term 'robot' was first used to denote fictional automata in a 1921 play R.U.R. by the Czech writer, Karel ?apek. However, Josef ?apek was named by his brother Karel as the true inventor of the term robot.[33][34]

In 1928, one of the first humanoid robots was exhibited at the annual exhibition of the Model Engineers Society in London. Invented by W. H. Richards, the robot Eric's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control.[35]

Westinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair.[36][37] Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.

Modern autonomous robots
The first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors - essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named Elmer and Elsie, were constructed between 1948 and 1949 and were often described as tortoises due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.

Walter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's turtles may be found in the form of BEAM robotics.[38]


U.S. Patent 2,988,237, issued in 1961 to Devol.
The first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry.[39] Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them.[40] Devol’s patent for the first digitally operated programmable robotic arm represents the foundation of the modern robotics industry.[41]

The first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company.[42] In 1973, a robot with six electromechanically driven axes was patented[43][44][45] by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.

Commercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.[46]

Future development and trends
Further information: Future of robotics
External video
 Atlas, The Next Generation
Various techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent "generation" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions. Another new type of robot is just recently introduced which acts both as a smartphone and robot and is named RoboHon.[47]

As robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System is an open-source set of programs being developed at Stanford University, the Massachusetts Institute of Technology and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a "Windows for robots" system with its Robotics Developer Studio, which has been available since 2007.[48]

Japan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.[49]

Many future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction.[50][51] As early as 1982 people were confident that someday robots would:[52] 1. clean parts by removing molding flash 2. spray paint automobiles with absolutely no human presence 3. pack things in boxes—for example, orient and nest chocolate candies in candy boxes 4. make electrical cable harness 5. load trucks with boxes—a packing problem 6. handle soft goods, such as garments and shoes 7. shear sheep 8. prosthesis 9. cook fast food and work in other service industries 10. household robot.

Generally such predictions are overly optimistic in timescale.

New functionalities and prototypes
In 2008, Caterpillar Inc. developed a dump truck which can drive itself without any human operator.[53] Many analysts believe that self-driving trucks may eventually revolutionize logistics. [54] By 2014, Caterpillar had a self-driving dump truck which is expected to greatly change the process of mining. In 2015, these Caterpillar trucks were actively used in mining operations in Australia by the mining company Rio Tinto Coal Australia. [55] [56] [57] [58] Some analysts believe that within the next few decades, most trucks will be self-driving. [59]

A literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.[60]

Baxter is a new robot which is different from other industrial robots because it can learn. A worker could teach Baxter how to perform a task by moving its hands in the desired motion and having Baxter memorize them. Extra dials, buttons, and controls are available on Baxter's arm for more precision and features. Any regular worker could program Baxter and it only takes a matter of minutes, unlike usual industrial robots that take extensive programs and coding in order to be used. This means Baxter needs no programming in order to operate. No software engineers are needed. This also means Baxter can be taught to perform multiple, more complicated tasks. [61]

Etymology
See also: Glossary of robotics

A scene from Karel ?apek's 1920 play R.U.R. (Rossum's Universal Robots), showing three robots.
The word robot was introduced to the public by the Czech interwar writer Karel ?apek in his play R.U.R. (Rossum's Universal Robots), published in 1920.[62] The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called robots. The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieve self-awareness and incite robots all around the world to rise up against the humans).

Karel ?apek himself did not coin the word. He wrote a short letter in reference to an etymology in the Oxford English Dictionary in which he named his brother, the painter and writer Josef ?apek, as its actual originator.[62]

In an article in the Czech journal Lidov? noviny in 1933, he explained that he had originally wanted to call the creatures labo?i ("workers", from Latin labor). However, he did not like the word, and sought advice from his brother Josef, who suggested "roboti". The word robota means literally "corv?e", "serf labor", and figuratively "drudgery" or "hard work" in Czech and also (more general) "work", "labor" in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as robot in Hungarian). Traditionally the robota (Hungarian robot) was the work period a serf (corv?e) had to give for his lord, typically 6 months of the year. The origin of the word is the Old Church Slavonic (Old Bulgarian) rabota "servitude" ("work" in contemporary Bulgarian and Russian), which in turn comes from the Proto-Indo-European root *orbh-. Robot is cognate with the German root Arbeit (work).[63][64]

The word robotics, used to describe this field of study,[2] was coined by the science fiction writer Isaac Asimov. Asimov created the "Three Laws of Robotics" which are a recurring theme in his books. These have since been used by many others to define laws used in fact and fiction.

Modern robots

A laparoscopic robotic surgery machine
Mobile robot
Main articles: Mobile robot and Automated guided vehicle
Mobile robots[65] have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the automated guided vehicle or automatic guided vehicle (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers.[citation needed] AGVs are discussed later in this article.

Mobile robots are also found in industry, military and security environments. They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.[citation needed]

Mobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.[citation needed]

Industrial robots (manipulating)
Main articles: Industrial robot and Manipulator

A Pick and Place robot in a factory
Industrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.

The International Organization for Standardization gives a definition of a manipulating industrial robot in ISO 8373:

"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications."[66]

This definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees.[67]

Service robot
Main article: Service robot
Most commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term "service robot" is less well-defined. The International Federation of Robotics has proposed a tentative definition, "A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations."[68]

Educational robot
Main article: Educational robotics
Robots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.[69][70]

There are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST LEGO League, Junior FIRST LEGO League, and FIRST Tech Challenge competitions.

There have also been devices shaped like robots such as the teaching computer, Leachim (1974), and 2-XL (1976), a robot shaped game / teaching toy based on an 8-track tape player, both invented Michael J. Freeman.

Modular robot
Main article: Self-reconfiguring modular robot
Modular robots are a new breed of robots that are designed to increase the utilization of robots by modularizing their architecture.[71] The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots. Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U and H shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These “ANAT robots” can be designed with “n” DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases. Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.

Modular robotic technology is currently being applied in hybrid transportation,[72] industrial automation,[73] duct cleaning[74] and handling. Many research centres and universities have also studied this technology, and have developed prototypes.

Collaborative robots
A collaborative robot or cobot is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.[75]

The collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.[citation needed]

Rethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks.[76] Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer.[77] As of May 2014, 190 companies in the US have bought Baxters and they are being used commercially in the UK.[7]

Robots in society

TOPIO, a humanoid robot, played ping pong at Tokyo International Robot Exhibition (IREX) 2009.[78][79]
Roughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa.[80] 40% of all the robots in the world are in Japan,[81] making Japan the country with the highest number of robots.

Autonomy and ethical questions
Main articles: Roboethics and Ethics of artificial intelligence

An android, or robot designed to resemble a human, can appear comforting to some people and disturbing to others[82]
As robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior,[83] and whether robots might be able to claim any kind of social, cultural, ethical or legal rights.[84] One scientific team has said that it is possible that a robot brain will exist by 2019.[85] Others predict robot intelligence breakthroughs by 2050.[86] Recent advances have made robotic behavior more sophisticated.[87] The social impact of intelligent robots is subject of a 2010 documentary film called Plug & Pray.[88]

Vernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this "the Singularity".[89] He suggests that it may be somewhat or possibly very dangerous for humans.[90] This is discussed by a philosophy called Singularitarianism.

In 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[89] Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns.[91][92][93] In 2015, the Nao alderen robots were shown to have a capability for a degree of self-awareness. Researchers at the Rensselaer Polytechnic Institute AI and Reasoning Lab in New York conducted an experiment where a robot became aware of itself, and corrected its answer to a question once it had realised this.[94]

Military robots
Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[95] There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots.[96] The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[97][98] One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.[99]

One robot in particular, the EATR, has generated public concerns [100] over its fuel source, as it can continually refuel itself using organic substances.[101] Although the engine for the EATR is designed to run on biomass and vegetation[102] specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.[103]

Manuel De Landa has noted that "smart missiles" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.[104]

Relationship to unemployment
Main article: Technological unemployment
A recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.[105]

Lawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to revise redundancy laws.[106]

Contemporary uses

A general-purpose robot acts as a guide during the day and a security guard at night
See also: List of robots
At present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.

Robots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. Of course, all robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.

General-purpose autonomous robots
Main article: Autonomous robot
General-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in.[citation needed] Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.

Factory robots
Car production
Over the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.

Packaging
Industrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.

Electronics
Mass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy.[107] Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.[108]

Automated guided vehicles (AGVs)

An intelligent AGV drops-off goods without needing lines or beacons in the workspace
Mobile robots, following markers or wires in the floor, or using vision[109] or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.[110]

Early AGV-style robots
Limited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.

Interim AGV technologies
Developed to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.

Intelligent AGVs (i-AGVs)
Such as SmartLoader,[111] SpeciMinder,[112] ADAM,[113] Tug[114] Eskorta,[115] and MT 400 with Motivity[116] are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.

Dirty, dangerous, dull or inaccessible tasks
There are many jobs which humans would rather leave to robots. The job may be boring, such as domestic cleaning, or dangerous, such as exploring inside a volcano.[117] Other jobs are physically inaccessible, such as exploring another planet,[118] cleaning the inside of a long pipe, or performing laparoscopic surgery.[119]

Space probes
Almost every unmanned space probe ever launched was a robot.[120][121] Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, and others.

Telerobots

A U.S. Marine Corps technician prepares to use a telerobot to detonate a buried improvised explosive device near Camp Fallujah, Iraq
Teleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time.[119] They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely.[122] Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets.[123][124] Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).[125]

Automated fruit harvesting machines
Robots are used to automate picking fruit on orchards at a cost lower than that of human pickers.

Domestic robots

The Roomba domestic vacuum cleaner robot does a single, menial job
Domestic robots are simple robots dedicated to a single task work in home use. They are used in simple but unwanted jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.

Military robots
Main article: Military robot
Military robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.[126][127][128]

Unmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own.[129] The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection.[130] Flight trials are expected to begin in 2011.[131][132]

The AAAI has studied this topic in depth[83] and its president has commissioned a study to look at this issue.[133]

Some have suggested a need to build "Friendly AI", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.[134] Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea[135] having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics.[136][137] An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee.[138] Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as "Robot Legal Studies."[139] Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.[140]

Mining robots
Mining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous vehicle fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia.[141]

Drilling, longwall and rockbreaking machines are now also available as autonomous robots.[142] The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths.[143] Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination.[144] These systems greatly enhance the safety and efficiency of mining operations.

Healthcare
Robots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.

Home automation for the elderly and disabled
Further information: Disability robot

The Care-Providing Robot FRIEND
Robots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1,[145] through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.

The population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them.[146][147] Humans make the best carers, but where they are unavailable, robots are gradually being introduced.[148]

FRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.

Pharmacies
Main article: Pharmacy automation

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2009)
Script Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form. The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient’s medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it’s the correct drug for the correct patient and then seals the vials and sends it out front to be picked up. The robot is a very time efficient device that the pharmacy depends on to fill prescriptions.

McKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors. The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to its either stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify its pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.

Research robots
See also: Robotics research
While most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.[citation needed]

Bionic and biomimetic robots
Further information: Bionics
Further information: Biomimetics
One approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.

Nanorobots
Further information: Nanorobotics

A microfabricated electrostatic gripper holding some silicon nanowires.[149]
Nanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10?9 meters). Also known as "nanobots" or "nanites", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest.[150] Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog,[151] manufacturing, weaponry and cleaning.[152] Some people have suggested that if there were nanobots which could reproduce, the earth would turn into "grey goo", while others argue that this hypothetical outcome is nonsense.[153][154]

Reconfigurable robots
Main article: Self-reconfiguring modular robot
A few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task,[155] like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.[156]

Soft-bodied robots
Robots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids) look and feel different from robots with rigid skeletons, and can have different behaviors.[157]

Swarm robots
Main article: Swarm robotics

A swarm of robots from the open-source micro-robotic project
Inspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project[158] and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors.[159][160] Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.[161]

Haptic interface robots
Further information: Haptic technology
Robotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called "haptic interfaces", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of "virtual" objects, which users can experience through their sense of touch.[162]

Robots in popular culture

Toy robots on display at the Museo del Objeto del Objeto in Mexico City.
See also: List of fictional robots and androids and Droid (robot)
Literature
Main article: Robots in literature
Robotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also "bionic men/women", or humans with significant mechanical enhancements) have become a staple of science fiction.

The first reference in Western literature to mechanical servants appears in Homer's Iliad. In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots.[163] According to the Rieu translation, "Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods." Of course, the words "robot" or "android" are not used to describe them, but they are nevertheless mechanical devices human in appearance. "The first use of the word Robot was in Karel ?apek's play R.U.R. (Rossum's Universal Robots) (written in 1920)". Writer Karel ?apek was born in Czechoslovakia (Czech Republic).

Possibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992)[164] who published over five-hundred books.[165] Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works.[166][167] Asimov carefully considered the problem of the ideal set of instructions robots might be given in order to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law.[168] These were introduced in his 1942 short story "Runaround", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: "A robot may not harm humanity, or, by inaction, allow humanity to come to harm"; the rest of the laws are modified sequentially to acknowledge this.

According to the Oxford English Dictionary, the first passage in Asimov's short story "Liar!" (1941) that mentions the First Law is the earliest recorded use of the word robotics. Asimov was not initially aware of this; he assumed the word already existed by analogy with mechanics, hydraulics, and other similar terms denoting branches of applied knowledge.[169]

Cinema
Robots appear in many films. Most of the robots in cinema are fictional. Two of the most famous are R2-D2 and C-3PO from the Star Wars franchise.

Problems depicted in popular culture

The Italian film The Mechanical Man (1921) is one of the first movies to show a battle between robots
Fears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race. (See The Mechanical Man, The Terminator, Runaway, RoboCop, the Replicators in Stargate, the Cylons in Battlestar Galactica, the Cybermen and Daleks in Doctor Who, The Matrix, Enthiran and I, Robot.) Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are 2001: A Space Odyssey, Red Planet and Enthiran. Another common theme is the reaction, sometimes called the "uncanny valley", of unease and even revulsion at the sight of robots that mimic humans too closely.[82] Frankenstein (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or monster advancing beyond its creator. In the TV show, Futurama, the robots are portrayed as humanoid figures that live alongside humans, not as robotic butlers. They still work in industry, but these robots carry out daily lives. Other problems may include events pertaining to robot surrogates (e.g. the movie Surrogates) where tissue of living organisms is interchanged with robotic systems. These problems can leave many possibilities where electronic viruses or an electro magnetic pulse (EMP) can destroy not only the robot but kill the host/operator as well.

Do it yourself
From Wikipedia, the free encyclopedia
  (Redirected from Do-it-yourself)
For other uses of "Do it yourself", see Do it yourself (disambiguation).
"DIY" redirects here. For other uses, see DIY (disambiguation).

[hide]This article has multiple issues. Please help improve it (see how) or discuss these issues on the talk page.
This article possibly contains original research. (November 2011)
This article needs additional citations for verification. (September 2010)
Part of a series on
Individualism
Topics and concepts[show]
Thinkers[show]
Philosophies[show]
Principal concerns[show]
v t e

Boy building a model airplane, Texas, 1942 (photograph by Arthur Rothstein for the Farm Security Administration)
Do it yourself, also known as DIY, is the method of building, modifying, or repairing something without the direct aid of experts or professionals. Academic research describes DIY as behaviors where "individuals engage raw and semi-raw materials and component parts to produce, transform, or reconstruct material possessions, including those drawn from the natural environment (e.g. landscaping)".[1] DIY behavior can be triggered by various motivations previously categorized as marketplace motivations (economic benefits, lack of product availability, lack of product quality, need for customization), and identity enhancement (craftsmanship, empowerment, community seeking, uniqueness)[2]

The term "do-it-yourself" has been associated with consumers since at least 1912 primarily in the domain of home improvement and maintenance activities.[3] The phrase "do it yourself" had come into common usage (in standard English) by the 1950s,[4] in reference to the emergence of a trend of people undertaking home improvement and various other small craft and construction projects as both a creative-recreational and cost-saving activity.


Girl decorating a gingerbread house
Subsequently, the term DIY has taken on a broader meaning that covers a wide range of skill sets. DIY is associated with the international alternative rock, punk rock, and indie rock music scenes; indymedia networks, pirate radio stations, and the zine community. In this context, DIY is related to the Arts and Crafts movement, in that it offers an alternative to modern consumer culture's emphasis on relying on others to satisfy needs. The abbreviation DIY is also widely used in the military as a way to teach commanders or other types of units to take responsibility, so that they'd be able to do things themselves just as a preparation for their own future.

Contents  [hide] 
1	History
2	Home improvement
3	Fashion
4	Subculture
5	Groups and Publications
6	See also
7	References
8	External links
History[edit]

Crude jungle gym, People's Park, Berkeley, ca. 1960s
Italian archaeologists unearthed the ruins of a 6th-century BC Greek structure in southern Italy that came with detailed assembly instructions and is being called an "ancient IKEA building". The structure was a temple-like building discovered at Torre Satriano, near the southern city of Potenza, in Basilicata, a region where local people mingled with Greeks who settled along the southern coast known as Magna Graecia and in Sicily from the 8th century BC onwards. Professor Christopher Smith, director of the British School at Rome, said that the discovery was "the clearest example yet found of mason's marks of the time. It looks as if someone was instructing others how to mass-produce components and put them together in this way". Much like the instruction booklets, various sections of the luxury building were inscribed with coded symbols showing how the pieces slotted together. The characteristics of these inscriptions indicate they date back to around the 6th century BC, which tallies with the architectural evidence suggested by the decoration. The building was built by Greek artisans coming from the Spartan colony of Taranto in Apulia.[5][6][7]

Home improvement[edit]

Shelves attached to a toy vehicle
See also: Home improvement
The DIY movement is a re-introduction (often to urban and suburban dwellers) of the old pattern of personal involvement and use of skills in upkeep of a house or apartment, making clothes; maintenance of cars, computers, websites; or any material aspect of living. The philosopher Alan Watts (from the "Houseboat Summit" panel discussion in a 1967 edition of the San Francisco Oracle) reflected a growing sentiment:

Our educational system, in its entirety, does nothing to give us any kind of material competence. In other words, we don't learn how to cook, how to make clothes, how to build houses, how to make love, or to do any of the absolutely fundamental things of life. The whole education that we get for our children in school is entirely in terms of abstractions. It trains you to be an insurance salesman or a bureaucrat, or some kind of cerebral character.[8]

In the 1970s, DIY spread through the North American population of college- and recent-college-graduate age groups. In part, this movement involved the renovation of affordable, rundown older homes. But it also related to various projects expressing the social and environmental vision of the 1960s and early 1970s. The young visionary Stewart Brand, working with friends and family, and initially using the most basic of typesetting and page-layout tools, published the first edition of The Whole Earth Catalog (subtitled Access to Tools) in late 1968.


Fiberglass dome house, California, in style of the Whole Earth Catalog building techniques
The first Catalog, and its successors, used a broad definition of the term "tools". There were informational tools, such as books (often technical in nature), professional journals, courses, classes, and the like. There were specialized, designed items, such as carpenters' and masons' tools, garden tools, welding equipment, chainsaws, fiberglass materials and so on; even early personal computers. The designer J. Baldwin acted as editor to include such items, writing many of the reviews. The Catalog's publication both emerged from and spurred the great wave of experimentalism, convention-breaking, and do-it-yourself attitude of the late 1960s. Often copied, the Catalog appealed to a wide cross-section of people in North America and had a broad influence.

For decades, magazines such as Popular Mechanics and Mechanix Illustrated offered a way for readers to keep current on useful practical skills and techniques. DIY home improvement books began to flourish in the 1970s, first created as collections of magazine articles. An early, extensive line of DIY how-to books was created by Sunset Books, based upon previously published articles from their magazine, Sunset, based in California. Time-Life, Better Homes and Gardens, and other publishers soon followed suit.


Electronics World 1959, home assembled amplifier
In the mid-1990s, DIY home-improvement content began to find its way onto the World Wide Web. HouseNet was the earliest bulletin-board style site where users could share information. HomeTips.com, established in early 1995, was among the first Web-based sites to deliver free extensive DIY home-improvement content created by expert authors.[citation needed] Since the late 1990s, DIY has exploded on the Web through thousands of sites.

In the 1970s, when home video (VCRs) came along, DIY instructors quickly grasped its potential for demonstrating processes by audio-visual means. In 1979, the PBS television series This Old House, starring Bob Vila, premiered and this spurred a DIY television revolution. The show was immensely popular, educating people on how to improve their living conditions (and the value of their house) without the expense of paying someone else to do (as much of) the work. In 1994, the HGTV Network cable television channel was launched in the United States and Canada, followed in 1999 by the DIY Network cable television channel. Both were launched to appeal to the growing percentage of North Americans interested in DIY topics, from home improvement to knitting. Such channels have multiple shows showing how to stretch one's budget to achieve professional-looking results (Design Cents, Design on a Dime, etc.) while doing the work yourself. Toolbelt Diva specifically caters to female DIYers.

Beyond magazines and television, the scope of home improvement DIY continues to grow online where most mainstream media outlets now have extensive DIY-focused informational websites such as This Old House, Martha Stewart, Hometalk, and the DIY Network. These are often extensions of their magazine or television brand. The growth of independent online DIY resources is also spiking.[9] The number of homeowners who blog about their experiences continues to grow, along with DIY websites from smaller organizations.



Fashion[edit]

Homemade lightbox
DIY amongst the fashion community is popular. With ideas being shared on social media such as YouTube about clothing, jewellery and hair styles. Techniques include distressing jeans, bleaching jeans, redesigning an old shirt, and studding denim. This trend is becoming popular.

Subculture[edit]
Main articles: DIY ethic and Maker culture
The terms "DIY" and "do-it-yourself" are also used to describe:


Zines, London
Self-publishing books, zines, and alternative comics
Bands or solo artists releasing their music on self-funded record labels.
Trading of mixtapes as part of cassette culture
Homemade stuffs based on the principles of "Recycle, Reuse & Reduce" (the 3R's). A common term in many Environmental movements encouraging people to reuse old, used objects found in their homes and to recycle simple materials like paper.[10]
Crafts such as knitting, crochet, sewing, handmade jewelry, ceramics
Designing business cards, invitations and so on
Creating punk or indie musical merchandise through the use of recycling thrift store or discarded materials, usually decorated with art applied by silk screen.[11]
Independent game development and game modding
Contemporary roller derby
Skateparks built by skateboarders without paid professional assistance
Building musical electronic circuits such as the Atari Punk Console and create circuit bending noise machines from old children toys.
Modifying ("mod'ing") common products to allow extended or unintended uses, commonly referred to by the internet term, "life-hacking". Related to jury-rigging i.e. sloppy/ unikely mods
DIY electronics like littleBits
DIY science: using open-source hardware to make scientific equipment to conduct citizen science or simply low-cost traditional science [12]
Using low-cost single-board computers, such as Arduino and Raspberry Pi, as embedded systems with various applications
DIY bio

Drink mixing robot
DIY as a subculture could be said to have begun with the punk movement of the 1970s.[13] Instead of traditional means of bands reaching their audiences through large music labels, bands began recording, manufacturing albums and merchandise, booking their own tours, and creating opportunities for smaller bands to get wider recognition and gain cult status through repetitive low-cost DIY touring. The burgeoning zine movement took up coverage of and promotion of the underground punk scenes, and significantly altered the way fans interacted with musicians. Zines quickly branched off from being hand-made music magazines to become more personal; they quickly became one of the youth culture's gateways to DIY culture. This led to tutorial zines showing others how to make their own shirts, posters, zines, books, food, etc.

Groups and Publications[edit]

This section is in a list format that may be better presented using prose. You can help by converting this section to prose, if appropriate. Editing help is available. (November 2011)

Maker Faire, Detroit, 2011
Publications and websites that focus on DIY and/or crafting content include:

Bazaar Bizarre - An informal craft show and a book with the same name emphasizing subcultural aesthetics
Craft - A magazine publication focusing on projects using cheap materials
Craftster - An online crafting community making use of traditional crafting techniques
CrimethInc. - An anarchist collective encouraging self-publishing
Destination DIY - An Oregon radio show discussing DIY projects
Hometalk - A social networking site focusing home improvement
Instructables - A website featuring user-published DIY projects of a wide range of topics
Lifehacker - A website revolving around life tips
Make - A magazine publication covering a wide array of complex projects
Maker Faire - An event held by Make Magazine
Microcosm Publishing - A magazine encouraging self-dependence
Popular Mechanics - A magazine featuring popular technology
ReadyMade (magazine) - A publication focusing on sustainable design and independent music
TechShop - A chain of workshops enabling people of various skill level to use industrial equipment
See also[edit]

Duct tape wallet
Bricolage
Circuit bending
Edupunk
Hackerspace
Handyman
Instructables
Junk box
Maker culture
Number 8 wire
Open Design
Project GreenOman
Prosumer
Ready-to-assemble furniture
3D printing
How-to
C (programming language)
From Wikipedia, the free encyclopedia
C
Text in light blue serif capital letters on white background and very large light blue sans-serif letter C.
The C Programming Language[1] (often referred to as "K&R"), the seminal book on C
Paradigm	Imperative (procedural), structured
Designed by	Dennis Ritchie
Developer	Dennis Ritchie & Bell Labs (creators); ANSI X3J11 (ANSI C); ISO/IEC JTC1/SC22/WG14 (ISO C)
First appeared	1972; 44 years ago[2]
Stable release	C11 / December 2011; 4 years ago
Typing discipline	Static, weak, manifest, nominal
OS	Cross-platform
Filename extensions	.c, .h
Major implementations
GCC, Clang, Intel C, MSVC, Pelles C, Watcom C
Dialects
Cyclone, Unified Parallel C, Split-C, Cilk, C*
Influenced by
B (BCPL, CPL), ALGOL 68,[3] Assembly, PL/I, FORTRAN
Influenced
Numerous: AMPL, AWK, csh, C++, C--, C#, Objective-C, BitC, D, Go, Rust, Java, JavaScript, Julia, Limbo, LPC, Perl, PHP, Pike, Processing, Python, Seed7, Vala, Verilog (HDL)[4]
 C Programming at Wikibooks
C (/?si?/, as in the letter c) is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.

C was originally developed by Dennis Ritchie between 1969 and 1973 at Bell Labs,[5] and used to re-implement the Unix operating system.[6] It has since become one of the most widely used programming languages of all time,[7][8] with C compilers from various vendors available for the majority of existing computer architectures and operating systems. C has been standardized by the American National Standards Institute (ANSI) since 1989 (see ANSI C) and subsequently by the International Organization for Standardization (ISO).

Contents  [hide] 
1	Design
2	Overview
2.1	Relations to other languages
3	History
3.1	Early developments
3.2	K&R C
3.3	ANSI C and ISO C
3.4	C99
3.5	C11
3.6	Embedded C
4	Syntax
4.1	Character set
4.2	Keywords
4.3	Operators
5	"Hello, world" example
6	Data types
6.1	Pointers
6.2	Arrays
6.3	Array–pointer interchangeability
7	Memory management
8	Libraries
9	Language tools
10	Uses
11	Related languages
12	See also
13	Notes
14	References
15	Further reading
16	External links
Design[edit]
C is an imperative (procedural) language. It was designed to be compiled using a relatively straightforward compiler, to provide low-level access to memory, to provide language constructs that map efficiently to machine instructions, and to require minimal run-time support. C was therefore useful for many applications that had formerly been coded in assembly language, such as in system programming.

Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant and portably written C program can be compiled for a very wide variety of computer platforms and operating systems with few changes to its source code. The language has become available on a very wide range of platforms, from embedded microcontrollers to supercomputers.

Overview[edit]
Like most imperative languages in the ALGOL tradition, C has facilities for structured programming and allows lexical variable scope and recursion, while a static type system prevents many unintended operations. In C, all executable code is contained within subroutines, which are called "functions" (although not in the strict sense of functional programming). Function parameters are always passed by value. Pass-by-reference is simulated in C by explicitly passing pointer values. C program source text is free-format, using the semicolon as a statement terminator and curly braces for grouping blocks of statements.

The C language also exhibits the following characteristics:

There is a small, fixed number of keywords, including a full set of flow of control primitives: for, if/else, while, switch, and do/while. There is one namespace, and user-defined names are not distinguished from keywords by any kind of sigil.
There are a large number of arithmetical and logical operators, such as +, +=, ++, &, ~, etc.
More than one assignment may be performed in a single statement.
Function return values can be ignored when not needed.
Typing is static, but weakly enforced: all data has a type, but implicit conversions can be performed; for instance, characters can be used as integers.
Declaration syntax mimics usage context. C has no "define" keyword; instead, a statement beginning with the name of a type is taken as a declaration. There is no "function" keyword; instead, a function is indicated by the parentheses of an argument list.
User-defined (typedef) and compound types are possible.
Heterogeneous aggregate data types (struct) allow related data elements to be accessed and assigned as a unit.
Array indexing is a secondary notation, defined in terms of pointer arithmetic. Unlike structs, arrays are not first-class objects; they cannot be assigned or compared using single built-in operators. There is no "array" keyword, in use or definition; instead, square brackets indicate arrays syntactically, for example month[11].
Enumerated types are possible with the enum keyword. They are not tagged, and are freely interconvertible with integers.
Strings are not a separate data type, but are conventionally implemented as null-terminated arrays of characters.
Low-level access to computer memory is possible by converting machine addresses to typed pointers.
Procedures (subroutines not returning values) are a special case of function, with an untyped return type void.
Functions may not be defined within the lexical scope of other functions.
Function and data pointers permit ad hoc run-time polymorphism.
A preprocessor performs macro definition, source code file inclusion, and conditional compilation.
There is a basic form of modularity: files can be compiled separately and linked together, with control over which functions and data objects are visible to other files via static and extern attributes.
Complex functionality such as I/O, string manipulation, and mathematical functions are consistently delegated to library routines.
C does not include some features found in newer, more modern high-level languages, including object orientation and garbage collection.

Relations to other languages[edit]
Many later languages have borrowed directly or indirectly from C, including C++, D, Go, Rust, Java, JavaScript, Limbo, LPC, C#, Objective-C, Perl, PHP, Python, Verilog (hardware description language),[4] and Unix's C shell. These languages have drawn many of their control structures and other basic features from C. Most of them (with Python being the most dramatic exception) are also very syntactically similar to C in general, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.

History[edit]
Early developments[edit]

Ken Thompson (left) with Dennis Ritchie (right, the inventor of the C programming language)
The origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Ritchie and Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was developed in assembly language. The developers were considering rewriting the system using the B language, Thompson's simplified version of BCPL.[9] However B's inability to take advantage of some of the PDP-11's features, notably byte addressability, led to C.

The development of C started in 1972 on the PDP-11 Unix system[10] and first appeared in Version 2 Unix.[11] The language was not initially designed with portability in mind, but soon ran on different platforms as well: a compiler for the Honeywell 6000 was written within the first year of C's history, while an IBM System/370 port followed soon.[1][10] The name of C simply continued the alphabetic order started by B.[12]

Also in 1972, a large part of Unix was rewritten in C.[13] By 1973, with the addition of struct types, the C language had become powerful enough that most of the Unix's kernel was now in C.

Unix was one of the first operating system kernels implemented in a language other than assembly. (Earlier instances include the Multics system (written in PL/I), and MCP (Master Control Program) for the Burroughs B5000 written in ALGOL in 1961.) Circa 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.[10]

K&R C[edit]

The cover of the book, The C Programming Language
In 1978, Brian Kernighan and Dennis Ritchie published the first edition of The C Programming Language.[1] This book, known to C programmers as "K&R", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as K&R C. The second edition of the book[14] covers the later ANSI C standard, described below.

K&R introduced several language features:

standard I/O library
long int data type
unsigned int data type
compound assignment operators of the form =op (such as =-) were changed to the form op= (that is, -=) to remove the semantic ambiguity created by such constructs as i=-10, which had been interpreted as i =- 10 (decrement i by 10) instead of the possibly intended i = -10 (let i be -10)
Even after the publication of the 1989 ANSI standard, for many years K&R C was still considered the "lowest common denominator" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.

In early versions of C, only functions that returned a non-int value needed to be declared if used before the function definition; a function used without any previous declaration was assumed to return type int, if its value was used.

For example:

long some_function();
/* int */ other_function();

/* int */ calling_function()
{
    long test1;
    register /* int */ test2;

    test1 = some_function();
    if (test1 > 0)
          test2 = 0;
    else
          test2 = other_function();
    return test2;
}
The int type specifiers which are commented out could be omitted in K&R C, but are required in later standards.

Since K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments. Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.

In the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC[15]) and some other vendors. These included:

void functions (i.e., functions with no return value)
functions returning struct or union types (rather than pointers)
assignment for struct data types
enumerated types
The large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.

ANSI C and ISO C[edit]
Main article: ANSI C
During the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.

In 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 "Programming Language C". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.

In 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms "C89" and "C90" refer to the same programming language.

ANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.

One of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), void pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.

C89 is supported by current C compilers, and most C code being written today is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.

In cases where code must be compilable by either standard-conforming or K&R C-based compilers, the __STDC__ macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.

After the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995 Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.[citation needed]

C99[edit]
Main article: C99
The C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as "C99". It has since been amended three times by Technical Corrigenda.[16]

C99 introduced several new features, including inline functions, several new data types (including long long int and a complex type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with //, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.

C99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has int implicitly assumed. A standard macro __STDC_VERSION__ is defined with value 199901L to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.[17]

C11[edit]
Main article: C11 (C standard revision)
In 2007, work began on another revision of the C standard, informally called "C1X" until its official publication on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.

The C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro __STDC_VERSION__ is defined as 201112L to indicate that C11 support is available.

Embedded C[edit]
Main article: Embedded C
Historically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.

In 2008, the C Standards Committee published a technical report extending the C language[18] to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.

Syntax[edit]
Main article: C syntax
C has a formal grammar specified by the C standard.[19] Unlike languages such as FORTRAN 77, C source code is free-form which allows arbitrary use of whitespace to format code, rather than column-based or text-line-based restrictions; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters /* and */, or (since C99) following // until the end of the line. Comments delimited by /* and */ do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.[20]

C source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as struct, union, and enum, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as char and int specify built-in types. Sections of code are enclosed in braces ({ and }, sometimes called "curly brackets") to limit the scope of declarations and to act as a single statement for control structures.

As an imperative language, C uses statements to specify actions. The most common statement is an expression statement, consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by if(-else) conditional execution and by do-while, while, and for iterative execution (looping). The for statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. break and continue can be used to leave the innermost enclosing loop statement or skip to its reinitialization. There is also a non-structured goto statement which branches directly to the designated label within the function. switch selects a case to be executed based on the value of an integer expression.

Expressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next "sequence point"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (&&, ||, ?: and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.

Kernighan and Ritchie say in the Introduction of The C Programming Language: "C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better."[21] The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.

Character set[edit]
The basic C source character set includes the following characters:

Lowercase and uppercase letters: a–z A–Z
Decimal digits: 0–9
Graphic characters: ! " # % & ' ( ) * + , - . / : ; < = > ? [ \ ] ^ _ { | } ~
Whitespace characters: space, horizontal tab, vertical tab, form feed, newline
Newline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.

Additional multibyte encoded characters may be used in string literals, but they are not entirely portable. The latest C standard (C11) allows multinational Unicode characters to be embedded portably within C source text by using \uXXXX or \UXXXXXXXX encoding (where the X denotes a hexadecimal character), although this feature is not yet widely implemented.

The basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.

Keywords[edit]
C89 has 32 keywords (reserved words with special meaning):

auto
break
case
char
const
continue
default
do
double
else
enum
extern
float
for
goto
if
int
long
register
return
short
signed
sizeof
static
struct
switch
typedef
union
unsigned
void
volatile
while
C99 adds five more keywords:

_Bool
_Complex
_Imaginary
inline
restrict
C11 adds seven more keywords:[22]

_Alignas
_Alignof
_Atomic
_Generic
_Noreturn
_Static_assert
_Thread_local
Most of the recently added keywords begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved keyword called entry, but this was seldom implemented, and has now been removed as a reserved word.[23]

Operators[edit]
Main article: Operators in C and C++
C supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:

arithmetic: +, -, *, /, %
assignment: =
augmented assignment: +=, -=, *=, /=, %=, &=, |=, ^=, <<=, >>=
bitwise logic: ~, &, |, ^
bitwise shifts: <<, >>
boolean logic: !, &&, ||
conditional evaluation: ? :
equality testing: ==, !=
calling functions: ( )
increment and decrement: ++, --
member selection: ., ->
object size: sizeof
order relations: <, <=, >, >=
reference and dereference: &, *, [ ]
sequencing: ,
subexpression grouping: ( )
type conversion: (typename)
C uses the = operator, reserved in mathematics to express equality, to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. The similarity between C's operator for assignment and that for equality (==) has been criticized[by whom?] as it makes it easy to accidentally substitute one for the other. In many cases, each may be used in the context of the other without a compilation error (although some compilers produce warnings). For example, the conditional expression in if(a=b+1) is true if a is not zero after the assignment.[24] Additionally, C's operator precedence is non-intuitive, such as == binding more tightly than & and | in expressions like x & 1 == 0, which would need to be written (x & 1) == 0 to be properly evaluated.[25]

"Hello, world" example[edit]
The "hello, world" example, which appeared in the first edition of K&R, has become the model for an introductory program in most programming textbooks, regardless of programming language. The program prints "hello, world" to the standard output, which is usually a terminal or screen display.

The original version was:[26]

main()
{
    printf("hello, world\n");
}
A standard-conforming "hello, world" program is:[a]

#include <stdio.h>

int main(void)
{
    printf("hello, world\n");
}
The first line of the program contains a preprocessing directive, indicated by #include. This causes the compiler to replace that line with the entire text of the stdio.h standard header, which contains declarations for standard input and output functions such as printf. The angle brackets surrounding stdio.h indicate that stdio.h is located using a search strategy that prefers headers in the compiler's include path to other headers having the same name; double quotes are used to include local or project-specific header files.[discuss]

The next line indicates that a function named main is being defined. The main function serves a special purpose in C programs; the run-time environment calls the main function to begin program execution. The type specifier int indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the main function, is an integer. The keyword void as a parameter list indicates that this function takes no arguments.[b]

The opening curly brace indicates the beginning of the definition of the main function.

The next line calls (diverts execution to) a function named printf, which is supplied from a system library. In this call, the printf function is passed (provided with) a single argument, the address of the first character in the string literal "hello, world\n". The string literal is an unnamed array with elements of type char, set up automatically by the compiler with a final 0-valued character to mark the end of the array (printf needs to know this). The \n is an escape sequence that C translates to a newline character, which on output signifies the end of the current line. The return value of the printf function is of type int, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the printf function succeeded.) The semicolon ; terminates the statement.

The closing curly brace indicates the end of the code for the main function. According to the C99 specification and newer, the main function, unlike any other function, will implicitly return a status of 0 upon reaching the } that terminates the function. This is interpreted by the run-time system as an exit code indicating successful execution.[27]

Data types[edit]
Main article: C variable types and declarations

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2012) (Learn how and when to remove this template message)
The type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal.[28] There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, characters, and enumerated types (enum). C99 added a boolean datatype. There are also derived types including arrays, pointers, records (struct), and untagged unions (union).

C is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a type cast to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.

Some find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: "declaration reflects use".)[29]

C's usual arithmetic conversions allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.

Pointers[edit]
C supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be dereferenced to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type. Pointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers. Many data types, such as trees, are commonly implemented as dynamically allocated struct objects linked together using pointers. Pointers to functions are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch) or as callbacks to be invoked by event handlers.[27]

A null pointer value explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no "next" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a null pointer constant can be written as 0, with or without explicit casting to a pointer type, or as the NULL macro defined by several standard headers. In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.

Void pointers (void *) point to objects of unspecified type, and can therefore be used as "generic" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.[27]

Careless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may be deallocated and reused (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.

Arrays[edit]
See also: C string
Array types in C are traditionally of a fixed, static size specified at compile time. (The more recent C99 standard also allows a form of variable-length arrays.) However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's malloc function, and treat it as an array. C's unification of arrays and pointers means that declared arrays and these dynamically allocated simulated arrays are virtually interchangeable.

Since arrays are always accessed (in effect) via pointers, array accesses are typically not checked against the underlying array size, although some compilers may provide bounds checking as an option.[30] Array bounds violations are therefore possible and rather common in carelessly written code, and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions. If bounds checking is desired, it must be done manually.

C does not have a special provision for declaring multidimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting "multidimensional array" can be thought of as increasing in row-major order.

Multidimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, since arrays are passed merely as pointers, the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this is to allocate the array with an additional "row vector" of pointers to the columns.)

C99 introduced "variable-length arrays" which address some, but not all, of the issues with ordinary C arrays.

Array–pointer interchangeability[edit]
The subscript notation x[i] (where x designates a pointer) is a syntactic sugar for *(x+i).[31] Taking advantage of the compiler's knowledge of the pointer type, the address that x + i points to is not the base address (pointed to by x) incremented by i bytes, but rather is defined to be the base address incremented by i multiplied by the size of an element that x points to. Thus, x[i] designates the i+1th element of the array.

Furthermore, in most expression contexts (a notable exception is as operand of sizeof), the name of an array is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.

The size of an element can be determined by applying the operator sizeof to any dereferenced element of x, as in n = sizeof *x or n = sizeof x[0], and the number of elements in a declared array A can be determined as sizeof A / sizeof A[0]. The latter only applies to array names: variables declared with subscripts (int A[20]). Due to the semantics of C, it is not possible to determine the entire size of arrays through pointers to arrays or those created by dynamic allocation (malloc); code such as sizeof arr / sizeof arr[0] (where arr designates a pointer) will not work since the compiler assumes the size of the pointer itself is being requested.[32][33] Since array name arguments to sizeof are not converted to pointers, they do not exhibit such ambiguity. However, arrays created by dynamic allocation are accessed by pointers rather than true array variables, so they suffer from the same sizeof issues as array pointers.

Thus, despite this apparent equivalence between array and pointer variables, there is still a distinction to be made between them. Even though the name of an array is, in most expression contexts, converted into a pointer (to its first element), this pointer does not itself occupy any storage; the array name is not an l-value, and its address is a constant, unlike a pointer variable. Consequently, what an array "points to" cannot be changed, and it is impossible to assign a new address to an array name. Array contents may be copied, however, by using the memcpy function, or by accessing the individual elements.

Memory management[edit]
One of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three distinct ways to allocate memory for objects:[27]

Static memory allocation: space for the object is provided in the binary at compile-time; these objects have an extent (or lifetime) as long as the binary which contains them is loaded into memory.
Automatic memory allocation: temporary objects can be stored on the stack, and this space is automatically freed and reusable after the block in which they are declared is exited.
Dynamic memory allocation: blocks of memory of arbitrary size can be requested at run-time using library functions such as malloc from a region of memory called the heap; these blocks persist until subsequently freed for reuse by calling the library function realloc or free
These three approaches are appropriate in different situations and have various tradeoffs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.

Where possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary.[27] Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on malloc for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)

Unless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.

Another issue is that heap memory allocation has to be synchronized with its actual usage in any program in order for it to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before free() is called, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a memory leak. Conversely, it is possible for memory to be freed but continue to be referenced, leading to unpredictable results. Typically, the symptoms will appear in a portion of the program far removed from the actual error, making it difficult to track down the problem. (Such issues are ameliorated in languages with automatic garbage collection.)

Libraries[edit]
The C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single "archive" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., -lm, shorthand for "math library").[27]

The most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation. (Implementations which target limited environments such as embedded systems may provide only a subset of the standard library.) This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, stdio.h) specify the interfaces for these and other standard library facilities.

Another common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.

Since many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.[27]

Language tools[edit]

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2014) (Learn how and when to remove this template message)
Tools have been created to help C programmers avoid some of the problems inherent in the language, such as statements with undefined behavior or statements that are not a good practice because they are likely to result in unintended behavior or run-time errors.

Automated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.[34]

There are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as array bounds checking, buffer overflow detection, serialization, and automatic garbage collection.

Tools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.

Uses[edit]

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2012) (Learn how and when to remove this template message)

The TIOBE index graph from 2002 to 2015, showing a comparison of the popularity of various programming languages[35]
C is widely used for "system programming", including implementing operating systems and embedded system applications, due to a combination of desirable characteristics such as code portability and efficiency, ability to access specific hardware addresses, ability to pun types to match externally imposed data access requirements, and low run-time demand on system resources. C can also be used for website programming using CGI as a "gateway" for information between the Web application, the server, and the browser.[36] Some reasons for choosing C over interpreted languages are its speed, stability, and near-universal availability.[37]

One consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. The primary implementations of Python, Perl 5 and PHP, for example, are all written in C.

Due to its thin layer of abstraction and low overhead, C allows efficient implementations of algorithms and data structures, which is useful for programs that perform a lot of computations. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica and MATLAB are completely or partially written in C.

C is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, it is not necessary to develop machine-specific code generators. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, which support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--.

C has also been widely used to implement end-user applications, but much of that development has shifted to newer, higher-level languages.

Related languages[edit]

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (February 2016) (Learn how and when to remove this template message)
C has directly or indirectly influenced many later languages such as C#, D, Go, Java, JavaScript, Limbo, LPC, Perl, PHP, Python, and Unix's C shell. The most pervasive influence has been syntactical: all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models and/or large-scale program structures that differ from those of C, sometimes radically.

Several C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.

When object-oriented languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.

The C++ programming language was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax.[38] C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.

Objective-C was originally a very "thin" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.

In addition to C++ and Objective-C, Ch, Cilk and Unified Parallel C are nearly supersets of C.

See also[edit]
Portal icon	Computer programming portal
Portal icon	Information technology portal
Comparison of Pascal and C
Comparison of programming languages
International Obfuscated C Code Contest
List of C-based programming languages
List of C compilers
C++
From Wikipedia, the free encyclopedia
C++
Paradigm	Multi-paradigm: procedural, functional, object-oriented, generic[1]
Designed by	Bjarne Stroustrup
First appeared	1983; 33 years ago
Stable release	ISO/IEC 14882:2014 / 15 December 2014; 16 months ago
Typing discipline	Static, nominative, partially inferred
Implementation language	C++
OS	Cross-platform
Filename extensions	.cc .cpp .cxx .C .c++ .h .hh .hpp .hxx .h++
Website	isocpp.org
Major implementations
LLVM Clang, GCC, Microsoft Visual C++, Embarcadero C++Builder, Intel C++ Compiler, IBM XL C++
Influenced by
C, Simula, ALGOL 68, Ada, CLU, ML
Influenced
Ada 95, C99, C#,[2] Chapel,[3] D, Java, Lua, Rust, Python, Perl, PHP
 C++ Programming at Wikibooks
C++ (pronounced as cee plus plus, /?si? pl?s pl?s/) is a general-purpose programming language. It has imperative, object-oriented and generic programming features, while also providing facilities for low-level memory manipulation.

It was designed with a bias toward system programming and embedded, resource-constrained and large systems, with performance, efficiency and flexibility of use as its design highlights.[4] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[4] including desktop applications, servers (e.g. e-commerce, web search or SQL servers), and performance-critical applications (e.g. telephone switches or space probes).[5] C++ is a compiled language, with implementations of it available on many platforms and provided by various organizations, including the FSF, LLVM, Microsoft, Intel and IBM.

C++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2014 as ISO/IEC 14882:2014 (informally known as C++14).[6] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, ISO/IEC 14882:2003, standard. The current C++14 standard supersedes these and C++11, with new features and an enlarged standard library. Before the initial standardization in 1998, C++ was developed by Bjarne Stroustrup at Bell Labs since 1979, as an extension of the C language as he wanted an efficient and flexible language similar to C, which also provided high-level features for program organization.

Many other programming languages have been influenced by C++, including C#, Java, and newer versions of C (after 1998).

Contents  [hide] 
1	History
1.1	Etymology
1.2	Philosophy
1.3	Standardization
2	Language
2.1	Object storage
2.1.1	Static storage duration objects
2.1.2	Thread storage duration objects
2.1.3	Automatic storage duration objects
2.1.4	Dynamic storage duration objects
2.2	Templates
2.3	Objects
2.3.1	Encapsulation
2.3.2	Inheritance
2.4	Operators and operator overloading
2.5	Polymorphism
2.5.1	Static polymorphism
2.5.2	Dynamic polymorphism
2.5.2.1	Inheritance
2.5.2.2	Virtual member functions
2.6	Lambda expressions
2.7	Exception handling
3	Standard library
4	Compatibility
4.1	With C
5	Criticism
6	See also
7	References
8	Further reading
9	External links
History[edit]

Bjarne Stroustrup, the creator of C++
In 1979, Bjarne Stroustrup, a Danish computer scientist, began work on the predecessor to C++, "C with Classes".[7] The motivation for creating a new language originated from Stroustrup's experience in programming for his Ph.D. thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his Ph.D. experience, Stroustrup set out to enhance the C language with Simula-like features.[8] C was chosen because it was general-purpose, fast, portable and widely used. As well as C and Simula's influences, other languages also influenced C++, including ALGOL 68, Ada, CLU and ML.

Initially, Stroustrup's "C with Classes" added features to the C compiler, Cpre, including classes, derived classes, strong typing, inlining and default arguments.[9]

In 1983, C with Classes was renamed to C++ ("++" being the increment operator in C), adding new features that included virtual functions, function name and operator overloading, references, constants, type-safe free-store memory allocation (new/delete), improved type checking, and BCPL style single-line comments with two forward slashes (//). Furthermore, it included the development of a standalone compiler for C++, Cfront.

In 1985, the first edition of The C++ Programming Language was released, which became the definitive reference for the language, as there was not yet an official standard.[10] The first commercial implementation of C++ was released in October of the same year.[7]

In 1989, C++ 2.0 was released, followed by the updated second edition of The C++ Programming Language in 1991.[11] New features in 2.0 included multiple inheritance, abstract classes, static member functions, const member functions, and protected members. In 1990, The Annotated C++ Reference Manual was published. This work became the basis for the future standard. Later feature additions included templates, exceptions, namespaces, new casts, and a boolean type.

After the 2.0 update, C++ evolved relatively slowly until, in 2011, the C++11 standard was released, adding numerous new features, enlarging the standard library further, and providing more facilities to C++ programmers. After a minor C++14 update, released in December 2014, various new additions are planned for 2017.

Etymology[edit]
According to Stroustrup: "the name signifies the evolutionary nature of the changes from C".[12] This name is credited to Rick Mascitti (mid-1983)[9] and was first used in December 1983. When Mascitti was questioned informally in 1992 about the naming, he indicated that it was given in a tongue-in-cheek spirit. The name stems from C's "++" operator (which increments the value of a variable) and a common naming convention of using "+" to indicate an enhanced computer program.

During C++'s development period, the language had been referred to as "new C" and "C with Classes"[9][13] before acquiring its final name.

Philosophy[edit]
Throughout C++'s life, its development and evolution has been informally governed by a set of rules that its evolution should follow:[8]

It must be driven by actual problems and its features should be useful immediately in real world programs.
Every feature should be implementable (with a reasonably obvious way to do so).
Programmers should be free to pick their own programming style, and that style should be fully supported by C++.
Allowing a useful feature is more important than preventing every possible misuse of C++.
It should provide facilities for organising programs into well-defined separate parts, and provide facilities for combining separately developed parts.
No implicit violations of the type system (but allow explicit violations; that is, those explicitly requested by the programmer).
User-created types need to have the same support and performance as built-in types.
Unused features should not negatively impact created executables (e.g. in lower performance).
There should be no language beneath C++ (except assembly language).
C++ should work alongside other existing programming languages, rather than fostering its own separate and incompatible programming environment.
If the programmer's intent is unknown, allow the programmer to specify it by providing manual control.
Standardization[edit]
Year	C++ Standard	Informal name
1998	ISO/IEC 14882:1998[14]	C++98
2003	ISO/IEC 14882:2003[15]	C++03
2011	ISO/IEC 14882:2011[6]	C++11
2014	ISO/IEC 14882:2014[16]	C++14
2017	to be determined	C++17
C++ is standardized by an ISO working group known as JTC1/SC22/WG21. So far, it published four revisions of the C++ standard and is currently working on the next revision, C++17.

In 1998, the ISO working group standardized C++ for the first time as ISO/IEC 14882:1998, which is informally known as C++98. In 2003, it published a new version of the C++ standard called ISO/IEC 14882:2003, which fixed problems identified in C++98.

The next major revision of the standard was informally referred to as "C++0x", but it was not released until 2011.[17] C++11 (14882:2011) included many additions to both the core language and the standard library.[6]

In 2014, C++14 (also known as C++1y) was released as a small extension to C++11, featuring mainly bug fixes and small improvements.[18] The Draft International Standard ballot procedures completed in mid-August 2014.[19]

After C++14, a major revision, informally known as C++17 or C++1z, is planned for 2017,[18] which is almost feature-complete.[20]

As part of the standardization process, ISO also publishes technical reports and specifications:

ISO/IEC TR 18015:2006[21] on the use of C++ in embedded systems and on performance implications of C++ language and library features,
ISO/IEC TR 19768:2007[22] (also known as the C++ Technical Report 1) on library extensions mostly integrated into C++11,
ISO/IEC TR 29124:2010[23] on special mathematical functions,
ISO/IEC TR 24733:2011[24] on decimal floating point arithmetic,
ISO/IEC TS 18822:2015[25] on the standard filesystem library,
ISO/IEC TS 19570:2015[26] on parallel versions of the standard library algorithms,
ISO/IEC TS 19841:2015[27] on software transactional memory,
ISO/IEC TS 19568:2015[28] on a new set of library extensions, some of which are already integrated into C++17,
ISO/IEC TS 19217:2015[29] on the C++ Concepts
More technical specifications are in development and pending approval, including concurrency library extensions, a networking standard library, ranges, and modules.[30]

Language[edit]
The C++ language has two main components: a direct mapping of hardware features provided primarily by the C subset, and zero-overhead abstractions based on those mappings. Stroustrup describes C++ as "a light-weight abstraction programming language [designed] for building and using efficient and elegant abstractions";[4] and "offering both hardware access and abstraction is the basis of C++. Doing it efficiently is what distinguishes it from other languages".[31]

C++ inherits most of C's syntax. The following is Bjarne Stroustrup's version of the Hello world program that uses the C++ Standard Library stream facility to write a message to standard output:[32][33]

#include <iostream>

int main()
{
	std::cout << "Hello, world!\n";
}
Within functions that define a non-void return type, failure to return a value before control reaches the end of the function results in undefined behaviour (compilers typically provide the means to issue a diagnostic in such a case).[34] The sole exception to this rule is the main function, which implicitly returns a value of zero.[35]

Object storage[edit]
As in C, C++ supports four types of memory management: static storage duration objects, thread storage duration objects, automatic storage duration objects, and dynamic storage duration objects.[36]

Static storage duration objects[edit]
Static storage duration objects are created before main() is entered (see exceptions below) and destroyed in reverse order of creation after main() exits. The exact order of creation is not specified by the standard (though there are some rules defined below) to allow implementations some freedom in how to organize their implementation. More formally, objects of this type have a lifespan that "shall last for the duration of the program".[37]

Static storage duration objects are initialized in two phases. First, "static initialization" is performed, and only after all static initialization is performed, "dynamic initialization" is performed. In static initialization, all objects are first initialized with zeros; after that, all objects that have a constant initialization phase are initialized with the constant expression (i.e. variables initialized with a literal or constexpr). Though it is not specified in the standard, the static initialization phase can be completed at compile time and saved in the data partition of the executable. Dynamic initialization involves all object initialization done via a constructor or function call (unless the function is marked with constexpr, in C++11). The dynamic initialization order is defined as the order of declaration within the compilation unit (i.e. the same file). No guarantees are provided about the order of initialization between compilation units.

Thread storage duration objects[edit]
Variables of this type are very similar to Static Storage duration objects. The main difference is the creation time is just prior to thread creation and destruction is done after the thread has been joined.[38]

Automatic storage duration objects[edit]
The most common variable types in C++ are local variables inside a function or block, and temporary variables.[39] The common feature about automatic variables is that they have a lifetime that is limited to the scope of the variable. They are created and potentially initialized at the point of declaration (see below for details) and destroyed in the reverse order of creation when the scope is left.

Local variables are created as the point of execution passes the declaration point. If the variable has a constructor or initializer this is used to define the initial state of the object. Local variables are destroyed when the local block or function that they are declared in is closed. C++ destructors for local variables are called at the end of the object lifetime, allowing a discipline for automatic resource management termed RAII, which is widely used in C++.

Member variables are created when the parent object is created. Array members are initialized from 0 to the last member of the array in order. Member variables are destroyed when the parent object is destroyed in the reverse order of creation. i.e. If the parent is an "automatic object" then it will be destroyed when it goes out of scope which triggers the destruction of all its members.

Temporary variables are created as the result of expression evaluation and are destroyed when the statement containing the expression has been fully evaluated (usually at the ; at the end of a statement).

Dynamic storage duration objects[edit]
Main article: new and delete (C++)
These objects have a dynamic lifespan and are created with new call and destroyed with an explicit call to delete.[40]

Templates[edit]
See also: Template metaprogramming and Generic programming
C++ templates enable generic programming. C++ supports function, class, alias and variable templates. Templates may be parameterized by types, compile-time constants, and other templates. Templates are implemented by instantiation at compile-time. To instantiate a template, compilers substitute specific arguments for a template's parameters to generate a concrete function or class instance. Some substitutions are not possible; these are eliminated by an overload resolution policy described by the phrase "Substitution failure is not an error" (SFINAE). Templates are a powerful tool that can be used for generic programming, template metaprogramming, and code optimization, but this power implies a cost. Template use may increase code size, because each template instantiation produces a copy of the template code: one for each set of template arguments, however, this is the same or smaller amount of code that would be generated if the code was written by hand.[41] This is in contrast to run-time generics seen in other languages (e.g., Java) where at compile-time the type is erased and a single template body is preserved.

Templates are different from macros: while both of these compile-time language features enable conditional compilation, templates are not restricted to lexical substitution. Templates are aware of the semantics and type system of their companion language, as well as all compile-time type definitions, and can perform high-level operations including programmatic flow control based on evaluation of strictly type-checked parameters. Macros are capable of conditional control over compilation based on predetermined criteria, but cannot instantiate new types, recurse, or perform type evaluation and in effect are limited to pre-compilation text-substitution and text-inclusion/exclusion. In other words, macros can control compilation flow based on pre-defined symbols but cannot, unlike templates, independently instantiate new symbols. Templates are a tool for static polymorphism (see below) and generic programming.

In addition, templates are a compile time mechanism in C++ that is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram prior to runtime.

In summary, a template is a compile-time parameterized function or class written without knowledge of the specific arguments used to instantiate it. After instantiation, the resulting code is equivalent to code written specifically for the passed arguments. In this manner, templates provide a way to decouple generic, broadly applicable aspects of functions and classes (encoded in templates) from specific aspects (encoded in template parameters) without sacrificing performance due to abstraction.

Objects[edit]
Main article: C++ classes
C++ introduces object-oriented programming (OOP) features to C. It offers classes, which provide the four features commonly present in OOP (and some non-OOP) languages: abstraction, encapsulation, inheritance, and polymorphism. One distinguishing feature of C++ classes compared to classes in other programming languages is support for deterministic destructors, which in turn provide support for the Resource Acquisition is Initialization (RAII) concept.

Encapsulation[edit]
Encapsulation is the hiding of information to ensure that data structures and operators are used as intended and to make the usage model more obvious to the developer. C++ provides the ability to define classes and functions as its primary encapsulation mechanisms. Within a class, members can be declared as either public, protected, or private to explicitly enforce encapsulation. A public member of the class is accessible to any function. A private member is accessible only to functions that are members of that class and to functions and classes explicitly granted access permission by the class ("friends"). A protected member is accessible to members of classes that inherit from the class in addition to the class itself and any friends.

The OO principle is that all of the functions (and only the functions) that access the internal representation of a type should be encapsulated within the type definition. C++ supports this (via member functions and friend functions), but does not enforce it: the programmer can declare parts or all of the representation of a type to be public, and is allowed to make public entities that are not part of the representation of the type. Therefore, C++ supports not just OO programming, but other weaker decomposition paradigms, like modular programming.

It is generally considered good practice to make all data private or protected, and to make public only those functions that are part of a minimal interface for users of the class. This can hide the details of data implementation, allowing the designer to later fundamentally change the implementation without changing the interface in any way.[42][43]

Inheritance[edit]
Inheritance allows one data type to acquire properties of other data types. Inheritance from a base class may be declared as public, protected, or private. This access specifier determines whether unrelated and derived classes can access the inherited public and protected members of the base class. Only public inheritance corresponds to what is usually meant by "inheritance". The other two forms are much less frequently used. If the access specifier is omitted, a "class" inherits privately, while a "struct" inherits publicly. Base classes may be declared as virtual; this is called virtual inheritance. Virtual inheritance ensures that only one instance of a base class exists in the inheritance graph, avoiding some of the ambiguity problems of multiple inheritance.

Multiple inheritance is a C++ feature not found in most other languages, allowing a class to be derived from more than one base class; this allows for more elaborate inheritance relationships. For example, a "Flying Cat" class can inherit from both "Cat" and "Flying Mammal". Some other languages, such as C# or Java, accomplish something similar (although more limited) by allowing inheritance of multiple interfaces while restricting the number of base classes to one (interfaces, unlike classes, provide only declarations of member functions, no implementation or member data). An interface as in C# and Java can be defined in C++ as a class containing only pure virtual functions, often known as an abstract base class or "ABC". The member functions of such an abstract base class are normally explicitly defined in the derived class, not inherited implicitly. C++ virtual inheritance exhibits an ambiguity resolution feature called dominance.

Operators and operator overloading[edit]
Operators that cannot be overloaded
Operator	Symbol
Scope resolution operator	::
Conditional operator	?:
dot operator	.
Member selection operator	.*
"sizeof" operator	sizeof
"typeid" operator	typeid
Main article: Operators in C and C++
C++ provides more than 35 operators, covering basic arithmetic, bit manipulation, indirection, comparisons, logical operations and others. Almost all operators can be overloaded for user-defined types, with a few notable exceptions such as member access (. and .*) as well as the conditional operator. The rich set of overloadable operators is central to making user-defined types in C++ seem like built-in types.

Overloadable operators are also an essential part of many advanced C++ programming techniques, such as smart pointers. Overloading an operator does not change the precedence of calculations involving the operator, nor does it change the number of operands that the operator uses (any operand may however be ignored by the operator, though it will be evaluated prior to execution). Overloaded "&&" and "||" operators lose their short-circuit evaluation property.

Polymorphism[edit]
See also: Polymorphism in object-oriented programming
Polymorphism enables one common interface for many implementations, and for objects to act differently under different circumstances.

C++ supports several kinds of static (compile-time) and dynamic (run-time) polymorphisms, supported by the language features described above. Compile-time polymorphism does not allow for certain run-time decisions, while run-time polymorphism typically incurs a performance penalty.

Static polymorphism[edit]
See also: Parametric polymorphism and ad hoc polymorphism
Function overloading allows programs to declare multiple functions having the same name but with different arguments (i.e. ad hoc polymorphism). The functions are distinguished by the number or types of their formal parameters. Thus, the same function name can refer to different functions depending on the context in which it is used. The type returned by the function is not used to distinguish overloaded functions and would result in a compile-time error message.

When declaring a function, a programmer can specify for one or more parameters a default value. Doing so allows the parameters with defaults to optionally be omitted when the function is called, in which case the default arguments will be used. When a function is called with fewer arguments than there are declared parameters, explicit arguments are matched to parameters in left-to-right order, with any unmatched parameters at the end of the parameter list being assigned their default arguments. In many cases, specifying default arguments in a single function declaration is preferable to providing overloaded function definitions with different numbers of parameters.

Templates in C++ provide a sophisticated mechanism for writing generic, polymorphic code (i.e. parametric polymorphism). In particular, through the Curiously Recurring Template Pattern, it's possible to implement a form of static polymorphism that closely mimics the syntax for overriding virtual functions. Because C++ templates are type-aware and Turing-complete, they can also be used to let the compiler resolve recursive conditionals and generate substantial programs through template metaprogramming. Contrary to some opinion, template code will not generate a bulk code after compilation with the proper compiler settings.[41]

Dynamic polymorphism[edit]
Inheritance[edit]
See also: Subtyping
Variable pointers and references to a base class type in C++ can also refer to objects of any derived classes of that type. This allows arrays and other kinds of containers to hold pointers to objects of differing types (references cannot be directly held in containers). This enables dynamic (run-time) polymorphism, where the referred objects can behave differently depending on their (actual, derived) types

C++ also provides the dynamic_cast operator, which allows code to safely attempt conversion of an object, via a base reference/pointer, to a more derived type: downcasting. The attempt is necessary as often one does not know which derived type is referenced. (Upcasting, conversion to a more general type, can always be checked/performed at compile-time via static_cast, as ancestral classes are specified in the derived class's interface, visible to all callers.) dynamic_cast relies on run-time type information (RTTI), metadata in the program that enables differentiating types and their relationships. If a dynamic_cast to a pointer fails, the result is the nullptr constant, whereas if the destination is a reference (which cannot be null), the cast throws an exception. Objects known to be of a certain derived type can be cast to that with static_cast, bypassing RTTI and the safe runtime type-checking of dynamic_cast, so this should be used only if the programmer is very confident the cast is, and will always be, valid.

Virtual member functions[edit]
Ordinarily, when a function in a derived class overrides a function in a base class, the function to call is determined by the type of the object. A given function is overridden when there exists no difference in the number or type of parameters between two or more definitions of that function. Hence, at compile time, it may not be possible to determine the type of the object and therefore the correct function to call, given only a base class pointer; the decision is therefore put off until runtime. This is called dynamic dispatch. Virtual member functions or methods[44] allow the most specific implementation of the function to be called, according to the actual run-time type of the object. In C++ implementations, this is commonly done using virtual function tables. If the object type is known, this may be bypassed by prepending a fully qualified class name before the function call, but in general calls to virtual functions are resolved at run time.

In addition to standard member functions, operator overloads and destructors can be virtual. As a rule of thumb, if any function in the class is virtual, the destructor should be as well. As the type of an object at its creation is known at compile time, constructors, and by extension copy constructors, cannot be virtual. Nonetheless a situation may arise where a copy of an object needs to be created when a pointer to a derived object is passed as a pointer to a base object. In such a case, a common solution is to create a clone() (or similar) virtual function that creates and returns a copy of the derived class when called.

A member function can also be made "pure virtual" by appending it with = 0 after the closing parenthesis and before the semicolon. A class containing a pure virtual function is called an abstract data type. Objects cannot be created from abstract data types; they can only be derived from. Any derived class inherits the virtual function as pure and must provide a non-pure definition of it (and all other pure virtual functions) before objects of the derived class can be created. A program that attempts to create an object of a class with a pure virtual member function or inherited pure virtual member function is ill-formed.

Lambda expressions[edit]
C++ provides support for anonymous functions, which are also known as lambda expressions and have the following form:

[capture](parameters) -> return_type { function_body }
The [capture] list supports the definition of closures. Such lambda expressions are defined in the standard as syntactic sugar for an unnamed function object. An example lambda function may be defined as follows:

[](int x, int y) -> int { return x + y; }
Exception handling[edit]
Exception handling is used to communicate the existence of a runtime problem or error from where it was detected to where the issue can be handled.[45] It permits this to be done in a uniform manner and separately from the main code, while detecting all errors.[46] Should an error occur, an exception is thrown (raised), which is then caught by the nearest suitable exception handler. The exception causes the current scope to be exited, and also each outer scope (propagation) until a suitable handler is found, calling in turn the destructors of any objects in these exited scopes.[47] At the same time, an exception is presented as an object carrying the data about the detected problem.[48]

The exception-causing code is placed inside a try block. The exceptions are handled in separate catch blocks (the handlers); each try block can have multiple exception handlers, as it is visible in the example below.[49]

#include <iostream>
#include <vector>
#include <stdexcept>

int main() {
    try {
        std::vector<int> vec{3,4,3,1};
        int i{vec.at(4)}; // Throws an exception, std::out_of_range (indexing for vec is from 0-3 not 1-4)
    }

    // An exception handler, catches std::out_of_range, which is thrown by vec.at(4)
    catch (std::out_of_range& e) {
        std::cerr << "Accessing a non-existent element: " << e.what() << '\n';
    }

    // To catch any other standard library exceptions (they derive from std::exception)
    catch (std::exception& e) {
        std::cerr << "Exception thrown: " << e.what() << '\n';
    }

    // Catch any unrecognised exceptions (i.e. those which don't derive from std::exception)
    catch (...) {
        std::cerr << "Some fatal error\n";
    }
}
It is also possible to raise exceptions purposefully, using the throw keyword; these exceptions are handled in the usual way. In some cases, exceptions cannot be used due to technical reasons. One such example is a critical component of an embedded system, where every operation must be guaranteed to complete within a specified amount of time. This cannot be determined with exceptions as no tools exist to determine the minimum time required for an exception to be handled.[50]

Standard library[edit]
Main article: C++ Standard Library
The C++ standard consists of two parts: the core language and the standard library. C++ programmers expect the latter on every major implementation of C++; it includes vectors, lists, maps, algorithms (find, for_each, binary_search, random_shuffle, etc.), sets, queues, stacks, arrays, tuples, input/output facilities (iostream, for reading from and writing to the console and files), smart pointers for automatic memory management, regular expression support, multi-threading library, atomics support (allowing a variable to be read or written to by at most one thread at a time without any external synchronisation), time utilities (measurement, getting current time, etc.), a system for converting error reporting that doesn't use C++ exceptions into C++ exceptions, a random number generator and a slightly modified version of the C standard library (to make it comply with the C++ type system).

A large part of the C++ library is based on the Standard Template Library (STL). Useful tools provided by the STL include containers as the collections of objects (such as vectors and lists), iterators that provide array-like access to containers, and algorithms that perform operations such as searching and sorting.

Furthermore, (multi)maps (associative arrays) and (multi)sets are provided, all of which export compatible interfaces. Therefore, using templates it is possible to write generic algorithms that work with any container or on any sequence defined by iterators. As in C, the features of the library are accessed by using the #include directive to include a standard header. C++ provides 105 standard headers, of which 27 are deprecated.

The standard incorporates the STL that was originally designed by Alexander Stepanov, who experimented with generic algorithms and containers for many years. When he started with C++, he finally found a language where it was possible to create generic algorithms (e.g., STL sort) that perform even better than, for example, the C standard library qsort, thanks to C++ features like using inlining and compile-time binding instead of function pointers. The standard does not refer to it as "STL", as it is merely a part of the standard library, but the term is still widely used to distinguish it from the rest of the standard library (input/output streams, internationalization, diagnostics, the C library subset, etc.).[51]

Most C++ compilers, and all major ones, provide a standards conforming implementation of the C++ standard library.

Compatibility[edit]
To give compiler vendors greater freedom, the C++ standards committee decided not to dictate the implementation of name mangling, exception handling, and other implementation-specific features. The downside of this decision is that object code produced by different compilers is expected to be incompatible. There were, however, attempts to standardize compilers for particular machines or operating systems (for example C++ ABI),[52] though they seem to be largely abandoned now.

With C[edit]
For more details on this topic, see Compatibility of C and C++.
C++ is often considered to be a superset of C, but this is not strictly true.[53] Most C code can easily be made to compile correctly in C++, but there are a few differences that cause some valid C code to be invalid or behave differently in C++. For example, C allows implicit conversion from void* to other pointer types, but C++ does not (for type safety reasons). Also, C++ defines many new keywords, such as new and class, which may be used as identifiers (for example, variable names) in a C program.

Some incompatibilities have been removed by the 1999 revision of the C standard (C99), which now supports C++ features such as line comments (//), and declarations mixed with code. On the other hand, C99 introduced a number of new features that C++ did not support, were incompatible or redundant in C++, such as variable-length arrays, native complex-number types (however, the std::complex class in the C++ standard library is closely compatible), designated initializers, compound literals, and the restrict keyword.[54] Some of the C99-introduced features were included in the subsequent version of the C++ standard, C++11 (out of those which were not redundant).[55][56][57]

To intermix C and C++ code, any function declaration or definition that is to be called from/used both in C and C++ must be declared with C linkage by placing it within an extern "C" {/*...*/} block. Such a function may not rely on features depending on name mangling (i.e., function overloading).

Criticism[edit]
Main article: Criticism of C++
Despite its widespread adoption, many programmers have criticized the C++ language, including Linus Torvalds,[58] Richard Stallman,[59] and Ken Thompson.[60] Issues include a lack of reflection or garbage collection, slow compilation times, and verbose error messages, particularly from template metaprogramming.[61] Because C++ introduces an additional memory footprint on programs due to internally generated vtables and constructors,[citation needed] programmers such as Torvalds prefer C over C++ for low-level, performance-critical and portable system software.[58]

To avoid the problems that exist in C++, and to increase productivity,[62] some people suggest alternative languages newer than C++, such as D, Go, Rust and Vala.[63]
Input/output
From Wikipedia, the free encyclopedia
  (Redirected from I/O)
"I/O" redirects here. For other uses, see I/O (disambiguation).
For uses of the term input-output in economics, see Input-output model.

This article does not cite any sources. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2013) (Learn how and when to remove this template message)
In computing, input/output or I/O (or, informally, io or IO) is the communication between an information processing system, such as a computer, and the outside world, possibly a human or another information processing system. Inputs are the signals or data received by the system and outputs are the signals or data sent from it. The term can also be used as part of an action; to "perform I/O" is to perform an input or output operation. I/O devices are used by a human (or other system) to communicate with a computer. For instance, a keyboard or mouse is an input device for a computer, while monitors and printers are output devices. Devices for communication between computers, such as modems and network cards, typically perform both input and output operations.

Note that the designation of a device as either input or output depends on perspective. Mouse and keyboards take physical movements that the human user outputs and convert them into input signals that a computer can understand; the output from these devices is the computer's input. Similarly, printers and monitors take signals that a computer outputs as input, and they convert these signals into a representation that human users can understand. From the human user's perspective, the process of reading or seeing these representations is receiving input; this type of interaction between computers and humans is studied in the field of human–computer interaction.

In computer architecture, the combination of the CPU and main memory, to which the CPU can read or write directly using individual instructions, is considered the brain of a computer. Any transfer of information to or from the CPU/memory combo, for example by reading data from a disk drive, is considered I/O. The CPU and its supporting circuitry may provide memory-mapped I/O that is used in low-level computer programming, such as in the implementation of device drivers, or may provide access to I/O channels. An I/O algorithm is one designed to exploit locality and perform efficiently when exchanging data with a secondary storage device, such as a disk drive.

Contents  [hide] 
1	Interface
1.1	Higher-level implementation
2	Channel I/O
3	Port-mapped I/O
4	See also
Interface[edit]
An I/O interface is required whenever the I/O device is driven by the processor. The interface must have necessary logic to interpret the device address generated by the processor. Handshaking should be implemented by the interface using appropriate commands (like BUSY, READY, and WAIT), and the processor can communicate with an I/O device through the interface. If different data formats are being exchanged, the interface must be able to convert serial data to parallel form and vice versa. There must be provision for generating interrupts and the corresponding type numbers for further processing by the processor if required.

A computer that uses memory-mapped I/O accesses hardware by reading and writing to specific memory locations, using the same assembly language instructions that computer would normally use to access memory.

Higher-level implementation[edit]
Higher-level operating system and programming facilities employ separate, more abstract I/O concepts and primitives. For example, most operating systems provide application programs with the concept of files. The C and C++ programming languages, and operating systems in the Unix family, traditionally abstract files and devices as streams, which can be read or written, or sometimes both. The C standard library provides functions for manipulating streams for input and output.

In the context of the ALGOL 68 programming language, the input and output facilities are collectively referred to as transput. The ALGOL 68 transput library recognizes the following standard files/devices: stand in, stand out, stand errors and stand back.

An alternative to special primitive functions is the I/O monad, which permits programs to just describe I/O, and the actions are carried out outside the program. This is notable because the I/O functions would introduce side-effects to any programming language, but this allows purely functional programming to be practical.

Channel I/O[edit]
Channel I/O requires the use of instructions that are specifically designed to perform I/O operations. The I/O instructions address the channel or the channel and device; the channel asynchronously accesses all other required addressing and control information. This is similar to DMA, but more flexible.

Port-mapped I/O[edit]
Port-mapped I/O also requires the use of special I/O instructions. Typically one or more ports are assigned to the device, each with a special purpose. The port numbers are in a separate address space from that used by normal instructions.
Computer science is the scientific and practical approach to computation and its applications. It is the systematic study of the feasibility, structure, expression, and 
mechanization of the methodical procedures (or algorithms) that underlie the acquisition, representation, processing, storage, communication of, and access to 
information. An alternate, more succinct definition of computer science is the study of automating algorithmic processes that scale. A computer scientist specializes in the 
theory of computation and the design of computational systems.[1]
Its fields can be divided into a variety of theoretical and practical disciplines. Some fields, such as computational complexity theory (which explores the fundamental 
properties of computational and intractable problems), are highly abstract, while fields such as computer graphics emphasize real world visual applications. Still other 
fields focus on challenges in implementing computation. For example, programming language theory considers various approaches to the description of computation, 
while the study of computer programming itself investigates various aspects of the use of programming language and complex systems. Human computer interaction 
considers the challenges in making computers and computations useful, usable, and universally accessible to humans.
Personal computer
From Wikipedia, the free encyclopedia
A personal computer (PC) is a general-purpose computer whose size, capabilities, and original sale price make it useful for individuals, and is intended to be operated directly by an end-user with no intervening computer time-sharing models that allowed larger, more expensive minicomputer and mainframe systems to be used by many people, usually at the same time.

Software applications for most personal computers include, but are not limited to, word processing, spreadsheets, databases, web browsers and e-mail clients, digital media playback, games and many personal productivity and special-purpose software applications. Modern personal computers often have connections to the Internet, allowing access to the World Wide Web and a wide range of other resources. Personal computers may be connected to a local area network (LAN), either by a cable or a wireless connection. A personal computer may be a laptop computer or a desktop computer running an operating system such as Windows, Linux (and the various operating systems based on it), or Macintosh OS.

Early computer owners usually had to write their own programs to do anything useful with the machines, which even did not include an operating system. The very earliest microcomputers, equipped with a front panel, required hand-loading of a bootstrap program to load programs from external storage (paper tape, cassettes, or eventually diskettes). Before very long, automatic booting from permanent read-only memory became universal. Today's users have access to a wide range of commercial software, freeware and free and open-source software, which are provided in ready-to-run or ready-to-compile form. Software for personal computers, such as applications and video games, are typically developed and distributed independently from the hardware or OS manufacturers, whereas software for many mobile phones and other portable systems is approved and distributed through a centralized online store.[1][2]

Since the early 1990s, Microsoft operating systems and Intel hardware dominated much of the personal computer market, first with MS-DOS and then with Windows. Popular alternatives to Microsoft's Windows operating systems include Apple's OS X and free open-source Unix-like operating systems such as Linux and BSD. AMD provides the major alternative to Intel's processors. ARM architecture processors now outnumber Intel's (and compatibles) in smartphones and tablets, that are also personal computers, outnumbering the traditional kind.

Contents  [hide] 
1  History
1.1 Market and sales
1.2 Average selling price
2 Terminology
3 Types
3.1 Stationary
3.1.1 Workstation
3.1.2 Desktop computer
3.1.2.1 Gaming computer
3.1.2.2 Single unit
3.1.3 Nettop
3.1.4 Home theater PC
3.2 Portable
3.2.1 Laptop
3.2.1.1 Desktop replacement
3.2.2 Netbook
3.2.3 Tablet
3.2.4 Ultra-mobile PC
3.2.5 Pocket PC
4 Hardware
4.1 Computer case
4.2 Power supply unit
4.3 Processor
4.4 Motherboard
4.5 Main memory
4.6 Hard disk
4.7 Visual display unit
4.8 Video card
4.9 Keyboard
4.10  Mouse
4.11  Other components
5 Software
5.1 Operating system
5.1.1 Microsoft Windows
5.1.2 OS X
5.1.3 Linux
5.2 Applications
5.3 Gaming
6 Toxicity
6.1 Electronic waste regulation
7 See also
8 Notes
9 References
10  Further reading
11  External links
History[edit]
Main article: History of personal computers
The Programma 101 was the first commercial "desktop personal computer", produced by the Italian company Olivetti and invented by the Italian engineer Pier Giorgio Perotto, inventor of the magnetic card system. The project started in 1962. It was launched at the 1964 New York World's Fair, and volume production began in 1965, the computer retailing for  3,200.[3][unreliable source ]

NASA bought at least ten Programma 101s and used them for the calculations for the 1969 Apollo 11 Moon landing. The ABC Network used the Programma 101 to predict the presidential election of 1968, and the U.S. military used the machine to plan their operations in the Vietnam War. The Programma 101 was used in schools, hospitals, government offices. This marked the beginning of the era of the personal computer.

In 1968, Hewlett-Packard was ordered to pay about  900,000 in royalties to Olivetti after their Hewlett-Packard 9100A was ruled to have copied some of the solutions adopted in the Programma 101, including the magnetic card, the architecture and other similar components.[3]

The Soviet MIR series of computers was developed from 1965 to 1969 in a group headed by Victor Glushkov. It was designed as a relatively small-scale computer for use in engineering and scientific applications and contained a hardware implementation of a high-level programming language. Another innovative feature for that time was the user interface combining a keyboard with a monitor and light pen for correcting texts and drawing on screen.[4]

In what was later to be called the Mother of All Demos, SRI researcher Douglas Engelbart in 1968 gave a preview of what would become the staples of daily working life in the 21st century: e-mail, hypertext, word processing, video conferencing and the mouse. The demonstration required technical support staff and a mainframe time-sharing computer that were far too costly for individual business use at the time.


Commodore PET in 1983 (at American Museum of Science and Energy)
By the early 1970s, people in academic or research institutions had the opportunity for single-person use of a computer system in interactive mode for extended durations, although these systems would still have been too expensive to be owned by a single person.

Early personal computers   generally called microcomputers   were often sold in a kit form and in limited volumes, and were of interest mostly to hobbyists and technicians. Minimal programming was done with toggle switches to enter instructions, and output was provided by front panel lamps. Practical use required adding peripherals such as keyboards, computer displays, disk drives, and printers. Micral N was the earliest commercial, non-kit microcomputer based on a microprocessor, the Intel 8008. It was built starting in 1972 and about 90,000 units were sold.

In 1973 the IBM Los Gatos Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT and full function keyboard. SCAMP emulated an IBM 1130 minicomputer in order to run APL\1130.[5] In 1973 APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because SCAMP was the first to emulate APL\1130 performance on a portable, single user computer, PC Magazine in 1983 designated SCAMP a "revolutionary concept" and "the world's first personal computer".[5][6] This seminal, single user portable computer now resides in the Smithsonian Institution, Washington, D.C.. Successful demonstrations of the 1973 SCAMP prototype led to the IBM 5100 portable microcomputer launched in 1975 with the ability to be programmed in both APL and BASIC for engineers, analysts, statisticians and other business problem-solvers. In the late 1960s such a machine would have been nearly as large as two desks and would have weighed about half a ton.[5]

Another seminal product in 1973 was the Xerox Alto, developed at Xerox's Palo Alto Research Center (PARC), it had a graphical user interface (GUI) which later served as inspiration for Apple Computer's Macintosh, and Microsoft's Windows operating system. Also in 1973 Hewlett Packard introduced fully BASIC programmable microcomputers that fit entirely on top of a desk, including a keyboard, a small one-line display and printer. The Wang 2200 microcomputer of 1973 had a full-size cathode ray tube (CRT) and cassette tape storage.[7] These were generally expensive specialized computers sold for business or scientific uses. The introduction of the microprocessor, a single chip with all the circuitry that formerly occupied large cabinets, led to the proliferation of personal computers after 1975.


IBM Personal Computer XT in 1988
In 1976 Steve Jobs and Steve Wozniak sold the Apple I computer circuit board, which was fully prepared and contained about 30 chips. The Apple I computer differed from the other hobby computers of the time at the beckoning of Paul Terrell owner of the Byte Shop who gave Steve Jobs his first purchase order for 50 Apple I computers only if the computers were assembled and tested and not a kit computer so he would have computers to sell to everyone, not just people that could assemble a computer kit. The Apple I as delivered was still a kit computer as it did not have a power supply, case, or keyboard as delivered to the Byte Shop.

The first successfully mass marketed personal computer was the Commodore PET introduced in January 1977, but back-ordered and not available until later in the year.[8] At the same time, the Apple II (usually referred to as the "Apple") was introduced[9] (June 1977), and the TRS-80 from Tandy Corporation / Tandy Radio Shack in summer 1977, delivered in September in a small number. Mass-market ready-assembled computers allowed a wider range of people to use computers, focusing more on software applications and less on development of the processor hardware.


The 8-bit PMD 85 personal computer produced in 1985 1990 by the Tesla company in the former socialist Czechoslovakia. This computer was produced locally (in Pie  any) due to a lack of foreign currency with which to buy systems from the West.

IBM 5150, released in 1981
During the early 1980s, home computers were further developed for household use, with software for personal productivity, programming and games. They typically could be used with a television already in the home as the computer display, with low-detail blocky graphics and a limited color range, and text about 40 characters wide by 25 characters tall. Sinclair Research,[10] a UK company, produced the ZX Series   the ZX80 (1980), ZX81 (1981), and the ZX Spectrum; the latter was introduced in 1982, and totaled 8 million unit sold. Following came the Commodore 64, totaled 17 million units sold.[11][12]

In the same year, the NEC PC-98 was introduced, which was a very popular personal computer that sold in more than 18 million units.[13] Another famous personal computer, the revolutionary Amiga 1000, was unveiled by Commodore on July 23, 1985. The Amiga 1000 featured a multitasking, windowing operating system, color graphics with a 4096-color palette, stereo sound, Motorola 68000 CPU, 256 kB RAM, and 880 kB 3.5-inch disk drive, for US 1,295.[14]

Somewhat larger and more expensive systems (for example, running CP/M), or sometimes a home computer with additional interfaces and devices, although still low-cost compared with minicomputers and mainframes, were aimed at office and small business use, typically using "high resolution" monitors capable of at least 80 column text display, and often no graphical or color drawing capability.

Workstations were characterized by high-performance processors and graphics displays, with large-capacity local disk storage, networking capability, and running under a multitasking operating system.

Eventually, due to the influence of the IBM PC on the personal computer market, personal computers and home computers lost any technical distinction. Business computers acquired color graphics capability and sound, and home computers and game systems users used the same processors and operating systems as office workers. Mass-market computers had graphics capabilities and memory comparable to dedicated workstations of a few years before. Even local area networking, originally a way to allow business computers to share expensive mass storage and peripherals, became a standard feature of personal computers used at home.

In 1982 "The Computer" was named Machine of the Year by Time Magazine.

In the 2010s, several companies such as Hewlett-Packard and Sony sold off their PC and laptop divisions. As a result, the personal computer was declared dead several times during this time.[15]

Market and sales[edit]
See also: Market share of personal computer vendors

Personal computers worldwide in million distinguished by developed and developing world
In 2001, 125 million personal computers were shipped in comparison to 48,000 in 1977.[16] More than 500 million personal computers were in use in 2002 and one billion personal computers had been sold worldwide from the mid-1970s up to this time. Of the latter figure, 75% were professional or work related, while the rest were sold for personal or home use. About 81.5% of personal computers shipped had been desktop computers, 16.4% laptops and 2.1% servers. The United States had received 38.8% (394 million) of the computers shipped, Europe 25% and 11.7% had gone to the Asia-Pacific region, the fastest-growing market as of 2002. The second billion was expected to be sold by 2008.[17] Almost half of all households in Western Europe had a personal computer and a computer could be found in 40% of homes in United Kingdom, compared with only 13% in 1985.[18]

The global personal computer shipments were 350.9 million units in 2010,[19] 308.3 million units in 2009[20] and 302.2 million units in 2008.[21][22] The shipments were 264 million units in the year 2007, according to iSuppli,[23] up 11.2% from 239 million in 2006.[24] In 2004, the global shipments were 183 million units, an 11.6% increase over 2003.[25] In 2003, 152.6 million computers were shipped, at an estimated value of  175 billion.[26] In 2002, 136.7 million PCs were shipped, at an estimated value of  175 billion.[26] In 2000, 140.2 million personal computers were shipped, at an estimated value of  226 billion.[26] Worldwide shipments of personal computers surpassed the 100-million mark in 1999, growing to 113.5 million units from 93.3 million units in 1998.[27] In 1999, Asia had 14.1 million units shipped.[28]

As of June 2008, the number of personal computers in use worldwide hit one billion,[29] while another billion is expected to be reached by 2014. Mature markets like the United States, Western Europe and Japan accounted for 58% of the worldwide installed PCs. The emerging markets were expected to double their installed PCs by 2012 and to take 70% of the second billion PCs. About 180 million computers (16% of the existing installed base) were expected to be replaced and 35 million to be dumped into landfill in 2008. The whole installed base grew 12% annually.[30][31]

Based on International Data Corporation (IDC) data for Q2 2011, for the first time China surpassed US in PC shipments by 18.5 million and 17.7 million respectively. This trend reflects the rising of emerging markets as well as the relative stagnation of mature regions.[32]

In the developed world, there has been a vendor tradition to keep adding functions to maintain high prices of personal computers. However, since the introduction of the One Laptop per Child foundation and its low-cost XO-1 laptop, the computing industry started to pursue the price too. Although introduced only one year earlier, there were 14 million netbooks sold in 2008.[33] Besides the regular computer manufacturers, companies making especially rugged versions of computers have sprung up, offering alternatives for people operating their machines in extreme weather or environments.[34]

2014 worldwide PC vendor unit shipment estimates
Source  Date  Lenovo  HP  Dell  Acer Inc. Asus  Others
IDC[35] Q2 2014 19.6% 18.3% 14.0% 8.2%  6.2%  33.6%
Gartner[36] Q2 2014 19.2% 17.7% 13.3% 7.9%  6.9%  35.0%
In 2011, Deloitte consulting firm predicted that, smartphones and tablet computers as computing devices would surpass the PCs sales[37] (as has happened since 2012). As of 2013, worldwide sales of PCs had begun to fall as many consumers moved to tablets and smartphones for gifts and personal use. Sales of 90.3 million units in the 4th quarter of 2012 represented a 4.9% decline from sales in the 4th quarter of 2011.[38] Global PC sales fell sharply in the first quarter of 2013, according to IDC data. The 14% year-over-year decline was the largest on record since the firm began tracking in 1994, and double what analysts had been expecting.[39][40] The decline of Q2 2013 PC shipments marked the fifth straight quarter of falling sales.[41] "This is horrific news for PCs," remarked an analyst. "It s all about mobile computing now. We have definitely reached the tipping point."[39] Data from Gartner Inc. showed a similar decline for the same time period.[39] China's Lenovo Group bucked the general trend as strong sales to first time buyers in the developing world allowed the company's sales to stay flat overall.[39] Windows 8, which was designed to look similar to tablet/smartphone software, was cited as a contributing factor in the decline of new PC sales. "Unfortunately, it seems clear that the Windows 8 launch not only didn t provide a positive boost to the PC market, but appears to have slowed the market," said IDC Vice President Bob O Donnell.[40]

In August 2013, Credit Suisse published research findings that attributed around 75% of the operating profit share of the PC industry to Microsoft (operating system) and Intel (semiconductors).[42]

According to IDC, in 2013 PC shipments dropped by 9.8% as the greatest drop-ever in line with consumers trends to use mobile devices.[43]

Average selling price[edit]
Selling prices of personal computers steadily declined due to lower costs of production and manufacture, while the capabilities of computers increased. In 1975, an Altair kit sold for only around US  400, but required customers to solder components into circuit boards; peripherals required to interact with the system in alphanumeric form instead of blinking lights would add another  2,000, and the resultant system was only of use to hobbyists.[44]

At their introduction in 1981, the US  1,795 price of the Osborne 1 and its competitor Kaypro was considered an attractive price point; these systems had text-only displays and only floppy disks for storage. By 1982, Michael Dell observed that a personal computer system selling at retail for about  3,000 US was made of components that cost the dealer about  600; typical gross margin on a computer unit was around  1,000.[45] The total value of personal computer purchases in the US in 1983 was about  4 billion, comparable to total sales of pet food. By late 1998, the average selling price of personal computer systems in the United States had dropped below  1,000.[46]

For Microsoft Windows systems, the average selling price (ASP) showed a decline in 2008/2009, possibly due to low-cost netbooks, drawing  569 for desktop computers and  689 for laptops at U.S. retail in August 2008. In 2009, ASP had further fallen to  533 for desktops and to  602 for notebooks by January and to  540 and  560 in February.[47] According to research firm NPD, the average selling price of all Windows portable PCs has fallen from  659 in October 2008 to  519 in October 2009.[48]

Terminology[edit]
"PC" is an initialism for "personal computer". However, it is sometimes used in a different sense, referring to a personal computer with an Intel x86-compatible processor, very often running (but not necessarily limited to) Microsoft Windows, which is a combination sometimes also called Wintel, although large portion of PCs are not shipped with preinstalled Windows operating systems. Some PCs, including the OLPC XOs, are equipped with x86 or x64 processors but not designed to run Microsoft Windows. "PC" is used in contrast with "Mac", an Apple Macintosh computer.[49][50][51][52] This sense of the word is used in the Get a Mac advertisement campaign that ran between 2006 and 2009, as well as its rival, I'm a PC campaign, that appeared in 2008. Since Apple's transition to Intel processors starting 2005, all Macintosh computers are now PCs.[53]

Types[edit]
Stationary[edit]
Workstation[edit]

Sun SPARCstation 1+ from the early 1990s, with a 25 MHz RISC processor
Main article: Workstation
A workstation is a high-end personal computer designed for technical, mathematical, or scientific applications. Intended primarily to be used by one person at a time, they are commonly connected to a local area network and run multi-user operating systems. Workstations are used for tasks such as computer-aided design, drafting and modeling, computation-intensive scientific and engineering calculations, image processing, architectural modeling, and computer graphics for animation and motion picture visual effects.[54]

Desktop computer[edit]
Main article: Desktop computer

A Dell OptiPlex desktop computer
Prior to the widespread usage of PCs, a computer that could fit on a desk was remarkably small, leading to the "desktop" nomenclature. More recently, the phrase usually indicates a particular style of computer case. Desktop computers come in a variety of styles ranging from large vertical tower cases to small models which can be tucked behind an LCD monitor. In this sense, the term "desktop" refers specifically to a horizontally oriented case, usually intended to have the display screen placed on top to save desk space. Most modern desktop computers have separate screens and keyboards.

Gaming computer[edit]
Main article: Gaming computer
A gaming computer is a standard desktop computer that typically has high-performance hardware, such as a more powerful video card, processor and memory, in order to handle the requirements of demanding video games, which are often simply called "PC games".[55] A number of companies, such as Alienware, manufacture prebuilt gaming computers, and companies such as Razer and Logitech market mice, keyboards and headsets geared toward gamers.

Single unit[edit]
Further information: All-in-one computer
Single-unit PCs (also known as all-in-one PCs) are a subtype of desktop computers that combine the monitor and case of the computer within a single unit. The monitor often utilizes a touchscreen as an optional method of user input, but separate keyboards and mice are normally still included. The inner components of the PC are often located directly behind the monitor and many of such PCs are built similarly to laptops.

Nettop[edit]
Main article: Nettop
A subtype of desktops, called nettops, was introduced by Intel in February 2008, characterized by low cost and lean functionality. A similar subtype of laptops (or notebooks) is the netbook, described below. The product line features the new Intel Atom processor, which specifically enables nettops to consume less power and fit into small enclosures.

Home theater PC[edit]
Main article: Home theater PC

An Antec Fusion V2 home theater PC, with a keyboard placed on top of it.
A home theater PC (HTPC) is a convergence device that combines the functions of a personal computer and a digital video recorder. It is connected to a TV set or an appropriately sized computer display, and is often used as a digital photo viewer, music and video player, TV receiver, and digital video recorder. HTPCs are also referred to as media center systems or media servers. The general goal in a HTPC is usually to combine many or all components of a home theater setup into one box. More recently, HTPCs gained the ability to connect to services providing on-demand movies and TV shows.

HTPCs can be purchased pre-configured with the required hardware and software needed to add television programming to the PC, or can be cobbled together out of discrete components, what is commonly done with software support from MythTV, Windows Media Center, GB-PVR, SageTV, Famulent or LinuxMCE.

Portable[edit]
Laptop[edit]
Main article: Laptop

A modern laptop computer
A laptop computer, also called a notebook, is a small personal computer designed for portability. Usually, all of the hardware and interfaces needed to operate a laptop, such as the graphics card, audio devices or USB ports (previously parallel and serial ports), are built into a single unit. Laptops contain high-capacity batteries that can power the device for extensive periods of time, enhancing portability. Once the battery charge is depleted, it will have to be recharged through a power outlet. In the interests of saving power, weight and space, laptop graphics cards are in many cases integrated into the CPU or chipset and use system RAM, resulting in reduced graphics performance when compared to an equivalent desktop machine. For this reason, desktop or gaming computers are usually preferred to laptop PCs for gaming purposes.

One of the drawbacks of laptops is that, due to the size and configuration of components, usually relatively little can be done to upgrade the overall computer from its original design. Internal upgrades are either not manufacturer-recommended, can damage the laptop if done with poor care or knowledge, or in some cases impossible, making the desktop PC more modular. Some internal upgrades, such as memory and hard disk drive upgrades are often easily performed, while a display or keyboard upgrade is usually impossible. Just as desktops, laptops also have the same possibilities for connecting to a wide variety of devices, including external displays, mice, cameras, storage devices and keyboards, which may be attached externally through USB ports and other less common ports such as external video. Laptops are also a little more expensive compared to desktops, as the components for laptops themselves are expensive.

A subtype of notebooks, called subnotebook, has most of the features of a standard laptop computer, but with smaller physical dimensions. Subnotebooks are larger than hand-held computers, and usually run full versions of desktop or laptop operating systems. Ultra-Mobile PCs (UMPC) are usually considered subnotebooks, or more specifically, subnotebook tablet PCs, which are described below. Netbooks are sometimes considered to belong to this category, though they are sometimes separated into a category of their own (see below).

Desktop replacement[edit]
Main article: Desktop replacement computer

An Acer Aspire desktop replacement laptop
A desktop replacement computer (DTR) is a personal computer that provides the full capabilities of a desktop computer while remaining mobile. Such computers are often actually larger, bulkier laptops. Because of their increased size, this class of computers usually includes more powerful components and a larger display than generally found in smaller portable computers, and can have a relatively limited battery capacity or none at all in some cases. Some use a limited range of desktop components to provide better performance at the expense of battery life. Desktop replacement computers are sometimes called desknotes, as a portmanteau of words "desktop" and "notebook," though the term is also applied to desktop replacement computers in general.[56]

Netbook[edit]
Main article: Netbook

An HP netbook
Netbooks, also called mini notebooks or subnotebooks, are a subgroup of laptops[57] acting as a category of small, lightweight and inexpensive laptop computers suited for general computing tasks and accessing web-based applications. They are often marketed as "companion devices", with an intention to augment other ways in which a user can access computer resources.[57] Walt Mossberg called them a "relatively new category of small, light, minimalist and cheap laptops."[58] By August 2009, CNET called netbooks "nothing more than smaller, cheaper notebooks."[57]

Initially, the primary defining characteristic of netbooks was the lack of an optical disc drive, requiring it to be a separate external device. This has become less important as flash memory devices have gradually increased in capacity, replacing the writable optical disc (e.g. CD-RW, DVD-RW) as a transportable storage medium.

At their inception in late 2007   as smaller notebooks optimized for low weight and low cost[59]   netbooks omitted key features (e.g., the optical drive), featured smaller screens and keyboards, and offered reduced specifications and computing power. Over the course of their evolution, netbooks have ranged in their screen sizes from below five inches[60] to over 13 inches,[61] with weights around ~1 kg (2-3 pounds). Often significantly less expensive than other laptops,[62] by mid-2009 netbooks had been offered to users "free of charge", with an extended service contract purchase of a cellular data plan.[63]

In the short period since their appearance, netbooks have grown in size and features, converging with new smaller and lighter notebooks. By mid-2009, CNET noted that "the specs are so similar that the average shopper would likely be confused as to why one is better than the other," noting "the only conclusion is that there really is no distinction between the devices."[57]

Tablet[edit]
Main article: Tablet computer

HP Compaq tablet PC with rotating/removable keyboard
A tablet is a type of portable PC that de-emphasizes the use of traditional input devices (such as a mouse or keyboard) by using a touchscreen display, which can be controlled using either a stylus pen or finger. Some tablets may use a "hybrid" or "convertible" design, offering a keyboard that can either be removed as an attachment, or a screen that can be rotated and folded directly over top the keyboard.

Some tablets may run a traditional PC operating system such as Windows or Linux; Microsoft attempted to enter the tablet market in 2002 with its Microsoft Tablet PC specifications, for tablets and convertible laptops running Windows XP. However, Microsoft's early attempts were overshadowed by the release of Apple's iPad; following in its footsteps, most modern tablets use slate designs and run mobile operating systems such as Android and iOS, giving them functionality similar to smartphones. In response, Microsoft built its Windows 8 operating system to better accommodate these new touch-oriented devices.[64] Many tablet computers have USB ports, to which a keyboard or mouse can be connected.

Ultra-mobile PC[edit]
Main article: Ultra-mobile PC

A Samsung Q1 ultra-mobile PC
The ultra-mobile PC (UMPC) is a specification for small-configuration tablet PCs. It was developed as a joint development exercise by Microsoft, Intel and Samsung, among others. Current UMPCs typically feature the Windows XP, Windows Vista, Windows 7, or Linux operating system, and low-voltage Intel Atom or VIA C7-M processors.

Pocket PC[edit]
Main article: Pocket PC

An O2 pocket PC
A pocket PC is a hardware specification for a handheld-sized computer (personal digital assistant, PDA) that runs the Microsoft Windows Mobile operating system. It may have the capability to run an alternative operating system like NetBSD or Linux. Pocket PCs have many of the capabilities of modern desktop PCs.

Numerous applications are available for handhelds adhering to the Microsoft Pocket PC specification, many of which are freeware. Some of these devices also include mobile phone features, actually representing a smartphone. Microsoft-compliant Pocket PCs can also be used with many other add-ons like GPS receivers, barcode readers, RFID readers and cameras. In 2007, with the release of Windows Mobile 6, Microsoft dropped the name Pocket PC in favor of a new naming scheme: devices without an integrated phone are called Windows Mobile Classic instead of Pocket PC, while devices with an integrated phone and a touch screen are called Windows Mobile Professional.[65]

Hardware[edit]

An exploded view of a modern personal computer and peripherals:
Scanner
CPU (Microprocessor)
Memory (RAM)
Expansion cards (graphics cards, etc.)
Power supply
Optical disc drive
Storage (Hard disk or SSD)
Motherboard
Speakers
Monitor
System software
Application software
Keyboard
Mouse
External hard disk
Printer
Main article: Personal computer hardware
Computer hardware is a comprehensive term for all physical parts of a computer, as distinguished from the data it contains or operates on, and the software that provides instructions for the hardware to accomplish tasks. The boundary between hardware and software might be slightly blurry, with the existence of firmware that is software "built into" the hardware.

Mass-market consumer computers use highly standardized components and so are simple for an end user to assemble into a working system. A typical desktop computer consists of a computer case that holds the power supply, motherboard, hard disk drive, and often an optical disc drive. External devices such as a computer monitor or visual display unit, keyboard, and a pointing device are usually found in a personal computer.

The motherboard connects all processor, memory and peripheral devices together. The RAM, graphics card and processor are in most cases mounted directly onto the motherboard. The central processing unit (microprocessor chip) plugs into a CPU socket, while the memory modules plug into corresponding memory sockets. Some motherboards have the video display adapter, sound and other peripherals integrated onto the motherboard, while others use expansion slots for graphics cards, network cards, or other I/O devices. The graphics card or sound card may employ a break out box to keep the analog parts away from the electromagnetic radiation inside the computer case. Disk drives, which provide mass storage, are connected to the motherboard with one cable, and to the power supply through another cable. Usually, disk drives are mounted in the same case as the motherboard; expansion chassis are also made for additional disk storage. For extended amounts of data, a tape drive can be used or extra hard disks can be put together in an external case.

The keyboard and the mouse are external devices plugged into the computer through connectors on an I/O panel on the back of the computer case. The monitor is also connected to the I/O panel, either through an onboard port on the motherboard, or a port on the graphics card.

Capabilities of the personal computers hardware can sometimes be extended by the addition of expansion cards connected via an expansion bus. Standard peripheral buses often used for adding expansion cards in personal computers include PCI, PCI Express (PCIe), and AGP (a high-speed PCI bus dedicated to graphics adapters, found in older computers). Most modern personal computers have multiple physical PCI Express expansion slots, with some of the having PCI slots as well.

Computer case[edit]
Main article: Computer case

A stripped ATX case lying on its side.
A computer case is an enclosure that contains the main components of a computer. They are usually constructed from steel or aluminum combined with plastic, although other materials such as wood have been used. Cases are available in different sizes and shapes; the size and shape of a computer case is usually determined by the configuration of the motherboard that it is designed to accommodate, since this is the largest and most central component of most computers.

The most popular style for desktop computers is ATX, although microATX and similar layouts became very popular for a variety of uses. Companies like Shuttle Inc. and AOpen have popularized small cases, for which FlexATX is the most common motherboard size.

Power supply unit[edit]
Main article: Power supply unit (computer)

Computer power supply unit with top cover removed.
The power supply unit (PSU) converts general-purpose mains AC electricity to direct current (DC) for the other components of the computer. The rated output capacity of a PSU should usually be about 40% greater than the calculated system power consumption needs obtained by adding up all the system components. This protects against overloading the supply, and guards against performance degradation.

Processor[edit]
Main article: Central processing unit

AMD Athlon 64 X2 CPU.
The central processing unit, or CPU, is a part of a computer that executes instructions of a software program. In newer PCs, the CPU contains over a million transistors in one integrated circuit chip called the microprocessor. In most cases, the microprocessor plugs directly into the motherboard. The chip generates so much heat that the PC builder is required to attach a special cooling device to its surface; thus, modern CPUs are equipped with a fan attached via heat sink.

IBM PC compatible computers use an x86-compatible microprocessor, manufactured by Intel, AMD, VIA Technologies or Transmeta. Apple Macintosh computers were initially built with the Motorola 680x0 family of processors, then switched to the PowerPC series; in 2006, they switched to x86-compatible processors made by Intel.

Motherboard[edit]
Main article: Motherboard

A motherboard without processor, memory and expansion cards, cables
The motherboard, also referred to as system board or main board, is the primary circuit board within a personal computer, and other major system components plug directly into it or via a cable. A motherboard contains a microprocessor, the CPU supporting circuitry (mostly integrated circuits) that provide the interface between memory and input/output peripheral circuits, main memory, and facilities for initial setup of the computer immediately after power-on (often called boot firmware or, in IBM PC compatible computers, a BIOS or UEFI).

In many portable and embedded personal computers, the motherboard houses nearly all of the PC's core components. Often a motherboard will also contain one or more peripheral buses and physical connectors for expansion purposes. Sometimes a secondary daughter board is connected to the motherboard to provide further expandability or to satisfy space constraints.

Main memory[edit]
Main article: Primary storage

1GB DDR SDRAM PC-3200 module
A PC's main memory is a fast primary storage device that is directly accessible by the CPU, and is used to store the currently executing program and immediately needed data. PCs use semiconductor random access memory (RAM) of various kinds such as DRAM, SDRAM or SRAM as their primary storage. Which exact kind is used depends on cost/performance issues at any particular time.

Main memory is much faster than mass storage devices like hard disk drives or optical discs, but is usually volatile, meaning that it does not retain its contents (instructions or data) in the absence of power, and is much more expensive for a given capacity than is most mass storage. As a result, main memory is generally not suitable for long-term or archival data storage.

Hard disk[edit]
Main article: Hard disk drive

A Western Digital 250 GB hard disk drive
Mass storage devices store programs and data even when the power is off; they do require power to perform read and write functions during usage. Although flash memory has dropped in cost, the prevailing form of mass storage in personal computers is still the hard disk drive.

If the mass storage controller provides additional ports for expandability, a PC may also be upgraded by the addition of extra hard disk or optical disc drives. For example, BD-ROMs, DVD-RWs, and various optical disc recorders may all be added by the user to certain PCs. Standard internal storage device connection interfaces are PATA, Serial ATA and SCSI.

Solid state drives (SSDs) are a much faster replacement for traditional mechanical hard disk drives, but are also more expensive in terms of cost per gigabyte.

Visual display unit[edit]
Main article: Visual display unit
A visual display unit, computer monitor or just display, is a piece of electrical equipment, usually separate from the computer case, which displays visual images without producing a permanent computer record. A display device is usually either a CRT or some form of flat panel such as a TFT LCD. Multi-monitor setups are also quite common.

The display unit houses an electronic circuitry that generates its picture from signals received from the computer. Within the computer, either integral to the motherboard or plugged into it as an expansion card, there is pre-processing circuitry to convert the microprocessor's output data to a format compatible with the display unit's circuitry. The images from computer monitors originally contained only text, but as graphical user interfaces emerged and became common, they began to display more images and multimedia content.

The term "monitor" is also used, particularly by technicians in broadcasting television, where a picture of the broadcast data is displayed to a highly standardized reference monitor for confidence checking purposes.

Video card[edit]
Main article: Video card

An ATI Radeon video card
The video card otherwise called a graphics card, graphics adapter or video adapter processes the graphics output from the motherboard and transmits it to the display. It is an essential part of modern multimedia-enriched computing. On older models, and today on budget models, graphics circuitry may be integrated with the motherboard, but for modern and flexible machines, they are connected by the PCI, AGP, or PCI Express interface.

When the IBM PC was introduced, most existing business-oriented personal computers used text-only display adapters and had no graphics capability. Home computers at that time had graphics compatible with television signals, but with low resolution by modern standards owing to the limited memory available to the eight-bit processors available at the time.

Keyboard[edit]
Main article: Keyboard (computing)

A "Model M" IBM computer keyboard from the early 1980s. Commonly called the "Clicky Keyboard" due to its buckling spring key spring design, which gives the keyboard its iconic 'Click' sound with each keystroke.
A keyboard is an arrangement of buttons that each correspond to a function, letter, or number. They are the primary devices used for inputting text. In most cases, they contain an array of keys specifically organized with the corresponding letters, numbers, and functions printed or engraved on the button. They are generally designed around an operators language, and many different versions for different languages exist.

In English, the most common layout is the QWERTY layout, which was originally used in typewriters. They have evolved over time, and have been modified for use in computers with the addition of function keys, number keys, arrow keys, and keys specific to an operating system. Often, specific functions can be achieved by pressing multiple keys at once or in succession, such as inputting characters with accents or opening a task manager. Programs use keyboard shortcuts very differently and all use different keyboard shortcuts for different program specific operations, such as refreshing a web page in a web browser or selecting all text in a word processor.

Mouse[edit]
Main article: Mouse (computing)

A selection of computer mice built between 1986 and 2007
A computer mouse is a small handheld device that users hold and slide across a flat surface, pointing at various elements of a graphical user interface with an on-screen cursor, and selecting and moving objects using the mouse buttons. Almost all modern personal computers include a mouse; it may be plugged into a computer's rear mouse socket, or as a USB device, or, more recently, may be connected wirelessly via an USB dongle or Bluetooth link.

In the past, mice had a single button that users could press down on the device to "click" on whatever the pointer on the screen was hovering over. Modern mice have two, three or more buttons, providing a "right click" function button on the mouse, which performs a secondary action on a selected object, and a scroll wheel, which users can rotate using their fingers to "scroll" up or down. The scroll wheel can also be pressed down, and therefore be used as a third button. Some mouse wheels may be tilted from side to side to allow sideways scrolling. Different programs make use of these functions differently, and may scroll horizontally by default with the scroll wheel, open different menus with different buttons, etc. These functions may be also user-defined through software utilities.

Mice traditionally detected movement and communicated with the computer with an internal "mouse ball", and used optical encoders to detect rotation of the ball and tell the computer where the mouse has moved. However, these systems were subject to low durability, accuracy and required internal cleaning. Modern mice use optical technology to directly trace movement of the surface under the mouse and are much more accurate, durable and almost maintenance free. They work on a wider variety of surfaces and can even operate on walls, ceilings or other non-horizontal surfaces.

Other components[edit]

A proper ergonomic design of a personal computer workplace is necessary to prevent repetitive strain injuries, which can develop over time and can lead to long-term disability.[66]
All computers require either fixed or removable storage for their operating system, programs and user-generated material. Early home computers used compact audio cassettes for file storage; these were at the time a very low cost storage solution, but were displaced by floppy disk drives when manufacturing costs dropped, by the mid-1980s.

Initially, the 5.25-inch and 3.5-inch floppy drives were the principal forms of removable storage for backup of user files and distribution of software. As memory sizes increased, the capacity of the floppy did not keep pace; the Zip drive and other higher-capacity removable media were introduced but never became as prevalent as the floppy drive.

By the late 1990s, the optical drive, in CD and later DVD and Blu-ray Disc forms, became the main method for software distribution, and writeable media provided means for data backup and file interchange. As a result, floppy drives became uncommon in desktop personal computers since about 2000, and were dropped from many laptop systems even earlier.[note 1]

A second generation of tape recorders was provided when videocassette recorders were pressed into service as backup media for larger disk drives. All these systems were less reliable and slower than purpose-built magnetic tape drives. Such tape drives were uncommon in consumer-type personal computers but were a necessity in business or industrial use.

Interchange of data such as photographs from digital cameras is greatly expedited by installation of a card reader, which is often compatible with several forms of flash memory devices. It is usually faster and more convenient to move large amounts of data by removing the card from the mobile device, instead of communicating with the mobile device through a USB interface.

A USB flash drive performs much of the data transfer and backup functions formerly done with floppy drives, Zip disks and other devices. Mainstream operating systems for personal computers provide built-in support for USB flash drives, allowing interchange even between computers with different processors and operating systems. The compact size and lack of moving parts or dirt-sensitive media, combined with low cost and high capacity, have made USB flash drives a popular and useful accessory for any personal computer user.

The operating system can be located on any storage, but is typically installed on a hard disk or solid-state drive. A Live CD represents the concept of running an operating system directly from a CD. While this is slow compared to storing the operating system on a hard disk drive, it is typically used for installation of operating systems, demonstrations, system recovery, or other special purposes. Large flash memory is currently more expensive than hard disk drives of similar size (as of mid-2014) but are starting to appear in laptop computers because of their low weight, small size and low power requirements.

Computer communications involve internal modem cards, modems, network adapter cards, and routers. Common peripherals and adapter cards include headsets, joysticks, microphones, printers, scanners, sound adapter cards (as a separate card rather than located on the motherboard), speakers and webcams.

Software[edit]
Main article: Computer software

A screenshot of the OpenOffice.org Writer software
Computer software is any kind of computer program, procedure, or documentation that performs some task on a computer system.[67] The term includes application software such as word processors that perform productive tasks for users, system software such as operating systems that interface with computer hardware to provide the necessary services for application software, and middleware that controls and co-ordinates distributed systems.

Software applications are common for word processing, Internet browsing, Internet faxing, e-mail and other digital messaging, multimedia playback, playing of computer game, and computer programming. The user of a modern personal computer may have significant knowledge of the operating environment and application programs, but is not necessarily interested in programming nor even able to write programs for the computer. Therefore, most software written primarily for personal computers tends to be designed with simplicity of use, or "user-friendliness" in mind. However, the software industry continuously provide a wide range of new products for use in personal computers, targeted at both the expert and the non-expert user.

Operating system[edit]
Main article: Operating system
See also: Usage share of operating systems
An operating system (OS) manages computer resources and provides programmers with an interface used to access those resources. An operating system processes system data and user input, and responds by allocating and managing tasks and internal system resources as a service to users and programs of the system. An operating system performs basic tasks such as controlling and allocating memory, prioritizing system requests, controlling input and output devices, facilitating computer networking, and managing files.

Common contemporary desktop operating systems are Microsoft Windows, OS X, Linux, Solaris and FreeBSD. Windows, OS X, and Linux all have server and personal variants. With the exception of Microsoft Windows, the designs of each of them were inspired by or directly inherited from the Unix operating system, which was developed at Bell Labs beginning in the late 1960s and spawned the development of numerous free and proprietary operating systems.

Microsoft Windows[edit]
Main article: Microsoft Windows
Microsoft Windows is the collective brand name of several operating systems made by Microsoft which, as of 2015, are installed on PCs built by HP, Dell and Lenovo, the three remaining high volume manufacturers.[68] Microsoft first introduced an operating environment named Windows in November 1985,[69] as an add-on to MS-DOS and in response to the growing interest in graphical user interfaces (GUIs)[70][71] generated by Apple's 1984 introduction of the Macintosh.[72] As of August 2015, the most recent client and server version of Windows are Windows 10 and Windows Server 2012 R2, respectively. Windows Server 2016 is currently in Beta Testing.

OS X[edit]
Main article: OS X
OS X (formerly Mac OS X) is a line of operating systems developed, marketed and sold by Apple Inc.. OS X is the successor to the original Mac OS, which had been Apple's primary operating system since 1984. OS X is a Unix-based graphical operating system, and Snow Leopard, Leopard, Lion, Mountain Lion, Mavericks and Yosemite are its version codenames. The most recent version of OS X is codenamed El Capitan.

On iPhone, iPad and iPod, versions of iOS (which is an OS X derivative) are available from iOS 1.0 to the recent iOS 9. The iOS devices, however, are not considered PCs.

Linux[edit]
Main article: Linux

A Linux distribution running KDE Plasma Workspaces 4.
Linux is a family of Unix-like computer operating systems. Linux is one of the most prominent examples of free software and open source development: typically all underlying source code can be freely modified, used, and redistributed by anyone.[73] The name "Linux" refers to the Linux kernel, started in 1991 by Linus Torvalds. The system's utilities and libraries usually come from the GNU operating system, announced in 1983 by Richard Stallman. The GNU contribution is the basis for the alternative name GNU/Linux.[74]

Known for its use in servers, with the LAMP application stack as one of prominent examples, Linux is supported by corporations such as Dell, Hewlett-Packard, IBM, Novell, Oracle Corporation, Red Hat, Canonical Ltd. and Sun Microsystems. It is used as an operating system for a wide variety of computer hardware, including desktop computers, netbooks, supercomputers,[75] video game systems such as the Steam Machine or PlayStation 3 (until this option was removed remotely by Sony in 2010[76]), several arcade games, and embedded devices such as mobile phones, portable media players, routers, and stage lighting systems.

Applications[edit]

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (April 2014)
Main article: Application software

A screenshot of GIMP, which is a raster graphics editor
Generally, a computer user uses application software to carry out a specific task. System software supports applications and provides common services such as memory management, network connectivity and device drivers, all of which may be used by applications but are not directly of interest to the end user. A simplified analogy in the world of hardware would be the relationship of an electric light bulb (an application) to an electric power generation plant (a system): the power plant merely generates electricity, not itself of any real use until harnessed to an application like the electric light that performs a service that benefits the user.

Typical examples of software applications are word processors, spreadsheets, and media players. Multiple applications bundled together as a package are sometimes referred to as an application suite. Microsoft Office and OpenOffice.org, which bundle together a word processor, a spreadsheet, and several other discrete applications, are typical examples. The separate applications in a suite usually have a user interface that has some commonality making it easier for the user to learn and use each application. Often, they may have some capability to interact with each other in ways beneficial to the user; for example, a spreadsheet might be able to be embedded in a word processor document even though it had been created in the separate spreadsheet application.

End-user development tailors systems to meet the user's specific needs. User-written software include spreadsheet templates, word processor macros, scientific simulations, graphics and animation scripts; even email filters are a kind of user software. Users create this software themselves and often overlook how important it is.

Gaming[edit]
PC gaming is popular among the high-end PC market. According to an April 2014 market analysis, Gaming platforms like Steam (software), Uplay, Origin, and GOG.com (as well as competitive e-sports titles like League of Legends) are largely responsible for PC systems overtaking console revenue in 2013.[77]

Toxicity[edit]
Toxic chemicals found in some computer hardware include lead, mercury, cadmium, chromium, plastic (PVC), and barium. Overall, a computer is about 17% lead, copper, zinc, mercury, and cadmium; 23% is plastic, 14% is aluminum, and 20% is iron.

Lead is found in a cathode ray tube (CRT) display, and on all of the printed circuit boards and most expansion cards. Mercury is located in the screen's fluorescent lamp, in the laser light generators in the optical disk drive, and in the round, silver-looking batteries on the motherboard. Plastic is found mostly in the housing of the computation and display circuitry.

While daily end-users are not exposed to these toxic elements, the danger arises during the computer recycling process, which involves manually breaking down hardware and leads to the exposure of a measurable amount of lead or mercury. A measurable amount of lead or mercury can easily cause serious brain damage or ruin drinking water supplies. Computer recycling is best handled by the electronic waste (e-waste) industry, and kept segregated from the general community dump.

Electronic waste regulation[edit]
Main article: Computer recycling
Personal computers have become a large contributor to the 50 million tons of discarded electronic waste that is being generated annually, according to the United Nations Environment Programme. To address the electronic waste issue affecting developing countries and the environment, extended producer responsibility (EPR) acts have been implemented in various countries and states.[78]

Organizations, such as the Silicon Valley Toxics Coalition, Basel Action Network, Toxics Link India, SCOPE, and Greenpeace have contributed to these efforts. In the absence of comprehensive national legislation or regulation on the export and import of electronic waste, the Silicon Valley Toxics Coalition and BAN (Basel Action Network) teamed up with 32 electronic recyclers in the US and Canada to create an e-steward program for the orderly disposal of manufacturers and customers electronic waste. The Silicon Valley Toxics Coalition founded the Electronics TakeBack Coalition, a coalition that advocates for the production of environmentally friendly products. The TakeBack Coalition works with policy makers, recyclers, and smart businesses to get manufacturers to take full responsibility of their products.

There are organizations opposing EPR regulation, such as the Reason Foundation. They see flaws in two principal tenants of EPR: First EPR relies on the idea that if the manufacturers have to pay for environmental harm, they will adapt their practices. Second EPR assumes the current design practices are environmentally inefficient. The Reason Foundation claims that manufacturers naturally move toward reduced material and energy use.

See also[edit]
Portal icon Computer Science portal
Portal icon Electronics portal
Computer case
Computer virus
Desktop computer
Desktop replacement computer
e-waste
IBM 5100
Information and communication technologies for development
Laptop
List of computer system manufacturers
Market share of personal computer vendors
Personal Computer Museum
Portable computer
Public computer
Quiet PC
PC game
Notes[edit]
Jump up ^ The NeXT computer introduced in 1988 did not include a floppy drive, which at the time was unusual.
References[edit]
Jump up ^ Conlon, Tom (January 29, 2010), The iPad s Closed System: Sometimes I Hate Being Right, Popular Science, retrieved 2010-10-14, The iPad is not a personal computer in the sense that we currently understand.
Jump up ^ "Steve Jobs Offers World 'Freedom From Porn'", Gawker, May 15, 2010, retrieved 2010-10-14
^ Jump up to: a b "The incredible story of the first PC, from 1965". Pingdom. Retrieved August 28, 2012.
Jump up ^ Pospelov, Dmitry.               -                         [MIR series of computers. The first personal computers]. Glushkov Foundation (in Russian). Institute of Applied Informatics. Retrieved November 19, 2012.
^ Jump up to: a b c IBM Archives
Jump up ^ PC Magazine, Vol. 2, No. 6, November 1983, ‘ SCAMP: The Missing Link in the PC's Past ‘ 
Jump up ^ Jim Battle (August 9, 2008). "The Wang 2200". Wang2200.org. Jim Battle. Retrieved November 13, 2013.
Jump up ^ What's New (February 1978), "Commodore Ships First PET Computers", BYTE (Byte Publications) 3 (2): 190 Commodore press release. "The PET computer made its debut recently as the first 100 units were shipped to waiting customers in mid-October 1977."
Jump up ^ "Apple II History". Apple II History. Retrieved May 8, 2014.
Jump up ^ "Sinclair Research website". Retrieved 2014-08-06.
Jump up ^ Reimer, Jeremy (November 2, 2009). "Personal Computer Market Share: 1975 2004". Retrieved 2009-07-17.
Jump up ^ Reimer, Jeremy (December 2, 2012). "Personal Computer Market Share: 1975 2004". Retrieved 2013-02-09.
Jump up ^ "Computing Japan". Computing Japan (LINC Japan). 54-59: 18. 1999. Retrieved February 6, 2012. ...its venerable PC 9800 series, which has sold more than 18 million units over the years, and is the reason why NEC has been the number one PC vendor in Japan for as long as anyone can remember.
Jump up ^ Polsson, Ken. "Chronology of Amiga Computers". Retrieved May 9, 2014.
Jump up ^ Angler, Martin. "Obituary: The PC is Dead". JACKED IN. Retrieved February 12, 2014.
Jump up ^ Kanellos, Michael. "PCs: More than 1 billion served". CNET News. Retrieved August 9, 2001.
Jump up ^ Kanellos, Michael (June 30, 2002). "personal computers: More than 1 billion served". CNET News. Retrieved 2010-10-14.
Jump up ^ "Computers reach one billion mark". BBC News. July 1, 2002. Retrieved 2010-10-14.
Jump up ^ Global PC shipments grew 13.8 percent in 2010   Gartner study, Jan 13, 2011, retrieved at September 12, 2011
Jump up ^ Laptop Sales Soaring Amid Wider PC Growth: Gartner, May 27, 2010, Andy Patrizio, earthweb.com, retrieved at September 12, 2011
Jump up ^ Worldwide PC Shipments in 2008, March 16, 2009, ZDNet, retrieved at September 12, 2011
Jump up ^ PC Sales Up for 2008, but Barely, January 14, 2009, Andy Patrizio, internetnews.com, retrieved at September 12, 2011
Jump up ^ ISuppli Raises 2007 Computer Sales Forecast, pcworld.com, retrieved January 13, 2009
Jump up ^ iSuppli raises 2007 computer sales forecast, Macworld UK, retrieved January 13, 2009
Jump up ^ Global PC Sales Leveling Off, newsfactor.com, retrieved January 13, 2009
^ Jump up to: a b c HP back on top of PC market, retrieved January 13, 2009
Jump up ^ Yates, Nona (January 24, 2000). "Dell Passes Compaq as Top PC Seller in U.S". Los Angeles Times. Retrieved January 13, 2009.
Jump up ^ Economic recovery bumps AP 1999 PC shipments to record high, zdnetasia.com, retrieved January 13, 2009
Jump up ^ "Worldwide PC use to reach 1 billion by 2008: report". CBC News. Retrieved June 12, 2007.
Jump up ^ "Gartner Says More than 1 Billion PCs In Use Worldwide and Headed to 2 Billion Units by 2014" (Press release). Gartner. June 23, 2008. Retrieved 2010-10-14.
Jump up ^ Tarmo Virki (June 23, 2008). "Computers in use pass 1 billion mark: Gartner". Reuters. Retrieved 2010-10-14.
Jump up ^ "China hits tech milestone: PC shipments pass US". August 23, 2011.
Jump up ^ "4P Computing   Negroponte's 14 Million Laptop Impact". OLPC News. December 11, 2008. Retrieved 2010-10-14.
Jump up ^ Conrad H. Blickenstorfer. "Rugged PC leaders". Ruggedpcreview.com. Retrieved 2010-10-14.
Jump up ^ "PC Rebound in Mature Regions Stabilizes Market, But Falls Short of Overall Growth in the Second Quarter of 2014". International Data Corporation.
Jump up ^ "After Two Years of Decline, Worldwide PC Shipments Experienced Flat Growth in Second Quarter of 2014". Gartner.
Jump up ^ Tablets, smartphones to outsell PCs http://news.yahoo.com/s/afp/20110210/tc_afp/itinternettelecomequipmentmobileconsumerproduct
Jump up ^ "Gartner Says Declining Worldwide PC Shipments in Fourth Quarter of 2012 Signal Structural Shift of PC Market". Gartner.Com (Press release). January 14, 2013. Retrieved January 18, 2013.
^ Jump up to: a b c d "Feeble PC industry stumbles to steep sales drop during 1st quarter as Windows makeover flops". Washington Times. Associated Press. April 10, 2013. Archived from the original on April 19, 2013. Retrieved April 11, 2013.
^ Jump up to: a b Nick Wingfield (April 10, 2013). "PC Sales Still in a Slump, Despite New Offerings". New York Times. Retrieved April 11, 2013.
Jump up ^ "Steve Ballmer's retirement leaves Microsoft in a replacement crisis". August 24, 2013.
Jump up ^ "The Apple Vs. Samsung Title Fight for Mobile Supremacy". The Financialist. Credit Suisse. August 8, 2013. Retrieved August 13, 2013.
Jump up ^ John Fingas (March 4, 2014). "PC shipments faced their steepest-ever drop in 2013".
Jump up ^ Marvin B. Sussman Personal Computers and the Family Routledge, 1985 ISBN 0-86656-361-X, page 90
Jump up ^ Kateri M. Drexler Icons of business: an encyclopedia of mavericks, movers, and shakers, Volume 1, Greenwood Publishing Group, 2007 ISBN 0-313-33863-9 page 102
Jump up ^ Nancy Weil, Average PC Price drops below  1000, PC World December 1998, Retrieved November 17, 2010
Jump up ^ Joe Wilcox (April 16, 2009). "Netbooks Are Destroying the Laptop Market and Microsoft Needs to Act Now". eWeek. Retrieved 2010-10-14.
Jump up ^ Shane O'Neill (December 2, 2009). "Falling PC Prices Pit Microsoft Against PC Makers". Retrieved 2010-10-14.
Jump up ^ "Mac* vs. PC Debate". intel.com. Intel. Retrieved 5 October 2014.
Jump up ^ Finnie, Scot (8 June 2007). "Mac vs. PC cost analysis: How does it all add up ". Computerworld. Computerworld, Inc. Retrieved 5 October 2014.
Jump up ^ Ackerman, Dan (22 August 2013). "Don't buy a new PC or Mac before you read this". CNET. CBS Interactive. Retrieved 5 October 2014.
Jump up ^ Haslam, Karen (11 December 2013). "Mac or PC  Ten reasons why Macs are better than PCs". Macworld. IDG. Retrieved 5 October 2014.
Jump up ^ Hoffman, Chris. "Macs Are PCs! Can We Stop Pretending They Aren t ". How-To Geek. Retrieved 15 October 2015.
Jump up ^ Ralston, Anthony; Reilly, Edwin (1993). "Workstation". Encyclopedia of Computer Science (Third ed.). New York: Van Nostrand Reinhold. ISBN 0-442-27679-6.
Jump up ^ Houghton, Andy. "Evolution of Custom Gaming PCs: What Really Made the Difference". digitalstorm.com. Retrieved 28 September 2015.
Jump up ^ Desktop notebooks stake their claim, accessed October 19, 2007
^ Jump up to: a b c d Erica Ogg (August 20, 2009). "Time to drop the Netbook label". CNN.
Jump up ^ Walt Mossberg (August 6, 2009). "New Netbook Offers Long Battery Life and Room to Type". The Wall Street Journal Online, Personal Technology.
Jump up ^ "Cheap PCs Weigh on Microsoft". Business Technologies, The Wall Street Journal. December 8, 2008.
Jump up ^ "UMID Netbook Only 4.8". Elitezoom.com. Retrieved 2010-10-14.
Jump up ^ "CES 2009   MSI Unveils the X320 "MacBook Air Clone" Netbook". Futurelooks.com. 2009-01-07. Retrieved 2010-10-14.
Jump up ^ Netbook Trends and Solid-State Technology Forecast (PDF). pricegrabber.com. p. 7. Retrieved 2009-01-28.
Jump up ^ Light and Cheap, Netbooks Are Poised to Reshape PC Industry, The New York Times, April 1, 2009, retrieved 2010-10-14, AT&T announced on Tuesday that customers in Atlanta could get a type of compact PC called a netbook for just 50 US  if they signed up for an Internet service plan... 'The era of a perfect Internet computer for 99 US  is coming this year,' said Jen-Hsun Huang, the chief executive of Nvidia, a maker of PC graphics chips that is trying to adapt to the new technological order.
Jump up ^ "Tablet PC Redux ". Paul Thurrott's Supersite for Windows. Retrieved October 6, 2013.
Jump up ^ New Windows Mobile 6 Devices :: Jun/Jul 2007 Archived March 4, 2008, at the Wayback Machine.
Jump up ^ Berkeley Lab. Integrated Safety Management: Ergonomics. Website. Retrieved July 9, 2008. Archived August 5, 2009, at the Wayback Machine.
Jump up ^ "Wordreference.com: WordNet 2.0". Princeton University, Princeton, NJ. Retrieved 2007-08-19.
Jump up ^ Tim Bajarin (October 29, 2015). "Microsoft Sees That Apple Has Been Right All Along". Re/code. Retrieved October 31, 2015. At the moment, it looks like Microsoft will only have three serious PC partners doing any volume   HP, Dell and Lenovo.
Jump up ^ "A history of Windows: Highlights from the first 25 years".
Jump up ^ Mary Bellis. "The Unusual History of Microsoft Windows". About.com. Retrieved 2010-10-14.
Jump up ^ "IDC: Consolidation to Windows won't happen". Linuxworld. Retrieved 2010-10-14.
Jump up ^ "Thirty Years of Mac: 1984   Macintosh". Apple. Retrieved May 8, 2014.
Jump up ^ "Linux Online Ä About the Linux Operating System". Linux.org. Retrieved 2007-07-06.
Jump up ^ Weeks, Alex (2004). "1.1". Linux System Administrator's Guide (version 0.9 ed.). Retrieved 2007-01-18.
Jump up ^ Lyons, Daniel (March 15, 2005). "Linux rules supercomputers". Forbes. Retrieved 2007-02-22.
Jump up ^ Patrick Seybold (March 28, 2010). "PS3 Firmware (v3.21) Update". PlayStation.Blog. Retrieved March 29, 2010.
Jump up ^ Mark Serrels. "PC Gaming Revenue Has Now Overtaken Console Gaming". kotaku.com.au. Retrieved June 8, 2015.
Jump up ^ Nash, Jennifer; Bosso, Christopher (2013). "Extended Producer Responsibility in the United States: Full Speed Ahead " (PDF). Journal of Industrial Ecology 17 (2   RPP-2013-04): 175 185. doi:10.1111/j.1530-9290.2012.00572.x. Retrieved August 23, 2014.
Intel
From Wikipedia, the free encyclopedia
This article is about the company. For the information gathering term, see Intelligence assessment.
Coordinates: 37 23 16.54 N 121 57 48.74 W

Intel Corporation
Intel-logo.svg
The current logo, used since December 30, 2005
Intelheadquarters.jpg
Headquarters in Santa Clara, California
Type
Public company
Traded as NASDAQ: INTC
Dow Jones Industrial Average Component
NASDAQ-100 Component
S&P 500 Component
Industry  Semiconductors
Founded July 18, 1968; 47 years ago
Founders  Gordon Moore
Robert Noyce
Headquarters  Santa Clara, California, United States
Area served
Worldwide
Key people
Andy Bryant
(Chairman)
Brian Krzanich
(CEO)
Products  Bluetooth chipsets
flash memory
microprocessors
motherboard chipsets
Network interface controllers
mobile phones
solid state drives
central processing units
Revenue Decrease US 55.4 billion (2015)[1]
Operating income
Decrease US 14.0 billion (2015)[1]
Net income
Decrease US 11.4 billion (2015)[1]
Total assets  Decrease US 103.065 billion (2015)[1]
Total equity  Decrease US 61.085 billion (2015)[1]
Number of employees
107,300, 51% located in the U.S. (2015)[1]
Slogan  Experience What's Inside.
Website www.intel.com
Intel Corporation (better known as Intel, stylized as intel) is an American multinational technology company headquartered in Santa Clara, California. Intel is one of the world's largest and highest valued semiconductor chip makers, based on revenue.[2] It is the inventor of the x86 series of microprocessors, the processors found in most personal computers. Intel supplies processors for computer system manufacturers such as Apple, Samsung, HP and Dell. Intel also makes motherboard chipsets, network interface controllers and integrated circuits, flash memory, graphics chips, embedded processors and other devices related to communications and computing.

Intel Corporation was founded on July 18, 1968 by semiconductor pioneers Robert Noyce and Gordon Moore and widely associated with the executive leadership and vision of Andrew Grove, Intel combines advanced chip design capability with a leading-edge manufacturing capability.

Intel was an early developer of SRAM and DRAM memory chips, which represented the majority of its business until 1981. Although Intel created the world's first commercial microprocessor chip in 1971, it was not until the success of the personal computer (PC) that this became its primary business. During the 1990s, Intel invested heavily in new microprocessor designs fostering the rapid growth of the computer industry. During this period Intel became the dominant supplier of microprocessors for PCs, and was known for aggressive and anti-competitive tactics in defense of its market position, particularly against Advanced Micro Devices (AMD), as well as a struggle with Microsoft for control over the direction of the PC industry.[3][4]

Intel was ranked #56 on the 2015 rankings of the world's most valuable brands published by Millward Brown Optimor.[5]

The Open Source Technology Center at Intel hosts PowerTOP and LatencyTOP, and supports other open-source projects such as Wayland, Intel Array Building Blocks, Threading Building Blocks (TBB), and Xen.[6][7]

The name "Intel" was conceived as portmanteau of the words integrated and electronics. The fact that "intel" is the term for intelligence information also made the name appropriate.[8]

Contents  [hide] 
1 Current Operations
1.1 Operating segments
1.2 Top customers
1.3 Market share
1.3.1 Market share in early 2011
1.3.2 Historical market share
1.3.3 Major competitors
2 Corporate history
2.1 Origins
2.2 Early history
2.3 Slowing demand and challenges to dominance in 2000
2.4 Litigation issues
2.5 Regaining of momentum (2005-2007)
2.6 Sale of XScale processor business (2006)
2.7 Acquisitions (2010-2015)
3 Acquisition table (2010-2016)
3.1 Expansions (2008-2011)
3.2 Opening up the foundries to other manufacturers (2013)
4 Product and market history
4.1 SRAMS and the microprocessor
4.2 From DRAM to microprocessors
4.3 Intel, x86 processors, and the IBM PC
4.3.1 386 microprocessor
4.3.2 486, Pentium, and Itanium
4.3.3 Pentium flaw
4.3.4 "Intel Inside" and other 1990s programs
4.4 Solid-state drives (SSD)
4.5 Supercomputers
4.6 Competition, antitrust and espionage
4.7 Use of Intel products by Apple Computer (2005-present)
4.8 Core 2 Duo advertisement controversy (2007)
4.9 Introduction of Classmate PC (2011)
4.10  Introduction of new mobile processor technology (2011)
4.11  Update to server chips (2011)
4.12  Introduction of Ivy Bridge 22 nm processors (2011)
4.13  Development of Personal Office Energy Monitor (POEM) (2011)
4.14  IT Manager 3: Unseen Forces
4.15  Car Security System (2011)
4.16  High-Bandwidth Digital Content Protection
4.17  Move from Wintel desktop to open mobile platforms (2013-2014)
4.18  Introduction of Haswell processors (2013)
4.19  Wearable fashion (2014)
5 Corporate affairs
5.1 Leadership and corporate structure
5.2 Employment
5.2.1 Diversity
5.3 Economic impact in Oregon in 2009
5.4 School funding in New Mexico in 1997
5.5 Ultrabook fund (2011)
5.6 Advertising and brand management
5.6.1 Intel Inside
5.6.2 Sonic logo
5.6.3 Processor naming strategy
5.7 Open source support
5.8 PC declining sales
6 Controversies
6.1 Patent infringement litigation (2006-2007)
6.2 Anti-competitive allegations and litigation (2005-2009)
6.2.1 Allegations by Japan Fair Trade Commission (2005)
6.2.2 Allegations by the European Union (2007-2008)
6.2.3 Allegations by regulators in South Korea (2007)
6.2.4 Allegations by regulators in the United States (2008-2010)
6.3 Corporate responsibility record
6.4 Religious controversy in Israel (2009)
6.5 Age discrimination complaints
6.6 Land dispute in Israel
7 See also
8 References
9 External links
Current Operations[edit]
Operating segments[edit]
Client Computing Group - 58% of 2015 revenues - produces hardware components used in desktop and notebook computers.[1]
Data Center Group - 29% of 2015 revenues - produces hardware components used in server, network, and storage platforms.[1]
Internet of Things Group - 4% of 2015 revenues - offers platforms designed for retail, transportation, industrial, buildings and home use.[1]
Software and Services - 4% of 2015 revenues - produces software, particularly security and antivirus software.[1]
Other - 5% of 2015 revenues - primarily the Non-Volatile Memory Solutions Group and the New Devices Group.[1]
Top customers[edit]
In 2015, HP Inc. and HP Enterprise collectively accounted for 18% of the company's revenue, Dell accounted for 15% of the company's revenue, and Lenovo accounted for 13% of the company's revenue.[1]

Market share[edit]
Further information: Semiconductor sales leaders by year
Market share in early 2011[edit]
According to IDC, while Intel enjoyed the biggest market share in both the overall worldwide PC microprocessor market (79.3%) and the mobile PC microprocessor (84.4%) in the second quarter of 2011, the numbers decreased by 1.5% and 1.9% compared to the first quarter of 2011.[9][10]

Historical market share[edit]
In the 1980s, Intel was among the top ten sellers of semiconductors (10th in 1987) in the world. In 1991, Intel became the biggest chip maker by revenue and has held the position ever since. Other top semiconductor companies include TSMC, Advanced Micro Devices, Samsung, Texas Instruments, Toshiba and STMicroelectronics.

Major competitors[edit]
Competitors in PC chip sets include Advanced Micro Devices, VIA Technologies, Silicon Integrated Systems, and Nvidia. Intel's competitors in networking include NXP Semiconductors, Infineon, Broadcom Limited, Marvell Technology Group and Applied Micro Circuits Corporation, and competitors in flash memory include Spansion, Samsung, Qimonda, Toshiba, STMicroelectronics, and SK Hynix.

The only major competitor in the x86 processor market is Advanced Micro Devices (AMD), with which Intel has had full cross-licensing agreements since 1976: each partner can use the other's patented technological innovations without charge after a certain time.[11] However, the cross-licensing agreement is canceled in the event of an AMD bankruptcy or takeover.[12]

Some smaller competitors such as VIA Technologies produce low-power x86 processors for small factor computers and portable equipment. However, the advent of such mobile computing devices, in particular, smartphones, has in recent years led to a decline in PC sales.[13] Since over 95% of the world's smartphones are currently powered by processors designed by ARM Holdings, ARM has become a major competitor for Intel's processor market. ARM is also planning to make inroads into the PC and server market.[14]

Intel has been involved in several disputes regarding violation of antitrust laws, which are noted below.

Corporate history[edit]
Further information: Timeline of Intel
Origins[edit]

Andy Grove, Robert Noyce and Gordon Moore (1978)

The old Intel logo used from July 18, 1968, until December 29, 2005
Intel was founded in Mountain View, California in 1968 by Gordon E. Moore (of "Moore's Law" fame), a chemist, and Robert Noyce, a physicist and co-inventor of the integrated circuit. Arthur Rock (investor and venture capitalist) helped them find investors, while Max Palevsky was on the board from an early stage.[15] Moore and Noyce had left Fairchild Semiconductor to found Intel. Rock was not an employee, but he was an investor and was chairman of the board.[16][17] The total initial investment in Intel was  2.5 million convertible debentures and  10,000 from Rock. Just 2 years later, Intel became a public company via an initial public offering (IPO), raising  6.8 million ( 23.50 per share).[16] Intel's third employee was Andy Grove,[18] a chemical engineer, who later ran the company through much of the 1980s and the high-growth 1990s.

In deciding on a name, Moore and Noyce quickly rejected "Moore Noyce",[19] homophone for "more noise"   an ill-suited name for an electronics company, since noise in electronics is usually very undesirable and typically associated with bad interference. Instead they used the name NM Electronics before renaming their company Integrated Electronics or "Intel" for short.[20] Since "Intel" was already trademarked by the hotel chain Intelco, they had to buy the rights for the name.[16][21]

Early history[edit]
At its founding, Intel was distinguished by its ability to make semiconductors. Its first product, in 1969, was the 3101 Schottky TTL bipolar 64-bit static random-access memory (SRAM), which was nearly twice as fast as earlier Schottky diode implementations by Fairchild and the Electrotechnical Laboratory in Tsukuba, Japan.[22][23] In the same year Intel also produced the 3301 Schottky bipolar 1024-bit read-only memory (ROM)[24] and the first commercial metal oxide semiconductor field-effect transistor (MOSFET) silicon gate SRAM chip, the 256-bit 1101.[16][25][26] Intel's business grew during the 1970s as it expanded and improved its manufacturing processes and produced a wider range of products, still dominated by various memory devices.


Federico Faggin, the designer of Intel 4004
While Intel created the first commercially available microprocessor (Intel 4004) in 1971[16] and one of the first microcomputers in 1972,[25][27] by the early 1980s its business was dominated by dynamic random-access memory chips. However, increased competition from Japanese semiconductor manufacturers had, by 1983, dramatically reduced the profitability of this market. The growing success of the IBM personal computer, based on an Intel microprocessor, was among factors that convinced Gordon Moore (CEO since 1975) to shift the company's focus to microprocessors, and to change fundamental aspects of that business model. Moore's decision to sole-source Intel's 386 chip played into the company's continuing success.

By the end of the 1980s, buoyed by its fortuitous position as microprocessor supplier to IBM and IBM's competitors within the rapidly growing personal computer market, Intel embarked on a 10-year period of unprecedented growth as the primary (and most profitable) hardware supplier to the PC industry, part of the winning 'Wintel' combination. Moore handed over to Andy Grove in 1987. By launching its Intel Inside marketing campaign in 1991, Intel was able to associate brand loyalty with consumer selection, so that by the end of the 1990s, its line of Pentium processors had become a household name.

Slowing demand and challenges to dominance in 2000[edit]
After 2000, growth in demand for high-end microprocessors slowed. Competitors, notably AMD (Intel's largest competitor in its primary x86 architecture market), garnered significant market share, initially in low-end and mid-range processors but ultimately across the product range, and Intel's dominant position in its core market was greatly reduced.[28] In the early 2000s then-CEO Craig Barrett attempted to diversify the company's business beyond semiconductors, but few of these activities were ultimately successful.

Litigation issues[edit]
Intel had also for a number of years been embroiled in litigation. US law did not initially recognize intellectual property rights related to microprocessor topology (circuit layouts), until the Semiconductor Chip Protection Act of 1984, a law sought by Intel and the Semiconductor Industry Association (SIA).[29] During the late 1980s and 1990s (after this law was passed), Intel also sued companies that tried to develop competitor chips to the 80386 CPU.[30] The lawsuits were noted to significantly burden the competition with legal bills, even if Intel lost the suits.[30] Antitrust allegations had been simmering since the early 1990s and had been the cause of one lawsuit against Intel in 1991. In 2004 and 2005, AMD brought further claims against Intel related to unfair competition.

Regaining of momentum (2005-2007)[edit]
In 2005, CEO Paul Otellini reorganized the company to refocus its core processor and chipset business on platforms (enterprise, digital home, digital health, and mobility).

In 2007, Intel unveiled its Core microarchitecture to widespread critical acclaim;[31] the product range was perceived as an exceptional leap in processor performance that at a stroke regained much of its leadership of the field.[32][33] In 2008, Intel had another "tick," when it introduced the Penryn microarchitecture, which was 45 nm. Later that year, Intel released a processor with the Nehalem architecture. Nehalem had positive reviews.[34]

Sale of XScale processor business (2006)[edit]
On June 27, 2006, the sale of Intel's XScale assets was announced. Intel agreed to sell the XScale processor business to Marvell Technology Group for an estimated  600 million and the assumption of unspecified liabilities. The move was intended to permit Intel to focus its resources on its core x86 and server businesses, and the acquisition completed on November 9, 2006.[35]

Acquisitions (2010-2015)[edit]
In 2010, Intel purchased McAfee, a manufacturer of computer security technology for  7.68 billion.[36] As a condition for regulatory approval of the transaction, Intel agreed to provide rival security firms with all necessary information that would allow their products to use Intel's chips and personal computers.[37] After the acquisition, Intel had about 90,000 employees, including about 12,000 software engineers.[38]

On August 30, 2010, Intel and Infineon Technologies announced that Intel would acquire Infineon's Wireless Solutions business.[39] Intel planned to use Infineon's technology in laptops, smart phones, netbooks, tablets and embedded computers in consumer products, eventually integrating its wireless modem into Intel's silicon chips.[40]

In March 2011, Intel bought most of the assets of Cairo-based SySDSoft.[41]

In July 2011, Intel announced that it had agreed to acquire Fulcrum Microsystems Inc., a company specializing in network switches.[42] The company was previously included on the EE Times list of 60 Emerging Startups.[42]

On October 1, 2011, Intel reached a deal to acquire Telmap, an Israeli-based navigation software company. The purchase price was not disclosed, but Israeli media reported values around  300 million to  350 million.[43]

In July 2012, Intel Corporation agreed to buy 10 percent shares of ASML Holding NV for  2.1 billion and another  1 billon for 5 percent shares that need shareholder approval to fund relevant research and development efforts, as part of a EUR3.3 billion ( 4.1 billion) deal to accelerate the development of 450-millimeter wafer technology and extreme ultra-violet lithography by as much as two years.[44]

In July 2013, Intel confirmed the acquisition of Omek Interactive, an Israeli company that makes technology for gesture-based interfaces, without disclosing the monetary value of the deal. An official statement from Intel read: "The acquisition of Omek Interactive will help increase Intel's capabilities in the delivery of more immersive perceptual computing experiences." One report estimated the value of the acquisition between US 30 million and  50 million.[45]

The acquisition of a Spanish natural language recognition startup named Indisys was announced on September 13, 2013. The terms of the deal were not disclosed but an email from an Intel representative stated: "Intel has acquired Indisys, a privately held company based in Seville, Spain. The majority of Indisys employees joined Intel. We signed the agreement to acquire the company on May 31 and the deal has been completed." Indysis explains that its artificial intelligence (AI) technology "is a human image, which converses fluently and with common sense in multiple languages and also works in different platforms."[46]

In December 2014, Intel bought PasswordBox.[47]

In January 2015, Intel purchased a 30% stake in Vuzix, a smart glasses manufacturer. The deal was worth  24.8 million.[48]

In February 2015, Intel announced its agreement to purchase German network chipmaker Lantiq, to aid in its expansion of its range of chips in devices with Internet connection capability.[49]

In June 2015, Intel announced its agreement to purchase FPGA design company Altera for  16.7 billion, in its largest acquisition to date.[50] The acquisition completed in December 2015.[51]

In October 2015, Intel bought cognitive computing company Saffron Technology for an undisclosed price.[52]

Acquisition table (2010-2016)[edit]
Number  Acquisition announcement date Company Business  Country Price Used as or integrated with  Ref(s).
1 June 4, 2009  Wind River Systems  Embedded Systems   US  884M Software  [53]
2 August 19, 2010 McAfee  Security   US  7.6B Software  [54]
3 August 30, 2010 Infineon (partial)  Wireless   Germany   1.4B Mobile CPUs [55]
4 March 17, 2011  Silicon Hive  DSP  Netherlands  N/A Mobile CPUs [56]
5 September 29, 2011  Telmap  Software   Israel N/A Location Services [57]
6 April 13, 2013  Mashery Cloud Software   US  180M Software  [58]
7 May 3, 2013 Aepona  SDN  Ireland  N/A Software  [59]
8 May 6, 2013 Stonesoft Corporation Security   Finland   389M Software  [60]
9 July 16, 2013 Omek Interactive  Gesture  Israel N/A Software  [45]
10  September 13, 2013  Indisys Natural language processing  Spain  N/A Software  [46]
11  March 25, 2014  BASIS Wearable   US N/A New Devices [61]
12  August 13, 2014 Avago Technologies (partial)  Semiconductor  US  650M Communications Processors [62]
13  December 1, 2014  PasswordBox Security   Canada N/A Software  [63]
14  January 5, 2015 Vuzix Wearable   US  24.8M  New Devices [64]
15  February 2, 2015  Lantiq  Telecom  Germany  undisclosed Gateways  [65]
16  June 1, 2015  Altera  Semiconductor  US  16.7B  FPGA  [50]
17  June 18, 2015 Recon Wearable   US  175M New Devices [66]
18  October 26, 2015  Saffron Technology  Cognitive computing  US undisclosed Software  [52]
19  January 4, 2016 Ascending Technologies  UAVs   DE undisclosed New Technology  [67]
20  March 9, 2016 Replay Technologies Video technology   Israel undisclosed 3D video technology [68]
21  April 5, 2016 Yogitech  IoT security and Advanced Driver Assistance Systems.   Italy  undisclosed Software  [69]
Expansions (2008-2011)[edit]
In 2008, Intel spun off key assets of a solar startup business effort to form an independent company, SpectraWatt Inc. In 2011, SpectraWatt filed for bankruptcy.[70]

In February 2011, Intel began to build a new microprocessor manufacturing facility in Chandler, Arizona, completed in 2013 at a cost of  5 billion.[71] The building was never used.[72] The company produces three-quarters of its products in the United States, although three-quarters of its revenue comes from overseas.[73]

In April 2011, Intel began a pilot project with ZTE Corporation to produce smartphones using the Intel Atom processor for China's domestic market.

In December 2011, Intel announced that it reorganized several of its business units into a new mobile and communications group[74] be responsible for the company's smartphone, tablet and wireless efforts.

Opening up the foundries to other manufacturers (2013)[edit]
Finding itself with excess fab capacity after the failure of the Ultrabook to gain market traction and with PC sales declining, in 2013 Intel reached a foundry agreement to produce chips for Altera using 14-nm process. General Manager of Intel's custom foundry division Sunit Rikhi indicated that Intel would pursue further such deals in the future.[75] This was after poor sales of Windows 8 hardware caused a major retrenchment for most of the major semiconductor manufacturers, except for Qualcomm, which continued to see healthy purchases from its largest customer, Apple.[76]

As of July 2013, five companies were using Intel's fabs via the Intel Custom Foundry division: Achronix, Tabula, Netronome, Microsemi, and Panasonic most are Field-programmable gate array (FPGA) makers, but Netronome designs network processors. Only Achronix began shipping chips made by Intel using the 22-nm Tri-Gate process.[77][78] Several other customers also exist but were not announced at the time.[79]

The Alliance for Affordable Internet (A4AI) was launched in October 2013 and Intel is part of the coalition of public and private organisations that also includes Facebook, Google, and Microsoft. Led by Sir Tim Berners-Lee, the A4AI seeks to make Internet access more affordable so that access is broadened in the developing world, where only 31% of people are online. Google will help to decrease internet access prices so that they fall below the UN Broadband Commission's worldwide target of 5% of monthly income.[80]

Product and market history[edit]
SRAMS and the microprocessor[edit]
Intel's first products were shift register memory and random-access memory integrated circuits, and Intel grew to be a leader in the fiercely competitive DRAM, SRAM, and ROM markets throughout the 1970s. Concurrently, Intel engineers Marcian Hoff, Federico Faggin, Stanley Mazor and Masatoshi Shima invented Intel's first microprocessor. Originally developed for the Japanese company Busicom to replace a number of ASICs in a calculator already produced by Busicom, the Intel 4004 was introduced to the mass market on November 15, 1971, though the microprocessor did not become the core of Intel's business until the mid-1980s. (Note: Intel is usually given credit with Texas Instruments for the almost-simultaneous invention of the microprocessor)

From DRAM to microprocessors[edit]
In 1983, at the dawn of the personal computer era, Intel's profits came under increased pressure from Japanese memory-chip manufacturers, and then-president Andy Grove focused the company on microprocessors. Grove described this transition in the book Only the Paranoid Survive. A key element of his plan was the notion, then considered radical, of becoming the single source for successors to the popular 8086 microprocessor.

Until then, the manufacture of complex integrated circuits was not reliable enough for customers to depend on a single supplier,[clarification needed] but Grove began producing processors in three geographically distinct factories,[which ] and ceased licensing the chip designs to competitors such as Zilog and AMD.[citation needed] When the PC industry boomed in the late 1980s and 1990s, Intel was one of the primary beneficiaries.

Intel, x86 processors, and the IBM PC[edit]

The die from an Intel 8742, an 8-bit microcontroller that includes a CPU running at 12 MHz, 128 bytes of RAM, 2048 bytes of EPROM, and I/O in the same chip
Despite the ultimate importance of the microprocessor, the 4004 and its successors the 8008 and the 8080 were never major revenue contributors at Intel. As the next processor, the 8086 (and its variant the 8088) was completed in 1978, Intel embarked on a major marketing and sales campaign for that chip nicknamed "Operation Crush", and intended to win as many customers for the processor as possible. One design win was the newly created IBM PC division, though the importance of this was not fully realized at the time.

IBM introduced its personal computer in 1981, and it was rapidly successful. In 1982, Intel created the 80286 microprocessor, which, two years later, was used in the IBM PC/AT. Compaq, the first IBM PC "clone" manufacturer, produced a desktop system based on the faster 80286 processor in 1985 and in 1986 quickly followed with the first 80386-based system, beating IBM and establishing a competitive market for PC-compatible systems and setting up Intel as a key component supplier.

In 1975, the company had started a project to develop a highly advanced 32-bit microprocessor, finally released in 1981 as the Intel iAPX 432. The project was too ambitious and the processor was never able to meet its performance objectives, and it failed in the marketplace. Intel extended the x86 architecture to 32 bits instead.[81][82]

386 microprocessor[edit]
During this period Andrew Grove dramatically redirected the company, closing much of its DRAM business and directing resources to the microprocessor business. Of perhaps greater importance was his decision to "single-source" the 386 microprocessor. Prior to this, microprocessor manufacturing was in its infancy, and manufacturing problems frequently reduced or stopped production, interrupting supplies to customers. To mitigate this risk, these customers typically insisted that multiple manufacturers produce chips they could use to ensure a consistent supply. The 8080 and 8086-series microprocessors were produced by several companies, notably AMD. Grove made the decision not to license the 386 design to other manufacturers, instead producing it in three geographically distinct factories: Santa Clara, California; Hillsboro, Oregon; and Chandler, a suburb of Phoenix, Arizona. He convinced customers that this would ensure consistent delivery. As the success of Compaq's Deskpro 386 established the 386 as the dominant CPU choice, Intel achieved a position of near-exclusive dominance as its supplier. Profits from this funded rapid development of both higher-performance chip designs and higher-performance manufacturing capabilities, propelling Intel to a position of unquestioned leadership by the early 1990s.

486, Pentium, and Itanium[edit]
Intel introduced the 486 microprocessor in 1989, and in 1990 formally established a second design team, designing the processors code-named "P5" and "P6" in parallel and committing to a major new processor every two years, versus the four or more years such designs had previously taken. Engineers Vinod Dham and Rajeev Chandrasekhar (Member of Parliament, India) were key figures on the core team that invented the 486 chip and later, Intel's signature Pentium chip. The P5 was earlier known as "Operation Bicycle," referring to the cycles of the processor. The P5 was introduced in 1993 as the Intel Pentium, substituting a registered trademark name for the former part number (numbers, such as 486, are hard to register as a trademark). The P6 followed in 1995 as the Pentium Pro and improved into the Pentium II in 1997. New architectures were developed alternately in Santa Clara, California and Hillsboro, Oregon.

The Santa Clara design team embarked in 1993 on a successor to the x86 architecture, codenamed "P7". The first attempt was dropped a year later, but quickly revived in a cooperative program with Hewlett-Packard engineers, though Intel soon took over primary design responsibility. The resulting implementation of the IA-64 64-bit architecture was the Itanium, finally introduced in June 2001. The Itanium's performance running legacy x86 code did not meet expectations, and it failed to compete effectively with x86-64, which was AMD's 64-bit extensions to the original x86 architecture (Intel uses the name Intel 64, previously EM64T). As of 2012, Intel continues to develop and deploy the Itanium; known planning continues into 2014.

The Hillsboro team designed the Willamette processors (initially code-named P68), which were marketed as the Pentium 4.[citation needed]

Pentium flaw[edit]
Main article: Pentium FDIV bug
In June 1994, Intel engineers discovered a flaw in the floating-point math subsection of the P5 Pentium microprocessor. Under certain data-dependent conditions, the low-order bits of the result of a floating-point division would be incorrect. The error could compound in subsequent calculations. Intel corrected the error in a future chip revision.

In October 1994, Thomas Nicely, Professor of Mathematics at Lynchburg College, independently discovered the bug. He contacted Intel, but received no response. On October 30, he posted a message on the Internet.[83] Word of the bug spread quickly and reached the industry press. The bug was easy to replicate; a user could enter specific numbers into the calculator on the operating system. Consequently, many users did not accept Intel's statements that the error was minor and "not even an erratum." During Thanksgiving, in 1994, The New York Times ran a piece by journalist John Markoff spotlighting the error. Intel changed its position and offered to replace every chip, quickly putting in place a large end-user support organization. This resulted in a  475 million charge against Intel's 1994 revenue.[84]

The "Pentium flaw" incident, Intel's response to it, and the surrounding media coverage propelled Intel from being a technology supplier generally unknown to most computer users to a household name. Dovetailing with an uptick in the "Intel Inside" campaign, the episode is considered to have been a positive event for Intel, changing some of its business practices to be more end-user focused and generating substantial public awareness, while avoiding a lasting negative impression.[85]

"Intel Inside" and other 1990s programs[edit]
During this period, Intel undertook two major supporting programs. The first is widely known: the 1991 "Intel Inside" marketing and branding campaign. The idea of ingredient branding was new at the time with only Nutrasweet and a few others making attempts to do so.[86] This campaign established Intel, which had been a component supplier little-known outside the PC industry, as a household name.

The second program is little-known: Intel's Systems Group began, in the early 1990s, manufacturing PC "motherboards", the main board component of a personal computer, and the one into which the processor (CPU) and memory (RAM) chips are plugged.[87] Shortly after, Intel began manufacturing fully configured "white box" systems for the dozens of PC clone companies that rapidly sprang up.[citation needed] At its peak in the mid-1990s, Intel manufactured over 15% of all PCs, making it the third-largest supplier at the time.[citation needed]

During the 1990s, Intel Architecture Labs (IAL) was responsible for many of the hardware innovations of the personal computer, including the PCI Bus, the PCI Express (PCIe) bus, the Universal Serial Bus (USB). IAL's software efforts met with a more mixed fate; its video and graphics software was important in the development of software digital video,[citation needed] but later its efforts were largely overshadowed by competition from Microsoft. The competition between Intel and Microsoft was revealed in testimony by IAL Vice-President Steven McGeady at the Microsoft antitrust trial.


Solid-state drives (SSD)[edit]

It has been suggested that this section be split into an article titled Intel solid-state drives. (Discuss) (January 2014)

An Intel X25-M SSD
On September 8, 2008, Intel began shipping its first mainstream solid-state drives, the X18-M and X25-M with 80GB and 160GB storage capacities.[88] Reviews measured high performance with these MLC-based drives.[89][90][91][92] Intel released its SLC-based Enterprise X25-E Extreme SSDs on October 15 that same year in capacities of 32GB and 64GB.[93]

In July 2009, Intel refreshed its X25-M and X18-M lines by moving from a 50-nanometer to a 34-nanometer process. These new drives, dubbed by the press as the X25-M and X18-M G2[94][95] (or generation 2), reduced prices by up to 60 percent while offering lower latency and improved performance.[96]

On February 1, 2010, Intel and Micron announced that they were gearing up for production of NAND flash memory using a new 25-nanometer process.[97] In March of that same year, Intel entered the budget SSD segment with its X25-V drives with an initial capacity of 40GB.[98] The SSD 310, Intel's first mSATA drive was released on December 2010, providing X25-M G2 performance in a much smaller package.[99][100]

March 2011 saw the introduction of two new SSD lines from Intel. The first, the SSD 510, used a SATA 6 Gigabit per second interface to reach speeds of up to 500 MegaBytes per second.[101] The drive, which uses a controller from Marvell Technology Group,[102] was released using 34 nm NAND Flash and came in capacities of 120GB and 250GB. The second product announcement, the SSD 320, is the successor to Intel's earlier X25-M. It uses the new 25 nm process that Intel and Micron announced in 2010, and was released in capacities of 40 GB, 80 GB, 120 GB, 160 GB, 300 GB and 600 GB.[103] Sequential read performance maxes out at 270 MB/s due to the older SATA 3 Gbit/s interface, and sequential write performance varies greatly based on the size of the drive with sequential write performance of the 40 GB model peaking at 45 MB/s and the 600 GB at 220 MB/s.[104]

Micron and Intel announced that they were producing their first 20 nm MLC NAND flash on April 14, 2011.[105]

In February 2012, Intel launched the SSD 520 series solid state drives using the SandForce SF-2200 controller with sequential read and write speeds of 550 and 520 MB/s respectively with random read and write IOPS as high as 80,000. These drives will replace the 510 series.[106] Intel has released the budget 330 series solid state drive in 60, 120, and 180GB capacities using 25 nm flash memory and a SandForce controller that have replaced the 320 series.[107][108]
Introduction of Haswell processors (2013)[edit]
In June 2013, Intel unveiled its fourth generation of Intel Core processors (Haswell) in an event named Computex in Taipei.[177]

Wearable fashion (2014)[edit]
On January 6, 2014, Intel announced that it was "teaming with the Council of Fashion Designers of America, Barneys New York and Opening Ceremony around the wearable tech field."[178]

Intel has developed a reference design for wearable smart earbuds that provide biometric and fitness information. The Intel smart earbuds provide full stereo audio, and monitor heart rate, while the applications on the user s phone keep track of run distance and calories burned.

Corporate affairs[edit]
Leadership and corporate structure[edit]

Paul Otellini, Craig Barrett and Sean Maloney (2006)
Robert Noyce was Intel's CEO at its founding in 1968, followed by co-founder Gordon Moore in 1975. Andy Grove became the company's president in 1979 and added the CEO title in 1987 when Moore became chairman. In 1998, Grove succeeded Moore as Chairman, and Craig Barrett, already company president, took over. On May 18, 2005, Barrett handed the reins of the company over to Paul Otellini, who previously was the company president and COO and who was responsible for Intel's design win in the original IBM PC. The board of directors elected Otellini as President and CEO, and Barrett replaced Grove as Chairman of the Board. Grove stepped down as chairman, but is retained as a special adviser. In May 2009, Barrett stepped down as chairman of the Board and was succeeded by Jane Shaw. In May 2012, Intel vice chairman Andy Bryant, who had previously held the posts of CFO (1994) and Chief Administrative Officer (2007) at Intel, succeeded Shaw as executive chairman.[179]

In November 2012, president and CEO Paul Otellini announced that he would step down in May 2013 at the age of 62, three years before the company's mandatory retirement age. During a six-month transition period, Intel's board of directors commenced a search process for the next CEO, in which it considered both internal managers and external candidates such as Sanjay Jha and Patrick Gelsinger.[180] Financial results revealed that, under Otellini, Intel's revenue increased by 55.8 percent (US 34.2 to 53.3 billion), while its net income increased by 46.7% (US 7.5 billion to 11 billion).[181]

On May 2, 2013, Executive Vice President and COO Brian Krzanich was elected as Intel's sixth CEO,[182] a selection that became effective on May 16, 2013 at the company's annual meeting. Reportedly, the board concluded that an insider could proceed with the role and exert an impact more quickly, without the need to learn Intel's processes, and Krzanich was selected on such a basis.[183] Intel's software head Ren e James was selected as president of the company, a role that is second to the CEO position.[184]

As of May 2013, Intel's board of directors consists of Andy Bryant, John Donahoe, Frank Yeary, Ambassador Charlene Barshefsky, Susan Decker, Reed Hundt, Paul Otellini, James Plummer, David Pottruck, and David Yoffie. The board was described by former Financial Times journalist Tom Foremski as "an exemplary example of corporate governance of the highest order" and received a rating of ten from GovernanceMetrics International, a form of recognition that has only been awarded to twenty-one other corporate boards worldwide.[185]

Employment[edit]

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2008)

Intel microprocessor facility in Costa Rica was responsible in 2006 for 20% of Costa Rican exports and 4.9% of the country's GDP.[186]
The firm promotes very heavily from within, most notably in its executive suite. The company has resisted the trend toward outsider CEOs. Paul Otellini was a 30-year veteran of the company when he assumed the role of CEO. All of his top lieutenants have risen through the ranks after many years with the firm. In many cases, Intel's top executives have spent their entire working careers with Intel.[citation needed]

Intel has a mandatory retirement policy for its CEOs when they reach age 65. Andy Grove retired at 62, while both Robert Noyce and Gordon Moore retired at 58. Grove retired as Chairman and as a member of the board of directors in 2005 at age 68.

Intel's headquarters are located in Santa Clara, California, and the company has operations around the world. Its largest workforce concentration anywhere is in Washington County, Oregon[187] (in the Portland metropolitan area's "Silicon Forest"), with 18,600 employees at several facilities.[188] Outside the United States, the company has facilities in China, Costa Rica, Malaysia, Israel, Ireland, India, Russia, Argentina and Vietnam, in 63 countries and regions internationally. In the U.S. Intel employs significant numbers of people in California, Colorado, Massachusetts, Arizona, New Mexico, Oregon, Texas, Washington and Utah. In Oregon, Intel is the state's largest private employer.[188][189] The company is the largest industrial employer in New Mexico while in Arizona the company has over 10,000 employees.[citation needed]

Intel invests heavily in research in China and about 100 researchers   or 10% of the total number of researchers from Intel   are located in Beijing.[190]

In 2011, the Israeli government offered Intel  290 million to expand in the country. As a condition, Intel will have to employ 1,500 more workers in Kiryat Gat and between 600 1000 workers in the north.[191]

In January 2014, it was reported that Intel would cut about 5,000 jobs from its work force of 107,000. The announcement was made a day after it reported earnings that missed analyst targets.[192]

In March 2014, it was reported that Intel would embark upon a  6 billion plan to expand its activities in Israel. The plan calls for continued investment in existing and new Intel plants until 2030. As of 2014 Intel employs 10,000 workers at four development centers and two production plants in Israel.[193]

Diversity[edit]
Intel has a Diversity Initiative, including employee diversity groups as well as supplier diversity programs.[194] Like many companies with employee diversity groups, they include groups based on race and nationality as well as sexual identity and religion. In 1994, Intel sanctioned one of the earliest corporate Gay, Lesbian, Bisexual, and Transgender employee groups,[195] and supports a Muslim employees group,[196] a Jewish employees group,[197] and a Bible-based Christian group.[198][199]

Intel received a 100% rating on the first Corporate Equality Index released by the Human Rights Campaign in 2002. It has maintained this rating in 2003 and 2004. In addition, the company was named one of the 100 Best Companies for Working Mothers in 2005 by Working Mother magazine.[citation needed]

In January 2015, Intel announced the investment of  300 million over the next five years to enhance gender and racial diversity in their own company as well as the technology industry as a whole.[200][201][202][203][204]

In February 2016, Intel released its Global Diversity & Inclusion 2015 Annual Report.[205] The male-female mix of US employees was reported as 75.2% men and 24.8% women. For US employees in technical roles, the mix was reported as 79.8% male and 20.1% female.[205] NPR reports that Intel is facing a retention problem (particularly for African Americans), not just a pipeline problem.[206]

Economic impact in Oregon in 2009[edit]
In 2011, ECONorthwest conducted an economic impact analysis of Intel's economic contribution to the state of Oregon. The report found that in 2009 "the total economic impacts attributed to Intel's operations, capital spending, contributions and taxes amounted to almost  14.6 billion in activity, including  4.3 billion in personal income and 59,990 jobs."[207] Through multiplier effects, every 10 Intel jobs supported, on average, was found to create 31 jobs in other sectors of the economy.[208]

School funding in New Mexico in 1997[edit]
In Rio Rancho, New Mexico, Intel is the leading employer.[209] In 1997, a community partnership between Sandoval County and Intel Corporation funded and built Rio Rancho High School.[210][211]

Ultrabook fund (2011)[edit]
In 2011, Intel Capital announced a new fund to support startups working on technologies in line with the company's concept for next generation notebooks.[212] The company is setting aside a  300 million fund to be spent over the next three to four years in areas related to ultrabooks.[212] Intel announced the ultrabook concept at Computex in 2011. The ultrabook is defined as a thin (less than 0.8 inches [~2 cm] thick[213]) notebook that utilizes Intel processors[213] and also incorporates tablet features such as a touch screen and long battery life.[212][213]

At the Intel Developers Forum in 2011, four Taiwan ODMs showed prototype ultrabooks that used Intel's Ivy Bridge chips.[214] Intel plans to improve power consumption of its chips for ultrabooks, like new Ivy Bridge processors in 2013, which will only have 10W default thermal design power.[215]

Intel's goal for Ultrabook's price is below  1000;[213] however, according to two presidents from Acer and Compaq, this goal will not be achieved if Intel does not lower the price of its chips.[216]

Advertising and brand management[edit]
Intel Inside[edit]
Intel has become one of the world's most recognizable computer brands following its long-running Intel Inside campaign. The idea for "Intel Inside" came out of a meeting between Intel and one of the major computer resellers, MicroAge.[217]

In the late 1980s, Intel's market share was being seriously eroded by upstart competitors such as American Micro Devices (now AMD), Zilog, and others who had started to sell their less expensive microprocessors to computer manufacturers. This was because, by using cheaper processors, manufacturers could make cheaper computers and gain more market share in an increasingly price-sensitive market. In 1989, Intel's Dennis Carter visited MicroAge's headquarters in Tempe, Arizona, to meet with MicroAge's VP of Marketing, Ron Mion. MicroAge had become one of the largest distributors of Compaq, IBM, HP, and others and thus was a primary   although indirect   driver of demand for microprocessors. Intel wanted MicroAge to petition its computer suppliers to favor Intel chips. However, Mion felt that the marketplace should decide which processors they wanted. Intel's counterargument was that it would be too difficult to educate PC buyers on why Intel microprocessors were worth paying more for ... and they were right.[217] But Mion felt that the public didn't really need to fully understand why Intel chips were better, they just needed to feel they were better. So Mion proposed a market test. Intel would pay for a MicroAge billboard somewhere saying, "If you're buying a personal computer, make sure it has Intel inside." In turn, MicroAge would put "Intel Inside" stickers on the Intel-based computers in their stores in that area. To make the test easier to monitor, Mion decided to do the test in Boulder, Colorado, where it had a single store. Virtually overnight, the sales of personal computers in that store dramatically shifted to Intel-based PCs. Intel very quickly adopted "Intel Inside" as its primary branding and rolled it out worldwide.[217]

As is often the case with computer lore, other tidbits have been combined to explain how things evolved. "Intel Inside" has not escaped that tendency and there are other "explanations" that had been floating around.

Intel's branding campaign started with "The Computer Inside" tagline in 1990 in US and Europe. The Japan chapter of Intel proposed an "Intel in it" tagline and kicked off the Japanese campaign by hosting EKI-KON (meaning "Station Concert" in Japanese) at the Tokyo railway station dome on Christmas Day, December 25, 1990. Several months later, "The Computer Inside" incorporated the Japan idea to become "Intel Inside" which eventually elevated to the worldwide branding campaign in 1991, by Intel marketing manager Dennis Carter.[218] The case study of the Inside Intel Inside was put together by Harvard Business School.[219] The five-note jingle was introduced in 1994 and by its tenth anniversary was being heard in 130 countries around the world. The initial branding agency for the "Intel Inside" campaign was DahlinSmithWhite Advertising of Salt Lake City. The Intel swirl logo was the work of DahlinSmithWhite art director Steve Grigg under the direction of Intel president and CEO Andy Grove.[citation needed]


The Intel Inside logo from 1991 to 2006
The Intel Inside advertising campaign sought public brand loyalty and awareness of Intel processors in consumer computers.[220] Intel paid some of the advertiser's costs for an ad that used the Intel Inside logo and xylo-marimba jingle.[221]


2009 2011 badge design
In 2008, Intel planned to shift the emphasis of its Intel Inside campaign from traditional media such as television and print to newer media such as the Internet.[222] Intel required that a minimum of 35% of the money it provided to the companies in its co-op program be used for online marketing.[222] The Intel 2010 annual financial report indicated that  1.8 billion (6% of the gross margin and nearly 16% of the total net income) was allocated to all advertising with Intel Inside being part of that.[223]

Sonic logo[edit]
The famous D   D   G   D   A  xylophone/xylomarimba jingle, sonic logo, tag, audio mnemonic was produced by Musikvergnuegen and written by Walter Werzowa, once a member of the Austrian 1980s sampling band Edelweiss.[224] The sonic Intel logo has undergone substantial changes in tone since the introduction of the Pentium III, Pentium 4, and Core processors, yet keeps the same jingle.

Processor naming strategy[edit]
In 2006, Intel expanded its promotion of open specification platforms beyond Centrino, to include the Viiv media center PC and the business desktop Intel vPro.

In mid-January 2006, Intel announced that they were dropping the long running Pentium name from their processors. The Pentium name was first used to refer to the P5 core Intel processors and was done to comply with court rulings that prevent the trademarking of a string of numbers, so competitors could not just call their processor the same name, as had been done with the prior 386 and 486 processors (both of which had copies manufactured by IBM and AMD). They phased out the Pentium names from mobile processors first, when the new Yonah chips, branded Core Solo and Core Duo, were released. The desktop processors changed when the Core 2 line of processors were released. By 2009 Intel was using a good-better-best strategy with Celeron being good, Pentium better, and the Intel Core family representing the best the company has to offer.[225]

According to spokesman Bill Calder, Intel has maintained only the Celeron brand, the Atom brand for netbooks and the vPro lineup for businesses. Since late 2009, Intel's mainstream processors have been called Celeron, Pentium, Core i3, Core i5, and Core i7, in order of performance from lowest to highest. The first generation core products carry a 3 digit name, such as i5 750, and the second generation products carry a 4 digit name, such as the i5 2500. In both cases, a K at the end of it shows that it is an unlocked processor, enabling additional overclocking abilities (for instance, 2500K). vPro products will carry the Intel Core i7 vPro processor or the Intel Core i5 vPro processor name.[226] In October 2011, Intel started to sell its Core i7-2700K "Sandy Bridge" chip to customers worldwide.[227]

Since 2010, "Centrino" is only be applied to Intel's WiMAX and Wi-Fi technologies.[226]

Open source support[edit]
Intel has a significant participation in the open source communities since 1999.[228] For example, in 2006 Intel released MIT-licensed X.org drivers for their integrated graphic cards of the i965 family of chipsets. Intel released FreeBSD drivers for some networking cards,[229] available under a BSD-compatible license,[230] which were also ported to OpenBSD.[230] Binary firmware files for non-wireless Ethernet devices were also released under a BSD licence allowing free redistribution.[231] Intel ran the Moblin project until April 23, 2009, when they handed the project over to the Linux Foundation. Intel also runs the LessWatts.org campaigns.[232]

However, after the release of the wireless products called Intel Pro/Wireless 2100, 2200BG/2225BG/2915ABG and 3945ABG in 2005, Intel was criticized for not granting free redistribution rights for the firmware that must be included in the operating system for the wireless devices to operate.[233] As a result of this, Intel became a target of campaigns to allow free operating systems to include binary firmware on terms acceptable to the open source community. Linspire-Linux creator Michael Robertson outlined the difficult position that Intel was in releasing to open source, as Intel did not want to upset their large customer Microsoft.[234] Theo de Raadt of OpenBSD also claimed that Intel is being "an Open Source fraud" after an Intel employee presented a distorted view of the situation at an open-source conference.[235] In spite of the significant negative attention Intel received as a result of the wireless dealings, the binary firmware still has not gained a license compatible with free software principles.[236]

PC declining sales[edit]
Due to PC declining sales and the condition prediction will be worse which Intel support the chipsets, so in 2016 Intel cuts 12,000 jobs. In 2014, Intel has also cut 5,000 jobs.[237]

Controversies[edit]
Patent infringement litigation (2006-2007)[edit]
In October 2006, a Transmeta lawsuit was filed against Intel for patent infringement on computer architecture and power efficiency technologies.[238] The lawsuit was settled in October 2007, with Intel agreeing to pay US 150 million initially and US 20 million per year for the next five years. Both companies agreed to drop lawsuits against each other, while Intel was granted a perpetual non-exclusive license to use current and future patented Transmeta technologies in its chips for 10 years.[239]

Anti-competitive allegations and litigation (2005-2009)[edit]
See also: AMD v. Intel
In September 2005, Intel filed a response to an AMD lawsuit,[240] disputing AMD's claims, and claiming that Intel's business practices are fair and lawful. In a rebuttal, Intel deconstructed AMD's offensive strategy and argued that AMD struggled largely as a result of its own bad business decisions, including underinvestment in essential manufacturing capacity and excessive reliance on contracting out chip foundries.[241] Legal analysts predicted the lawsuit would drag on for a number of years, since Intel's initial response indicated its unwillingness to settle with AMD.[242][243] In 2008 a court date was finally set,[244] but in 2009 Intel settled with a  1.25 billion payout to AMD (see below).[245]

On November 4, 2009, New York's attorney general filed an antitrust lawsuit against Intel Corp, claiming the company used "illegal threats and collusion" to dominate the market for computer microprocessors.

On November 12, 2009, AMD agreed to drop the antitrust lawsuit against Intel in exchange for  1.25 billion.[245] A joint press release published by the two chip makers stated "While the relationship between the two companies has been difficult in the past, this agreement ends the legal disputes and enables the companies to focus all of our efforts on product innovation and development."[246][247]

Main article: High-Tech Employee Antitrust Litigation
An antitrust lawsuit[248] and a class-action suit relating to cold calling employees of other companies has been settled. [249]

Allegations by Japan Fair Trade Commission (2005)[edit]
In 2005, the local Fair Trade Commission found that Intel violated the Japanese Antimonopoly Act. The commission ordered Intel to eliminate discounts that had discriminated against AMD. To avoid a trial, Intel agreed to comply with the order.[250][251][252][253]

Allegations by the European Union (2007-2008)[edit]
In July 2007, the European Commission accused Intel of anti-competitive practices, mostly against AMD.[254] The allegations, going back to 2003, include giving preferential prices to computer makers buying most or all of their chips from Intel, paying computer makers to delay or cancel the launch of products using AMD chips, and providing chips at below standard cost to governments and educational institutions.[255] Intel responded that the allegations were unfounded and instead qualified its market behavior as consumer-friendly.[255] General counsel Bruce Sewell responded that the Commission had misunderstood some factual assumptions as to pricing and manufacturing costs.[256]

In February 2008, Intel stated that its office in Munich had been raided by European Union regulators. Intel reported that it was cooperating with investigators.[257] Intel faced a fine of up to 10% of its annual revenue, if found guilty of stifling competition.[258] AMD subsequently launched a website promoting these allegations.[259][260] In June 2008, the EU filed new charges against Intel.[261] In May 2009, the EU found that Intel had engaged in anti-competitive practices and subsequently fined Intel €1.06 billion (US 1.44 billion), a record amount. Intel was found to have paid companies, including Acer, Dell, HP, Lenovo and NEC,[262] to exclusively use Intel chips in their products, and therefore harmed other companies including AMD.[262][263][264] The European Commission said that Intel had deliberately acted to keep competitors out of the computer chip market and in doing so had made a "serious and sustained violation of the EU's antitrust rules".[262] In addition to the fine, Intel was ordered by the Commission to immediately cease all illegal practices.[262] Intel has stated that they will appeal against the Commission's verdict. In June 2014, the General Court, which sits below the European Court of Justice, rejected the appeal.[262]

Allegations by regulators in South Korea (2007)[edit]
In September 2007, South Korean regulators accused Intel of breaking antitrust law. The investigation began in February 2006, when officials raided Intel's South Korean offices. The company risked a penalty of up to 3% of its annual sales, if found guilty.[265] In June 2008, the Fair Trade Commission ordered Intel to pay a fine of US 25.5 million for taking advantage of its dominant position to offer incentives to major Korean PC manufacturers on the condition of not buying products from AMD.[266]

Allegations by regulators in the United States (2008-2010)[edit]
New York started an investigation of Intel in January 2008 on whether the company violated antitrust laws in pricing and sales of its microprocessors.[267] In June 2008, the Federal Trade Commission also began an antitrust investigation of the case.[268] In December 2009, the FTC announced it would initiate an administrative proceeding against Intel in September 2010.[269][270][271][272]

In November 2009, following a two-year investigation, New York Attorney General Andrew Cuomo sued Intel, accusing them of bribery and coercion, claiming that Intel bribed computer makers to buy more of their chips than those of their rivals, and threatened to withdraw these payments if the computer makers were perceived as working too closely with its competitors. Intel has denied these claims.[273]

On July 22, 2010, Dell agreed to a settlement with the U.S. Securities and Exchange Commission (SEC) to pay  100M in penalties resulting from charges that Dell did not accurately disclose accounting information to investors. In particular, the SEC charged that from 2002 to 2006, Dell had an agreement with Intel to receive rebates in exchange for not using chips manufactured by AMD. These substantial rebates were not disclosed to investors, but were used to help meet investor expectations regarding the company's financial performance; "These exclusivity payments grew from 10 percent of Dell's operating income in FY 2003 to 38 percent in FY 2006, and peaked at 76 percent in the first quarter of FY 2007.".[274] Dell eventually did adopt AMD as a secondary supplier in 2006, and Intel subsequently stopped their rebates, causing Dell's financial performance to fall.[275][276][277]

Corporate responsibility record[edit]
Intel has been accused by some residents of Rio Rancho, New Mexico of allowing VOCs to be released in excess of their pollution permit. One resident claimed that a release of 1.4 tons of carbon tetrachloride was measured from one acid scrubber during the fourth quarter of 2003 but an emission factor allowed Intel to report no carbon tetrachloride emissions for all of 2003.[278]

Another resident alleges that Intel was responsible for the release of other VOCs from their Rio Rancho site and that a necropsy of lung tissue from two deceased dogs in the area indicated trace amounts of toluene, hexane, ethylbenzene, and xylene isomers,[279] all of which are solvents used in industrial settings but also commonly found in gasoline, retail paint thinners and retail solvents. During a sub-committee meeting of the New Mexico Environment Improvement Board, a resident claimed that Intel's own reports documented more than 1,580 pounds (720 kg) of VOCs were released in June and July 2006.[280]

Intel's environmental performance is published annually in their corporate responsibility report.[281]

In its 2012 rankings on the progress of consumer electronics companies relating to conflict minerals, the Enough Project rated Intel the best of 24 companies, calling it a "Pioneer of progress".[282] In 2014, chief executive Brian Krzanich urged the rest of the industry to follow Intel's lead by also shunning conflict minerals.[283]

Religious controversy in Israel (2009)[edit]
Orthodox Jews have protested against Intel operating in Israel on Saturday, Shabbat. Intel ringed its office with barbed wire before the protest, but there was no violence.[284] As of December 2009, the situation has been stable for Intel Israel while some employees reported working overtime on Shabbat.

Age discrimination complaints[edit]
Intel has faced complaints of age discrimination in firing and layoffs. Intel was sued in 1993 by nine former employees, over allegations that they were laid off because they were over the age of 40.[285]

A group called FACE Intel (Former and Current Employees of Intel) claims that Intel weeds out older employees. FACE Intel claims that more than 90 percent of people who have been laid off or fired from Intel are over the age of 40. Upside magazine requested data from Intel breaking out its hiring and firing by age, but the company declined to provide any.[286] Intel has denied that age plays any role in Intel's employment practices.[287] FACE Intel was founded by Ken Hamidi, who was fired from Intel in 1995 at the age of 47.[286] Hamidi was blocked in a 1999 court decision from using Intel's email system to distribute criticism of the company to employees,[288] which overturned in 2003 in Intel Corp. v. Hamidi.

Land dispute in Israel[edit]
According to the Christian Science Monitor, Intel's plant at Kiryat Gat was built on the site of former Palestinian villages of Al-Faluja and Iraq al-Manshiyya; and Al-Awda   the Palestinian Right to Return Coalition   a supporter of Boycott, Divestment and Sanctions, has called on Intel to close this plant.[289]

See also[edit]
Portal icon San Francisco Bay Area portal
Portal icon Companies portal
5 nm The Quantum tunneling leakage Wall
ASCI Red
AMD
Comparison of ATI Graphics Processing Units
Comparison of Intel processors
Comparison of Nvidia graphics processing units
Cyrix
Engineering sample (CPU)
Graphics Processing Unit (GPU)
Intel Driver Update Utility
Intel GMA (Graphics Media Accelerator)
Intel Museum
Intel Science Talent Search
Intel Developer Zone (Intel DZ)
List of Intel chipsets
List of Intel CPU microarchitectures
List of Intel manufacturing sites
List of Intel microprocessors
List of Semiconductor Fabrication Plants
Semiconductor sales leaders by year
Wintel
Intel related biographical articles on Wikipedia:

Andy Grove
Bill Gaede
Bob Colwell
Craig Barrett (chief executive)
Gordon Moore
Justin Rattner
Pat Gelsinger
Paul Otellini
Robert Noyce
Sean Maloney

Advanced Micro Devices
From Wikipedia, the free encyclopedia
  (Redirected from AMD)
"AMD" redirects here. For other uses, see AMD (disambiguation).
Advanced Micro Devices, Inc.
AMD Logo
Amdheadquarters.jpg
Headquarters in Sunnyvale, California
Type
Public
Traded as NASDAQ: AMD
Industry  Semiconductors
Founded May 1, 1969; 46 years ago
Founder Jerry Sanders
Headquarters  One AMD Place,[1]
Sunnyvale, California, United States
Area served
Worldwide
Key people
Lisa Su (CEO)[2]
Bruce Claflin (Executive Chairman)
Products  Microprocessors
Motherboard chipsets
Graphics processing units
Random-access memory[3]
TV tuner cards[4]
Revenue Decrease  3.991 billion (2015)[5]
Operating income
Decrease - 481 million (2015)[5]
Net income
Decrease - 660 million (2015)[5]
Total assets  Decrease US  3.109 billion (2015)[5]
Total equity  Decrease - 412 million(2015)[5]
Number of employees
9,139 (2015)[5]
Divisions SeaMicro
Slogan  Enabling today.
Inspiring tomorrow.
Website www.amd.com
Advanced Micro Devices, Inc. (AMD) is an American multinational semiconductor company based in Sunnyvale, California, United States, that develops computer processors and related technologies for business and consumer markets. While initially it manufactured its own processors, the company became fabless after GlobalFoundries was spun off in 2009. AMD's main products include microprocessors, motherboard chipsets, embedded processors and graphics processors for servers, workstations and personal computers, and embedded systems applications.

AMD is the second-largest supplier and only significant rival to Intel in the market for x86-based microprocessors. Since acquiring ATI in 2006, AMD and its competitor Nvidia have dominated the discrete graphics processor unit (GPU) market.[6]

Contents  [hide] 
1 Company history
1.1 First twelve years
1.2 Technology exchange agreement with Intel
2 History of CPUs and APUs
2.1 IBM PC and the x86 architecture
2.2 K5, K6, Athlon, Duron, and Sempron
2.3 Athlon 64, Opteron and Phenom
2.4 Fusion becomes the AMD APU, and new microarchitectures
2.5 ARM architecture-based chip
2.6 Zen based CPUs and APUs
3 Other products and technologies
3.1 Graphics products (discrete and AMD APU technology)
3.2 AMD motherboard chipsets
3.3 AMD Live!
3.4 AMD Quad FX platform
3.5 Server platform
3.6 Desktop platforms
3.7 Embedded systems
3.8 Other initiatives
3.9 Software
4 Production and fabrication
5 Corporate affairs
5.1 Partnerships
5.2 Litigation with Intel
5.3 Guinness World Record Achievement
5.4 Corporate social responsibility
6 See also
7 References
8 Notes
9 External links
Company history[edit]

AMD campus in Markham, Ontario, Canada, formerly ATI headquarters

AMD's LEED-certified Lone Star campus in Austin, Texas
First twelve years[edit]
Advanced Micro Devices was formally incorporated on May 1, 1969, by Jerry Sanders, along with seven of his colleagues from Fairchild Semiconductor.[7][8] Sanders, an electrical engineer who was the director of marketing at Fairchild, had like many Fairchild executives grown frustrated with the increasing lack of support, opportunity, and flexibility within that company, and decided to leave to start his own semiconductor company.[9] The previous year Robert Noyce, who had invented the first practical integrated circuit or microchip in 1959 at Fairchild,[10] had left Fairchild together with Gordon Moore and founded the semiconductor company Intel in July 1968.[11]

In September 1969, AMD moved from its temporary location in Santa Clara to Sunnyvale, California.[12] To immediately secure a customer base, AMD initially became a second source supplier of microchips designed by Fairchild and National Semiconductor.[13][14] AMD first focused on producing logic chips.[15] The company guaranteed quality control to United States Military Standard, an advantage in the early computer industry since unreliability in microchips was a distinct problem that customers   including computer manufacturers, the telecommunications industry, and instrument manufacturers   wanted to avoid.[13][16][17][18]

In November 1969, the company manufactured its first product, the Am9300, a 4-bit MSI shift register, which began selling in 1970.[18][19] Also in 1970, AMD produced its first proprietary product, the Am2501 logic counter, which was highly successful.[20][21] Its best-selling product in 1971 was the Am2505, the fastest multiplier available.[20][22]

In 1971, AMD entered the RAM chip market, beginning with the Am3101, a 64-bit bipolar RAM.[22][23] That year AMD also greatly increased the sales volume of its linear integrated circuits, and by year end the company's total annual sales reached  4.6 million.[20][24]

AMD went public in September 1972.[13][25][26] The company was a second source for Intel MOS/LSI circuits by 1973, with products such as Am14/1506 and Am14/1507, dual 100-bit dynamic shift registers.[27][28] By 1975, AMD was producing 212 products   of which 49 were proprietary, including the Am9102 (a static N-channel 1024-bit RAM)[29] and three low-power Schottky MSI circuits: Am25LS07, Am25LS08, and Am25LS09.[30]

Intel had created the first microprocessor, its 4-bit 4004, in 1971.[31][32] By 1975, AMD entered the microprocessor market with the Am9080, a reverse-engineered clone of the Intel 8080,[33][34][35] and the Am2900 bit-slice microprocessor family.[34] When Intel began installing microcode in its microprocessors in 1976, it entered into a cross-licensing agreement with AMD, granting AMD a copyright license to the microcode in its microprocessors and peripherals, effective October 1976.[30][36][37][38][39]

In 1977, AMD entered into a joint venture with Siemens, a German engineering conglomerate wishing to enhance its technology expertise and enter the U.S. market.[40] Siemens purchased 20% of AMD's stock, giving AMD an infusion of cash to increase its product lines.[40][41][42] That year the two companies also jointly established Advanced Micro Computers, located in Silicon Valley and in Germany, giving AMD an opportunity to enter the microcomputer development and manufacturing field,[40][43][44][45] in particular based on AMD's second-source Zilog Z8000 microprocessors.[46][47] When the two companies' vision for Advanced Micro Computers diverged, AMD bought out Siemens' stake in the U.S. division in 1979.[48][49] AMD closed its Advanced Micro Computers subsidiary in late 1981, after switching focus to manufacturing second-source Intel x86 microprocessors.[46][50][51]

Total sales in fiscal year 1978 topped  100 million,[43] and in 1979, AMD debuted on the New York Stock Exchange.[21] In 1979, production also began in AMD's new semiconductor fab in Austin;[21] the company already had overseas assembly facilities in Penang and Manila,[52] and it began construction on a semiconductor fab in San Antonio in 1981.[53] In 1980, AMD began supplying semiconductor products for telecommunications, an industry undergoing rapid expansion and innovation.[54]

Technology exchange agreement with Intel[edit]
Intel had introduced the first x86 microprocessors in 1978.[55] In 1981, IBM created its PC, and wanted Intel's x86 processors, but only under the condition that Intel also provide a second-source manufacturer for its patented x86 microprocessors.[16] Intel and AMD entered into a 10-year technology exchange agreement, first signed in October 1981[50][56] and formally executed in February 1982.[39] The terms of the agreement were that each company could acquire the right to become a second-source manufacturer for semiconductor products developed by the other; that is, each party could "earn" the right to manufacture and sell a product developed by the other, if agreed to, by exchanging the manufacturing rights to a product of equivalent technical complexity. The technical information and licenses needed to make and sell a part would be exchanged for a royalty to the developing company.[38] The 1982 agreement also extended the 1976 AMD Intel cross-licensing agreement through 1995.[38][39] The agreement included the right to invoke arbitration of disagreements, and after five years the right of either party to end the agreement with one year's notice.[38] The main result of the 1982 agreement was that AMD became a second-source manufacturer of Intel's x86 microprocessors and related chips, and Intel provided AMD with database tapes for its 8086, 80186, and 80286 chips.[39]

Beginning in 1982, AMD began volume-producing second-source Intel-licensed 8086, 8088, 80186, and 80188 processors, and by 1984 its own Am286 clone of Intel's 80286 processor, for the rapidly growing market of IBM PCs and IBM clones.[16][57] It also continued its successful concentration on proprietary bipolar chips.[58] In 1983, it introduced INT.STD.1000, the highest manufacturing quality standard in the industry.[18][53]

The company continued to spend greatly on research and development,[59] and in addition to other breakthrough products, created the world's first 512K EPROM in 1984.[60] That year AMD was listed in the book The 100 Best Companies to Work for in America,[53][61] and based on 1984 income it made the Fortune 500 list for the first time in 1985.[62][63]

By mid-1985, however, the microchip market experienced a severe downturn, mainly due to longterm aggressive trade practices (dumping) from Japan, but also due to a crowded and non-innovative chip market in the U.S.[64] AMD rode out the mid-1980s crisis by aggressively innovating and modernizing,[65] devising the Liberty Chip program of designing and manufacturing one new chip or chip set per week for 52 weeks in fiscal year 1986,[53][66] and by heavily lobbying the U.S. government until sanctions and restrictions were put into place to prevent predatory Japanese pricing.[67] During this time period, AMD withdrew from the DRAM market,[68] and at the same time made some headway into the CMOS market, which it had lagged in entering, having focused instead on bipolar chips.[69]

AMD had some success in the mid-1980s with the AMD7910 and AMD7911 "World Chip" FSK modem, one of the first multi-standard devices that covered both Bell and CCITT tones at up to 1200 baud half duplex or 300/300 full duplex.[70] Beginning in 1986, AMD embraced the perceived shift toward RISC with their own AMD Am29000 (29k) processor;[71] the 29k survived as an embedded processor.[72][73] The company also increased its EPROM memory market share in the late 1980s.[74] Throughout the 1980s, AMD was a second-source supplier of Intel x86 processors. In 1991, it introduced its own 386-compatible Am386, an AMD-designed chip. Creating its own chips, AMD began to compete directly with Intel.[75]

AMD had a large and successful flash memory business, even during the dotcom bust.[76] In 2003, to divest some manufacturing and aid its overall cash flow, which was under duress from aggressive microprocessor competition from Intel, AMD spun-off its flash memory business and manufacturing into Spansion, a joint venture with Fujitsu, which had been co-manufacturing flash memory with AMD since 1993.[77][78] AMD divested itself of Spansion in December 2005, in order to focus on the microprocessor market, and Spansion went public in an IPO.[79]

AMD announced the acquisition of the graphics processor company ATI Technologies on July 24, 2006. AMD paid  4.3 billion in cash and 58 million shares of its stock, for a total of approximately  5.4 billion. The transaction completed on October 25, 2006.[80] On August 30, 2010, AMD announced that it would retire the ATI brand name for its graphics chipsets in favor of the AMD brand name.[81][82]

In October 2008, AMD announced plans to spin off manufacturing operations in the form of a multibillion-dollar joint venture with Advanced Technology Investment Co., an investment company formed by the government of Abu Dhabi. The new venture is called GlobalFoundries Inc. The partnership and spin-off gave AMD an infusion of cash and allowed AMD to focus solely on chip design.[83] To assure the Abu Dhabi investors of the new venture's success, CEO Hector Ruiz stepped down as CEO of AMD in July 2008, while remaining Executive Chairman, in preparation to becoming Chairman of Global Foundries in March 2009.[84][85] President and COO Dirk Meyer became AMD's CEO.[86] Recessionary losses necessitated AMD cutting 1,100 jobs in 2009.[87]

In August 2011, AMD announced that former Lenovo executive Rory Read would be joining the company as CEO, replacing Meyer.[88] AMD announced in November 2011 plans to lay off more than 10% (1,400) of its employees from across all divisions worldwide.[89] In October 2012, it announced plans to lay off an additional 15% of its workforce to reduce costs in the face of declining sales revenue.[90]

AMD acquired the low-power server manufacturer SeaMicro in early 2012, with an eye to bringing out an ARM architecture server chip.[91]

On October 8, 2014, AMD announced that Rory Read had stepped down after three years as president and chief executive officer.[2] He was succeeded by Lisa Su, a key lieutenant who had been serving as chief operating officer since June.[92]

On October 16, 2014, AMD announced a new restructuring plan along with its Q3 results. Effective July 1, 2014, AMD reorganized into two business groups: Computing and Graphics, which primarily includes desktop and notebook processors and chipsets, discrete GPUs, and professional graphics; and Enterprise, Embedded and Semi-Custom, which primarily includes server and embedded processors, dense servers, semi-custom SoC products, engineering services, and royalties. As part of this restructuring AMD announced that 7% of its global workforce would be laid off by the end of 2014.[93]

History of CPUs and APUs[edit]

Early AMD 9080 Processor (AMD AM9080ADC / C8080A), 1977

AMD D8086, 1978
See also: List of AMD microprocessors
IBM PC and the x86 architecture[edit]
Main articles: Am286, Am386, Am486 and Am5x86
In February 1982, AMD signed a contract with Intel, becoming a licensed second-source manufacturer of 8086 and 8088 processors. IBM wanted to use the Intel 8088 in its IBM PC, but IBM's policy at the time was to require at least two sources for its chips. AMD later produced the Am286 under the same arrangement. In 1984 Intel, in order to shore up its advantage in the marketplace, internally decided to no longer cooperate with AMD in supplying product information, and delayed and eventually refused to convey the technical details of the Intel 80386 to AMD.[94] In 1987, AMD invoked arbitration over the issue, and Intel reacted by cancelling the 1982 technological-exchange agreement altogether.[95][96] After three years of testimony, AMD eventually won in arbitration in 1992, but Intel disputed this decision. Another long legal dispute followed, ending in 1994 when the Supreme Court of California sided with the arbitrator and AMD.[97][98]

In 1990, Intel also countersued AMD, reneging on AMD's right to use derivatives of Intel's microcode for its cloned processors.[99] In the face of uncertainty during the legal dispute, AMD was forced to develop clean room designed versions of Intel code for its x386 and x486 processors, the former long after Intel had released its own x386 in 1985.[100] In March 1991, AMD released the Am386, its clone of the Intel 386 processor.[53] By October of the same year it had sold one million units.[53]

In 1993, AMD introduced the first of the Am486 family of processors,[21] which proved popular with a large number of original equipment manufacturers, including Compaq, which signed an exclusive agreement using the Am486.[13][101][102] Another Am486-based processor, the Am5x86, was released in November 1995 and continued AMD's success as a fast, cost-effective processor.[103][104]

Finally, in an agreement effective 1996, AMD received the rights to the microcode in Intel's x386 and x486 processor families, but not the rights to the microcode in the following generations of processors.[105][106]

K5, K6, Athlon, Duron, and Sempron[edit]
Main articles: AMD K5, AMD K6, Athlon, Duron and Sempron
AMD's first in-house x86 processor was the K5, which was launched in 1996.[107] The "K" was a reference to Kryptonite. (In comic books, the only substance which could harm Superman was Kryptonite. This is a reference to Intel's hegemony over the market, i.e., an anthropomorphization of them as Superman.[108]) The numeral "5" refers to the fifth generation of x86 processors; rival Intel had previously introduced its line of fifth-generation x86 processors as Pentium because the U.S. Trademark and Patent Office had ruled that mere numbers could not be trademarked.[109]

In 1996, AMD purchased NexGen, specifically for the rights to their Nx series of x86-compatible processors. AMD gave the NexGen design team their own building, left them alone, and gave them time and money to rework the Nx686. The result was the K6 processor, introduced in 1997. Although the K6 was based on Socket 7, variants such as K6-3/450 were faster than Intel's Pentium II (sixth-generation processor).

The K7 was AMD's seventh-generation x86 processor, making its debut on June 23, 1999, under the brand name Athlon. Unlike previous AMD processors, it could not be used on the same motherboards as Intel's, due to licensing issues surrounding Intel's Slot 1 connector, and instead used a Slot A connector, referenced to the Alpha processor bus. The Duron was a lower-cost and limited version of the Athlon (64KB instead of 256KB L2 cache) in a 462-pin socketed PGA (socket A) or soldered directly onto the motherboard. Sempron was released as a lower-cost Athlon XP, replacing Duron in the socket A PGA era. It has since been migrated upward to all new sockets, up to AM3.

On October 9, 2001, the Athlon XP was released. On February 10, 2003, the Athlon XP with 512KB L2 Cache was released.[110]

Athlon 64, Opteron and Phenom[edit]
Main articles: Athlon 64, Opteron and Phenom (processor)
The K8 was a major revision of the K7 architecture, with the most notable features being the addition of a 64-bit extension to the x86 instruction set (called x86-64, AMD64, or x64), the incorporation of an on-chip memory controller, and the implementation of an extremely high performance point-to-point interconnect called HyperTransport, as part of the Direct Connect Architecture. The technology was initially launched as the Opteron server-oriented processor on April 22, 2003.[111] Shortly thereafter it was incorporated into a product for desktop PCs, branded Athlon 64.[112]

On April 21, 2005, AMD released the first dual core Opteron, an x86-based server CPU.[113] A month later, AMD released the Athlon 64 X2, the first desktop-based dual core processor family.[114] In May 2007, AMD abandoned the string "64" in its dual-core desktop product branding, becoming Athlon X2, downplaying the significance of 64-bit computing in its processors. Further updates involved improvements to the microarchitecture, and a shift of target market from mainstream desktop systems to value dual-core desktop systems. In 2008, AMD started to release dual-core Sempron processors exclusively in China, branded as the Sempron 2000 series, with lower HyperTransport speed and smaller L2 cache. Thus AMD completed its dual-core product portfolio for each market segment.

After K8 came K10. In September 2007, AMD released the first K10 processors   nine quad-core Third Generation Opteron processors   followed in November by the Phenom processor for desktop. K10 processors came in dual-core, triple-core,[115] and quad-core versions, with all cores on a single die. AMD released a new platform, codenamed "Spider", which utilized the new Phenom processor, as well as an R770 GPU and a 790 GX/FX chipset from the AMD 700 chipset series. However, AMD built the Spider at 65nm, which was uncompetitive with Intel's smaller and more power-efficient 45nm.

In January 2009, AMD released a new processor line dubbed Phenom II, a refresh of the original Phenom built using the 45 nm process. AMD's new platform, codenamed  Dragon , utilised the new Phenom II processor, and an ATI R770 GPU from the R700 GPU family, as well as a 790 GX/FX chipset from the AMD 700 chipset series. The Phenom II came in dual-core, triple-core and quad-core variants, all using the same die, with cores disabled for the triple-core and dual-core versions. The Phenom II resolved issues that the original Phenom had, including a low clock speed, a small L3 cache and a Cool'n'Quiet bug that decreased performance. The Phenom II cost less but was not performance-competitive with Intel's mid-to-high-range Core 2 Quads. The Phenom II also enhanced the Phenom's memory controller, allowing it to use DDR3 in a new native socket AM3, while maintaining backwards compatibility with AM2+, the socket used for the Phenom, and allowing the use of the DDR2 memory that was used with the platform.

In April 2010, AMD released a new Phenom II hexa-core (6-core) processor codenamed "Thuban". This was a totally new die based on the hexa-core  Istanbul  Opteron processor. It included AMD's  turbo core  technology, which allows the processor to automatically switch from 6 cores to 3 faster cores when more pure speed is needed. AMD's enthusiast platform, codenamed "Leo", utilized the new Phenom II, a new chipset from the AMD 800 chipset series and an ATI  Cypress  GPU from the Evergreen GPU series.

Ambox current red.svg
This section is outdated. Please update this article to reflect recent events or newly available information. (July 2014)
The Magny Cours and Lisbon server parts were released in 2010. The Magny Cours part came in 8 to 12 cores and the Lisbon part in 4 and 6 core parts. Magny Cours is focused on performance while the Lisbon part is focused on high performance per watt. Magny Cours is an MCM (Multi-Chip Module) with two hexa-core  Istanbul  Opteron parts. This will use a new G34 socket for dual and quad socket processors and thus will be marketed as Opteron 61xx series processors. Lisbon uses C32 socket certified for dual socket use or single socket use only and thus will be marketed as Opteron 41xx processors. Both will be built on a 45 nm SOI process.

Fusion becomes the AMD APU, and new microarchitectures[edit]
Main articles: AMD APU, AMD mobile platform, The AMD FX and Opteron Bulldozer microarchitecture, The AMD APU Bobcat microarchitecture and The Jaguar microarchitecture used in multiple processors and APUs
Following AMD's 2006 acquisition of Canadian graphics company ATI Technologies, an initiative codenamed Fusion was announced to integrate a CPU and GPU together on some of AMD's microprocessors, including a built in PCI Express link to accommodate separate PCI Express peripherals, eliminating the northbridge chip from the motherboard. The initiative intended to move some of the processing originally done on the CPU (e.g. floating-point unit operations) to the GPU, which is better optimized for some calculations. The Fusion was later renamed to the AMD APU (Accelerated Processing Unit).[116]

Llano was AMD's first APU built for laptops. Llano was the second APU released,[117] targeted at the mainstream market.[116] Incorporating a CPU and GPU on the same die, as well as northbridge functions, and using "Socket FM1" with DDR3 memory. The CPU part of the processor was based on the Phenom II "Deneb" processor. AMD suffered an unexpected decrease in revenue based on production problems for the Llano.[118]

Bulldozer is AMD's microarchitecture codename for server and desktop AMD FX processors first released on October 12, 2011. This family 15h microarchitecture is the successor to the family 10h (K10) microarchitecture design. Bulldozer is designed from scratch, not a development of earlier processors.[119] The core is specifically aimed at 10-125 W TDP computing products. AMD claims dramatic performance-per-watt efficiency improvements in high-performance computing (HPC) applications with Bulldozer cores. While hopes were very high that Bulldozer would bring AMD to be performance competitive with arch rival Intel once more, most benchmarks were disappointing. In some cases the new Bulldozer products were slower than the K10 model they were built to replace.[120][121][122]

Hondo is AMD's latest processor series used in Tablet computers.[123]

Piledriver is the name of AMD's microarchitecture used in some AMD FX processors released in 2012. This AMD FX series processor lineup is called Vishera, and targets the desktop performance market.

Jaguar is a x86-64 microarchitecture codename for a processor core that is used in various APUs from AMD aimed at the low-power/low-cost market. It is also used as the microarchitecture for the custom APUs in the PS4 and Xbox One (which contain CPU, GPU and memory).

Jaguar's predecessor, Bobcat, was revealed during a speech from AMD executive vice-president Henri Richard in Computex 2007 and was put into production Q1 2011.[117] One of the major supporters was executive vice-president Mario A. Rivas who felt it was difficult to compete in the x86 market with a single core optimized for the 10-100 W range and actively promoted the development of the simpler core with a target range of 1-10 watts. In addition, it was believed that the core could migrate into the hand-held space if the power consumption can be reduced to less than 1 W.[citation needed]

ARM architecture-based chip[edit]
Ambox current red.svg
This section is outdated. Please update this article to reflect recent events or newly available information. (April 2016)
AMD intends to release 64-bit ARM System on Chips (SoC) that will begin sampling in early 2014 and shipping in the second half of 2015. They will be for use in servers as a low-power alternative to current x86 chips. Their implementation using the ARM architecture is codenamed "Seattle", based on the Cortex A57 core design (ARMv8-A), and will contain 8 and 16 cores each. They will include the proprietary SeaMicro "Freedom Fabric", as well as support for 128 GB RAM, and 10 gigabit Ethernet.[124] This is to be followed by the custom ARM core K12 core, expected in 2016-2017.[125]

Zen based CPUs and APUs[edit]
Main article: Zen (microarchitecture)
Zen is a new architecture for x86-64 based CPUs and APUs, built from the ground up by a team led by Jim Keller, beginning with his arrival in 2012, and taping out before his departure in September 2015. Zen will be built on the 14 nm node and have a renewed focus on single-core performance and HSA compatibility.[126] Zen will be the first chip encompassing CPUs and APUs from AMD built for a single socket. It will also support DDR4. It is expected to be released mid-late 2016, following the availability of the AMD A10-7890K FM2+ desktop CPU.

Other products and technologies[edit]

AMD Radeon memory
Graphics products (discrete and AMD APU technology)[edit]
See also: List of AMD graphics processing units and List of AMD Accelerated Processing Unit microprocessors
AMD's portfolio of dedicated graphics processors as of 2015 includes product families and associated technologies aimed at the consumer, professional and high-performance computing markets, such as:

Radeon   brand for consumer line of graphics cards; the brand name originated with ATI. Mobility Radeon offers power-optimized versions of Radeon graphics chips for use in laptops.
AMD FirePro   brand for professional line of graphics cards for workstations. Succeeds the FireGL series of workstation CAD/CAM video cards, the FireMV series and the AMD FireStream series.
AMD FireStream   brand for discontinued product line targeting stream processing and GPGPU as used in various industries.
AMD FireMV   brand for discontinued product line targeting multi-monitor setups in professional environments.
As of 2015 technologies found in AMD products include:

AMD Eyefinity   facilitates multi-monitor setup of up to 6 monitors per graphics card
AMD TrueAudio   acceleration of audio calculations
Unified Video Decoder (UVD)   acceleration of video decoding
Video Coding Engine (VCE)   acceleration of video encoding
AMD Catalyst is a collection of proprietary device driver software available for Microsoft Windows and Linux.

Since 2007, AMD has participated in the development of free and open-source graphics device drivers. The programming specifications for a number of chipsets and features were published in several rounds. Employees hired by AMD for this purpose contribute code to the Direct Rendering Manager in the Linux kernel.

AMD motherboard chipsets[edit]
See also: Comparison of AMD chipsets
Before the launch of Athlon 64 processors in 2003, AMD designed chipsets for their processors spanning the K6 and K7 processor generations. The chipsets include the AMD-640, AMD-751 and the AMD-761 chipsets. The situation changed in 2003 with the release of Athlon 64 processors, and AMD chose not to further design its own chipsets for its desktop processors while opening the desktop platform to allow other firms to design chipsets. This was the  Open Platform Management Architecture  with ATI, VIA and SiS developing their own chipset for Athlon 64 processors and later Athlon 64 X2 and Athlon 64 FX processors, including the Quad FX platform chipset from Nvidia.

The initiative went further with the release of Opteron server processors as AMD stopped the design of server chipsets in 2004 after releasing the AMD-8111 chipset, and again opened the server platform for firms to develop chipsets for Opteron processors. As of today, Nvidia and Broadcom are the sole designing firms of server chipsets for Opteron processors.

As the company completed the acquisition of ATI Technologies in 2006, the firm gained the ATI design team for chipsets which previously designed the Radeon Xpress 200 and the Radeon Xpress 3200 chipsets. AMD then renamed the chipsets for AMD processors under AMD branding (for instance, the CrossFire Xpress 3200 chipset was renamed as AMD 580X CrossFire chipset). In February 2007, AMD announced the first AMD-branded chipset since 2004 with the release of the AMD 690G chipset (previously under the development codename RS690), targeted at mainstream IGP computing. It was the industry's first to implement a HDMI 1.2 port on motherboards, shipping for more than a million units. While ATI had aimed at releasing an Intel IGP chipset, the plan was scrapped and the inventories of Radeon Xpress 1250 (codenamed RS600, sold under ATI brand) was sold to two OEMs, Abit and ASRock. Although AMD stated the firm would still produce Intel chipsets, Intel had not granted the license of 1333 MHz FSB to ATI.

On November 15, 2007, AMD announced a new chipset series portfolio, the AMD 7-Series chipsets, covering from enthusiast multi-graphics segment to value IGP segment, to replace the AMD 480/570/580 chipsets and AMD 690 series chipsets, marking AMD's first enthusiast multi-graphics chipset. Discrete graphics chipsets were launched on November 15, 2007 as part of the codenamed Spider desktop platform, and IGP chipsets were launched at a later time in spring 2008 as part of the codenamed Cartwheel platform.

AMD returned to the server chipsets market with the AMD 800S series server chipsets. It includes support for up to six SATA 6.0 Gbit/s ports, the C6 power state, which is featured in Fusion processors and AHCI 1.2 with SATA FIS based switching support. This is a chipset family supporting Phenom processors and Quad FX enthusiast platform (890FX), IGP(890GX).

AMD Live![edit]
Main article: AMD Live!
As of 2007, AMD LIVE! was a platform marketing initiative focusing the consumer electronics segment, with an Active TV initiative for streaming Internet videos from web video services such as YouTube, into AMD Live! PC as well as connected digital TVs, together with a scheme for an ecosystem of certified peripherals for the ease of customers to identify peripherals for AMD LIVE! systems for digital home experience, called "AMD LIVE! Ready".[127]

AMD Quad FX platform[edit]
Main article: AMD Quad FX platform
The AMD Quad FX platform, being an extreme enthusiast platform,[clarification needed] allows two processors to connect through HyperTransport, which is a similar setup to dual-processor (2P) servers, excluding the use of buffered memory/registered memory DIMM modules, and a server motherboard, the current setup includes two Athlon 64 FX-70 series processors and a special motherboard.[citation needed] AMD pushed the platform for the surging demands for what AMD calls "megatasking",[128] the ability to do more tasks on a single system. The platform refreshes with the introduction of Phenom FX processors and the next-generation RD790 chipset, codenamed "FASN8".

Server platform[edit]
AMD's first multi-processor server platform, codenamed Fiorano, consists of AMD SR5690 + SP5100 server chipsets, supporting 45 nm, codenamed Shanghai Socket F+ processors and registered DDR2 memory. It was followed by the Maranello platform supporting 45 nm, codenamed Istanbul, Socket G34 processors with DDR3 memory. On single-processor platform, the codenamed Catalunya platform consists of codenamed Suzuka 45 nm quad-core processor with AMD SR5580 + SP5100 chipset and DDR3 support.[129]

AMD's x86 virtualization extension to the 64-bit x86 architecture is named AMD Virtualization, also known by the abbreviation AMD-V, and is sometimes referred to by the code name "Pacifica". AMD processors using Socket AM2, Socket S1, and Socket F include AMD Virtualization support. AMD Virtualization is also supported by release two (8200, 2200 and 1200 series) of the Opteron processors. The third generation (8300 and 2300 series) of Opteron processors will see an update in virtualization technology, specifically the Rapid Virtualization Indexing (also known by the development name Nested Page Tables), alongside the tagged TLB and Device Exclusion Vector (DEV).

AMD also promotes the "AMD I/O Virtualization Technology" (also known as IOMMU) for I/O virtualization.[130] The AMD IOMMU specification has been updated to version 1.2.[131] The specification describes the use of a HyperTransport architecture.

AMD's server initiatives include the following:

AMD Trinity, provides support for virtualization, security and management. Key features include AMD-V technology, codenamed Presidio trusted computing platform technology, I/O Virtualization and Open Management Partition.[132]
AMD Raiden, future clients similar to the Jack PC[133] to be connected through network to a blade server for central management, to reduce client form factor sizes with AMD Trinity features.
Torrenza, coprocessors support through interconnects such as HyperTransport, and PCI Express (though more focus was at HyperTransport enabled coprocessors), also opening processor socket architecture to other manufacturers, Sun and IBM are among the supporting consortium, with rumoured POWER7 processors would be socket-compatible to future Opteron processors. The move made rival Intel respond with the opening of front-side bus (FSB) architecture as well as Geneseo,[134] a collaboration project with IBM for coprocessors connected through PCI Express.
Various certified systems programs and platforms: AMD Commercial Stable Image Platform (CSIP), together with AMD Validated Server program, AMD True Server Solutions, AMD Thermally Tested Barebones Platforms and AMD Validated Server Program, providing certified systems for business from AMD.
Desktop platforms[edit]
Ambox current red.svg
This section is outdated. Please update this article to reflect recent events or newly available information. (December 2014)
Starting in 2007, AMD, following Intel, began using codenames for its desktop platforms such as Spider or Dragon. The platforms, unlike Intel's approach, will refresh every year, putting focus on platform specialization. The platform includes components such as AMD processors, chipsets, ATI graphics and other features, but continued to the open platform approach, and welcome components from other vendors such as VIA, SiS, and Nvidia, as well as wireless product vendors.

Updates to the platform includes the implementation of IOMMU I/O Virtualization with 45 nm generation of processors, and the AMD 800 chipset series in 2009.[135]

Embedded systems[edit]
Main articles: Alchemy (processor) and Geode (processor)
Ambox current red.svg
This section is outdated. Please update this article to reflect recent events or newly available information. (December 2014)
In February 2002, AMD acquired Alchemy Semiconductor for its Alchemy line of MIPS processors for the hand-held and portable media player markets. On June 13, 2006, AMD officially announced that the line was to be transferred to Raza Microelectronics, Inc., a designer of MIPS processors for embedded applications.[136]

In August 2003, AMD also purchased the Geode business which was originally the Cyrix MediaGX from National Semiconductor to augment its existing line of embedded x86 processor products. During the second quarter of 2004, it launched new low-power Geode NX processors based on the K7 Thoroughbred architecture with speeds of fanless processors 667 MHz and 1 GHz, and 1.4 GHz processor with fan, of TDP 25 W. This technology is used in a variety of embedded systems (Casino slot machines and customer kiosks for instance), several UMPC designs in Asia markets, as well as the OLPC XO-1 computer, an inexpensive laptop computer intended to be distributed to children in developing countries around the world. The Geode LX processor was announced in 2005 and is said will continue to be available through 2015.

For the past couple of years AMD has been introducing 64-bit processors into its embedded product line starting with the AMD Opteron processor. Leveraging the high throughput enabled through HyperTransport and the Direct Connect Architecture these server class processors have been targeted at high end telecom and storage applications. In 2007, AMD added the AMD Athlon, AMD Turion and Mobile AMD Sempron processors to its embedded product line. Leveraging the same 64-bit instruction set and Direct Connect Architecture as the AMD Opteron but at lower power levels, these processors were well suited to a variety of traditional embedded applications. Throughout 2007 and into 2008, AMD has continued to add both single-core Mobile AMD Sempron and AMD Athlon processors and dual-core AMD Athlon X2 and AMD Turion processors to its embedded product line and now offers embedded 64-bit solutions starting with 8W TDP Mobile AMD Sempron and AMD Athlon processors for fan-less designs up to multi-processor systems leveraging multi-core AMD Opteron processors all supporting longer than standard availability.[137]

The ATI acquisition included the Imageon and Xilleon product lines. In late 2008, the entire handheld division was sold off to Qualcomm, who have since produced the Adreno series. The Xilleon division was sold to Broadcom.

In April 2007, AMD announced the release of the M690T integrated graphics chipset for embedded designs. This enabled AMD to offer complete processor and chipset solutions targeted at embedded applications requiring high performance 3D and video such as emerging digital signage, kiosk and Point of Sale applications. The M690T was followed by the M690E specifically for embedded applications which removed the TV output, which required Macrovision licensing for OEMs, and enabled native support for dual TMDS outputs, enabling dual independent DVI interfaces.

In 2008, AMD announced the Radeon E2400, the first discrete GPU in their embedded product line offering the same long term availability as their other embedded products. That was followed in 2009 with the higher performance Radeon E4690 discrete GPU.

In 2009, AMD announced their first BGA packaged e64 architecture processors, known as the ASB1 family.

In 2010, AMD announced a second generation BGA platform referred to as ASB2. They also announced several new AM3 based processors with support for DDR3 memory.

In January 2011, AMD announced the AMD Embedded G-Series Accelerated Processing Unit. The first Fusion family APU for embedded applications. This announcement was followed by announcements for the high performance AMD Radeon E6760 and the value-conscious Radeon E6460 discrete GPUs. These solutions all added support for DirectX 11, OpenGL 4.1 and OpenCL 1.1.

In May 2012, AMD Announced the AMD Embedded R-Series[138] Accelerated Processing Unit. This family of products incorporates the Bulldozer CPU architecture, and Discrete-class AMD Radeon  HD 7000G Series graphics.

AMD Embedded solutions offer 5+ year product life.

Other initiatives[edit]
50x15, digital inclusion, with targeted 50% of world population to be connected through Internet via affordable computers by the year of 2015.
The Green Grid,[139] founded by AMD together with other founders, such as IBM, Sun and Microsoft, to seek lower power consumption for grids.
Codenamed SIMFIRE   interoperability testing tool for the Desktop and mobile Architecture for System Hardware (DASH) open architecture.
Software[edit]
AMD develops the AMD CodeXL tool suite which includes a GPU debugger, a GPU profiler, a CPU profiler and an OpenCL static kernel analyzer. CodeXL is freely available at AMD developer tools website.
AMD Stream SDK and AMD APP SDK (Accelerated Parallel Processing) SDK to enable AMD graphics processing cores (GPU), working in concert with the system s x86 cores (CPU), to execute heterogeneously to accelerate many applications beyond just graphics[140]
AMD has also taken an active part in developing coreboot, and open source projects aimed at replacing the proprietary BIOS firmware.
Other AMD software includes the AMD Core Math Library, and open-source software including the AMD Performance Library, and the CodeAnalyst performance profiler.
AMD contributes to open source projects, including working with Sun Microsystems to enhance OpenSolaris and Sun xVM on the AMD platform.[141] AMD also maintains its own Open64 compiler distribution and contributes its changes back to the community.[142]
In 2008, AMD released the low-level programming specifications for its GPUs, and works with the X.Org Foundation to develop drivers for AMD graphics cards.[143][144]
Extensions for software parallelism (xSP), aimed at speeding up programs to enable multi-threaded and multi-core processing, announced in Technology Analyst Day 2007. One of the initiatives being discussed since August 2007 is the Light Weight Profiling (LWP), providing internal hardware monitor with runtimes, to observe information about executing process and help the re-design of software to be optimized with multi-core and even multi-threaded programs. Another one is the extension of Streaming SIMD Extension (SSE) instruction set, the SSE5.
Production and fabrication[edit]
Main article: GlobalFoundries
Ever since the spin-off of AMD's fabrication plants in early 2009, GlobalFoundries has been responsible for producing AMD's processors.

GlobalFoundries' main microprocessor manufacturing facilities are located in Dresden, Germany. Additionally, highly integrated microprocessors are manufactured in Taiwan made by third-party manufacturers under strict license from AMD. Between 2003 and 2005, they constructed a second manufacturing plant (300 mm 90 nm process SOI) in the same complex in order to increase the number of chips they could produce, thus becoming more competitive with Intel. The new plant was named "Fab 36", in recognition of AMD's 36 years of operation, and reached full production in mid-2007. Fab 36 was renamed to "Fab 1" during the spin-off of AMD's manufacturing business during the creation of GlobalFoundries. In July 2007, AMD announced that they completed the conversion of Fab 1 Module 1 from 90 nm to 65 nm. They then shifted their focus to the 45 nm conversion.[145]

Corporate affairs[edit]
Partnerships[edit]
AMD utilizes strategic industry partnerships to further its business interests as well as to rival Intel's dominance and resources.

A partnership between AMD and Alpha Processor Inc. developed HyperTransport, a point-to-point interconnect standard which was turned over to an industry standards body for finalization. It is now used in modern motherboards that are compatible with AMD processors.

AMD also formed a strategic partnership with IBM, under which AMD gained silicon on insulator (SOI) manufacturing technology, and detailed advice on 90 nm implementation. AMD announced that the partnership would extend to 2011 for 32 nm and 22 nm fabrication-related technologies.[146]

To facilitate processor distribution and sales, AMD is loosely partnered with end-user companies, such as HP, Compaq, ASUS, Acer, and Dell.

In 1993, AMD established a 50-50 partnership with Fujitsu called FASL, and merged into a new company called FASL LLC in 2003. The joint venture went public under the name Spansion and ticker symbol SPSN in December 2005, with AMD shares drop to 37%. AMD no longer directly participates in the Flash memory devices market now as AMD entered into a non-competition agreement, as of December 21, 2005, with Fujitsu and Spansion, pursuant to which it agreed not to directly or indirectly engage in a business that manufactures or supplies standalone semiconductor devices (including single chip, multiple chip or system devices) containing only Flash memory.[147]

On May 18, 2006, Dell announced that it would roll out new servers based on AMD's Opteron chips by year's end, thus ending an exclusive relationship with Intel. In September 2006, Dell began offering AMD Athlon X2 chips in their desktop line-up.

In June 2011, HP announced new business and consumer notebooks equipped with the latest versions of AMD APUs   accelerated processing units. AMD will power HP's Intel-based business notebooks as well.[148]

In the spring of 2013, AMD announced that it would be powering all three major next-generation consoles.[149] The Xbox One and Sony PlayStation 4 are both powered by a custom-built AMD APU, and the Nintendo Wii U is powered by an AMD GPU.[150] According to AMD, having their processors in all three of these consoles will greatly assist developers with cross-platform development to competing consoles and PCs as well as increased support for their products across the board.[151]

Litigation with Intel[edit]
See also: AMD v. Intel

AMD processor with Intel logo
AMD has a long history of litigation with former partner and x86 creator Intel.[152][153][154]

In 1986, Intel broke an agreement it had with AMD to allow them to produce Intel's micro-chips for IBM; AMD filed for arbitration in 1987 and the arbitrator decided in AMD's favor in 1992. Intel disputed this, and the case ended up in the Supreme Court of California. In 1994, that court upheld the arbitrator's decision and awarded damages for breach of contract.
In 1990, Intel brought a copyright infringement action alleging illegal use of its 287 microcode. The case ended in 1994 with a jury finding for AMD and its right to use Intel's microcode in its microprocessors through the 486 generation.
In 1997, Intel filed suit against AMD and Cyrix Corp. for misuse of the term MMX. AMD and Intel settled, with AMD acknowledging MMX as a trademark owned by Intel, and with Intel granting AMD rights to market the AMD K6 MMX processor.
In 2005, following an investigation, the Japan Federal Trade Commission found Intel guilty on a number of violations. On June 27, 2005, AMD won an antitrust suit against Intel in Japan, and on the same day, AMD filed a broad antitrust complaint against Intel in the U.S. Federal District Court in Delaware. The complaint alleges systematic use of secret rebates, special discounts, threats, and other means used by Intel to lock AMD processors out of the global market. Since the start of this action, the court has issued subpoenas to major computer manufacturers including Acer, Dell, Lenovo, HP and Toshiba.
In November 2009, Intel agreed to pay AMD  1.25bn and renew a five-year patent cross-licensing agreement as part of a deal to settle all outstanding legal disputes between them.[155]
Guinness World Record Achievement[edit]
On August 31, 2011, in Austin, Texas, AMD achieved a Guinness World Record for the "Highest frequency of a computer processor": 8.429 GHz.[156] The company ran an 8-core FX-8150 processor with only one active module (two cores), and cooled with liquid helium.[157] The previous record was 8.308 GHz, with an Intel Celeron 352 (one core).

On November 1, 2011, geek.com reported that Andre Yang, an overclocker from Taiwan, used an FX-8150 to set another record: 8.461 GHz.[158]

Corporate social responsibility[edit]
In its 2012 report on progress relating to conflict minerals, the Enough Project rated AMD the fifth most progressive of 24 consumer electronics companies.[159]

See also[edit]
Portal icon San Francisco Bay Area portal
Portal icon Companies portal
Bill Gaede
3DNow!
Cool'n'Quiet
PowerNow!
Semiconductor sales leaders by year
Comparison of AMD Chipsets
Comparison of AMD graphics processing units
Comparison of ATI Chipsets
Comparison of AMD Processors
Intel PRO/Wireless
From Wikipedia, the free encyclopedia
  (Redirected from Intel Driver Update Utility)
Intel PRO/Wireless is a series of Intel wireless products.

Contents  [hide] 
1 History
2 Hardware
3 See also
4 References
5 External links
History[edit]
After the release of the wireless products called Intel Pro/Wireless 2100, 2200BG/2225BG/2915ABG and 3945ABG in 2005, Intel was criticized for not granting free redistribution rights for the firmware necessary to be included in the operating systems for the wireless devices to operate.[1] As a result of this, Intel became a target of campaigns to allow free operating systems to include binary firmwares on terms acceptable to the open source community. Linspire-Linux creator Michael Robertson outlined the difficult position that Intel was in releasing to Open Source, as Intel did not want to upset their large customer Microsoft.[2] Theo de Raadt of OpenBSD also claimed that Intel is being "an Open Source fraud" after an Intel employee presented a distorted view of the situation on an open-source conference.[3] In spite of the negative attention Intel received as a result of the wireless dealings, the binary firmware still has not gained a license compatible with free software principles.

Hardware[edit]
Model Name  Supported 802.11 protocols  Form Factor
PRO/Wireless 2011B  b PC-Card
PRO/Wireless 2100 b Mini PCI
PRO/Wireless 2100A  ab  Mini PCI
PRO/Wireless 2200BG bg  Mini PCI
PRO/Wireless 2915ABG  abg Mini PCI
PRO/Wireless 3945ABG  abg Mini PCIe
PRO/Wireless 5100ABGN abgn  Mini PCIe
The successor to the PRO/Wireless series is Intel Wireless WiFi Link.

See also[edit]
Comparison of open-source wireless drivers
FIPS 140
Wireless LAN
References[edit]
Jump up ^ Varghese, Sam (2005-03-01). "OpenBSD to support more wireless chipsets". theage.com.au (The Age Company Ltd). Retrieved 2007-08-05.
Jump up ^ Robertson, Michael (2003-03-19). "Is Intel's "Centrino" Techno-Latin for "No Linux "". michaelrobertson.com. Retrieved 2007-08-05.
Jump up ^ "Intel: Only "Open" for Business". OpenBSD Journal. 2006-09-30. Retrieved 2007-08-05. |first1= missing |last1= in Authors list (help)
Motherboard
From Wikipedia, the free encyclopedia

Motherboard for an Acer desktop personal computer, showing the typical components and interfaces that are found on a motherboard. This model was made by Foxconn in 2007, and follows the ATX layout (known as the "form factor") usually employed for desktop computers. It is designed to work with AMD's Athlon 64 processor

Intel D945GCPE A microATX Motherboard LGA775 for Intel Pentium 4, D, XE, Dual-Core, Core 2 (circa 2007)
A motherboard (sometimes alternatively known as the mainboard, system board, baseboard, planar board or logic board,[1] or colloquially, a mobo) is the main printed circuit board (PCB) found in general purpose microcomputers and other expandable systems. It holds and allows communication between many of the crucial electronic components of a system, such as the central processing unit (CPU) and memory, and provides connectors for other peripherals. Unlike a backplane, a motherboard usually contains significant sub-systems such as the central processor, the chipset's input/output and memory controllers, interface connectors, and other components integrated for general purpose use.

Motherboard specifically refers to a PCB with expansion capability and as the name suggests, this board is often referred to as the "mother" of all components attached to it, which often include peripherals, interface cards, and daughtercards: sound cards, video cards, network cards, hard drives, or other forms of persistent storage; TV tuner cards, cards providing extra USB or FireWire slots and a variety of other custom components.

Similarly, the term mainboard is applied to devices with a single board and no additional expansions or capability, such as controlling boards in laser printers, televisions, washing machines and other embedded systems with limited expansion abilities.

Contents  [hide] 
1 History
2 Design
2.1 Form factor
2.2 CPU sockets
2.3 Integrated peripherals
2.4 Peripheral card slots
2.5 Temperature and reliability
2.6 Air pollution and reliability
3 Bootstrapping using the Basic input output system
4 See also
5 References
6 External links
History[edit]
Prior to the invention of the microprocessor, a digital computer consisted of multiple printed circuit boards in a card-cage case with components connected by a backplane, a set of interconnected sockets. In very old designs, copper wires were the discrete connections between card connector pins, but printed circuit boards soon became the standard practice. The Central Processing Unit (CPU), memory, and peripherals were housed on individual printed circuit boards, which were plugged into the backplate. The ubiquitous S-100 bus of the 1970s is an example of this type of backplane system.

The most popular computers of the 1980s such as the Apple II and IBM PC had published schematic diagrams and other documentation which permitted rapid reverse-engineering and third-party replacement motherboards. Usually intended for building new computers compatible with the exemplars, many motherboards offered additional performance or other features and were used to upgrade the manufacturer's original equipment.

During the late 1980s and 1990s, it became economical to move an increasing number of peripheral functions onto the motherboard. In the late 1980s, personal computer motherboards began to include single ICs (also called Super I/O chips) capable of supporting a set of low-speed peripherals: keyboard, mouse, floppy disk drive, serial ports, and parallel ports. By the late 1990s, many personal computer motherboards included consumer grade embedded audio, video, storage, and networking functions without the need for any expansion cards at all; higher-end systems for 3D gaming and computer graphics typically retained only the graphics card as a separate component. Business PCs, workstations, and servers were more likely to need expansion cards, either for more robust functions, or for higher speeds; those systems often had fewer embedded components.

Laptop and notebook computers that were developed in the 1990s integrated the most common peripherals. This even included motherboards with no upgradeable components, a trend that would continue as smaller systems were introduced after the turn of the century (like the tablet computer and the netbook). Memory, processors, network controllers, power source, and storage would be integrated into some systems.

Design[edit]

The Octek Jaguar V motherboard from 1993.[2] This board has few onboard peripherals, as evidenced by the 6 slots provided for ISA cards and the lack of other built-in external interface connectors. Note the large AT keyboard connector at the back right is its only peripheral interface.

The motherboard of a Samsung Galaxy SII; almost all functions of the device are integrated into a very small board
A motherboard provides the electrical connections by which the other components of the system communicate. Unlike a backplane, it also contains the central processing unit and hosts other subsystems and devices.

A typical desktop computer has its microprocessor, main memory, and other essential components connected to the motherboard. Other components such as external storage, controllers for video display and sound, and peripheral devices may be attached to the motherboard as plug-in cards or via cables; in modern microcomputers it is increasingly common to integrate some of these peripherals into the motherboard itself.

An important component of a motherboard is the microprocessor's supporting chipset, which provides the supporting interfaces between the CPU and the various buses and external components. This chipset determines, to an extent, the features and capabilities of the motherboard.

Modern motherboards include:

Sockets (or slots) in which one or more microprocessors may be installed. In the case of CPUs in ball grid array packages, such as the VIA C3, the CPU is directly soldered to the motherboard.[3]
Slots into which the system's main memory is to be installed (typically in the form of DIMM modules containing DRAM chips)
A chipset which forms an interface between the CPU's front-side bus, main memory, and peripheral buses
Non-volatile memory chips (usually Flash ROM in modern motherboards) containing the system's firmware or BIOS
A clock generator which produces the system clock signal to synchronize the various components
Slots for expansion cards (the interface to the system via the buses supported by the chipset)
Power connectors, which receive electrical power from the computer power supply and distribute it to the CPU, chipset, main memory, and expansion cards. As of 2007, some graphics cards (e.g. GeForce 8 and Radeon R600) require more power than the motherboard can provide, and thus dedicated connectors have been introduced to attach them directly to the power supply.[4]
Connectors for hard drives, typically SATA only. Disk drives also connect to the power supply.
Additionally, nearly all motherboards include logic and connectors to support commonly used input devices, such as USB for mouse devices and keyboards. Early personal computers such as the Apple II or IBM PC included only this minimal peripheral support on the motherboard. Occasionally video interface hardware was also integrated into the motherboard; for example, on the Apple II and rarely on IBM-compatible computers such as the IBM PC Jr. Additional peripherals such as disk controllers and serial ports were provided as expansion cards.

Given the high thermal design power of high-speed computer CPUs and components, modern motherboards nearly always include heat sinks and mounting points for fans to dissipate excess heat.

Form factor[edit]
Main article: Comparison of computer form factors
Motherboards are produced in a variety of sizes and shapes called computer form factor, some of which are specific to individual computer manufacturers. However, the motherboards used in IBM-compatible systems are designed to fit various case sizes. As of 2007, most desktop computer motherboards use the ATX standard form factor   even those found in Macintosh and Sun computers, which have not been built from commodity components. A case's motherboard and PSU form factor must all match, though some smaller form factor motherboards of the same family will fit larger cases. For example, an ATX case will usually accommodate a microATX motherboard.

Laptop computers generally use highly integrated, miniaturized and customized motherboards. This is one of the reasons that laptop computers are difficult to upgrade and expensive to repair. Often the failure of one laptop component requires the replacement of the entire motherboard, which is usually more expensive than a desktop motherboard due to the large number of integrated components and their custom shape and size.

CPU sockets[edit]
A CPU socket (central processing unit) or slot is an electrical component that attaches to a Printed Circuit Board (PCB) and is designed to house a CPU (also called a microprocessor). It is a special type of integrated circuit socket designed for very high pin counts. A CPU socket provides many functions, including a physical structure to support the CPU, support for a heat sink, facilitating replacement (as well as reducing cost), and most importantly, forming an electrical interface both with the CPU and the PCB. CPU sockets on the motherboard can most often be found in most desktop and server computers (laptops typically use surface mount CPUs), particularly those based on the Intel x86 architecture. A CPU socket type and motherboard chipset must support the CPU series and speed.

Integrated peripherals[edit]

Block diagram of a modern motherboard, which supports many on-board peripheral functions as well as several expansion slots
With the steadily declining costs and size of integrated circuits, it is now possible to include support for many peripherals on the motherboard. By combining many functions on one PCB, the physical size and total cost of the system may be reduced; highly integrated motherboards are thus especially popular in small form factor and budget computers.

Disk controllers for a floppy disk drive, up to 2 PATA drives, and up to 6 SATA drives (including RAID 0/1 support)
integrated graphics controller supporting 2D and 3D graphics, with VGA and TV output
integrated sound card supporting 8-channel (7.1) audio and S/PDIF output
Fast Ethernet network controller for 10/100 Mbit networking
USB 2.0 controller supporting up to 12 USB ports
IrDA controller for infrared data communication (e.g. with an IrDA-enabled cellular phone or printer)
Temperature, voltage, and fan-speed sensors that allow software to monitor the health of computer components.
Peripheral card slots[edit]
A typical motherboard will have a different number of connections depending on its standard and form factor.

A standard, modern ATX motherboard will typically have two or three PCI-Express 16x connection for a graphics card, one or two legacy PCI slots for various expansion cards, and one or two PCI-E 1x (which has superseded PCI). A standard EATX motherboard will have two to four PCI-E 16x connection for graphics cards, and a varying number of PCI and PCI-E 1x slots. It can sometimes also have a PCI-E 4x slot (will vary between brands and models).

Some motherboards have two or more PCI-E 16x slots, to allow more than 2 monitors without special hardware, or use a special graphics technology called SLI (for Nvidia) and Crossfire (for AMD). These allow 2 to 4 graphics cards to be linked together, to allow better performance in intensive graphical computing tasks, such as gaming, video editing, etc.

Temperature and reliability[edit]

A motherboard of a Vaio E series laptop (right)

A microATX motherboard with some faulty capacitors
Main article: Computer cooling
Motherboards are generally air cooled with heat sinks often mounted on larger chips, such as the Northbridge, in modern motherboards.[5] Insufficient or improper cooling can cause damage to the internal components of the computer, or cause it to crash. Passive cooling, or a single fan mounted on the power supply, was sufficient for many desktop computer CPU's until the late 1990s; since then, most have required CPU fans mounted on their heat sinks, due to rising clock speeds and power consumption. Most motherboards have connectors for additional case fans and integrated temperature sensors to detect motherboard and CPU temperatures and controllable fan connectors which the BIOS or operating system can use to regulate fan speed.[6] Alternatively computers can use a water cooling system instead of many fans.

Some small form factor computers and home theater PCs designed for quiet and energy-efficient operation boast fan-less designs. This typically requires the use of a low-power CPU, as well as careful layout of the motherboard and other components to allow for heat sink placement.

A 2003 study found that some spurious computer crashes and general reliability issues, ranging from screen image distortions to I/O read/write errors, can be attributed not to software or peripheral hardware but to aging capacitors on PC motherboards.[7] Ultimately this was shown to be the result of a faulty electrolyte formulation,[8] an issue termed capacitor plague.

Motherboards use electrolytic capacitors to filter the DC power distributed around the board. These capacitors age at a temperature-dependent rate, as their water based electrolytes slowly evaporate. This can lead to loss of capacitance and subsequent motherboard malfunctions due to voltage instabilities. While most capacitors are rated for 2000 hours of operation at 105  C (221  F),[9] their expected design life roughly doubles for every 10  C (50  F) below this. At 45  C (113  F) a lifetime of 15 years can be expected. This appears reasonable for a computer motherboard. However, many manufacturers deliver substandard capacitors,[10] which significantly reduce life expectancy. Inadequate case cooling and elevated temperatures easily exacerbate this problem. It is possible, but time-consuming, to find and replace failed capacitors on personal computer motherboards.

Air pollution and reliability[edit]
High rates of motherboard failures in China and India appear to be due to "sulfurous air pollution produced by coal" that's burned to generate electricity. Air pollution corrodes the circuitry, according to Intel researchers.[11]

Bootstrapping using the Basic input output system[edit]
Motherboards contain some non-volatile memory to initialize the system and load some startup software, usually an operating system, from some external peripheral device. Microcomputers such as the Apple II and IBM PC used ROM chips mounted in sockets on the motherboard. At power-up, the central processor would load its program counter with the address of the boot ROM and start executing instructions from the ROM. These instructions initialized and tested the system hardware, displayed system information on the screen, performed RAM checks, and then loaded an initial program from an external or peripheral device. If none was available, then the computer would perform tasks from other memory stores or display an error message, depending on the model and design of the computer and the ROM version. For example, both the Apple II and the original IBM PC had Microsoft Cassette BASIC in ROM and would start that if no program could be loaded from disk.

Most modern motherboard designs use a BIOS, stored in an EEPROM chip soldered to or socketed on the motherboard, to booting an operating system. Non-operating system boot programs are still supported on modern IBM PC-descended machines, but nowadays it is assumed that the boot program will be a complex operating system such as Microsoft Windows or Linux. When power is first supplied to the motherboard, the BIOS firmware tests and configures memory, circuitry, and peripherals. This Power-On Self Test (POST) may include testing some of the following things:

Video adapter
Cards inserted into slots, such as conventional PCI
Floppy drive
Temperatures, voltages, and fan speeds for hardware monitoring
CMOS used to store BIOS setup configuration
Keyboard and Mouse
Network controller
Optical drives: CD-ROM or DVD-ROM
SCSI hard drive
IDE, EIDE, or Serial ATA Hard disk drive
Security devices, such as a fingerprint reader or the state of a latching switch to detect intrusion
USB devices, such as a memory storage device
On recent motherboards the BIOS may also patch the central processor microcode if the BIOS detects that the installed CPU is one for which errata have been published.

See also[edit]
Accelerated Graphics Port
Computer case screws
CMOS battery
Daughterboard
List of computer hardware manufacturers
Memory Reference Code   the part of the BIOS which handles memory timings on Intel motherboards
Overclocking
Single-board computer
Switched-mode power supply applications
Symmetric multiprocessing
Static random-access memory
From Wikipedia, the free encyclopedia
Not to be confused with SDRAM or "Synchronous DRAM".

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2010)

A static RAM chip from a NES clone (2K x 8 bit)
Computer memory types
Volatile
RAM
DRAM (e.g., DDR SDRAM) SRAM
In development

T-RAM Z-RAM
Historical
Williams Kilburn tube (1946 47) Delay line memory (1947) Selectron tube (1953) Dekatron
Non-volatile
ROM
Mask ROM PROM EPROM EEPROM
NVRAM
Flash memory Solid-state storage
Early stage NVRAM
nvSRAM FeRAM MRAM PRAM
Mechanical
Magnetic tape Hard disk drive Optical disc drive
In development
3D XPoint CBRAM SONOS RRAM Racetrack memory NRAM Millipede memory FJG RAM
Historical
Paper data storage (1725) Drum memory (1932) Magnetic-core memory (1949) Plated wire memory (1957) Core rope memory (1960s) Thin-film memory (1962) Twistor memory (~1968) Bubble memory (~1970)
v t e
Static random-access memory (static RAM or SRAM) is type of semiconductor memory that uses bistable latching circuitry (flip-flop) to store each bit. SRAM exhibits data remanence,[1] but it is still volatile in the conventional sense that data is eventually lost when the memory is not powered.

The term static differentiates SRAM from DRAM (dynamic random-access memory) which must be periodically refreshed. SRAM is faster and more expensive than DRAM; it is typically used for CPU cache while DRAM is used for a computer's main memory.

Contents  [hide] 
1 Applications and uses
1.1 Characteristics
1.1.1 Clock rate and power
1.1.2 Embedded use
1.1.3 In computers
1.1.4 Hobbyists
2 Types of SRAM
2.1 Non-volatile SRAM
2.2 By transistor type
2.3 By function
2.4 By feature
2.5 By flip-flop type
3 Design
4 SRAM operation
4.1 Bus behavior
5 See also
6 References
Applications and uses[edit]

SRAM cells on the die of a STM32F103VGT6 microcontroller as seen by a scanning electron microscope. Manufactured by STMicroelectronics using a 180 nanometre process.

Comparison image of 180 nanometre SRAM cells on a STM32F103VGT6 microcontroller as seen by an optical microscope.
Characteristics[edit]
SRAM is more expensive and less dense than DRAM and is therefore not used for high-capacity, low-cost applications such as the main memory in personal computers.

Clock rate and power[edit]
The power consumption of SRAM varies widely depending on how frequently it is accessed; it can be as power-hungry as dynamic RAM, when used at high frequencies, and some ICs can consume many watts at full bandwidth. On the other hand, static RAM used at a somewhat slower pace, such as in applications with moderately clocked microprocessors, draws very little power and can have a nearly negligible power consumption when sitting idle   in the region of a few micro-watts. Several techniques have been proposed to manage power consumption of SRAM-based memory structures.[2]

Static RAM exists primarily as:

general purpose products
with asynchronous interface, such as the ubiquitous 28-pin 8K   8 and 32K   8 chips (often but not always named something along the lines of 6264 and 62C256 respectively), as well as similar products up to 16 Mbit per chip
with synchronous interface, usually used for caches and other applications requiring burst transfers, up to 18 Mbit (256K   72) per chip
integrated on chip
as RAM or cache memory in micro-controllers (usually from around 32 bytes up to 128 kilobytes)
as the primary caches in powerful microprocessors, such as the x86 family, and many others (from 8 KB, up to many megabytes)
to store the registers and parts of the state-machines used in some microprocessors (see register file)
on application specific ICs, or ASICs (usually in the order of kilobytes)
in FPGAs and CPLDs
Embedded use[edit]
Many categories of industrial and scientific subsystems, automotive electronics, and similar, contain static RAM.
Some amount (kilobytes or less) is also embedded in practically all modern appliances, toys, etc. that implement an electronic user interface.
Several megabytes may be used in complex products such as digital cameras, cell phones, synthesizers, etc.
SRAM in its dual-ported form is sometimes used for realtime digital signal processing circuits.[citation needed]

In computers[edit]
SRAM is also used in personal computers, workstations, routers and peripheral equipment: CPU register files, internal CPU caches and external burst mode SRAM caches, hard disk buffers, router buffers, etc. LCD screens and printers also normally employ static RAM to hold the image displayed (or to be printed). Static RAM was used for the main memory of some early personal computers such as the ZX80, TRS-80 Model 100 and Commodore VIC-20.

Hobbyists[edit]
Hobbyists, specifically homebuilt processor enthusiasts,[3] often prefer SRAM due to the ease of interfacing. It is much easier to work with than DRAM as there are no refresh cycles and the address and data buses are directly accessible rather than multiplexed. In addition to buses and power connections, SRAM usually requires only three controls: Chip Enable (CE), Write Enable (WE) and Output Enable (OE). In synchronous SRAM, Clock (CLK) is also included.[citation needed]

Types of SRAM[edit]
Non-volatile SRAM[edit]
Non-volatile SRAMs, or nvSRAMs, have standard SRAM functionality, but they save the data when the power supply is lost, ensuring preservation of critical information. nvSRAMs are used in a wide range of situations networking, aerospace, and medical, among many others[4]  where the preservation of data is critical and where batteries are impractical.

By transistor type[edit]
Bipolar junction transistor (used in TTL and ECL)   very fast but consumes a lot of power
MOSFET (used in CMOS)   low power and very common today
By function[edit]
Asynchronous   independent of clock frequency; data in and data out are controlled by address transition
Synchronous   all timings are initiated by the clock edge(s). Address, data in and other control signals are associated with the clock signals
In 1990s asynchronous SRAM used to be employed for fast access time. Asynchronous SRAM was used as main memory for small cache-less embedded processors used in everything from industrial electronics and measurement systems to hard disks and networking equipment, among many other applications. Nowadays, synchronous SRAM (e.g. DDR SRAM) is rather employed similarly like Synchronous DRAM - DDR SDRAM memory is rather used than asynchronous DRAM (Dynamic random-access memory). Synchronous memory interface is much faster as access time can be significantly reduced by employing pipeline architecture. Furthermore as DRAM is much cheaper than SRAM, SRAM is often replaced by DRAM, especially in the case when large volume of data is required. SRAM memory is however much faster for random (not block / burst) access. Therefore SRAM memory is mainly used for CPU cache, small on-chip memory, FIFOs or other small buffers.

By feature[edit]
ZBT (ZBT stands for zero bus turnaround)   the turnaround is the number of clock cycles it takes to change access to the SRAM from write to read and vice versa. The turnaround for ZBT SRAMs or the latency between read and write cycle is zero.
syncBurst (syncBurst SRAM or synchronous-burst SRAM)   features synchronous burst write access to the SRAM to increase write operation to the SRAM
DDR SRAM   Synchronous, single read/write port, double data rate I/O
Quad Data Rate SRAM   Synchronous, separate read and write ports, quadruple data rate I/O
By flip-flop type[edit]
Binary SRAM
Ternary SRAM
Design[edit]

A six-transistor CMOS SRAM cell.
A typical SRAM cell is made up of six MOSFETs. Each bit in an SRAM is stored on four transistors (M1, M2, M3, M4) that form two cross-coupled inverters. This storage cell has two stable states which are used to denote 0 and 1. Two additional access transistors serve to control the access to a storage cell during read and write operations. In addition to such six-transistor (6T) SRAM, other kinds of SRAM chips use 4, 8, 10 (4T, 8T, 10T SRAM), or more transistors per bit.[5][6][7] Four-transistor SRAM is quite common in stand-alone SRAM devices (as opposed to SRAM used for CPU caches), implemented in special processes with an extra layer of polysilicon, allowing for very high-resistance pull-up resistors.[8] The principal drawback of using 4T SRAM is increased static power due to the constant current flow through one of the pull-down transistors.


Four transistor SRAM provides advantages in density at the cost of manufacturing complexity. The resistors must have small dimensions and large values.
This is sometimes used to implement more than one (read and/or write) port, which may be useful in certain types of video memory and register files implemented with multi-ported SRAM circuitry.

Generally, the fewer transistors needed per cell, the smaller each cell can be. Since the cost of processing a silicon wafer is relatively fixed, using smaller cells and so packing more bits on one wafer reduces the cost per bit of memory.

Memory cells that use fewer than four transistors are possible   but, such 3T[9][10] or 1T cells are DRAM, not SRAM (even the so-called 1T-SRAM).

Access to the cell is enabled by the word line (WL in figure) which controls the two access transistors M5 and M6 which, in turn, control whether the cell should be connected to the bit lines: BL and BL. They are used to transfer data for both read and write operations. Although it is not strictly necessary to have two bit lines, both the signal and its inverse are typically provided in order to improve noise margins.

During read accesses, the bit lines are actively driven high and low by the inverters in the SRAM cell. This improves SRAM bandwidth compared to DRAMs   in a DRAM, the bit line is connected to storage capacitors and charge sharing causes the bitline to swing upwards or downwards. The symmetric structure of SRAMs also allows for differential signaling, which makes small voltage swings more easily detectable. Another difference with DRAM that contributes to making SRAM faster is that commercial chips accept all address bits at a time. By comparison, commodity DRAMs have the address multiplexed in two halves, i.e. higher bits followed by lower bits, over the same package pins in order to keep their size and cost down.

The size of an SRAM with m address lines and n data lines is 2m words, or 2m   n bits. The most common word size is 8 bits, meaning that a single byte can be read or written to each of 2m different words within the SRAM chip. Several common SRAM chips have 11 address lines (thus a capacity of 2m = 2,048 = 2k words) and an 8-bit word, so they are referred to as "2k   8 SRAM".

SRAM operation[edit]
An SRAM cell has three different states: standby (the circuit is idle), reading (the data has been requested) or writing (updating the contents). SRAM operating in read mode and write modes should have "readability" and "write stability", respectively. The three different states work as follows:

Standby
If the word line is not asserted, the access transistors M5 and M6 disconnect the cell from the bit lines. The two cross-coupled inverters formed by M1   M4 will continue to reinforce each other as long as they are connected to the supply.
Reading
In theory, reading only requires asserting the word line WL and reading the SRAM cell state by a single access transistor and bit line, e.g. M6, BL. Nevertheless bit lines are relatively long and have large parasitic capacitance. To speed-up reading, a more complex process is used in practice: The read cycle is started by precharging both bit lines BL and BL, i.e. driving the bit lines to a threshold voltage (midrange voltage between logical 1 and 0) by an external module (not shown in the figures). Then asserting the word line WL, enabling both the access transistors M5 and M6 which causes the bit line BL voltage to either slightly drop (bottom NMOS transistor M3 is ON and top PMOS transistor M4 is off) or rise (top PMOS transistor M4 is on). It should be noted that if BL voltage rises, the BL voltage drops, and vice versa. Then the BL and BL lines will have a small voltage difference between them. A sense amplifier will sense which line has the higher voltage and thus determine whether there was 1 or 0 stored. The higher the sensitivity of the sense amplifier, the faster the read operation.
Writing
The write cycle begins by applying the value to be written to the bit lines. If we wish to write a 0, we would apply a 0 to the bit lines, i.e. setting BL to 1 and BL to 0. This is similar to applying a reset pulse to an SR-latch, which causes the flip flop to change state. A 1 is written by inverting the values of the bit lines. WL is then asserted and the value that is to be stored is latched in. This works because the bit line input-drivers are designed to be much stronger than the relatively weak transistors in the cell itself so they can easily override the previous state of the cross-coupled inverters. In practice, access NMOS transistors M5 and M6 have to be stronger than either bottom NMOS (M1, M3) or top PMOS (M2, M4) transistors. This is easily obtained as PMOS transistors are much weaker than NMOS when same sized. Consequently when one transistor pair (e.g. M3 and M4) is only slightly overriden by the write process, the opposite transistors pair (M1 and M2) gate voltage is also changed. This means that the M1 and M2 transistors can be easier overriden, and so on. Thus, cross-coupled inverters magnify the writing process.
Bus behavior[edit]
RAM with an access time of 70 ns will output valid data within 70 ns from the time that the address lines are valid. But the data will remain for a hold time as well (5 10 ns). Rise and fall times also influence valid timeslots with approximately 5 ns. By reading the lower part of an address range, bits in sequence (page cycle) one can read with significantly shorter access time (30 ns).[11]
Dynamic random-access memory
From Wikipedia, the free encyclopedia
"DRAM" redirects here. For other uses, see Dram (disambiguation).
Computer memory types
Volatile
RAM
DRAM (e.g., DDR SDRAM) SRAM
In development

T-RAM Z-RAM
Historical
Williams Kilburn tube (1946 47) Delay line memory (1947) Selectron tube (1953) Dekatron
Non-volatile
ROM
Mask ROM PROM EPROM EEPROM
NVRAM
Flash memory Solid-state storage
Early stage NVRAM
nvSRAM FeRAM MRAM PRAM
Mechanical
Magnetic tape Hard disk drive Optical disc drive
In development
3D XPoint CBRAM SONOS RRAM Racetrack memory NRAM Millipede memory FJG RAM
Historical
Paper data storage (1725) Drum memory (1932) Magnetic-core memory (1949) Plated wire memory (1957) Core rope memory (1960s) Thin-film memory (1962) Twistor memory (~1968) Bubble memory (~1970)
v t e

A die photograph of the Micron Technology MT4C1024 DRAM integrated circuit. It has a capacity of 1 megabit equalient of 2^20 bits or 128 kB. [1]
Dynamic random-access memory (DRAM) is a type of random-access memory that stores each bit of data in a separate capacitor within an integrated circuit. The capacitor can be either charged or discharged; these two states are taken to represent the two values of a bit, conventionally called 0 and 1. Since even "nonconducting" transistors always leak a small amount, the capacitors will slowly discharge, and the information eventually fades unless the capacitor charge is refreshed periodically. Because of this refresh requirement, it is a dynamic memory as opposed to static random-access memory (SRAM) and other static types of memory. Unlike flash memory, DRAM is volatile memory (vs. non-volatile memory), since it loses its data quickly when power is removed.

DRAM is widely used in digital electronics where low-cost and high-capacity memory is required. One of the largest applications for DRAM is the main memory (colloquially called the "RAM") in modern computers; and as the main memories of components used in these computers such as graphics cards (where the "main memory" is called the graphics memory). In contrast, SRAM, which is faster and more expensive than DRAM, is typically used where speed is of greater concern than cost, such as the cache memories in processors.

The advantage of DRAM is its structural simplicity: only one transistor and a capacitor are required per bit, compared to four or six transistors in SRAM. This allows DRAM to reach very high densities. The transistors and capacitors used are extremely small; billions can fit on a single memory chip. Due to the dynamic nature of its memory cells, DRAM consumes relatively large amounts of power, with different ways for managing the power consumption.[2]

Contents  [hide] 
1 History
2 Principles of operation
2.1 Operations to read a data bit from a DRAM storage cell
2.2 To write to memory
2.3 Refresh rate
2.4 Memory timing
2.4.1 Timing abbreviations
3 DRAM cells
3.1 Capacitor design
3.2 Historical cell designs
3.3 Proposed cell designs
4 DRAM array structures
4.1 Bitline architecture
4.1.1 Open bitline arrays
4.1.2 Folded bitline arrays
4.1.3 Future array architectures
4.2 Row and column redundancy
5 Embedded DRAM (eDRAM)
6 Error detection and correction
7 Security
8 Packaging
8.1 General DRAM formats
8.2 Common DRAM modules
8.3 Memory size of a DRAM module
9 Versions
9.1 Asynchronous DRAM
9.1.1 Principles of operation
9.1.1.1 RAS Only Refresh (ROR)
9.1.1.2 CAS before RAS refresh (CBR)
9.1.1.3 Hidden refresh
9.1.2 Page mode DRAM
9.1.2.1 Extended data out DRAM (EDO DRAM)
9.1.3 Burst EDO DRAM (BEDO DRAM)
9.2 Synchronous dynamic RAM (SDRAM)
9.2.1 Single data rate synchronous DRAM (SDR SDRAM)
9.2.2 Double data rate synchronous DRAM (DDR SDRAM)
9.2.3 Direct Rambus DRAM (DRDRAM)
9.2.4 Reduced Latency DRAM (RLDRAM)
9.3 Graphics RAM
9.3.1 Video DRAM (VRAM)
9.3.2 Window DRAM (WRAM)
9.3.3 Multibank DRAM (MDRAM)
9.3.4 Synchronous graphics RAM (SGRAM)
9.3.5 Graphics double data rate SDRAM (GDDR SDRAM)
9.4 Pseudostatic RAM (PSRAM)
10  See also
11  Notes
12  References
13  External links
History[edit]

A schematic drawing depicting the cross-section of the original one-transistor, one-capacitor NMOS DRAM cell. It was patented in 1968.
The cryptanalytic machine code-named "Aquarius" used at Bletchley Park during World War II incorporated a hard-wired dynamic memory. Paper tape was read and the characters on it "were remembered in a dynamic store. ... The store used a large bank of capacitors, which were either charged or not, a charged capacitor representing cross (1) and an uncharged capacitor dot (0). Since the charge gradually leaked away, a periodic pulse was applied to top up those still charged (hence the term 'dynamic')".[3]

In 1964, Arnold Farber and Eugene Schlig, working for IBM, created a hard-wired memory cell, using a transistor gate and tunnel diode latch. They replaced the latch with two transistors and two resistors, a configuration that became known as the Farber-Schlig cell. In 1965, Benjamin Agusta and his team at IBM created a 16-bit silicon memory chip based on the Farber-Schlig cell, with 80 transistors, 64 resistors, and 4 diodes. In 1966, DRAM was invented by Dr. Robert Dennard at the IBM Thomas J. Watson Research Center. He was granted U.S. patent number 3,387,286 in 1968. Capacitors had been used for earlier memory schemes such as the drum of the Atanasoff Berry Computer, the Williams tube and the Selectron tube.

The Toshiba "Toscal" BC-1411 electronic calculator, which was introduced in November 1966,[4] used a form of DRAM built from discrete components.[5]

In 1969 Honeywell asked Intel to make a DRAM using a three-transistor cell that they had developed. This became the Intel 1102[6] in early 1970. However, the 1102 had many problems, prompting Intel to begin work on their own improved design, in secrecy to avoid conflict with Honeywell. This became the first commercially available DRAM, the Intel 1103, in October 1970, despite initial problems with low yield until the fifth revision of the masks. The 1103 was designed by Joel Karp and laid out by Pat Earhart. The masks were cut by Barbara Maness and Judy Garcia.[7]

The first DRAM with multiplexed row and column address lines was the Mostek MK4096 4 Kbit DRAM designed by Robert Proebsting and introduced in 1973. This addressing scheme uses the same address pins to receive the low half and the high half of the address of the memory cell being referenced, switching between the two halves on alternating bus cycles. This was a radical advance, effectively halving the number of address lines required, which enabled it to fit into packages with fewer pins, a cost advantage that grew with every jump in memory size. The MK4096 proved to be a very robust design for customer applications. At the 16 Kbit density, the cost advantage increased; the 16 Kbit Mostek MK4116 DRAM, introduced in 1976, achieved greater than 75% worldwide DRAM market share. However, as density increased to 64 Kbit in the early 1980s, Mostek was overtaken by Japanese DRAM manufacturers selling higher quality DRAMs using the same multiplexing scheme at below-cost prices.[citation needed] This generated friction between Japan and the United States.

Principles of operation[edit]

The principles of operation for reading a simple 4 by 4 DRAM array.

Basic structure of a DRAM cell array.
DRAM is usually arranged in a rectangular array of charge storage cells consisting of one capacitor and transistor per data bit. The figure to the right shows a simple example with a four-by-four cell matrix. Some DRAM matrices are many thousands of cells in height and width.[8][9]

The long horizontal lines connecting each row are known as word-lines. Each column of cells is composed of two bit-lines, each connected to every other storage cell in the column (the illustration to the right does not include this important detail). They are generally known as the "+" and " " bit lines.

Operations to read a data bit from a DRAM storage cell[edit]
The sense amplifiers are disconnected.[10]
The bit-lines are precharged to exactly equal voltages that are in between high and low logic levels (e.g., 0.5 V if the two levels are 0 and 1 V). The bit-lines are physically symmetrical to keep the capacitance equal, and therefore at this time their voltages are equal.[10]
The precharge circuit is switched off. Because the bit-lines are relatively long, they have enough capacitance to maintain the precharged voltage for a brief time. This is an example of dynamic logic.[10]
The desired row's word-line is then driven high to connect a cell's storage capacitor to its bit-line. This causes the transistor to conduct, transferring charge from the storage cell to the connected bit-line (if the stored value is 1) or from the connected bit-line to the storage cell (if the stored value is 0). Since the capacitance of the bit-line is typically much higher than the capacitance of the storage cell, the voltage on the bit-line increases very slightly if the storage cell's capacitor is discharged and decreases very slightly if the storage cell is charged (e.g., 0.54 and 0.45 V in the two cases). As the other bit-line holds 0.50 V there is a small voltage difference between the two twisted bit-lines.[10]
The sense amplifiers are now connected to the bit-lines pairs. Positive feedback then occurs from the cross-connected inverters, thereby amplifying the small voltage difference between the odd and even row bit-lines of a particular column until one bit line is fully at the lowest voltage and the other is at the maximum high voltage. Once this has happened, the row is "open" (the desired cell data is available).[10]
All storage cells in the open row are sensed simultaneously, and the sense amplifier outputs latched. A column address then selects which latch bit to connect to the external data bus. Reads of different columns in the same row can be performed without a row opening delay because, for the open row, all data has already been sensed and latched.[10]
While reading of columns in an open row is occurring, current is flowing back up the bit-lines from the output of the sense amplifiers and recharging the storage cells. This reinforces (i.e. "refreshes") the charge in the storage cell by increasing the voltage in the storage capacitor if it was charged to begin with, or by keeping it discharged if it was empty. Note that due to the length of the bit-lines there is a fairly long propagation delay for the charge to be transferred back to the cell's capacitor. This takes significant time past the end of sense amplification, and thus overlaps with one or more column reads.[10]
When done with reading all the columns in the current open row, the word-line is switched off to disconnect the storage cell capacitors (the row is "closed") from the bit-lines. The sense amplifier is switched off, and the bit-lines are precharged again.[10]
To write to memory[edit]
To store data, a row is opened and a given column's sense amplifier is temporarily forced to the desired high or low voltage state, thus causing the bit-line to charge or discharge the cell storage capacitor to the desired value. Due to the sense amplifier's positive feedback configuration, it will hold a bit-line at stable voltage even after the forcing voltage is removed. During a write to a particular cell, all the columns in a row are sensed simultaneously just as during reading, so although only a single column's storage-cell capacitor charge is changed, the entire row is refreshed (written back in), as illustrated in the figure to the right.[10]


Writing to a DRAM cell.
Refresh rate[edit]
Main article: Memory refresh
See also:   Security
Typically, manufacturers specify that each row must be refreshed every 64 ms or less, as defined by the JEDEC standard.

Some systems refresh every row in a burst of activity involving all rows every 64 ms. Other systems refresh one row at a time staggered throughout the 64 ms interval. For example, a system with 213 = 8,192 rows would require a staggered refresh rate of one row every 7.8  s which is 64 ms divided by 8,192 rows. A few real-time systems refresh a portion of memory at a time determined by an external timer function that governs the operation of the rest of a system, such as the vertical blanking interval that occurs every 10 20 ms in video equipment.

The row address of the row that will be refreshed next is maintained by external logic or a counter within the DRAM. A system that provides the row address (and the refresh command) does so to have greater control over when to refresh and which row to refresh. This is done to minimize conflicts with memory accesses, since such a system has both knowledge of the memory access patterns and the refresh requirements of the DRAM. When the row address is supplied by a counter within the DRAM, the system relinquishes control over which row is refreshed and only provides the refresh command. Some modern DRAMs are capable of self-refresh; no external logic is required to instruct the DRAM to refresh or to provide a row address.

Under some conditions, most of the data in DRAM can be recovered even if the DRAM has not been refreshed for several minutes.[11]

Memory timing[edit]
Main article: Memory timings
Many parameters are required to fully describe the timing of DRAM operation. Here are some examples for two timing grades of asynchronous DRAM, from a data sheet published in 1998:[12]

"50 ns" "60 ns" Description
tRC 84 ns 104 ns  Random read or write cycle time (from one full /RAS cycle to another)
tRAC  50 ns 60 ns Access time: /RAS low to valid data out
tRCD  11 ns 14 ns /RAS low to /CAS low time
tRAS  50 ns 60 ns /RAS pulse width (minimum /RAS low time)
tRP 30 ns 40 ns /RAS precharge time (minimum /RAS high time)
tPC 20 ns 25 ns Page-mode read or write cycle time (/CAS to /CAS)
tAA 25 ns 30 ns Access time: Column address valid to valid data out (includes address setup time before /CAS low)
tCAC  13 ns 15 ns Access time: /CAS low to valid data out
tCAS  8 ns  10 ns /CAS low pulse width minimum
Thus, the generally quoted number is the /RAS access time. This is the time to read a random bit from a precharged DRAM array. The time to read additional bits from an open page is much less.

When such a RAM is accessed by clocked logic, the times are generally rounded up to the nearest clock cycle. For example, when accessed by a 100 MHz state machine (i.e. a 10 ns clock), the 50 ns DRAM can perform the first read in five clock cycles, and additional reads within the same page every two clock cycles. This was generally described as "5 2 2 2" timing, as bursts of four reads within a page were common.

When describing synchronous memory, timing is described by clock cycle counts separated by hyphens. These numbers represent tCL tRCD tRP tRAS in multiples of the DRAM clock cycle time. Note that this is half of the data transfer rate when double data rate signaling is used. JEDEC standard PC3200 timing is 3 4 4 8[13] with a 200 MHz clock, while premium-priced high performance PC3200 DDR DRAM DIMM might be operated at 2 2 2 5 timing.[14]

PC-3200 (DDR-400) PC2-6400 (DDR2-800) PC3-12800 (DDR3-1600) Description
Typical Fast  Typical Fast  Typical Fast
cycles  time  cycles  time  cycles  time  cycles  time  cycles  time  cycles  time
tCL 3 15 ns 2 10 ns 5 12.5 ns 4 10 ns 9 11.25 ns  8 10 ns /CAS low to valid data out (equivalent to tCAC)
tRCD  4 20 ns 2 10 ns 5 12.5 ns 4 10 ns 9 11.25 ns  8 10 ns /RAS low to /CAS low time
tRP 4 20 ns 2 10 ns 5 12.5 ns 4 10 ns 9 11.25 ns  8 10 ns /RAS precharge time (minimum precharge to active time)
tRAS  8 40 ns 5 25 ns 16  40 ns 12  30 ns 27  33.75 ns  24  30 ns Row active time (minimum active to precharge time)
...Minimum random access time has improved from tRAC = 50 ns to tRCD + tCL = 22.5 ns, and even the premium 20 ns variety is only 2.5 times better compared to the typical case (~2.22 times better). CAS latency has improved even less, from tCAC = 13 ns to 10 ns. However, the DDR3 memory does achieve 32 times higher bandwidth; due to internal pipelining and wide data paths, it can output two words every 1.25 ns (1600 Mword/s), while the EDO DRAM can output one word per tPC = 20 ns (50 Mword/s).

Timing abbreviations[edit]
tCL   CAS latency
tCR   Command rate
tPTP   precharge to precharge delay
tRAS   RAS active time
tRCD   RAS to CAS delay
tREF   Refresh period
tRFC   Row refresh cycle time
tRP   RAS precharge
tRRD   RAS to RAS delay
tRTP   Read to precharge delay
tRTR   Read to read delay
tRTW   Read to write delay
tWR   Write recovery time
tWTP   Write to precharge delay
tWTR   Write to read delay
tWTW   Write to write delay
DRAM cells[edit]
Each bit of data in a DRAM is stored as a positive or negative electrical charge in a capacitive structure. The structure providing the capacitance, as well as the transistors that control access to it, is collectively referred to as a DRAM cell. They are the fundamental building block in DRAM arrays. Multiple DRAM memory cell variants exist, but the most commonly used variant in modern DRAMs is the one-transistor, one-capacitor (1T1C) cell. The transistor is used to admit current into the capacitor during writes, and to discharge the capacitor during reads. The access transistor is designed to maximize drive strength and minimize transistor-transistor leakage (Kenner, pg. 34).

The capacitor has two terminals, one of which is connected to its access transistor, and the other to either ground or VCC/2. In modern DRAMs, the latter case is more common, since it allows faster operation. In modern DRAMs, a voltage of +VCC/2 across the capacitor is required to store a logic one; and a voltage of -VCC/2 across the capacitor is required to store a logic zero. The electrical charge stored in the capacitor is measured in coulombs. For a logic one, the charge is: {\textstyle Q = {V_{CC} \over 2} \cdot C}, where Q is the charge in coulombs and C is the capacitance in farads. A logic zero has a charge of: {\textstyle Q = {-V_{CC} \over 2} \cdot C}.[15]

Reading or writing a logic one requires the wordline is driven to a voltage greater than the sum of VCC and the access transistor's threshold voltage (VTH). This voltage is called VCC pumped (VCCP). The time required to discharge a capacitor thus depends on what logic value is stored in the capacitor. A capacitor containing logic one begins to discharge when the voltage at the access transistor's gate terminal is above VCCP. If the capacitor contains a logic zero, it begins to discharge when the gate terminal voltage is above VTH.[16]

Capacitor design[edit]
Up until the mid-1980s, the capacitors in DRAM cells were co-planar with the access transistor (they were constructed on the surface of the substrate), thus they were referred to as planar capacitors. The drive to increase both density, and to a lesser extent, performance, required denser designs. This was strongly motivated by economics; a major consideration for DRAM devices, especially commodity DRAMs. The minimization of DRAM cell area can produce a denser device (which could be sold at a higher price), or a lower priced device with the same capacity. Starting in the mid-1980s, the capacitor has been moved above or below the silicon substrate in order to meet these objectives. DRAM cells featuring capacitors above the substrate are referred to as stacked or folded plate capacitors; whereas those with capacitors buried beneath the substrate surface are referred to as trench capacitors. In the 2000s, manufacturers were sharply divided by the type of capacitor used by their DRAMs, and the relative cost and long-term scalability of both designs has been the subject of extensive debate. The majority of DRAMs, from major manufactures such as Hynix, Micron Technology, Samsung Electronics use the stacked capacitor structure, whereas smaller manufacturers such Nanya Technology use the trench capacitor structure (Jacob, pp. 355 357).

The capacitor in the stacked capacitor scheme is constructed above the surface of the substrate. The capacitor is constructed from an oxide-nitride-oxide (ONO) dielectric sandwiched in between two layers of polysilicon plates (the top plate is shared by all DRAM cells in an IC), and its shape can be a rectangle, a cylinder, or some other more complex shape. There are two basic variations of the stacked capacitor, based on its location relative to the bitline capacitor-over-bitline (COB) and capacitor-under-bitline (CUB). In a former variation, the capacitor is underneath the bitline, which is usually made of metal, and the bitline has a polysilicon contact that extends downwards to connect it to the access transistor's source terminal. In the latter variation, the capacitor is constructed above the bitline, which is almost always made of polysilicon, but is otherwise identical to the COB variation. The advantage the COB variant possesses is the ease of fabricating the contact between the bitline and the access transistor's source as it is physically close to the substrate surface. However, this requires the active area to be laid out at a 45-degree angle when viewed from above, which makes it difficult to ensure that the capacitor contact does not touch the bitline. CUB cells avoid this, but suffer from difficulties in inserting contacts in between bitlines, since the size of features this close to the surface are at or near the minimum feature size of the process technology (Kenner, pp. 33 42).

The trench capacitor is constructed by etching a deep hole into the silicon substrate. The substrate volume surrounding the hole is then heavily doped to produce a buried n+ plate and to reduce resistance. A layer of oxide-nitride-oxide dielectric is grown or deposited, and finally the hole is filled by depositing doped polysilicon, which forms the top plate of the capacitor. The top the capacitor is connected to the access transistor's drain terminal via a polysilicon strap (Kenner, pp. 42&ndash44). A trench capacitor's depth-to-width ratio in DRAMs of the mid-2000s can exceed 50:1 (Jacob, p. 357).

Trench capacitors have numerous advantages. Since the capacitor is buried in the bulk of the substrate instead of lying on its surface, the area it occupies can be minimized to what is required to connect it to the access transistor's drain terminal without decreasing the capacitor's size, and thus capacitance (Jacob, pp. 356 357). Alternatively, the capacitance can be increased by etching a deeper hole without any increase to surface area (Kenner, pg. 44). Another advantage of the trench capacitor is that its structure is under the layers of metal interconnect, allowing them to be more easily made planar, which enables it to be integrated in a logic-optimized process technology, which have many levels of interconnect above the substrate. The fact that the capacitor is under the logic means that it is constructed before the transistors are. This allows high-temperature processes to fabricate the capacitors, which would otherwise be degrading the logic transistors and their performance. This makes trench capacitors suitable for constructing embedded DRAM (eDRAM) (Jacob, p. 357). Disadvantages of trench capacitors are difficulties in reliably constructing the capacitor's structures within deep holes and in connecting the capacitor to the access transistor's drain terminal (Kenner, pg. 44).

Historical cell designs[edit]
First-generation DRAM ICs (those with capacities of 1 Kbit), of which the first was the Intel 1103, used a three-transistor, one-capacitor (3T1C) DRAM cell. By the second-generation, the requirement to increase density by fitting more bits in a given area, or the requirement to reduce cost by fitting the same amount of bits in a smaller area, lead to the almost universal adaptation of the 1T1C DRAM cell, although a couple of devices with 4 and 16 Kbit capacities continued to use the 3T1C cell for performance reasons (Kenner, p. 6). These performance advantages included, most significantly, the ability to read the state stored by the capacitor without discharging it, avoiding the need to write back what was read out (non-destructive read). A second performance advantage relates to the 3T1C cell has separate transistors for reading and writing; the memory controller can exploit this feature to perform atomic read-modify-writes, where a value is read, modified, and then written back as a single, indivisible operation (Jacob, p. 459).

Proposed cell designs[edit]
The drive to increase density and performance has lead to the one-transistor, zero-capacitor (1T) DRAM cell being a topic of research since the late-1990s. In 1T DRAM cells, there is a transistor for controlling access to a capacitive region used to store the bit of data, but this capacitance is not provided by a separate capacitor. Instead, the parasitic body capacitor inherent in silicon-on-insulator (SOI) transistors is used instead. Subsequently, 1T DRAM cells have the greatest density, and can be easily integrated with logic since they are constructed from the same SOI process technologies used for high-performance logic. A key difference from 1T1C DRAMs is that reads in 1T DRAM are non-destructive; the stored charge causes a detectable shift in the threshold voltage of the transistor. Refresh, however, is still required.[17] Performance-wise, access times are significantly better than capacitor-based DRAMs, but slightly worse than SRAM. Examples of such DRAMs include A-RAM and Z-RAM.

DRAM array structures[edit]
DRAM cells are laid out in a regular rectangular, grid-like pattern to facilitate their control and access via wordlines and bitlines. The physical layout of the DRAM cells in an array is typically designed so that two adjacent DRAM cells in a column share a single bitline contact to reduce their area. DRAM cell area is given as n F2, where n is a number derived from the DRAM cell design, and F is the smallest feature size of a given process technology. This scheme permits comparison of DRAM size over different process technology generations, as DRAM cell area scales at linear or near-linear rates over. The typical area for modern DRAM cells varies between 6 8 F2.

The horizontal wire, the wordline, is connected to the gate terminal of every access transistor in its row. The vertical bitline is connected to the source terminal of the transistors in its a column. The lengths of the wordlines and bitlines are limited. The wordline length is limited by the desired performance of the array, since propagation time of the signal that must transverse the wordline is determined by the RC time constant. The bitline length is limited by its capacitance (which increases with length), which must be kept within a range for proper sensing (as DRAMs operate by sensing the charge of the capacitor released onto the bitline). Bitline length is also limited by the amount of operating current the DRAM can draw and by how power can be dissipated, since these two characteristics are largely determined by the charging and discharging of the bitline.

Bitline architecture[edit]
Sense amplifiers are required to read the state contained in the DRAM cells. When the access transistor is activated, the electrical charge in the capacitor is shared with the bitline. The bitline's capacitance is much greater than that of the capacitor (approximately ten times). Thus, the change in bitline voltage is minute. Sense amplifiers are required to resolve the voltage differential into the levels specified by the logic signaling system. Modern DRAMs use differential sense amplifiers, and are accompanied by requirements as to how the DRAM arrays are constructed. Differential sense amplifiers work by driving their outputs to opposing extremes based on the relative voltages on pairs of bitlines. The sense amplifiers function effectively and efficient only if the capacitance and voltages of these bitline pairs are closely matched. Besides ensuring that the lengths of the bitlines and the number of attached DRAM cells attached to them are equal, two basic architectures to array design have emerged to provide for the requirements of the sense amplifiers: open and folded bitline arrays.

Open bitline arrays[edit]
The first generation (1 Kbit) DRAM ICs, up until the 64 Kbit generation (and some 256 Kbit generation devices) had open bitline array architectures. In these architectures, the bitlines are divided into multiple segments, and the differential sense amplifiers are placed in between bitline segments. Because the sense amplifiers are placed between bitline segments, to route their outputs outside the array, an additional layer of interconnect placed above those used to construct the wordlines and bitlines is required.

The DRAM cells that are on the edges of the array do not have adjacent segments. Since the differential sense amplifiers require identical capacitance and bitline lengths from both segments, dummy bitline segments are provided. The advantage of the open bitline array is a smaller array area, although this advantage is slightly diminished by the dummy bitline segments. The disadvantage that caused the near disappearance of this architecture is the inherent vulnerability to noise, which affects the effectiveness of the differential sense amplifiers. Since each bitline segment does not have any spatial relationship to the other, it is likely that noise would affect only one of the two bitline segments.

Folded bitline arrays[edit]
The folded bitline array architecture routes bitlines in pairs throughout the array. The close proximity of the paired bitlines provide superior common-mode noise rejection characteristics over open bitline arrays. The folded bitline array architecture began appearing in DRAM ICs during the mid-1980s, beginning with the 256 Kbit generation. This architecture is favored in modern DRAM ICs for its superior noise immunity.

This architecture is referred to as folded because it takes its basis from the open array architecture from the perspective of the circuit schematic. The folded array architecture appears to remove DRAM cells in alternate pairs (because two DRAM cells share a single bitline contact) from a column, then move the DRAM cells from an adjacent column into the voids.

The location where the bitline twists occupies additional area. To minimize area overhead, engineers select the simplest and most area-minimal twisting scheme that is able to reduce noise under the specified limit. As process technology improves to reduce minimum feature sizes, the signal to noise problem worsens, since coupling between adjacent metal wires is inversely proportional to their pitch. The array folding and bitline twisting schemes that are used must increase in complexity in order to maintain sufficient noise reduction. Schemes that have desirable noise immunity characteristics for a minimal impact in area is the topic of current research (Kenner, p. 37).

Future array architectures[edit]
Advances in process technology could result in open bitline array architectures being favored if it is able to offer better long-term area efficiencies; since folded array architectures require increasingly complex folding schemes to match any advance in process technology. The relationship between process technology, array architecture, and area efficiency is an active area of research.

Row and column redundancy[edit]
The first DRAM ICs did not have any redundancy. An IC with a defective DRAM cell would be discarded. Beginning with the 64 Kbit generation, DRAM arrays have included spare rows and columns to improve yields. Spare rows and columns provide tolerance of minor fabrication defects which have caused a small number of rows or columns to be inoperable. The defective rows and columns are physically disconnected from the rest of the array by a triggering a programmable fuse or by cutting the wire by a laser. The spare rows or columns are substituted in by remapping logic in the row and column decoders (Jacob, pp. 358 361).

Embedded DRAM (eDRAM)[edit]
Main article: EDRAM
DRAM that is integrated into an integrated circuit designed in a logic-optimized process, such as an application-specific integrated circuit (ASIC) or a microprocessor, is called embedded DRAM (eDRAM). Embedded DRAM requires DRAM cell designs that can be fabricated without preventing the fabrication of fast-switching transistors used in high-performance logic, and modification of the basic logic-optimized process technology to accommodate the process steps required to build DRAM cell structures.

Error detection and correction[edit]
Main articles: RAM parity and ECC memory
Electrical or magnetic interference inside a computer system can cause a single bit of DRAM to spontaneously flip to the opposite state. The majority of one-off ("soft") errors in DRAM chips occur as a result of background radiation, chiefly neutrons from cosmic ray secondaries, which may change the contents of one or more memory cells or interfere with the circuitry used to read/write them. Recent studies give widely varying error rates for single event upsets with over seven orders of magnitude difference, ranging from roughly one bit error, per hour, per gigabyte of memory to one bit error, per century, per gigabyte of memory.[18][19][20]

The problem can be mitigated by using redundant memory bits and additional circuitry that use these bits to detect and correct soft errors. In most cases, the detection and correction logic is performed by the memory controller; sometimes, the required logic is transparently implemented within DRAM chips or modules, enabling the ECC memory functionality for otherwise ECC-incapable systems.[21] The extra memory bits are used to record parity and to enable missing data to be reconstructed by error-correcting code (ECC). Parity allows the detection of all single-bit errors (actually, any odd number of wrong bits). The most common error-correcting code, a SECDED Hamming code, allows a single-bit error to be corrected and, in the usual configuration, with an extra parity bit, double-bit errors to be detected.

Recent studies give widely varying error rates with over seven orders of magnitude difference, ranging from 10 10 10 17 error/bit h, roughly one bit error, per hour, per gigabyte of memory to one bit error, per century, per gigabyte of memory.[18][19][20] The Schroeder et al. 2009 study reported a 32% chance that a given computer in their study would suffer from at least one correctable error per year, and provided evidence that most such errors are intermittent hard rather than soft errors.[22] A 2010 study at the University of Rochester also gave evidence that a substantial fraction of memory errors are intermittent hard errors.[23] Large scale studies on non-ECC main memory in PCs and laptops suggest that undetected memory errors account for a substantial number of system failures: the study reported a 1-in-1700 chance per 1.5% of memory tested (extrapolating to an approximately 26% chance for total memory) that a computer would have a memory error every eight months.[24]

Security[edit]
Main article: Data remanence
Although dynamic memory is only specified and guaranteed to retain its contents when supplied with power and refreshed every short period of time (often 64 ms), the memory cell capacitors often retain their values for significantly longer, particularly at low temperatures.[25] Under some conditions most of the data in DRAM can be recovered even if it has not been refreshed for several minutes.[26]

This property can be used to circumvent security and recover data stored in the main memory that is assumed to be destroyed at power-down. The computer could be quickly rebooted, and the contents of the main memory read out; or by removing a computer's memory modules, cooling them to prolong data remanence, then transferring them to a different computer to be read out. Such an attack was demonstrated to circumvent popular disk encryption systems, such as the open source TrueCrypt, Microsoft's BitLocker Drive Encryption, and Apple's FileVault.[25] This type of attack against a computer is often called a cold boot attack.

Packaging[edit]

It has been suggested that this section be merged into Memory module. (Discuss) Proposed since January 2016.
For economic reasons, the large (main) memories found in personal computers, workstations, and non-handheld game-consoles (such as PlayStation and Xbox) normally consist of dynamic RAM (DRAM). Other parts of the computer, such as cache memories and data buffers in hard disks,[citation needed] normally use static RAM (SRAM). However, since SRAM has high leakage power and low density, die-stacked DRAM has recently been used for designing multi-megabyte sized processor caches.[27]

Physically, most DRAM is packaged in black epoxy resin.

General DRAM formats[edit]

A 256 k x 4 bit 20-pin DIP DRAM on an early PC memory card (k = 1024), usually Industry Standard Architecture

Common DRAM packages. From top to bottom: DIP, SIPP, SIMM (30-pin), SIMM (72-pin), DIMM (168-pin), DDR DIMM (184-pin).

Two 8 GB DDR4-2133 288-pin ECC 1.2 V RDIMMs
Dynamic random access memory is produced as integrated circuits (ICs) bonded and mounted into plastic packages with metal pins for connection to control signals and buses. In early use individual DRAM ICs were usually either installed directly to the motherboard or on ISA expansion cards; later they were assembled into multi-chip plug-in modules (DIMMs, SIMMs, etc.). Some standard module types are:

DRAM chip (Integrated Circuit or IC)
Dual in-line Package (DIP)
DRAM (memory) modules
Single In-line Pin Package (SIPP)
Single In-line Memory Module (SIMM)
Dual In-line Memory Module (DIMM)
Rambus In-line Memory Module (RIMM), technically DIMMs but called RIMMs due to their proprietary slot.
Small outline DIMM (SO-DIMM), about half the size of regular DIMMs, are mostly used in notebooks, small footprint PCs (such as Mini-ITX motherboards), upgradable office printers and networking hardware like routers.
Small outline RIMM (SO-RIMM). Smaller version of the RIMM, used in laptops. Technically SO-DIMMs but called SO-RIMMs due to their proprietary slot.
Stacked vs. non-stacked RAM modules
Stacked RAM modules contain two or more RAM chips stacked on top of each other. This allows large modules to be manufactured using cheaper low density wafers. Stacked chip modules draw more power, and tend to run hotter than non-stacked modules. Stacked modules can be packaged using the older TSOP or the newer BGA style IC chips. Silicon dies connected with older wire bonding or newer TSV.
Several proposed stacked RAM approaches exist, with TSV and much wider interfaces, including Wide I/O, Wide I/O 2, Hybrid Memory Cube and High Bandwidth Memory.
Common DRAM modules[edit]
Common DRAM packages as illustrated to the right, from top to bottom (last three types are not present in the group picture, and the last type is available in a separate picture):

DIP 16-pin (DRAM chip, usually pre-fast page mode DRAM (FPRAM))
SIPP 30-pin (usually FPRAM)
SIMM 30-pin (usually FPRAM)
SIMM 72-pin (often extended data out DRAM (EDO DRAM) but FPRAM is not uncommon)
DIMM 168-pin (most SDRAM but were some extended data out DRAM (EDO DRAM))
DIMM 184-pin (DDR SDRAM)
RIMM 184-pin (RDRAM)
DIMM 240-pin (DDR2 SDRAM and DDR3 SDRAM)
DIMM 288-pin (DDR4 SDRAM)
Common SO-DIMM DRAM modules:

72-pin (32-bit)
144-pin (64-bit) used for SO-DIMM SDRAM
200-pin (72-bit) used for SO-DIMM DDR SDRAM and SO-DIMM DDR2 SDRAM
204-pin (64-bit) used for SO-DIMM DDR3 SDRAM
260-pin used for SO-DIMM DDR4 SDRAM
Memory size of a DRAM module[edit]
The exact number of bytes in a DRAM module is always an integral power of two. A 512 MB (as marked on a module) SDRAM DIMM, actually contains 512 MiB (mebibytes) = 512   220 bytes = 229 bytes = 536,870,912 bytes exactly, and might be made of 8 or 9 SDRAM chips, each containing exactly 512 Mib (mebibits) of storage, and each one contributing 8 bits to the DIMM's 64- or 72- bit width. For comparison, a 2 GB SDRAM module contains 2 GiB (gibibytes) = 2   230 bytes = 231 bytes = 2,147,483,648 bytes of memory, exactly. The module usually has 8 SDRAM chips of 256 MiB each.

Versions[edit]
While the fundamental DRAM cell and array has maintained the same basic structure (and performance) for many years, there have been many different interfaces for communicating with DRAM chips. When one speaks about "DRAM types", one is generally referring to the interface that is used.

DRAM can be divided into asynchronous and synchronous DRAM. In addition, graphics DRAM is specially designed for graphics tasks, and can be asynchronous or synchronous DRAM in nature. Pseudostatic RAM (PSRAM) have an architecture and interface that closely mimics the operation and interface of static RAM. Lastly, 1T DRAM uses a capacitorless design, as opposed to the usual 1T/1C (one transistor/one capacitor) designs of conventional DRAM.

Asynchronous DRAM[edit]
Principles of operation[edit]
An asynchronous DRAM chip has power connections, some number of address inputs (typically 12), and a few (typically one or four) bidirectional data lines. There are four active-low control signals:

RAS, the Row Address Strobe. The address inputs are captured on the falling edge of RAS, and select a row to open. The row is held open as long as RAS is low.
CAS, the Column Address Strobe. The address inputs are captured on the falling edge of CAS, and select a column from the currently open row to read or write.
WE, Write Enable. This signal determines whether a given falling edge of CAS is a read (if high) or write (if low). If low, the data inputs are also captured on the falling edge of CAS.
OE, Output Enable. This is an additional signal that controls output to the data I/O pins. The data pins are driven by the DRAM chip if RAS and CAS are low, WE is high, and OE is low. In many applications, OE can be permanently connected low (output always enabled), but it can be useful when connecting multiple memory chips in parallel.
This interface provides direct control of internal timing. When RAS is driven low, a CAS cycle must not be attempted until the sense amplifiers have sensed the memory state, and RAS must not be returned high until the storage cells have been refreshed. When RAS is driven high, it must be held high long enough for precharging to complete.

Although the DRAM is asynchronous, the signals are typically generated by a clocked memory controller, which limits their timing to multiples of the controller's clock cycle.

RAS Only Refresh (ROR)[edit]
Classic asynchronous DRAM is refreshed by opening each row in turn.

The refresh cycles are distributed across the entire refresh interval in such a way that all rows are refreshed within the required interval. To refresh one row of the memory array using RAS Only Refresh, the following steps must occur:

The row address of the row to be refreshed must be applied at the address input pins.
RAS must switch from high to low. CAS must remain high.
At the end of the required amount of time, RAS must return high.
This can be done by supplying a row address and pulsing RAS low; it is not necessary to perform any CAS cycles. An external counter is needed to iterate over the row addresses in turn.[28]

CAS before RAS refresh (CBR)[edit]
For convenience, the counter was quickly incorporated into the DRAM chips themselves. If the CAS line is driven low before RAS (normally an illegal operation), then the DRAM ignores the address inputs and uses an internal counter to select the row to open. This is known as CAS-before-RAS (CBR) refresh. This became the standard form of refresh for asynchronous DRAM, and is the only form generally used with SDRAM.

Hidden refresh[edit]
Given support of CAS-before-RAS refresh, it is possible to deassert RAS while holding CAS low to maintain data output. If RAS is then asserted again, this performs a CBR refresh cycle while the DRAM outputs remain valid. Because data output is not interrupted, this is known as hidden refresh.[29]

Page mode DRAM[edit]
Page mode DRAM is a minor modification to the first-generation DRAM IC interface which improved the performance of reads and writes to a row by avoiding the inefficiency of precharging and opening the same row repeatedly to access a different column. In Page mode DRAM, after a row was opened by holding RAS low, the row could be kept open, and multiple reads or writes could be performed to any of the columns in the row. Each column access was initiated by asserting CAS and presenting a column address. For reads, after a delay (tCAC), valid data would appear on the data out pins, which were held at high-Z before the appearance of valid data. For writes, the write enable signal and write data would be presented along with the column address.[30]

Page mode DRAM was later improved with a small modification which further reduced latency. DRAMs with this improvement were called fast page mode DRAMs (FPM DRAMs). In page mode DRAM, CAS was asserted before the column address was supplied. In FPM DRAM, the column address could be supplied while CAS was still deasserted. The column address propagated through the column address data path, but did not output data on the data pins until CAS was asserted. Prior to CAS being asserted, the data out pins were held at high-Z. FPM DRAM reduced tCAC latency.[31]

Static column is a variant of fast page mode in which the column address does not need to be stored in, but rather, the address inputs may be changed with CAS held low, and the data output will be updated accordingly a few nanoseconds later.[31]

Nibble mode is another variant in which four sequential locations within the row can be accessed with four consecutive pulses of CAS. The difference from normal page mode is that the address inputs are not used for the second through fourth CAS edges; they are generated internally starting with the address supplied for the first CAS edge.[31]

Extended data out DRAM (EDO DRAM)[edit]

A pair of 32 MB EDO DRAM modules.
EDO DRAM, sometimes referred to as Hyper Page Mode enabled DRAM, is similar to Fast Page Mode DRAM with the additional feature that a new access cycle can be started while keeping the data output of the previous cycle active. This allows a certain amount of overlap in operation (pipelining), allowing somewhat improved performance. It was 5% faster than FPM DRAM, which it began to replace in 1995, when Intel introduced the 430FX chipset that supported EDO DRAM.

To be precise, EDO DRAM begins data output on the falling edge of CAS, but does not stop the output when CAS rises again. It holds the output valid (thus extending the data output time) until either RAS is deasserted, or a new CAS falling edge selects a different column address.

Single-cycle EDO has the ability to carry out a complete memory transaction in one clock cycle. Otherwise, each sequential RAM access within the same page takes two clock cycles instead of three, once the page has been selected. EDO's performance and capabilities allowed it to somewhat replace the then-slow L2 caches of PCs. It created an opportunity to reduce the immense performance loss associated with a lack of L2 cache, while making systems cheaper to build. This was also good for notebooks due to difficulties with their limited form factor, and battery life limitations. An EDO system with L2 cache was tangibly faster than the older FPM/L2 combination.

Single-cycle EDO DRAM became very popular on video cards towards the end of the 1990s. It was very low cost, yet nearly as efficient for performance as the far more costly VRAM.

Burst EDO DRAM (BEDO DRAM)[edit]
An evolution of EDO DRAM, Burst EDO DRAM, could process four memory addresses in one burst, for a maximum of 5 1 1 1, saving an additional three clocks over optimally designed EDO memory. It was done by adding an address counter on the chip to keep track of the next address. BEDO also added a pipeline stage allowing page-access cycle to be divided into two parts. During a memory-read operation, the first part accessed the data from the memory array to the output stage (second latch). The second part drove the data bus from this latch at the appropriate logic level. Since the data is already in the output buffer, quicker access time is achieved (up to 50% for large blocks of data) than with traditional EDO.

Although BEDO DRAM showed additional optimization over EDO, by the time it was available the market had made a significant investment towards synchronous DRAM, or SDRAM [2]. Even though BEDO RAM was superior to SDRAM in some ways, the latter technology quickly displaced BEDO.

Synchronous dynamic RAM (SDRAM)[edit]
Main article: Synchronous dynamic random-access memory
SDRAM significantly revises the asynchronous memory interface, adding a clock (and a clock enable) line. All other signals are received on the rising edge of the clock.

The /RAS and /CAS inputs no longer act as strobes, but are instead, along with /WE, part of a 3-bit command:

SDRAM Command summary
/CS /RAS  /CAS  /WE Address Command
H x x x x Command inhibit (No operation)
L H H H x No operation
L H H L x Burst Terminate: stop a read or write burst in progress
L H L H column  Read from currently active row
L H L L column  Write to currently active row
L L H H row Activate a row for read and write
L L H L x Precharge (deactivate) the current row
L L L H x Auto refresh: Refresh one row of each bank, using an internal counter
L L L L mode  Load mode register: Address bus specifies DRAM operation mode.
The /OE line's function is extended to a per-byte "DQM" signal, which controls data input (writes) in addition to data output (reads). This allows DRAM chips to be wider than 8 bits while still supporting byte-granularity writes.

Many timing parameters remain under the control of the DRAM controller. For example, a minimum time must elapse between a row being activated and a read or write command. One important parameter must be programmed into the SDRAM chip itself, namely the CAS latency. This is the number of clock cycles allowed for internal operations between a read command and the first data word appearing on the data bus. The "Load mode register" command is used to transfer this value to the SDRAM chip. Other configurable parameters include the length of read and write bursts, i.e. the number of words transferred per read or write command.

The most significant change, and the primary reason that SDRAM has supplanted asynchronous RAM, is the support for multiple internal banks inside the DRAM chip. Using a few bits of "bank address" which accompany each command, a second bank can be activated and begin reading data while a read from the first bank is in progress. By alternating banks, an SDRAM device can keep the data bus continuously busy, in a way that asynchronous DRAM cannot.

Single data rate synchronous DRAM (SDR SDRAM)[edit]
See also: SDR SDRAM
Single data rate SDRAM (sometimes known as SDR) is a synchronous form of DRAM.

Double data rate synchronous DRAM (DDR SDRAM)[edit]
Main articles: DDR SDRAM, DDR2 SDRAM, DDR3 SDRAM and DDR4 SDRAM
Double data rate SDRAM (DDR) was a later development of SDRAM, used in PC memory beginning in 2000. Subsequent versions are numbered sequentially (DDR2, DDR3, etc.). DDR SDRAM internally performs double-width accesses at the clock rate, and uses a double data rate interface to transfer one half on each clock edge. DDR2 and DDR3 increased this factor to 4  and 8 , respectively, delivering 4-word and 8-word bursts over 2 and 4 clock cycles, respectively. The internal access rate is mostly unchanged (200 million per second for DDR-400, DDR2-800 and DDR3-1600 memory), but each access transfers more data.

Direct Rambus DRAM (DRDRAM)[edit]
Main article: RDRAM
Direct RAMBUS DRAM (DRDRAM) was developed by Rambus.

Reduced Latency DRAM (RLDRAM)[edit]
Main article: RLDRAM
Reduced Latency DRAM is a high performance double data rate (DDR) SDRAM that combines fast, random access with high bandwidth, mainly intended for networking and caching applications.

Graphics RAM[edit]
These are asynchronous and synchronous DRAMs designed for graphics-related tasks such as texture memory and framebuffers, and can be found on video cards.

Video DRAM (VRAM)[edit]
Main article: VRAM
VRAM is a dual-ported variant of DRAM that was once commonly used to store the frame-buffer in some graphics adaptors.

Window DRAM (WRAM)[edit]
WRAM is a variant of VRAM that was once used in graphics adaptors such as the Matrox Millenium and ATI 3D Rage Pro. WRAM was designed to perform better and cost less than VRAM. WRAM offered up to 25% greater bandwidth than VRAM and accelerated commonly used graphical operations such as text drawing and block fills.[32]

Multibank DRAM (MDRAM)[edit]
Multibank DRAM is a type of specialized DRAM developed by MoSys. It is constructed from small memory banks of 256 KB, which are operated in an interleaved fashion, providing bandwidths suitable for graphics cards at a lower cost to memories such as SRAM. MDRAM also allows operations to two banks in a single clock cycle, permitting multiple concurrent accesses to occur if the accesses were independent. MDRAM was primarily used in graphic cards, such as those featuring the Tseng Labs ET6x00 chipsets. Boards based upon this chipset often had the unusual capacity of 2.25 MB because of MDRAM's ability to be implemented more easily with such capacities. A graphics card with 2.25 MB of MDRAM had enough memory to provide 24-bit color at a resolution of 1024 768 a very popular setting at the time.

Synchronous graphics RAM (SGRAM)[edit]

MoSys SGRAM
SGRAM is a specialized form of SDRAM for graphics adaptors. It adds functions such as bit masking (writing to a specified bit plane without affecting the others) and block write (filling a block of memory with a single colour). Unlike VRAM and WRAM, SGRAM is single-ported. However, it can open two memory pages at once, which simulates the dual-port nature of other video RAM technologies.

Graphics double data rate SDRAM (GDDR SDRAM)[edit]
Main article: GDDR
Graphics double data rate SDRAM (GDDR SDRAM) is a type of specialized DDR SDRAM designed to be used as the main memory of graphics processing units (GPUs). GDDR SDRAM is distinct from commodity types of DDR SDRAM such as DDR3, although they share some core technologies. Their primary characteristics are higher clock frequencies for both the DRAM core and I/O interface, which provides greater memory bandwidth for GPUs. As of 2015, there are four successive generations of GDDR: GDDR2, GDDR3, GDDR4, and GDDR5.

Pseudostatic RAM (PSRAM)[edit]
PSRAM or PSDRAM is dynamic RAM with built-in refresh and address-control circuitry to make it behave similarly to static RAM (SRAM). It combines the high density of DRAM with the ease of use of true SRAM. PSRAM (made by Numonyx) is used in the Apple iPhone and other embedded systems such as XFlar Platform.[33]

Some DRAM components have a "self-refresh mode". While this involves much of the same logic that is needed for pseudo-static operation, this mode is often equivalent to a standby mode. It is provided primarily to allow a system to suspend operation of its DRAM controller to save power without losing data stored in DRAM, rather not to allow operation without a separate DRAM controller as is the case with PSRAM.

An embedded variant of PSRAM is sold by MoSys under the name 1T-SRAM. It is technically DRAM, but behaves much like SRAM. It is used in Nintendo Gamecube and Wii video game consoles.

See also[edit]
Portal icon Electronics portal
Portal icon Information technology portal
DRAM price fixing
Flash memory
List of device bit rates
Memory bank
Memory geometry
Row hammer
Flash memory is an electronic (solid-state) non-volatile computer storage medium that can be electrically erased and reprogrammed.

Toshiba developed flash memory from EEPROM (electrically erasable programmable read-only memory) in the early 1980s and introduced it to the market in 1984. The two main types of flash memory are named after the NAND and NOR logic gates. The individual flash memory cells exhibit internal characteristics similar to those of the corresponding gates.

Whereas EPROMs had to be completely erased before being rewritten, NAND-type flash memory may be written and read in blocks (or pages) which are generally much smaller than the entire device. NOR-type flash allows a single machine word (byte) to be written   to an erased location   or read independently.

The NAND type operates primarily in memory cards, USB flash drives, solid-state drives (those produced in 2009 or later), and similar products, for general storage and transfer of data. NAND or NOR flash memory is also often used to store configuration data in numerous digital products, a task previously made possible by EEPROM or battery-powered static RAM. One key disadvantage of flash memory is that it can endure relatively small number of write cycles in a specific block.[1]

Example applications of both types of flash memory include personal computers, PDAs, digital audio players, digital cameras, mobile phones, synthesizers, video games, scientific instrumentation, industrial robotics, and medical electronics. In addition to being non-volatile, flash memory offers fast read access times, although not as fast as static RAM or ROM.[2] Its mechanical shock resistance helps explain its popularity over hard disks in portable devices, as does its high durability, ability to withstand high pressure, temperature and immersion in water, etc.[3]

Although flash memory is technically a type of EEPROM, the term "EEPROM" is generally used to refer specifically to non-flash EEPROM which is erasable in small blocks, typically bytes.[citation needed] Because erase cycles are slow, the large block sizes used in flash memory erasing give it a significant speed advantage over non-flash EEPROM when writing large amounts of data. As of 2013,[needs update ] flash memory cost much less than byte-programmable EEPROM and had become the dominant memory type wherever a system required a significant amount of non-volatile solid-state storage.

Contents  [hide] 
1 History
2 Principles of operation
2.1 Floating-gate transistor
2.2 NOR flash
2.3 NAND flash
2.4 Vertical NAND
3 Limitations
3.1 Block erasure
3.2 Memory wear
3.3 Read disturb
3.4 X-ray effects
4 Low-level access
4.1 NOR memories
4.2 NAND memories
4.3 Standardization
5 Distinction between NOR and NAND flash
5.1 Write endurance
6 Flash file systems
7 Capacity
8 Transfer rates
9 Applications
9.1 Serial flash
9.2 Flash memory as a replacement for hard drives
9.3 Flash memory as RAM
9.4 Archival or long-term storage
10  Industry
11  Flash scalability
12  See also
13  References
14  External links
History[edit]

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2010)
Flash memory (both NOR and NAND types) was invented by Dr. Fujio Masuoka while working for Toshiba circa 1980.[4][5] According to Toshiba, the name "flash" was suggested by Masuoka's colleague, Sh ji Ariizumi, because the erasure process of the memory contents reminded him of the flash of a camera.[6] Masuoka and colleagues presented the invention at the IEEE 1984 International Electron Devices Meeting (IEDM) held in San Francisco.[7]

Intel Corporation saw the massive potential of the invention and introduced the first commercial NOR type flash chip in 1988.[8] NOR-based flash has long erase and write times, but provides full address and data buses, allowing random access to any memory location. This makes it a suitable replacement for older read-only memory (ROM) chips, which are used to store program code that rarely needs to be updated, such as a computer's BIOS or the firmware of set-top boxes. Its endurance may be from as little as 100 erase cycles for an on-chip flash memory,[9] to a more typical 10,000 or 100,000 erase cycles, up to 1,000,000 erase cycles.[10] NOR-based flash was the basis of early flash-based removable media; CompactFlash was originally based on it, though later cards moved to less expensive NAND flash.

NAND flash has reduced erase and write times, and requires less chip area per cell, thus allowing greater storage density and lower cost per bit than NOR flash; it also has up to 10 times the endurance of NOR flash. However, the I/O interface of NAND flash does not provide a random-access external address bus. Rather, data must be read on a block-wise basis, with typical block sizes of hundreds to thousands of bits. This makes NAND flash unsuitable as a drop-in replacement for program ROM, since most microprocessors and microcontrollers require byte-level random access. In this regard, NAND flash is similar to other secondary data storage devices, such as hard disks and optical media, and is thus, highly suitable for use in mass-storage devices, such as memory cards. The first NAND-based removable media format was SmartMedia in 1995, and many others have followed, including:

MultiMediaCard
Secure Digital
Memory Stick, and xD-Picture Card.
A new generation of memory card formats, including RS-MMC, miniSD and microSD, feature extremely small form factors. For example, the microSD card has an area of just over 1.5 cm2, with a thickness of less than 1 mm. microSD capacities range from 64 MB to 200 GB, as of March 2015.[11]


A flash memory cell.
Principles of operation[edit]
Flash memory stores information in an array of memory cells made from floating-gate transistors. In single-level cell (SLC) devices, each cell stores only one bit of information. In multi-level cell (MLC) devices, including triple-level cell (TLC) devices, can store more than one bit per cell.

The floating gate may be conductive (typically polysilicon in most kinds of flash memory) or non-conductive (as in SONOS flash memory).[12]

Floating-gate transistor[edit]
Main article: Floating-gate MOSFET
In flash memory, each memory cell resembles a standard MOSFET, except that the transistor has two gates instead of one. On top is the control gate (CG), as in other MOS transistors, but below this there is a floating gate (FG) insulated all around by an oxide layer. The FG is interposed between the CG and the MOSFET channel. Because the FG is electrically isolated by its insulating layer, electrons placed on it are trapped until they are removed by another application of electric field (e.g. Applied voltage or UV as in EPROM). Counter-intuitively, placing electrons on the FG sets the transistor to the logical "0" state. Once the FG is charged, the electrons in it screen (partially cancel) the electric field from the CG, thus, increasing the threshold voltage (VT1) of the cell. This means that now a higher voltage(VT2) must be applied to the CG to make the channel conductive. In order to read a value from the transistor, an intermediate voltage between the threshold voltages (VT1 & VT2) is applied to the CG. If the channel conducts at this intermediate voltage, the FG must be uncharged (if it were charged, we would not get conduction because the intermediate voltage is less than VT2), and hence, a logical "1" is stored in the gate. If the channel does not conduct at the intermediate voltage, it indicates that the FG is charged, and hence, a logical "0" is stored in the gate. The presence of a logical "0" or "1" is sensed by determining whether there is current flowing through the transistor when the intermediate voltage is asserted on the CG. In a multi-level cell device, which stores more than one bit per cell, the amount of current flow is sensed (rather than simply its presence or absence), in order to determine more precisely the level of charge on the FG.

Internal charge pumps[edit]
Despite the need for high programming and erasing voltages, virtually all flash chips today require only a single supply voltage, and produce the high voltages using on-chip charge pumps.

Over half the energy used by a 1.8 V NAND flash chip is lost in the charge pump itself. Since boost converters are inherently more efficient than charge pumps, researchers developing low-power SSDs have proposed returning to the dual Vcc/Vpp supply voltages used on all the early flash chips, driving the high Vpp voltage for all flash chips in a SSD with a single shared external boost converter.[13][14][15][16][17][18][19][20]

In spacecraft and other high-radiation environments, the on-chip charge pump is the first part of the flash chip to fail, although flash memories will continue to work in read-only mode at much higher radiation levels.[21]

NOR flash[edit]
In NOR flash, each cell has one end connected directly to ground, and the other end connected directly to a bit line. This arrangement is called "NOR flash" because it acts like a NOR gate: when one of the word lines (connected to the cell's CG) is brought high, the corresponding storage transistor acts to pull the output bit line low. NOR flash continues to be the technology of choice for embedded applications requiring a discrete non-volatile memory device. The low read latencies characteristic of NOR devices allow for both direct code execution and data storage in a single memory product.[22]


NOR flash memory wiring and structure on silicon

Programming a NOR memory cell (setting it to logical 0), via hot-electron injection.

Erasing a NOR memory cell (setting it to logical 1), via quantum tunneling.
Programming[edit]
A single-level NOR flash cell in its default state is logically equivalent to a binary "1" value, because current will flow through the channel under application of an appropriate voltage to the control gate, so that the bitline voltage is pulled down. A NOR flash cell can be programmed, or set to a binary "0" value, by the following procedure:

an elevated on-voltage (typically >5 V) is applied to the CG
the channel is now turned on, so electrons can flow from the source to the drain (assuming an NMOS transistor)
the source-drain current is sufficiently high to cause some high energy electrons to jump through the insulating layer onto the FG, via a process called hot-electron injection.
Erasing[edit]
To erase a NOR flash cell (resetting it to the "1" state), a large voltage of the opposite polarity is applied between the CG and source terminal, pulling the electrons off the FG through quantum tunneling. Modern NOR flash memory chips are divided into erase segments (often called blocks or sectors). The erase operation can be performed only on a block-wise basis; all the cells in an erase segment must be erased together. Programming of NOR cells, however, generally can be performed one byte or word at a time.


NAND flash memory wiring and structure on silicon
NAND flash[edit]
NAND flash also uses floating-gate transistors, but they are connected in a way that resembles a NAND gate: several transistors are connected in series, and the bit line is pulled low only if all the word lines are pulled high (above the transistors' VT). These groups are then connected via some additional transistors to a NOR-style bit line array in the same way that single transistors are linked in NOR flash.

Compared to NOR flash, replacing single transistors with serial-linked groups adds an extra level of addressing. Whereas NOR flash might address memory by page then word, NAND flash might address it by page, word and bit. Bit-level addressing suits bit-serial applications (such as hard disk emulation), which access only one bit at a time. Execute-in-place applications, on the other hand, require every bit in a word to be accessed simultaneously. This requires word-level addressing. In any case, both bit and word addressing modes are possible with either NOR or NAND flash.

To read data, first the desired group is selected (in the same way that a single transistor is selected from a NOR array). Next, most of the word lines are pulled up above the VT of a programmed bit, while one of them is pulled up to just over the VT of an erased bit. The series group will conduct (and pull the bit line low) if the selected bit has not been programmed.

Despite the additional transistors, the reduction in ground wires and bit lines allows a denser layout and greater storage capacity per chip. (The ground wires and bit lines are actually much wider than the lines in the diagrams.) In addition, NAND flash is typically permitted to contain a certain number of faults (NOR flash, as is used for a BIOS ROM, is expected to be fault-free). Manufacturers try to maximize the amount of usable storage by shrinking the size of the transistors.

Writing and erasing[edit]
NAND flash uses tunnel injection for writing and tunnel release for erasing. NAND flash memory forms the core of the removable USB storage devices known as USB flash drives, as well as most memory card formats and solid-state drives available today.

Vertical NAND[edit]
Vertical NAND (V-NAND) memory stacks memory cells vertically and uses a charge trap flash architecture. The vertical layers allow larger areal bit densities without requiring smaller individual cells.[23]

Structure[edit]
V-NAND uses a charge trap flash geometry (pioneered in 2002 by AMD)[citation needed] that stores charge on an embedded silicon nitride film. Such a film is more robust against point defects and can be made thicker to hold larger numbers of electrons. V-NAND wraps a planar charge trap cell into a cylindrical form.[23]

An individual memory cell is made up of one planar polysilicon layer containing a hole filled by multiple concentric vertical cylinders. The hole's polysilicon surface acts as the gate electrode. The outermost silicon dioxide cylinder acts as the gate dielectric, enclosing a silicon nitride cylinder that stores charge, in turn enclosing a silicon dioxide cylinder as the tunnel dielectric that surrounds a central rod of conducting polysilicon which acts as the conducting channel.[23]

Memory cells in different vertical layers do not interfere with each other, as the charges cannot move vertically through the silicon nitride storage medium, and the electric fields associated with the gates are closely confined within each layer. The vertical collection is electrically identical to the serial-linked groups in which conventional NAND flash memory is configured.[23]

Construction[edit]
Growth of a group of V-NAND cells begins with an alternating stack of conducting (doped) polysilicon layers and insulating silicon dioxide layers.[23]

The next step is to form a cylindrical hole through these layers. In practice, a 128 Gibit V-NAND chip with 24 layers of memory cells requires about 2.9 billion such holes. Next the hole's inner surface receives multiple coatings, first silicon dioxide, then silicon nitride, then a second layer of silicon dioxide. Finally, the hole is filled with conducting (doped) polysilicon.[23]

Performance[edit]
As of 2013, V-NAND flash architecture allows read and write operations twice as fast as conventional NAND and can last up to 10 times as long, while consuming 50 percent less power. They offer comparable physical bit density using 10-nm lithography, but may be able to increase bit density by up to two orders of magnitude.[23]

Limitations[edit]
Block erasure[edit]
One limitation of flash memory is that, although it can be read or programmed a byte or a word at a time in a random access fashion, it can be erased only a block at a time. This generally sets all bits in the block to 1. Starting with a freshly erased block, any location within that block can be programmed. However, once a bit has been set to 0, only by erasing the entire block can it be changed back to 1. In other words, flash memory (specifically NOR flash) offers random-access read and programming operations, but does not offer arbitrary random-access rewrite or erase operations. A location can, however, be rewritten as long as the new value's 0 bits are a superset of the over-written values. For example, a nibble value may be erased to 1111, then written as 1110. Successive writes to that nibble can change it to 1010, then 0010, and finally 0000. Essentially, erasure sets all bits to 1, and programming can only clear bits to 0. File systems designed for flash devices can make use of this capability, for example, to represent sector metadata.

Although data structures in flash memory cannot be updated in completely general ways, this allows members to be "removed" by marking them as invalid. This technique may need to be modified for multi-level cell devices, where one memory cell holds more than one bit.

Common flash devices such as USB flash drives and memory cards provide only a block-level interface, or flash translation layer (FTL), which writes to a different cell each time to wear-level the device. This prevents incremental writing within a block; however, it does not help the device from being prematurely worn out by intensive write patterns.

Memory wear[edit]
Another limitation is that flash memory has a finite number of program erase cycles (typically written as P/E cycles). Most commercially available flash products are guaranteed to withstand around 100,000 P/E cycles before the wear begins to deteriorate the integrity of the storage.[24] Micron Technology and Sun Microsystems announced an SLC NAND flash memory chip rated for 1,000,000 P/E cycles on 17 December 2008.[25]

The guaranteed cycle count may apply only to block zero (as is the case with TSOP NAND devices), or to all blocks (as in NOR). This effect is mitigated in some chip firmware or file system drivers by counting the writes and dynamically remapping blocks in order to spread write operations between sectors; this technique is called wear leveling. Another approach is to perform write verification and remapping to spare sectors in case of write failure, a technique called bad block management (BBM). For portable consumer devices, these wearout management techniques typically extend the life of the flash memory beyond the life of the device itself, and some data loss may be acceptable in these applications. For high reliability data storage, however, it is not advisable to use flash memory that would have to go through a large number of programming cycles. This limitation is meaningless for 'read-only' applications such as thin clients and routers, which are programmed only once or at most a few times during their lifetimes.

In December 2012, Taiwanese engineers from Macronix revealed their intention to announce at the 2012 IEEE International Electron Devices Meeting that it has figured out how to improve NAND flash storage read/write cycles from 10,000 to 100 million cycles using a  self-healing  process that uses a flash chip with  onboard heaters that could anneal small groups of memory cells. [26] The built-in thermal annealing replaces the usual erase cycle with a local high temperature process that not only erases the stored charge, but also repairs the electron-induced stress in the chip, giving write cycles of at least 100 million.[27] The result is a chip that can be erased and rewritten over and over, even when it should theoretically break down. As promising as Macronix s breakthrough could be for the mobile industry, however, there are no plans for a commercial product to be released any time in the near future.[28]

Read disturb[edit]
The method used to read NAND flash memory can cause nearby cells in the same memory block to change over time (become programmed). This is known as read disturb. The threshold number of reads is generally in the hundreds of thousands of reads between intervening erase operations. If reading continually from one cell, that cell will not fail but rather one of the surrounding cells on a subsequent read. To avoid the read disturb problem the flash controller will typically count the total number of reads to a block since the last erase. When the count exceeds a target limit, the affected block is copied over to a new block, erased, then released to the block pool. The original block is as good as new after the erase. If the flash controller does not intervene in time, however, a read disturb error will occur with possible data loss if the errors are too numerous to correct with an error-correcting code.[29][30]

X-ray effects[edit]
Most flash ICs come in ball grid array (BGA) packages, and even the ones that do not are often mounted on a PCB next to other BGA packages. After PCB Assembly, boards with BGA packages are often X-rayed to see if the balls are making proper connections to the proper pad, or if the BGA needs rework. These X-rays can erase programmed bits in a flash chip (convert programmed "0" bits into erased "1" bits). Erased bits ("1" bits) are not affected by X-rays.[31][32]

Some manufacturers are now making X-ray proof SD[33] and USB[34] memory devices.

Low-level access[edit]
The low-level interface to flash memory chips differs from those of other memory types such as DRAM, ROM, and EEPROM, which support bit-alterability (both zero to one and one to zero) and random access via externally accessible address buses.

NOR memory has an external address bus for reading and programming. For NOR memory, reading and programming are random-access, and unlocking and erasing are block-wise. For NAND memory, reading and programming are page-wise, and unlocking and erasing are block-wise.

NOR memories[edit]
Reading from NOR flash is similar to reading from random-access memory, provided the address and data bus are mapped correctly. Because of this, most microprocessors can use NOR flash memory as execute in place (XIP) memory, meaning that programs stored in NOR flash can be executed directly from the NOR flash without needing to be copied into RAM first. NOR flash may be programmed in a random-access manner similar to reading. Programming changes bits from a logical one to a zero. Bits that are already zero are left unchanged. Erasure must happen a block at a time, and resets all the bits in the erased block back to one. Typical block sizes are 64, 128, or 256 KiB.

Bad block management is a relatively new feature in NOR chips. In older NOR devices not supporting bad block management, the software or device driver controlling the memory chip must correct for blocks that wear out, or the device will cease to work reliably.

The specific commands used to lock, unlock, program, or erase NOR memories differ for each manufacturer. To avoid needing unique driver software for every device made, special Common Flash Memory Interface (CFI) commands allow the device to identify itself and its critical operating parameters.

Besides its use as random-access ROM, NOR flash can also be used as a storage device, by taking advantage of random-access programming. Some devices offer read-while-write functionality so that code continues to execute even while a program or erase operation is occurring in the background. For sequential data writes, NOR flash chips typically have slow write speeds, compared with NAND flash.

Typical NOR flash does not need an error correcting code.[35]

NAND memories[edit]
NAND flash architecture was introduced by Toshiba in 1989.[36] These memories are accessed much like block devices, such as hard disks. Each block consists of a number of pages. The pages are typically 512[37] or 2,048 or 4,096 bytes in size. Associated with each page are a few bytes (typically 1/32 of the data size) that can be used for storage of an error correcting code (ECC) checksum.

Typical block sizes include:

32 pages of 512+16 bytes each for a block size of 16 KiB
64 pages of 2,048+64 bytes each for a block size of 128 KiB[38]
64 pages of 4,096+128 bytes each for a block size of 256 KiB[39]
128 pages of 4,096+128 bytes each for a block size of 512 KiB.
While reading and programming is performed on a page basis, erasure can only be performed on a block basis.[40]

NAND devices also require bad block management by the device driver software, or by a separate controller chip. SD cards, for example, include controller circuitry to perform bad block management and wear leveling. When a logical block is accessed by high-level software, it is mapped to a physical block by the device driver or controller. A number of blocks on the flash chip may be set aside for storing mapping tables to deal with bad blocks, or the system may simply check each block at power-up to create a bad block map in RAM. The overall memory capacity gradually shrinks as more blocks are marked as bad.

NAND relies on ECC to compensate for bits that may spontaneously fail during normal device operation. A typical ECC will correct a one-bit error in each 2048 bits (256 bytes) using 22 bits of ECC code, or a one-bit error in each 4096 bits (512 bytes) using 24 bits of ECC code.[41] If the ECC cannot correct the error during read, it may still detect the error. When doing erase or program operations, the device can detect blocks that fail to program or erase and mark them bad. The data is then written to a different, good block, and the bad block map is updated.

Hamming codes are the most commonly used ECC for SLC NAND flash. Reed-Solomon codes and Bose-Chaudhuri-Hocquenghem codes are commonly used ECC for MLC NAND flash. Some MLC NAND flash chips internally generate the appropriate BCH error correction codes. [35]

Most NAND devices are shipped from the factory with some bad blocks. These are typically marked according to a specified bad block marking strategy. By allowing some bad blocks, the manufacturers achieve far higher yields than would be possible if all blocks had to be verified good. This significantly reduces NAND flash costs and only slightly decreases the storage capacity of the parts.

When executing software from NAND memories, virtual memory strategies are often used: memory contents must first be paged or copied into memory-mapped RAM and executed there (leading to the common combination of NAND + RAM). A memory management unit (MMU) in the system is helpful, but this can also be accomplished with overlays. For this reason, some systems will use a combination of NOR and NAND memories, where a smaller NOR memory is used as software ROM and a larger NAND memory is partitioned with a file system for use as a non-volatile data storage area.

NAND sacrifices the random-access and execute-in-place advantages of NOR. NAND is best suited to systems requiring high capacity data storage. It offers higher densities, larger capacities, and lower cost. It has faster erases, sequential writes, and sequential reads.

Standardization[edit]
A group called the Open NAND Flash Interface Working Group (ONFI) has developed a standardized low-level interface for NAND flash chips. This allows interoperability between conforming NAND devices from different vendors. The ONFI specification version 1.0[42] was released on 28 December 2006. It specifies:

a standard physical interface (pinout) for NAND flash in TSOP-48, WSOP-48, LGA-52, and BGA-63 packages
a standard command set for reading, writing, and erasing NAND flash chips
a mechanism for self-identification (comparable to the serial presence detection feature of SDRAM memory modules)
The ONFI group is supported by major NAND flash manufacturers, including Hynix, Intel, Micron Technology, and Numonyx, as well as by major manufacturers of devices incorporating NAND flash chips.[43]

One major flash device manufacturer, Toshiba, has chosen to use an interface of their own design known as Toggle Mode (and now Toggle V2.0). This interface isn't pin-to-pin compatible with the ONFI specification. The result is a product designed for one vendor's devices may not be able to use another vendor's devices.[44]

A group of vendors, including Intel, Dell, and Microsoft, formed a Non-Volatile Memory Host Controller Interface (NVMHCI) Working Group.[45] The goal of the group is to provide standard software and hardware programming interfaces for nonvolatile memory subsystems, including the "flash cache" device connected to the PCI Express bus.

Distinction between NOR and NAND flash[edit]
NOR and NAND flash differ in two important ways:

the connections of the individual memory cells are different
the interface provided for reading and writing the memory is different (NOR allows random-access for reading, NAND allows only page access)
These two are linked by the design choices made in the development of NAND flash. A goal of NAND flash development was to reduce the chip area required to implement a given capacity of flash memory, and thereby to reduce cost per bit and increase maximum chip capacity so that flash memory could compete with magnetic storage devices like hard disks.[citation needed]

NOR and NAND flash get their names from the structure of the interconnections between memory cells.[46] In NOR flash, cells are connected in parallel to the bit lines, allowing cells to be read and programmed individually. The parallel connection of cells resembles the parallel connection of transistors in a CMOS NOR gate. In NAND flash, cells are connected in series, resembling a NAND gate. The series connections consume less space than parallel ones, reducing the cost of NAND flash. It does not, by itself, prevent NAND cells from being read and programmed individually.

Each NOR flash cell is larger than a NAND flash cell   10 F2 vs 4 F2   even when using exactly the same semiconductor device fabrication and so each transistor, contact, etc. is exactly the same size because NOR flash cells require a separate metal contact for each cell.[47]

When NOR flash was developed, it was envisioned as a more economical and conveniently rewritable ROM than contemporary EPROM and EEPROM memories. Thus random-access reading circuitry was necessary. However, it was expected that NOR flash ROM would be read much more often than written, so the write circuitry included was fairly slow and could erase only in a block-wise fashion. On the other hand, applications that use flash as a replacement for disk drives do not require word-level write address, which would only add to the complexity and cost unnecessarily.[citation needed]

Because of the series connection and removal of wordline contacts, a large grid of NAND flash memory cells will occupy perhaps only 60% of the area of equivalent NOR cells[48] (assuming the same CMOS process resolution, for example, 130 nm, 90 nm, or 65 nm). NAND flash's designers realized that the area of a NAND chip, and thus the cost, could be further reduced by removing the external address and data bus circuitry. Instead, external devices could communicate with NAND flash via sequential-accessed command and data registers, which would internally retrieve and output the necessary data. This design choice made random-access of NAND flash memory impossible, but the goal of NAND flash was to replace mechanical hard disks, not to replace ROMs.

Attribute NAND  NOR
Main Application  File Storage  Code execution
Storage capacity  High  Low
Cost per bit  Better  
Active Power  Better  
Standby Power   Better
Write Speed Good  
Read Speed    Good
Write endurance[edit]
The write endurance of SLC floating-gate NOR flash is typically equal to or greater than that of NAND flash, while MLC NOR and NAND flash have similar endurance capabilities. Examples of endurance cycle ratings listed in datasheets for NAND and NOR flash are provided.[citation needed]

Type of flash memory  Endurance rating (Erases per block) Example(s) of flash memory
SLC NAND  100,000 Samsung OneNAND KFW4G16Q2M
MLC NAND  5,000 to 10,000 for medium-capacity applications;
1,000 to 3,000 for high-capacity applications Samsung K9G8G08U0M (Example for medium-capacity applications)
TLC NAND  1,000 Samsung 840
SLC (floating-gate) NOR 100,000 to 1,000,000  Numonyx M58BW (Endurance rating of 100,000 erases per block);
Spansion S29CD016J (Endurance rating of 1,000,000 erases per block)
MLC (floating-gate) NOR 100,000 Numonyx J3 flash
However, by applying certain algorithms and design paradigms such as wear leveling and memory over-provisioning, the endurance of a storage system can be tuned to serve specific requirements.[2][49]

Computation of NAND flash memory endurance is a challenging subject that depends on SLC/MLC/TLC memory type as well as the use pattern. In order to compute the longevity of the NAND flash, one must account for the size of the memory chip, the type of memory (e.g. SLC/MLC/TLC), and use pattern.

Flash file systems[edit]
Main article: Flash file system
Because of the particular characteristics of flash memory, it is best used with either a controller to perform wear leveling and error correction or specifically designed flash file systems, which spread writes over the media and deal with the long erase times of NOR flash blocks.[50] The basic concept behind flash file systems is the following: when the flash store is to be updated, the file system will write a new copy of the changed data to a fresh block, remap the file pointers, then erase the old block later when it has time.

In practice, flash file systems are used only for memory technology devices (MTDs), which are embedded flash memories that do not have a controller. Removable flash memory cards and USB flash drives have built-in controllers to perform wear leveling and error correction so use of a specific flash file system does not add any benefit.

Capacity[edit]
Multiple chips are often arrayed to achieve higher capacities[51] for use in consumer electronic devices such as multimedia players or GPSs. The capacity of flash chips generally follows Moore's Law because they are manufactured with many of the same integrated circuits techniques and equipment.

Consumer flash storage devices typically are advertised with usable sizes expressed as a small integer power of two (2, 4, 8, etc.) and a designation of megabytes (MB) or gigabytes (GB); e.g., 512 MB, 8 GB. This includes SSDs marketed as hard drive replacements, in accordance with traditional hard drives, which use decimal prefixes.[52] Thus, an SSD marked as "64 GB" is at least 64   1,0003 bytes (64 GB). Most users will have slightly less capacity than this available for their files, due to the space taken by file system metadata.

The flash memory chips inside them are sized in strict binary multiples, but the actual total capacity of the chips is not usable at the drive interface. It is considerably larger than the advertised capacity in order to allow for distribution of writes (wear leveling), for sparing, for error correction codes, and for other metadata needed by the device's internal firmware.

In 2005, Toshiba and SanDisk developed a NAND flash chip capable of storing 1 GB of data using multi-level cell (MLC) technology, capable of storing two bits of data per cell. In September 2005, Samsung Electronics announced that it had developed the world s first 2 GB chip.[53]

In March 2006, Samsung announced flash hard drives with a capacity of 4 GB, essentially the same order of magnitude as smaller laptop hard drives, and in September 2006, Samsung announced an 8 GB chip produced using a 40 nm manufacturing process.[54] In January 2008, SanDisk announced availability of their 16 GB MicroSDHC and 32 GB SDHC Plus cards.[55][56]

More recent flash drives (as of 2012) have much greater capacities, holding 64, 128, and 256 GB.[57]

A joint development at Intel and Micron will allow the production of 32 layer 3.5 terabyte (TB) NAND flash sticks and 10 TB standard-sized SSDs. The device includes 5 packages of 16 x 48 GB TLC dies, using a floating gate cell design.[58]

Flash chips continue to be manufactured with capacities under or around 1 MB, e.g., for BIOS-ROMs and embedded applications.

Transfer rates[edit]
Flash memory devices are typically much faster at reading than writing.[59] Performance also depends on the quality of storage controllers which become more critical when devices are partially full.[59] Even when the only change to manufacturing is die-shrink, the absence of an appropriate controller can result in degraded speeds.[60]

Applications[edit]
Serial flash[edit]
Serial flash is a small, low-power flash memory that uses a serial interface, typically Serial Peripheral Interface Bus (SPI), for sequential data access. When incorporated into an embedded system, serial flash requires fewer wires on the PCB than parallel flash memories, since it transmits and receives data one bit at a time. This may permit a reduction in board space, power consumption, and total system cost.

There are several reasons why a serial device, with fewer external pins than a parallel device, can significantly reduce overall cost:

Many ASICs are pad-limited, meaning that the size of the die is constrained by the number of wire bond pads, rather than the complexity and number of gates used for the device logic. Eliminating bond pads thus permits a more compact integrated circuit, on a smaller die; this increases the number of dies that may be fabricated on a wafer, and thus reduces the cost per die.
Reducing the number of external pins also reduces assembly and packaging costs. A serial device may be packaged in a smaller and simpler package than a parallel device.
Smaller and lower pin-count packages occupy less PCB area.
Lower pin-count devices simplify PCB routing.
There are two major SPI flash types. The first type is characterized by small pages and one or more internal SRAM page buffers allowing a complete page to be read to the buffer, partially modified, and then written back (for example, the Atmel AT45 DataFlash or the Micron Technology Page Erase NOR Flash). The second type has larger sectors. The smallest sectors typically found in an SPI flash are 4 kB, but they can be as large as 64 kB. Since the SPI flash lacks an internal SRAM buffer, the complete page must be read out and modified before being written back, making it slow to manage. SPI flash is cheaper than DataFlash and is therefore a good choice when the application is code shadowing.

The two types are not easily exchangeable, since they do not have the same pinout, and the command sets are incompatible.

Firmware storage[edit]
With the increasing speed of modern CPUs, parallel flash devices are often much slower than the memory bus of the computer they are connected to. Conversely, modern SRAM offers access times below 10 ns, while DDR2 SDRAM offers access times below 20 ns. Because of this, it is often desirable to shadow code stored in flash into RAM; that is, the code is copied from flash into RAM before execution, so that the CPU may access it at full speed. Device firmware may be stored in a serial flash device, and then copied into SDRAM or SRAM when the device is powered-up.[61] Using an external serial flash device rather than on-chip flash removes the need for significant process compromise (a process that is good for high-speed logic is generally not good for flash and vice versa). Once it is decided to read the firmware in as one big block it is common to add compression to allow a smaller flash chip to be used. Typical applications for serial flash include storing firmware for hard drives, Ethernet controllers, DSL modems, wireless network devices, etc.

Flash memory as a replacement for hard drives[edit]
Main article: Solid-state drive
One more recent application for flash memory is as a replacement for hard disks. Flash memory does not have the mechanical limitations and latencies of hard drives, so a solid-state drive (SSD) is attractive when considering speed, noise, power consumption, and reliability. Flash drives are gaining traction as mobile device secondary storage devices; they are also used as substitutes for hard drives in high-performance desktop computers and some servers with RAID and SAN architectures.

There remain some aspects of flash-based SSDs that make them unattractive. The cost per gigabyte of flash memory remains significantly higher than that of hard disks.[62] Also flash memory has a finite number of P/E cycles, but this seems to be currently under control since warranties on flash-based SSDs are approaching those of current hard drives.[63] In addition, deleted files on SSDs can remain for an indefinite period of time before being overwritten by fresh data; erasure or shred techniques or software that work well on magnetic hard disk drives have no effect on SSDs, compromising security and forensic examination.

For relational databases or other systems that require ACID transactions, even a modest amount of flash storage can offer vast speedups over arrays of disk drives.[64][65]

In June 2006, Samsung Electronics released the first flash-memory based PCs, the Q1-SSD and Q30-SSD, both of which used 32 GB SSDs, and were at least initially available only in South Korea.[66]

A solid-state drive was offered as an option with the first Macbook Air introduced in 2008, and from 2010 onwards, all Macbook Air laptops shipped with an SSD. Starting in late 2011, as part of Intel's Ultrabook initiative, an increasing number of ultra thin laptops are being shipped with SSDs standard.

There are also hybrid techniques such as hybrid drive and ReadyBoost that attempt to combine the advantages of both technologies, using flash as a high-speed non-volatile cache for files on the disk that are often referenced, but rarely modified, such as application and operating system executable files.

Flash memory as RAM[edit]
As of 2012, there are attempts to use flash memory as the main computer memory, DRAM.[67]

Archival or long-term storage[edit]
It is unclear how long flash memory will persist under archival conditions i.e., benign temperature and humidity with infrequent access with or without prophylactic rewrite. Anecdotal evidence[specify] suggests that the technology is reasonably robust on the scale of years.[citation needed]

Industry[edit]
One source states that, in 2008, the flash memory industry includes about US 9.1 billion in production and sales. Other sources put the flash memory market at a size of more than US 20 billion in 2006, accounting for more than eight percent of the overall semiconductor market and more than 34 percent of the total semiconductor memory market.[68] In 2012, the market was estimated at  26.8 billion.[69]

Flash scalability[edit]

The aggressive trend of the shrinking process design rule or technology node in NAND flash memory technology effectively accelerates Moore's Law.
Due to its relatively simple structure and high demand for higher capacity, NAND flash memory is the most aggressively scaled technology among electronic devices. The heavy competition among the top few manufacturers only adds to the aggressiveness in shrinking the design rule or process technology node.[30] While the expected shrink timeline is a factor of two every three years per original version of Moore's law, this has recently been accelerated in the case of NAND flash to a factor of two every two years.
A video card (also called a video adapter, display card, graphics card, graphics board, display adapter, graphics adapter or frame buffer[1]) is an expansion card which generates a feed of output images to a display (such as a computer monitor). Frequently, these are advertised as discrete or dedicated graphics cards, emphasizing the distinction between these and integrated graphics. Within the industry, video cards are sometimes called graphics add-in-boards, abbreviated as AIBs,[2] with the word "graphics" usually omitted.

Graphic cards
Connects to 
Motherboard via one of:

ISA
MCA
VLB
PCI
AGP
PCI-X
PCI Express
Others
Display via one of:

VGA connector
Digital Visual Interface
Composite video
S-Video
Component video
HDMI
DMS-59
DisplayPort
Others
Contents  [hide] 
1 History
2 Dedicated vs integrated graphics
3 Power demand
4 Size
5 Multi-card scaling
6 Device drivers
7 Industry
8 Size of market and impact of accelerated processing units on video card sales
9 Parts
9.1 Graphics Processing Unit
9.2 Heat sink
9.3 Video BIOS
9.4 Video memory
9.5 RAMDAC
9.6 Output interfaces
9.6.1 Video Graphics Array (VGA) (DE-15)
9.6.2 Digital Visual Interface (DVI)
9.6.3 Video In Video Out (VIVO) for S-Video, Composite video and Component video
9.6.4 High-Definition Multimedia Interface (HDMI)
9.6.5 DisplayPort
9.6.6 Other types of connection systems
9.7 Motherboard interfaces
10  See also
11  References
12  External links
History[edit]
Standards such as MDA, CGA, HGC, Tandy, PGC, EGA, VGA, MCGA, 8514 or XGA were introduced from 1982 to 1990 and supported by a variety of hardware manufacturers.

The majority of modern video cards are built with either AMD-sourced or Nvidia-sourced graphics chips.[2] Most video cards offer various functions such as accelerated rendering of 3D scenes and 2D graphics, MPEG-2/MPEG-4 decoding, TV output, or the ability to connect multiple monitors (multi-monitor). Video cards also have sound card capabilities to output sound - along with the video for connected TVs or monitors with integrated speakers.

Graphics cards weren't very useful for early computers, since they didn't have the capability to run graphic-based games or high resolution videos as modern computers do now.

Dedicated vs integrated graphics[edit]

Classical desktop computer architecture with a distinct graphics card over PCI Express. Typical bandwidths for given memory technologies, missing are the memory latency. Zero-copy between GPU and CPU is not possible, since both have their distinct physical memories. Data must be copied from one to the other to be shared.

Integrated graphics with partitioned main memory: a part of the system memory is allocated to the GPU exclusively. Zero-copy is not possible, data has to be copied, over the system memory bus, from one partition to the other.

Integrated graphics with unified main memory, to be found AMD "Kaveri" or PlayStation 4 (HSA).
As an alternative to the use of a video card, video hardware can be integrated into the motherboard or the CPU. Both approaches can be called integrated graphics. Motherboard-based implementations are sometimes called "on-board video" while CPU-based implementations are known as accelerated processing units (APUs). Almost all desktop computer motherboards with integrated graphics allow the disabling of the integrated graphics chip in BIOS, and have a PCI, or PCI Express (PCI-E) slot for adding a higher-performance graphics card in place of the integrated graphics. The ability to disable the integrated graphics sometimes also allows the continued use of a motherboard on which the on-board video has failed. Sometimes both the integrated graphics and a dedicated graphics card can be used simultaneously to feed separate displays. The main advantages of integrated graphics include cost, compactness, simplicity and low energy consumption. The performance disadvantage of integrated graphics arises because the graphics processor shares system resources with the CPU. A dedicated graphics card has its own random access memory (RAM), its own cooling system, and dedicated power regulators, with all components designed specifically for processing video images. Upgrading to a dedicated graphics card offloads work from the CPU and system RAM, so not only will graphics processing be faster, but the computer's overall performance may also improve.

Both of the dominant CPU makers, AMD and Intel, are moving to APUs. One of the reasons is that graphics processors are powerful parallel processors, and placing them on the CPU die allows their parallel processing ability to be harnessed for various computing tasks in addition to graphics processing. (See Heterogeneous System Architecture, which discusses AMD's implementation.) APUs are the newer integrated graphics technology and, as costs decline, will probably be used instead of integrated graphics on the motherboard in most future low and mid-priced home and business computers. As of late 2013, the best APUs provide graphics processing approaching mid-range mobile video cards[3] and are adequate for casual gaming. Users seeking the highest video performance for gaming or other graphics-intensive uses should still choose computers with dedicated graphics cards. (See Size of market and impact of accelerated processing units on video card sales, below.)

Beyond the enthusiast segment is the market for professional video cards for workstations used in the special effects industry, and in fields such as design, analysis and scientific research. Nvidia is a major player in the professional segment. In November, 2013, AMD introduced a so-called "Supercomputing" graphics card "designed for data visualization in finance, oil exploration, aeronautics and automotive, design and engineering, geophysics, life sciences, medicine and defense."[4]

Power demand[edit]
As the processing power of video cards has increased, so has their demand for electrical power. Current high-performance video cards tend to consume a great deal of power. For example, the thermal design power (TDP) for the GeForce GTX TITAN is 250 Watts.[5] While CPU and power supply makers have recently moved toward higher efficiency, power demands of GPUs have continued to rise, so the video card may be the biggest electricity user in a computer.[6][7] Although power supplies are increasing their power too, the bottleneck is due to the PCI-Express connection, which is limited to supplying 75 Watts.[8] Modern video cards with a power consumption over 75 Watts usually include a combination of six-pin (75W) or eight-pin (150W) sockets that connect directly to the power supply. Providing adequate cooling becomes a challenge in such computers. Computers with multiple video cards may need power supplies in the 1000W-1500W range. Heat extraction becomes a major design consideration for computers with two or more high end video cards.

Size[edit]
Video cards for desktop computers come in one of two size profiles, which can allow a graphics card to be added even to small form factor PCs. Some video cards are not of usual size, and are thus categorized as being low profile.[9][10] Video card profiles are based on width only, with low-profile cards taking up less than the width of a PCIe slot. Length and thickness can vary greatly, with high-end cards usually occupying two or three expansion slots, and with dual-GPU cards -such as the Nvidia GeForce GTX 690- generally exceeding 10" in length.[11]

Multi-card scaling[edit]
Some graphics cards can be linked together to allow scaling of the graphics processing across multiple cards. This is done using either the PCIe bus on the motherboard, or, more commonly, a data bridge. Generally, the cards must be of the same model to be linked, and most low power cards are not able to be linked in this way.[12] AMD and Nvidia both have proprietary methods of scaling, CrossFireX for AMD, and SLI for Nvidia. Cards from different chipset manufacturers, architectures cannot be used together for multi card scaling. If a graphics card has different sizes of memory, the lowest value will be used, with the higher values being disregarded. Currently, scaling on consumer grade cards can be done using up to four cards.[13][14][15]

Device drivers[edit]
The device driver usually supports one or multiple cards, and has to be specifically written for an operating system.

Industry[edit]
The primary suppliers of the GPUs (video chips or chipsets) used in video cards are AMD and Nvidia. In the third quarter of 2013, AMD had a 35.5% market share while Nvidia had a 64.5% market share,[16] according to Jon Peddie Research. In economics, this industry structure is termed a duopoly. AMD and Nvidia also build and sell video cards, which are termed graphics add-in-board (AIBs) in the industry. (See Comparison of Nvidia graphics processing units and Comparison of AMD graphics processing units.) In addition to marketing their own video cards, AMD and Nvidia sell their GPUs to authorized AIB suppliers, which AMD and Nvidia refer to as "partners".[2] The fact that Nvidia and AMD compete directly with their customer/partners complicates relationships in the industry. The fact that AMD and Intel are direct competitors in the CPU industry is also noteworthy, since AMD-based video cards may be used in computers with Intel CPUs. Intel's move to APUs may weaken AMD, which until now has derived a significant portion of its revenue from graphics components. As of the second quarter of 2013, there were 52 AIB suppliers.[2] These AIB suppliers may market video cards under their own brands, and/or produce video cards for private label brands and/or produce video cards for computer manufacturers. Some AIB suppliers such as MSI build both AMD-based and Nvidia-based video cards. Others, such as EVGA, build only Nvidia-based video cards, while XFX, now builds only AMD-based video cards. Several AIB suppliers are also motherboard suppliers. The largest AIB suppliers, based on global retail market share for graphics cards, include Taiwan-based Palit Microsystems, Hong Kong-based PC Partner (which markets AMD-based video cards under its Sapphire brand and Nvidia-based video cards under its Zotac brand), Taiwan-based computer-maker Asustek Computer (Asus), Taiwan-based Micro-Star International (MSI), Taiwan-based Gigabyte Technology,[17] Brea, California, USA-based EVGA (which also sells computer components such as power supplies) and Ontario, California USA-based XFX. (The parent corporation of XFX is based in Hong Kong.)

Size of market and impact of accelerated processing units on video card sales[edit]
Video card shipments totaled 14.5 million units in the third quarter of 2013, a 17% fall from Q3 2012 levels.[16] The traditional PC market is shrinking as tablet computers and smartphones gain share. Years ago, the move to integrated graphics on the motherboard greatly reduced the market for low end video cards. Now, AMD and Intel's accelerated processing units, which combine graphics processing with CPU functions on the CPU die itself, are putting further pressure on video card sales.[17] AMD introduced a line of combined processors which it calls the AMD A-Series APU Processors (A4, A6, A8, A10) while Intel, rather than marketing an exclusive line of APUs, introduced its "4th Generation Intel  Core  Processors", some of which are APUs. Those processors are described as offering "Superb visuals and graphics performance without the cost of a separate graphics card."[18] They are branded as having Intel HD Graphics or Intel Iris Pro Graphics. As an example, the Intel Core i7 4750HQ with Iris Pro Graphics 5200, an accelerated processing unit for notebook computers, allows users with mid-range graphics requirements to use a notebook computer without a video card. In a September, 2013 review of the Intel Core i7 4750HQ accelerated processing unit (which is closely related to the Intel processor with HD Graphics 5000 used in the MacBook Air,) the website hardware.info stated: "With its latest generation of integrated graphics, Intel set out to rival the performance of the mid-range mobile Nvidia GeForce GT 650M graphics card. And the tests leave no doubt about it, both 3DMark and the gaming benchmarks confirm that the [Intel] Iris Pro Graphics 5200 is on the same level of or slightly below that of the GT 650M."[3] (The GeForce GT 650M is not sold through retail channels, but an EVGA desktop GTX 650 was selling for around  120 in late 2013.[19]) Although the review notes that Intel's accelerated processing unit is not yet cost competitive, the technology is approaching competitiveness, at least with mid-range mobile dedicated video. (A video benchmarking website that tabulates user-submitted benchmarks shows Intel Iris Pro Graphics 5200, based on a very small sample of 8 submissions, scoring a G3D Mark of 912,[20] versus 1296 for the Nvidia GeForce GT 650M,[21] with higher scores being better. If the benchmark is linear, that puts the Iris Pro Graphics 5200's performance at about 70% of the GeForce GT 650M Intel was targeting. AMD's A10-5750M mobile APU with Radeon HD 8650G graphics scores 858 on this graphics benchmark.[22]) With anticipated price reductions, it is predicted that APUs will eventually replace low to mid-range dedicated video implementations. That will leave only the high-end enthusiast and professional market segments for video card vendors.

Parts[edit]

A Radeon HD 7970 with the cooler removed, showing the major components of the card.
A modern video card consists of a printed circuit board on which the components are mounted. These include:

Graphics Processing Unit[edit]
Main article: graphics processing unit
A graphics processing unit (GPU), also occasionally called visual processing unit (VPU), is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the building of images in a frame buffer intended for output to a display. Because of the large degree of programmable computational complexity for such a task, a modern video card is also a computer unto itself.

Heat sink[edit]
A heat sink is mounted on most modern graphics cards. A heat sink spreads out the heat produced by the graphics processing unit evenly throughout the heat sink and unit itself. The heat sink commonly has a fan mounted as well to cool the heat sink and the graphics processing unit. Not all cards have heat sinks, for example, some cards are liquid cooled, and instead have a waterblock; additionally, cards from the 1980s and early 1990s did not produce much heat, and did not require heatsinks.

Video BIOS[edit]
The video BIOS or firmware contains a minimal program for initial set up and control of the video card. It may contain information on the memory timing, operating speeds and voltages of the graphics processor, RAM, and other details which can sometimes be changed. The usual reason for doing this is to overclock the video card to allow faster video processing speeds, however, this has the potential to irreversibly damage the card with the possibility of cascaded damage to the motherboard.

The modern Video BIOS does not support all the functions of the video card, being only sufficient to identify and initialize the card to display one of a few frame buffer or text display modes. It does not support YUV to RGB translation, video scaling, pixel copying, compositing or any of the multitude of other 2D and 3D features of the video card.

Video memory[edit]
Type  Memory clock rate (MHz) Bandwidth (GB/s)
DDR 166   950 1.2   3.04
DDR2  2000   3600 128   200
GDDR5 900   7000  80   336.5
The memory capacity of most modern video cards ranges from 1 GB to 12 GB.[23] Since video memory needs to be accessed by the GPU and the display circuitry, it often uses special high-speed or multi-port memory, such as VRAM, WRAM, SGRAM, etc. Around 2003, the video memory was typically based on DDR technology. During and after that year, manufacturers moved towards DDR2, GDDR3, GDDR4 and GDDR5. The effective memory clock rate in modern cards is generally between 1 GHz to 7 GHz.

Video memory may be used for storing other data as well as the screen image, such as the Z-buffer, which manages the depth coordinates in 3D graphics, textures, vertex buffers, and compiled shader programs.

RAMDAC[edit]
The RAMDAC, or Random Access Memory Digital-to-Analog Converter, converts digital signals to analog signals for use by a computer display that uses analog inputs such as Cathode ray tube (CRT) displays. The RAMDAC is a kind of RAM chip that regulates the functioning of the graphics card. Depending on the number of bits used and the RAMDAC-data-transfer rate, the converter will be able to support different computer-display refresh rates. With CRT displays, it is best to work over 75 Hz and never under 60 Hz, in order to minimize flicker.[24] (With LCD displays, flicker is not a problem.[citation needed]) Due to the growing popularity of digital computer displays and the integration of the RAMDAC onto the GPU die, it has mostly disappeared as a discrete component. All current LCD/plasma monitors and TVs and projectors with only digital connections, work in the digital domain and do not require a RAMDAC for those connections. There are displays that feature analog inputs (VGA, component, SCART etc.) only. These require a RAMDAC, but they reconvert the analog signal back to digital before they can display it, with the unavoidable loss of quality stemming from this digital-to-analog-to-digital conversion.[25] With VGA standard being phased out in favor of digital, RAMDACs will begin to disappear from video cards.

Output interfaces[edit]

Video In Video Out (VIVO) for S-Video (TV-out), Digital Visual Interface (DVI) for High-definition television (HDTV), and DB-15 for Video Graphics Array (VGA)
The most common connection systems between the video card and the computer display are:

Video Graphics Array (VGA) (DE-15)[edit]

Video Graphics Array (VGA) (DE-15).
Main article: Video Graphics Array
Also known as D-sub, VGA is an analog-based standard adopted in the late 1980s designed for CRT displays, also called VGA connector. Some problems of this standard are electrical noise, image distortion and sampling error in evaluating pixels. Today, the VGA analog interface is used for high definition video including 1080p and higher. While the VGA transmission bandwidth is high enough to support even higher resolution playback, there can be picture quality degradation depending on cable quality and length. How discernible this quality difference is depends on the individual's eyesight and the display; when using a DVI or HDMI connection, especially on larger sized LCD/LED monitors or TVs, quality degradation, if present, is prominently visible. Blu-ray playback at 1080p is possible via the VGA analog interface, if Image Constraint Token (ICT) is not enabled on the Blu-ray disc.

Digital Visual Interface (DVI)[edit]

Digital Visual Interface (DVI-I).
Main article: Digital Visual Interface
Digital-based standard designed for displays such as flat-panel displays (LCDs, plasma screens, wide high-definition television displays) and video projectors. In some rare cases high end CRT monitors also use DVI. It avoids image distortion and electrical noise, corresponding each pixel from the computer to a display pixel, using its native resolution. It is worth to note that most manufacturers include DVI-I connector, allowing (via simple adapter) standard RGB signal output to an old CRT or LCD monitor with VGA input.

Video In Video Out (VIVO) for S-Video, Composite video and Component video[edit]
Included to allow the connection with televisions, DVD players, video recorders and video game consoles. They often come in two 10-pin mini-DIN connector variations, and the VIVO splitter cable generally comes with either 4 connectors (S-Video in and out + composite video in and out), or 6 connectors (S-Video in and out + component PB out + component PR out + component Y out [also composite out] + composite in).

High-Definition Multimedia Interface (HDMI)[edit]

High-Definition Multimedia Interface (HDMI)
Main article: HDMI
HDMI is a compact audio/video interface for transferring uncompressed video data and compressed/uncompressed digital audio data from an HDMI-compliant device ("the source device") to a compatible digital audio device, computer monitor, video projector, or digital television.[26] HDMI is a digital replacement for existing analog video standards. HDMI supports copy protection through HDCP.

DisplayPort[edit]

DisplayPort
Main article: DisplayPort
DisplayPort is a digital display interface developed by the Video Electronics Standards Association (VESA). The interface is primarily used to connect a video source to a display device such as a computer monitor, though it can also be used to transmit audio, USB, and other forms of data.[27] The VESA specification is royalty-free. VESA designed it to replace VGA, DVI, and LVDS. Backward compatibility to VGA and DVI by using adapter dongles enables consumers to use DisplayPort fitted video sources without replacing existing display devices. Although DisplayPort has much of the same functionality as HDMI, it is expected to complement the interface, not replace it.[28][29]

Other types of connection systems[edit]
Composite video Analog system with lower 480i resolution; it uses the RCA connector. The single pin connector carries all resolution, brightness and color information, making it the lowest quality dedicated video connection.[30]
Composite-video-cable.jpg
Component video It has three cables, each with RCA connector (YCBCR for digital component, or YPBPR for analog component); it is used in older projectors, video-game consoles, DVD players.[31] It can carry SDTV 480i and EDTV 480p resolutions, and HDTV resolutions 720p and 1080i, but not 1080p due to industry concerns about copy protection. Contrary to popular belief it looks equal to HDMI for the resolutions it carries,[32] but for best performance from Blu-ray, other 1080p sources like PPV, and 4K Ultra HD, a digital display connector is required.
Component video jack.jpg
DB13W3  An analog standard once used by Sun Microsystems, SGI and IBM.
DB13W3 Pinout.svg
DMS-59  A connector that provides two DVI or VGA outputs on a single connector. This is a DMS-59 port.
DMS-59.jpg
Motherboard interfaces[edit]
Main articles: Bus (computing) and Expansion card
Chronologically, connection systems between video card and motherboard were, mainly:

S-100 bus: Designed in 1974 as a part of the Altair 8800, it was the first industry-standard bus for the microcomputer industry.
ISA: Introduced in 1981 by IBM, it became dominant in the marketplace in the 1980s. It was an 8 or 16-bit bus clocked at 8 MHz.
NuBus: Used in Macintosh II, it was a 32-bit bus with an average bandwidth of 10 to 20 MB/s.
MCA: Introduced in 1987 by IBM it was a 32-bit bus clocked at 10 MHz.
EISA: Released in 1988 to compete with IBM's MCA, it was compatible with the earlier ISA bus. It was a 32-bit bus clocked at 8.33 MHz.
VLB: An extension of ISA, it was a 32-bit bus clocked at 33 MHz. Also referred to as VESA.
PCI: Replaced the EISA, ISA, MCA and VESA buses from 1993 onwards. PCI allowed dynamic connectivity between devices, avoiding the manual adjustments required with jumpers. It is a 32-bit bus clocked 33 MHz.
UPA: An interconnect bus architecture introduced by Sun Microsystems in 1995. It had a 64-bit bus clocked at 67 or 83 MHz.
USB: Although mostly used for miscellaneous devices, such as secondary storage devices and toys, USB displays and display adapters exist.
AGP: First used in 1997, it is a dedicated-to-graphics bus. It is a 32-bit bus clocked at 66 MHz.
PCI-X: An extension of the PCI bus, it was introduced in 1998. It improves upon PCI by extending the width of bus to 64 bits and the clock frequency to up to 133 MHz.
PCI Express: Abbreviated PCIe, it is a point to point interface released in 2004. In 2006 provided double the data-transfer rate of AGP. It should not be confused with PCI-X, an enhanced version of the original PCI specification.
In the attached table[33] is a comparison between a selection of the features of some of those interfaces.
Video Graphics Array (VGA) refers specifically to the display hardware first introduced with the IBM PS/2 line of computers in 1987,[1] but through its widespread adoption has also come to mean either an analog computer display standard, the 15-pin D-subminiature VGA connector or the 640x480 resolution itself.

VGA was the last IBM graphics standard to which the majority of PC clone manufacturers conformed, making it the lowest common denominator that virtually all post-1990 PC graphics hardware can be expected to implement. It was officially followed by IBM's Extended Graphics Array (XGA) standard, but was effectively superseded by numerous slightly different extensions to VGA made by clone manufacturers, collectively known as Super VGA.

Today, the VGA analog interface is used for high definition video, including resolutions of 1080p and higher. While the transmission bandwidth of VGA is high enough to support even higher resolution playback, there can be picture quality degradation depending on cable quality and length. How discernible this degradation is depends on the individual's eyesight and the display, though it is more noticeable when switching to and from digital inputs like HDMI or DVI.
(Total vertical sync and blanking time 1.43 ms; equivalent to line periods of A = 10, B = 2, C = 33, D = 480 and each complete frame = 525)

These timings are somewhat altered in "70Hz" mode, as although it uses the same line rate, its frame rate is not quite exactly 7/6ths that of "60Hz", despite 525 dividing cleanly into 7 - and, of course, 480/400 is itself a larger 6:5 ratio. Instead, it compromises on a 449-line frame (instead of the expected 450), with the back porch extended to 34 lines, and the front porch to 13, with an unaltered 2-line sync pulse - and the active image taking up 89% of the total scan period rather than 91%. The monitor is triggered into synchronising at the higher frame scan rate (and, with digital displays such as LCDs, the higher horizontal pixel density) by use of a positive-polarity VSync pulse, versus the negative pulse of 60 Hz mode.

Depending on manufacturer, the exact details of active period and front/back porch widths, particularly in the horizontal domain, may vary slightly. This does not usually cause a problem as the porches are merely intended to act as blanked-video buffers offering a little overscan space between the active area and the sync pulse (which triggers, in traditional CRT monitors, the phosphor beam deflection "flyback" to the upper or left hand side of the tube) and thus can be safely overrun into by a certain amount when everything else is operating correctly. The relationship between the front and back porches can also be altered within certain limits, which makes possible special features such as software-based image alignment with certain graphics cards (centering the image within the monitor frame by adjusting the location of the active screen area between the horizontal and vertical porches, rather than relying wholly upon the adjustment range offered by the monitor's own controls which can sometimes be less than satisfactory).

This buffer zone is typically what is exploited to achieve higher active resolutions in the various custom screen modes, by deliberately reducing porch widths and using the freed-up scan time for active pixels instead. This technique can achieve an absolute maximum of 704 pixels horizontally in 25 MHz mode and 792 at 28 MHz without altering the actual sync width (in real-world cases, e.g. with 800 pixel wide mode, the sync pulse would be shortened and a small porch area left in place to prevent obvious visual artefacting), and as much as 523 or 447 lines at the standard 60 and 70 Hz refresh rates (again, it is usually necessary to leave SOME porch lines intact, hence the usual maximum of 410 or 512 lines at these rates, and the 50 Hz maximum being 600 lines rather than 626). Conveniently, the practical limits of these techniques are not quite high enough to overflow the available memory capacity of typical 256KB cards (800x600 consuming 235KB, and even the theoretical 832x624 requiring "only" 254KB), so the only concerns remain those of monitor compatibility.

Typical uses of selected modes[edit]
640x400 @ 70 Hz is traditionally the video mode used for booting most VGA-compatible x86 personal computers[11] which show a graphical boot screen (text-mode boot uses 720x400 @ 70 Hz).

640x480 @ 60 Hz is the default Windows graphics mode (usually with 16 colors),[11] up to Windows 2000. It remains an option in XP and later versions via the boot menu "low resolution video" option and per-application compatibility mode settings.

320x200 @ 70 Hz is the most common mode for VGA-era PC games, using exactly the same timings as the 640x400 mode, but halving the pixel rate (and, in 256 colour mode, doubling the bit-depth of each pixel) and displaying each line of pixels twice.

The actual timings vary slightly from the defined standard. For example, for 640x480 @ 60 Hz, a 25.17  s active video time with a pixel frequency of 25.174 MHz gives 634 pixels, rather than the expected 640.

Connectors[edit]

A D-SUB connector (better known as VGA connector)
See also: VGA connector
VGA uses a DE-15 connector. This connector fits on the mounting tab of an ISA expansion card.


VGA BNC connectors
An alternative method of connecting VGA devices that maintains very high signal quality is the BNC connector, typically used as a group of five connectors, one each for Red, Green, Blue, Horizontal Sync, and Vertical Sync. With BNC, the coaxial wires are fully shielded end-to-end and through the interconnect so that no crosstalk or external interference is possible.

However, BNC connectors are relatively large compared to the DE 15, and some attention is needed to make sure each cable goes to the correct socket. Additionally, extra signal lines such as +5 V, DDC, and DDC2 are not supported using BNC connectors.

Standard text modes[edit]
See also: VGA compatible text mode
The BIOS offers some text modes for a VGA adapter, which have 80x25, 40x25, 80x43 or 80x50 text grid. Each cell may choose from one of 16 available colors for its foreground and eight colors for the background; the eight background colors allowed are the ones without the high-intensity bit set. Each character may also be made to blink; all that are set to blink will blink in unison. The blinking option for the entire screen can be exchanged for the ability to use all 16 colors for background. All of these options are the same as those on the CGA adapter as introduced by IBM.

Like EGA, VGA supports having up to 512 different simultaneous characters on screen, albeit in only 8 foreground colors, by rededicating one color bit as the highest bit of the character number. The glyphs on 80x25 mode are normally made of 9x16 pixels. Users may define their own character set by loading a custom font onto the card. As character data is only eight bits wide on VGA, just as on all of its predecessors, there is usually a blank pixel column between any two horizontally adjacent glyphs. However, some characters are normally made nine bits wide by repeating their last column instead of inserting a blank column, especially those defining horizontally connected IBM box-drawing characters. This functionality is hard-wired to the character numbers C0hex to DFhex, where all horizontally connecting characters are found in Codepage 437 and its most common derivatives. The same column-repeating trick was already used on the older MDA hardware with its 9x14 pixel glyphs, but on VGA it can be turned off when loading a font in which those character numbers do not represent box drawing characters.[12][13]

Monochrome modes[edit]
VGA adapters usually support both monochrome and color modes, though the monochrome mode is almost never used, and support for the full set of MDA text mode attributes (intense, underline) is often missing. Black and white text on nearly all modern VGA adapters is drawn by using gray colored text on a black background in color mode. VGA monochrome monitors intended primarily for text were sold, but most of them will work at least adequately with a VGA adapter in color mode. Occasionally, a faulty connection between a modern monitor and video card will cause the VGA part of the card to detect the monitor as monochrome; this will cause the BIOS and initial boot sequence to appear in greyscale. Usually, once the video card's drivers are loaded (for example, by continuing to boot into the operating system), they will override this detection and the monitor will return to color.

Addressing details[edit]

Examples of VGA images in 640x480x16 (top) and 320x200x256 modes (bottom). Dithering is used to mask color limitations.
The video memory of the VGA is mapped to the PC's memory via a window in the range between segments 0xA0000 and 0xBFFFF in the PC's real mode address space (A000:0000 and B000:FFFF in segment:offset notation). Typically, these starting segments are:

0xA0000 for EGA/VGA graphics modes (64 KB)
0xB0000 for monochrome text mode (32 KB)
0xB8000 for color text mode and CGA-compatible graphics modes (32 KB)
Due to the use of different address mappings for different modes, it is possible to have a monochrome adapter (i.e. MDA or Hercules) and a color adapter such as the VGA, EGA, or CGA installed in the same machine. At the beginning of the 1980s, this was typically used to display Lotus 1-2-3 spreadsheets in high-resolution text on a monochrome display and associated graphics on a low-resolution CGA display simultaneously. Many programmers also used such a setup with the monochrome card displaying debugging information while a program ran in graphics mode on the other card. Several debuggers, like Borland's Turbo Debugger, D86 (by Alan J. Cox) and Microsoft's CodeView could work in a dual monitor setup. Either Turbo Debugger or CodeView could be used to debug Windows. There were also DOS device drivers such as ox.sys, which implemented a serial interface simulation on the monochrome display and, for example, allowed the user to receive crash messages from debugging versions of Windows without using an actual serial terminal. It is also possible to use the "MODE MONO" command at the DOS prompt to redirect the output to the monochrome display. When a monochrome adapter was not present, it was possible to use the 0xB000 0xB7FF address space as additional memory for other programs (for example by adding the line "DEVICE=EMM386.EXE I=B000-B7FF" into config.sys, this memory would be made available to programs that can be "loaded high", that is loaded into high memory.)

Color palette[edit]
See also: List of monochrome and RGB palettes, 8-bit RGB, List of 16-bit computer hardware palettes and MCGA and VGA

VGA 256 default color palette
The VGA color system is backward compatible with the EGA and CGA adapters, and adds another level of indirection to support 256 8-bit-colors.

CGA was able to display 16 fixed colors, and EGA extended this by using 16 palette registers, each containing a color value from a 64-color palette. The default EGA palette values were chosen to look like the CGA colors, but it's possible to remap each color. (Note: This works in graphics and text modes.) The signals from the EGA palette entries will drive a set of six signal lines on the EGA output, with two lines corresponding to one color (off, dark, normal and bright, for red, green and blue).

VGA further extends this scheme by adding 256 color registers, 3 6 bit each for a total of 262,144 colors to choose from. These color registers are by default set to match the 64 default EGA colors. The values from these registers drive a DAC, which in turn drives three signal lines, one for red, green and blue. (Using analog signals allows a theoretically unlimited number of color values on a default VGA cable.)

Like EGA uses the CGA color value to address a palette entry, the VGA hardware will also use the palette entries not directly as signal levels but as indexes to the color registers. Therefore, in the 16-color-modes, the color value from the RAM will reference a palette register and that palette register will select a color register. E.g. the default palette entry for brown (on CGA: 4 (red) + 2(green), contains 0x14 (dark green + normal red) on the EGA palette. The corresponding VGA color register 0x14 is preset to (42,21,0, or #aa5500, also and off cause brown).

For 16-color-modes, using the 6-bit palette registers allows to use a block of up to 64 color registers, usually 0-63.

In the 256-color-modes, the palette registers are set to have no effect and the DAC is set to combine two 4-bit color values into the original
 8-bit-value; thereby the 8-bit color number in the video RAM is exactly the color register index to be used. Since the colors 0 ... 15 are still supposed to result in the CGA colors, the color registers are not preset to contain the EGA palette, instead it contains the 16 CGA colors in the first entries. The other entries are 16 gray levels from black to white and 9 groups of 24 color values. The 

Computer monitor
From Wikipedia, the free encyclopedia
  (Redirected from Computer display)
A computer monitor or a computer display is an electronic visual display for computers. A monitor usually comprises the display device, circuitry, casing, and power supply. The display device in modern monitors is typically a thin film transistor liquid crystal display (TFT-LCD) or a flat panel LED display, while older monitors used a cathode ray tubes (CRT). It can be connected to the computer via VGA, DVI, HDMI, DisplayPort, Thunderbolt, LVDS (Low-voltage differential signaling) or other proprietary connectors and signals.

Originally, computer monitors were used for data processing while television receivers were used for entertainment. From the 1980s onwards, computers (and their monitors) have been used for both data processing and entertainment, while televisions have implemented some computer functionality. The common aspect ratio of televisions, and computer monitors, has changed from 4:3 to 16:10, to 16:9.

Contents  [hide] 
1 History
2 Technologies
2.1 Cathode ray tube
2.2 Liquid crystal display
2.3 Organic light-emitting diode
3 Measurements of performance
3.1 Size
3.2 Aspect ratio
3.3 Resolution
4 Additional features
4.1 Power saving
4.2 Integrated accessories
4.3 Glossy screen
4.4 Curved designs
4.5 Directional screen
4.6 3D
4.7 Touch screen
4.8 Tablet screens
5 Mounting
5.1 Desktop
5.2 VESA mount
5.3 Rack mount
5.4 Panel mount
5.5 Open frame
6 Security vulnerabilities
7 See also
8 References
9 External links
History[edit]
Early electronic computers were fitted with a panel of light bulbs where the state of each particular bulb would indicate the on/off state of a particular register bit inside the computer. This allowed the engineers operating the computer to monitor the internal state of the machine, so this panel of lights came to be known as the 'monitor'. As early monitors were only capable of displaying a very limited amount of information, and were very transient, they were rarely considered for programme output. Instead, a line printer was the primary output device, while the monitor was limited to keeping track of the programme's operation.

As technology developed it was realized that the output of a CRT display was more flexible than a panel of light bulbs and eventually, by giving control of what was displayed to the programme itself, the monitor itself became a powerful output device in its own right.

Technologies[edit]
Further information: Comparison of CRT, LCD, Plasma, and OLED and History of display technology
Multiple technologies have been used for computer monitors. Until the 21st century most used cathode ray tubes but they have largely been superseded by LCD monitors.

Cathode ray tube[edit]
Main article: cathode ray tube
The first computer monitors used cathode ray tubes (CRTs). Prior to the advent of home computers in the late 1970s, it was common for a video display terminal (VDT) using a CRT to be physically integrated with a keyboard and other components of the system in a single large chassis. The display was monochrome and far less sharp and detailed than on a modern flat-panel monitor, necessitating the use of relatively large text and severely limiting the amount of information that could be displayed at one time. High-resolution CRT displays were developed for specialized military, industrial and scientific applications but they were far too costly for general use.

Some of the earliest home computers (such as the TRS-80 and Commodore PET) were limited to monochrome CRT displays, but color display capability was already a standard feature of the pioneering Apple II, introduced in 1977, and the specialty of the more graphically sophisticated Atari 800, introduced in 1979. Either computer could be connected to the antenna terminals of an ordinary color TV set or used with a purpose-made CRT color monitor for optimum resolution and color quality. Lagging several years behind, in 1981 IBM introduced the Color Graphics Adapter, which could display four colors with a resolution of 320 x 200 pixels, or it could produce 640 x 200 pixels with two colors. In 1984 IBM introduced the Enhanced Graphics Adapter which was capable of producing 16 colors and had a resolution of 640 x 350.[1]

By the end of the 1980s color CRT monitors that could clearly display 1024 x 768 pixels were widely available and increasingly affordable. During the following decade maximum display resolutions gradually increased and prices continued to fall. CRT technology remained dominant in the PC monitor market into the new millennium partly because it was cheaper to produce and offered viewing angles close to 180 degrees.[2] CRTs still offer some image quality advantages[clarification needed] over LCDs but improvements to the latter have made them much less obvious. The dynamic range of early LCD panels was very poor, and although text and other motionless graphics were sharper than on a CRT, an LCD characteristic known as pixel lag caused moving graphics to appear noticeably smeared and blurry.

Liquid crystal display[edit]
Main articles: Liquid-crystal display and Thin-film-transistor liquid-crystal display
There are multiple technologies that have been used to implement liquid crystal displays (LCD). Throughout the 1990s, the primary use of LCD technology as computer monitors was in laptops where the lower power consumption, lighter weight, and smaller physical size of LCDs justified the higher price versus a CRT. Commonly, the same laptop would be offered with an assortment of display options at increasing price points: (active or passive) monochrome, passive color, or active matrix color (TFT). As volume and manufacturing capability have improved, the monochrome and passive color technologies were dropped from most product lines.

TFT-LCD is a variant of LCD which is now the dominant technology used for computer monitors.[3]

The first standalone LCDs appeared in the mid-1990s selling for high prices. As prices declined over a period of years they became more popular, and by 1997 were competing with CRT monitors. Among the first desktop LCD computer monitors was the Eizo L66 in the mid-1990s, the Apple Studio Display in 1998, and the Apple Cinema Display in 1999. In 2003, TFT-LCDs outsold CRTs for the first time, becoming the primary technology used for computer monitors.[2] The main advantages of LCDs over CRT displays are that LCDs consume less power, take up much less space, and are considerably lighter. The now common active matrix TFT-LCD technology also has less flickering than CRTs, which reduces eye strain.[4] On the other hand, CRT monitors have superior contrast, have superior response time, are able to use multiple screen resolutions natively, and there is no discernible flicker if the refresh rate is set to a sufficiently high value. LCD monitors have now very high temporal accuracy and can be used for vision research.[5]

Organic light-emitting diode[edit]
Main article: Organic light-emitting diode
Organic light-emitting diode (OLED) monitors provide higher contrast and better viewing angles than LCDs but they require more power when displaying documents with white or bright backgrounds. In 2011, a 25-inch (64 cm) OLED monitor cost  7500, but the prices are expected to drop.[6]

Measurements of performance[edit]
The performance of a monitor is measured by the following parameters:

Luminance is measured in candelas per square meter (cd/m2 also called a Nit).
Aspect ratio is the ratio of the horizontal length to the vertical length. Monitors usually have the aspect ratio 4:3, 5:4, 16:10 or 16:9.
Viewable image size is usually measured diagonally, but the actual widths and heights are more informative since they are not affected by the aspect ratio in the same way. For CRTs, the viewable size is typically 1 in (25 mm) smaller than the tube itself.
Display resolution is the number of distinct pixels in each dimension that can be displayed. For a given display size, maximum resolution is limited by dot pitch.
Dot pitch is the distance between sub-pixels of the same color in millimeters. In general, the smaller the dot pitch, the sharper the picture will appear.
Refresh rate is the number of times in a second that a display is illuminated. Maximum refresh rate is limited by response time.
Response time is the time a pixel in a monitor takes to go from active (white) to inactive (black) and back to active (white) again, measured in milliseconds. Lower numbers mean faster transitions and therefore fewer visible image artifacts.
Contrast ratio is the ratio of the luminosity of the brightest color (white) to that of the darkest color (black) that the monitor is capable of producing.
Power consumption is measured in watts.
Delta-E: Color accuracy is measured in delta-E; the lower the delta-E, the more accurate the color representation. A delta-E of below 1 is imperceptible to the human eye. Delta-Es of 2 to 4 are considered good and require a sensitive eye to spot the difference.
Viewing angle is the maximum angle at which images on the monitor can be viewed, without excessive degradation to the image. It is measured in degrees horizontally and vertically.
Size[edit]
Main article: Display size

The area, height and width of displays with identical diagonal measurements vary dependent on aspect ratio.
On two-dimensional display devices such as computer monitors the display size or viewable image size is the actual amount of screen space that is available to display a picture, video or working space, without obstruction from the case or other aspects of the unit's design. The main measurements for display devices are: width, height, total area and the diagonal.

The size of a display is usually by monitor manufacturers given by the diagonal, i.e. the distance between two opposite screen corners. This method of measurement is inherited from the method used for the first generation of CRT television, when picture tubes with circular faces were in common use. Being circular, it was the external diameter of the glass envelope that described their size. Since these circular tubes were used to display rectangular images, the diagonal measurement of the rectangular image was smaller than the diameter of the tube's face (due to the thickness of the glass). This method continued even when cathode ray tubes were manufactured as rounded rectangles; it had the advantage of being a single number specifying the size, and was not confusing when the aspect ratio was universally 4:3.

With the introduction of flat panel technology, the diagonal measurement became the actual diagonal of the visible display. This meant that an eighteen-inch LCD had a larger visible area than an eighteen-inch cathode ray tube.

The estimation of the monitor size by the distance between opposite corners does not take into account the display aspect ratio, so that for example a 16:9 21-inch (53 cm) widescreen display has less area, than a 21-inch (53 cm) 4:3 screen. The 4:3 screen has dimensions of 16.8 in   12.6 in (43 cm   32 cm) and area 211 sq in (1,360 cm2), while the widescreen is 18.3 in   10.3 in (46 cm   26 cm), 188 sq in (1,210 cm2).

Aspect ratio[edit]
Main article: Display aspect ratio
Until about 2003, most computer monitors had a 4:3 aspect ratio and some had 5:4. Between 2003 and 2006, monitors with 16:9 and mostly 16:10 (8:5) aspect ratios became commonly available, first in laptops and later also in standalone monitors. Reasons for this transition was productive uses for such monitors, i.e. besides widescreen computer game play and movie viewing, are the word processor display of two standard letter pages side by side, as well as CAD displays of large-size drawings and CAD application menus at the same time.[7][8] In 2008 16:10 became the most common sold aspect ratio for LCD monitors and the same year 16:10 was the mainstream standard for laptops and notebook computers.[9]

In 2010 the computer industry started to move over from 16:10 to 16:9 because 16:9 was chosen to be the standard high-definition television display size, and because they were cheaper to manufacture.

In 2011 non-widescreen displays with 4:3 aspect ratios were only being manufactured in small quantities. According to Samsung this was because the "Demand for the old 'Square monitors' has decreased rapidly over the last couple of years," and "I predict that by the end of 2011, production on all 4:3 or similar panels will be halted due to a lack of demand."[10]

Resolution[edit]
Main article: Display resolution
The resolution for computer monitors has increased over time. From 320x200 during the early 1980s, to 800x600 during the late 1990s. Since 2009, the most commonly sold resolution for computer monitors is 1920x1080.[11] Before 2013 top-end consumer products were limited to 2560x1600 at 30 in (76 cm), excluding Apple products.[12] Apple introduced 2880x1800 with Retina MacBook Pro at 15.4 in (39 cm) on June 12, 2012, and introduced a 5120x2880 Retina iMac at 27 in (69 cm) on October 16, 2014. By 2015 all major display manufacturers had released 3840x2160 resolution displays.

Additional features[edit]
Power saving[edit]
Most modern monitors will switch to a power-saving mode if no video-input signal is received. This allows modern operating systems to turn off a monitor after a specified period of inactivity. This also extends the monitor's service life.

Some monitors will also switch themselves off after a time period on standby.

Most modern laptops provide a method of screen dimming after periods of inactivity or when the battery is in use. This extends battery life and reduces wear.

Integrated accessories[edit]
Many monitors have other accessories (or connections for them) integrated. This places standard ports within easy reach and eliminates the need for another separate hub, camera, microphone, or set of speakers. These monitors have advanced microprocessors which contain codec information, Windows Interface drivers and other small software which help in proper functioning of these functions.

Glossy screen[edit]
Main article: Glossy display
Some displays, especially newer LCD monitors, replace the traditional anti-glare matte finish with a glossy one. This increases color saturation and sharpness but reflections from lights and windows are very visible. Anti-reflective coatings are sometimes applied to help reduce reflections, although this only mitigates the effect.

Curved designs[edit]
In about 2009, NEC/Alienware together with Ostendo Technologies (based in Carlsbad, CA) were offering a curved (concave) 43-inch (110 cm) monitor that allows better viewing angles near the edges, covering 75% of peripheral vision. This monitor had 2880x900 resolution, LED backlight and was marketed as suitable both for gaming and office work, while for  6499 it was rather expensive.[13] While this particular monitor is no longer in production, most PC manufacturers now offer some sort of curved desktop display.

Directional screen[edit]
Narrow viewing angle screens are used in some security conscious applications.

3D[edit]
Main article: Stereo display
Newer monitors are able to display a different image for each eye, often with the help of special glasses, giving the perception of depth.

Active shutter
Main article: Active shutter 3D system
Polarized
Main article: Polarized 3D system
Autostereoscopic
Main article: Autostereoscopy
A directional screen which generates 3D images without headgear.

Touch screen[edit]
Main article: Touchscreen
These monitors use touching of the screen as an input method. Items can be selected or moved with a finger, and finger gestures may be used to convey commands. The screen will need frequent cleaning due to image degradation from fingerprints.

Tablet screens[edit]
Main article: Graphics tablet/screen hybrid
A combination of a monitor with a graphics tablet. Such devices are typically unresponsive to touch without the use of one or more special tools' pressure. Newer models however are now able to detect touch from any pressure and often have the ability to detect tilt and rotation as well.

Touch and tablet screens are used on LCDs as a substitute for the light pen, which can only work on CRTs.

Mounting[edit]
Computer monitors are provided with a variety of methods for mounting them depending on the application and environment.

Desktop[edit]
A desktop monitor is typically provided with a stand from the manufacturer which lifts the monitor up to a more ergonomic viewing height. The stand may be attached to the monitor using a proprietary method or may use, or be adaptable to, a Video Electronics Standards Association, VESA, standard mount. Using a VESA standard mount allows the monitor to be used with an after-market stand once the original stand is removed. Stands may be fixed or offer a variety of features such as height adjustment, horizontal swivel, and landscape or portrait screen orientation.

VESA mount[edit]
The Flat Display Mounting Interface (FDMI), also known as VESA Mounting Interface Standard (MIS) or colloquially as a VESA mount, is a family of standards defined by the Video Electronics Standards Association for mounting flat panel monitors, TVs, and other displays to stands or wall mounts.[14] It is implemented on most modern flat-panel monitors and TVs.

For Computer Monitors, the VESA Mount typically consists of four threaded holes on the rear of the display that will mate with an adapter bracket.

Rack mount[edit]
Rack mount computer monitors are available in two styles and are intended to be mounted into a 19-inch rack:


A fixed 19-inch (48 cm), 4:3 rack mount LCD monitor
Fixed
A fixed rack mount monitor is mounted directly to the rack with the LCD visible at all times. The height of the unit is measured in rack units (RU) and 8U or 9U are most common to fit 17-inch or 19-inch LCDs. The front sides of the unit are provided with flanges to mount to the rack, providing appropriately spaced holes or slots for the rack mounting screws. A 19-inch diagonal LCD is the largest size that will fit within the rails of a 19-inch rack. Larger LCDs may be accommodated but are 'mount-on-rack' and extend forward of the rack. There are smaller display units, typically used in broadcast environments, which fit multiple smaller LCDs side by side into one rack mount.


A 1U stowable clamshell 19-inch (48 cm), 4:3 rack mount LCD monitor with keyboard
Stowable
A stowable rack mount monitor is 1U, 2U or 3U high and is mounted on rack slides allowing the display to be folded down and the unit slid into the rack for storage. The display is visible only when the display is pulled out of the rack and deployed. These units may include only a display or may be equipped with a keyboard creating a KVM (Keyboard Video Monitor). Most common are systems with a single LCD but there are systems providing two or three displays in a single rack mount system.


A panel mount 19-inch (48 cm), 4:3 rack mount LCD monitor
Panel mount[edit]
A panel mount computer monitor is intended for mounting into a flat surface with the front of the display unit protruding just slightly. They may also be mounted to the rear of the panel. A flange is provided around the LCD, sides, top and bottom, to allow mounting. This contrasts with a rack mount display where the flanges are only on the sides. The flanges will be provided with holes for thru-bolts or may have studs welded to the rear surface to secure the unit in the hole in the panel. Often a gasket is provided to provide a water-tight seal to the panel and the front of the LCD will be sealed to the back of the front panel to prevent water and dirt contamination.

Open frame[edit]
An open frame monitor provides the LCD monitor and enough supporting structure to hold associated electronics and to minimally support the LCD. Provision will be made for attaching the unit to some external structure for support and protection. Open frame LCDs are intended to be built in to some other piece of equipment. An arcade video game would be a good example with the display mounted inside the cabinet. There is usually an open frame display inside all end-use displays with the end-use display simply providing an attractive protective enclosure. Some rack mount LCD manufacturers will purchase desk-top displays, take them apart, and discard the outer plastic parts, keeping the inner open-frame LCD for inclusion into their product.

Security vulnerabilities[edit]
According to an NSA document leaked to Der Spiegel, the NSA sometimes swaps the monitor cables on targeted computers with a bugged monitor cable in order to allow the NSA to remotely see what's displayed on the targeted computer monitor.[15]

Van Eck phreaking is the process of remotely displaying the contents of a CRT or LCD by detecting its electromagnetic emissions. It is named after Dutch computer researcher Wim van Eck, who in 1985 published the first paper on it, including proof of concept. Phreaking is the process of exploiting telephone networks, used here because of its connection to eavesdropping.[citation needed]

See also[edit]
History of display technology
Flat panel display
Multi-monitor
Vector monitor
Virtual desktopremaining 8 entries were black (see picture).[14]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks 
such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Further, algorithms for performing computations have existed 
since antiquity, even before the development of sophisticated computing equipment. The ancient Sanskrit treatise Shulba Sutras, or "Rules of the Chord", is a book of 
algorithms written in 800 BC for constructing geometric objects like altars using a peg and chord, an early precursor of the modern field of computational geometry.
Blaise Pascal designed and constructed the first working mechanical calculator, Pascal's calculator, in 1642.[2] In 1673, Gottfried Leibniz demonstrated a digital 
mechanical calculator, called the Stepped Reckoner.[3] He may be considered the first computer scientist and information theorist, for, among other reasons, documenting 
the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he released his simplified arithmometer, which was the 
first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic 
Computer A computer is a general purpose device that can be programmed to carry out a set of arithmetic or logical operations automatically. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem.

Conventionally, a computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logic operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices allow information to be retrieved from an external source, and the result of operations saved and retrieved.

Mechanical analog computers started appearing in the first century and were later used in the medieval era for astronomical calculations. In World War II, mechanical analog computers were used for specialized military applications such as calculating torpedo aiming. During this time the first electronic digital computers were developed. Originally they were the size of a large room, consuming as much power as several hundred modern personal computers (PCs).[1]

Modern computers based on integrated circuits are millions to billions of times more capable than the early machines, and occupy a fraction of the space.[2] Computers are small enough to fit into mobile devices, and mobile computers can be powered by small batteries. Personal computers in their various forms are icons of the Information Age and are generally considered as "computers". However, the embedded computers found in many devices from MP3 players to fighter aircraft and from electronic toys to industrial robots are the most numerous.The first known use of the word "computer" was in 1613 in a book called The Yong Mans Gleanings by English writer Richard Braithwait: "I haue read the truest computer of Times, and the best Arithmetician that euer breathed, and he reduceth thy dayes into a short number." It referred to a person who carried out calculations, or computations. The word continued with the same meaning until the middle of the 20th century. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.[3]

History
Main article: History of computing hardware
Pre twentieth century

The Ishango bone
Devices have been used to aid computation for thousands of years, mostly using one to one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers.[4][5] The use of counting rods is one example.


Suanpan (the number represented on this abacus is 6,302,715,408)
The abacus was initially used for arithmetic tasks. The Roman abacus was used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.


The ancient Greek designed Antikythera mechanism, dating between 150 to 100 BC, is the world's oldest analog computer.
The Antikythera mechanism is believed to be the earliest mechanical analog "computer", according to Derek J. de Solla Price.[6] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.

Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Ab  Rayh n al B r n  in the early 11th century.[7] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[8][9] and gear wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[10] Ab  Rayh n al B r n  invented the first mechanical geared lunisolar calendar astrolabe,[11] an early fixed wired knowledge processing machine[12] with a gear train and gear wheels,[13] circa 1000 AD.

The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.

The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.


A slide rule
The slide rule was invented around 1620 1630, shortly after the publication of the concept of the logarithm. It is a hand operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving time distance problems in light aircraft. To save space and for ease of reading, these are typically circular devices rather than the classic linear slide rule shape. A popular example is the E6B.

In the 1770s Pierre Jaquet Droz, a Swiss watchmaker, built a mechanical doll (automata) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically "programmed" to read instructions. Along with two other complex machines, the doll is at the Mus e d'Art et d'Histoire of Neuch tel, Switzerland, and still operates.[14]

The tide predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.

The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel and disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball and disk integrators.[15] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.

First general purpose computing device

A portion of Babbage's Difference engine.
Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer",[16] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general purpose computer that could be described in modern terms as Turing complete.[17][18]

The machine was about a century ahead of its time. All the parts for his machine had to be made by hand   this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.

Later analog computers

Sir William Thomson's third tide predicting machine design, 1879 81
During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.[19]

The first modern analog computer was a tide predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel and disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.[15]

The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious.

By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remain in use in some specialized applications such as education (control systems) and aircraft (slide rule).

Digital computer development
The principle of the modern computer was first described by mathematician and pioneering computer scientist Alan Turing, who set out the idea in his seminal 1936 paper,[20] On Computable Numbers. Turing reformulated Kurt G del's 1931 results on the limits of proof and computation, replacing G del's universal arithmetic based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the Entscheidungsproblem by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt.

He also introduced the notion of a 'Universal Machine' (now known as a Universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[21] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.

Electromechanical
By 1938 the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.


Replica of Zuse's Z3, the first fully automatic, digital (electromechanical) computer.
Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.[22]

In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer.[23][24] The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5 10 Hz.[25] Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard to implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.[26] The Z3 was Turing complete.[27][28]

Vacuum tubes and digital electronic circuits
Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[19] In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff Berry Computer (ABC) in 1942,[29] the first "automatic electronic digital computer".[30] This design was also all electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[31]


Colossus was the first electronic digital programmable computing device, and was used to break German ciphers during World War II.
During World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro mechanical bombes. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus.[31] He spent eleven months from early February 1943 designing and building the first Colossus.[32] After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944[33] and attacked its first message on 5 February.[31]

Colossus was the world's first electronic digital programmable computer.[19] It used a large number of valves (vacuum tubes). It had paper tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process.[34][35]


ENIAC was the first Turing complete device, and performed ballistics trajectory calculations for the United States Army.
The US built ENIAC[36] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing complete device and could compute any problem that would fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.

It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.[37]

Stored programs
Three tall racks containing electronic circuit boards
A section of the Manchester Small Scale Experimental Machine, the first stored program computer.
Early computing machines had fixed programs. Changing its function required the re wiring and re structuring of the machine.[31] With the proposal of the stored program computer this changed. A stored program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored program computer was laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored program digital computer. His 1945 report ‘Proposed Electronic Calculator  was the first specification for such a device. John von Neumann at the University of Pennsylvania, also circulated his First Draft of a Report on the EDVAC in 1945.[19]


Ferranti Mark 1, c. 1951.
The Manchester Small Scale Experimental Machine, nicknamed Baby, was the world's first stored program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.[38] It was designed as a testbed for the Williams tube the first random access digital storage device.[39] Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer.[40] As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1.

The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general purpose computer.[41] Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[42] In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 [43] and ran the world's first regular routine office computer job.

Transistors

A bipolar junction transistor
The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.

At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.[44] Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955,[45] built by the electronics division of the Atomic Energy Research Establishment at Harwell.[46][47]

Integrated circuits
The next great advance in computing power came with the advent of the integrated circuit. The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.[48]

The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor.[49] Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.[50] In his patent application of 6 February 1959, Kilby described his new device as "a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated".[51][52] Noyce also came up with his own idea of an integrated circuit half a year later than Kilby.[53] His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.

This new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor", it is largely undisputed that the first single chip microprocessor was the Intel 4004,[54] designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.[55]

Mobile computers become dominant
With the continued miniaturization of computing resources, and advancements in portable battery life, portable computers grew in popularity in the 2000s.[56] The same developments that spurred the growth of laptop computers and other portable computers allowed manufacturers to integrate computing resources into cellular phones. These so called smartphones and tablets run on a variety of operating systems and have become the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013.[57]

Programs
The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language.

In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.

Stored program architecture
Main articles: Computer program and Computer programming

Replica of the Small Scale Experimental Machine (SSEM), the world's first stored program computer, at the Museum of Science and Industry in Manchester, England
This section applies to most common RAM machine based computers.

In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called "jump" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that "remembers" the location it jumped from and another instruction to return to the instruction following that jump instruction.

Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.

Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:

  begin:
  addi  8,  0, 0           # initialize sum to 0
  addi  9,  0, 1           # set first number to add = 1
  loop:
  slti  10,  9, 1000       # check if the number is less than 1000
  beq  10,  0, finish      # if odd number is greater than n then exit
  add  8,  8,  9           # update sum
  addi  9,  9, 1           # get next number
  j loop                   # repeat the summing process
  finish:
  add  2,  8,  0           # put sum in output register
Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.

Machine code
In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program[citation needed], architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.

While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,[58] it is extremely tedious and potentially error prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember   a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.


A 1970s punched card containing one line from a FORTRAN program. The card reads: "Z(1) = Y + W(1)" and is labeled "PROJ039" for identification purposes.
Programming language
Main article: Programming language
Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.

Low level languages
Main article: Low level programming language
Machine languages and the assembly languages that represent them (collectively termed low level programming languages) tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a PDA or a hand held videogame) cannot understand the machine language of an Intel Pentium or the AMD Athlon 64 computer that might be in a PC.[59]

High level languages/Third Generation Language
Main article: High level programming language
Though considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually "compiled" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.[60] High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.

Fourth Generation Languages
These 4G languages are less procedural than 3G languages. The benefit of 4GL is that it provides ways to obtain information without requiring the direct help of a programmer. Example of 4GL is SQL.

Program design

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2012)
Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies. The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.

Bugs
Main article: Software bug

The actual first computer bug, a moth found trapped on a relay of the Harvard Mark II computer
Errors in computer programs are called "bugs". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to "hang", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.[61]

Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term "bugs" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.[62]

Components
Main articles: Central processing unit and Microprocessor
File:Computer Components.webm
Video demonstrating the standard components of a "slimline" computer
A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires.

Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a "1", and when off it represents a "0" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.

Control unit
Main articles: CPU design and Control unit

Diagram showing how a particular MIPS architecture instruction would be decoded by the control system
The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.[63] Control systems in advanced computers may change the order of execution of some instructions to improve performance.

A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.[64]

The control system's function is as follows note that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:

Read the code for the next instruction from the cell indicated by the program counter.
Decode the numerical code for the instruction into a set of commands or signals for each of the other systems.
Increment the program counter so it points to the next instruction.
Read whatever data the instruction requires from cells in memory (or perhaps from an input device). The location of this required data is typically stored within the instruction code.
Provide the necessary data to an ALU or register.
If the instruction requires an ALU or specialized hardware to complete, instruct the hardware to perform the requested operation.
Write the result from the ALU back to a memory location or to a register or perhaps an output device.
Jump back to step (1).
Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as "jumps" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).

The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.

Central processing unit (CPU)
The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid 1970s CPUs have typically been constructed on a single integrated circuit called a microprocessor.

Arithmetic logic unit (ALU)
Main article: Arithmetic logic unit
The ALU is capable of performing two classes of operations: arithmetic and logic.[65]

The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other ("is 64 greater than 65 ").

Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.

Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously.[66] Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.

Memory
Main article: Computer data storage

Magnetic core memory was the computer memory of choice throughout the 1960s, until it was replaced by semiconductor memory.
A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered "address" and can store a single number. The computer can be instructed to "put the number 123 into the cell numbered 1357" or to "add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595." The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.

In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or  128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.

The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.

Computer main memory comes in two principal varieties:

random access memory or RAM
read only memory or ROM
RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.[67]

In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.

Input/output (I/O)
Main article: Input/output

Hard disk drives are common storage devices used with computers.
I/O is the means by which a computer exchanges information with the outside world.[68] Devices that provide input or output to the computer are called peripherals.[69] On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.

I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics.[citation needed] Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O.

Multitasking
Main article: Computer multitasking
While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.[70]

One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running "at the same time". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed "time sharing" since each program is allocated a "slice" of time in turn.[71]

Before the era of cheap computers, the principal use for multitasking was to allow many people to share the same computer.

Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a "time slice" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.

Multiprocessing
Main article: Multiprocessing

Cray designed many supercomputers that used multiprocessing heavily.
Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower end markets as a result.

Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored program architecture and from general purpose computers.[72] They often feature thousands of CPUs, customized high speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large scale simulation, graphics rendering, and cryptography applications, as well as with other so called "embarrassingly parallel" tasks.

Networking and the Internet
Main articles: Computer networking and Internet

Visualization of a portion of the routes on the Internet
Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large scale example of such a system, which led to a number of special purpose commercial systems such as Sabre.[73]

In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.[74] The technologies that made the Arpanet possible spread and evolved.

In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high tech environments, but in the 1990s the spread of applications like e mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. "Wireless" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.

Computer architecture paradigms
There are many types of computer architectures:

Quantum computer vs. Chemical computer
Scalar processor vs. Vector processor
Non Uniform Memory Access (NUMA) computers
Register machine vs. Stack machine
Harvard architecture vs. von Neumann architecture
Cellular architecture
Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing.[75]

Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms.

The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.

Misconceptions
Main articles: Human computer and Harvard Computers

Women as computers in NACA High Speed Flight Station "Computer Room"
A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word "computer" is synonymous with a personal electronic computer, the modern[76] definition of a computer is literally: "A device that computes, especially a programmable [usually] electronic machine that performs high speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information."[77] Any device which processes information qualifies as a computer, especially if the processing is purposeful.[citation needed]

Unconventional computing
Main article: Unconventional computing
Historically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example.[citation needed] More realistically, modern computers are made out of transistors made of photolithographed semiconductors.

Future
There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.

Further topics
Glossary of computers
Artificial intelligence
A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning.

Hardware
Main articles: Computer hardware and Personal computer hardware
The term hardware covers all of those parts of a computer that are tangible objects. Circuits, displays, power supplies, cables, keyboards, printers and mice are all hardware.

History of computing hardware
Main article: History of computing hardware
First generation (mechanical/electromechanical)  Calculators Pascal's calculator, Arithmometer, Difference engine, Quevedo's analytical machines
Programmable devices  Jacquard loom, Analytical engine, IBM ASCC/Harvard Mark I, Harvard Mark II, IBM SSEC, Z1, Z2, Z3
Second generation (vacuum tubes)  Calculators Atanasoff Berry Computer, IBM 604, UNIVAC 60, UNIVAC 120
Programmable devices  Colossus, ENIAC, Manchester Small Scale Experimental Machine, EDSAC, Manchester Mark 1, Ferranti Pegasus, Ferranti Mercury, CSIRAC, EDVAC, UNIVAC I, IBM 701, IBM 702, IBM 650, Z22
Third generation (discrete transistors and SSI, MSI, LSI integrated circuits) Mainframes  IBM 7090, IBM 7080, IBM System/360, BUNCH
Minicomputer  HP 2116A, IBM System/32, IBM System/36, LINC, PDP 8, PDP 11
Fourth generation (VLSI integrated circuits)  Minicomputer  VAX, IBM System i
4 bit microcomputer Intel 4004, Intel 4040
8 bit microcomputer Intel 8008, Intel 8080, Motorola 6800, Motorola 6809, MOS Technology 6502, Zilog Z80
16 bit microcomputer  Intel 8088, Zilog Z8000, WDC 65816/65802
32 bit microcomputer  Intel 80386, Pentium, Motorola 68000, ARM
64 bit microcomputer[78]  Alpha, MIPS, PA RISC, PowerPC, SPARC, x86 64, ARMv8 A
Embedded computer Intel 8048, Intel 8051
Personal computer Desktop computer, Home computer, Laptop computer, Personal digital assistant (PDA), Portable computer, Tablet PC, Wearable computer
Theoretical/experimental  Quantum computer, Chemical computer, DNA computing, Optical computer, Spintronics based computer
Other hardware topics
Peripheral device (input/output)  Input Mouse, keyboard, joystick, image scanner, webcam, graphics tablet, microphone
Output  Monitor, printer, loudspeaker
Both  Floppy disk drive, hard disk drive, optical disc drive, teleprinter
Computer buses  Short range RS 232, SCSI, PCI, USB
Long range (computer networking)  Ethernet, ATM, FDDI
Software
Main article: Computer software
Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. When software is stored in hardware that cannot easily be modified (such as BIOS ROM in an IBM PC compatible), it is sometimes called "firmware".

Operating system /System Software Unix and BSD  UNIX System V, IBM AIX, HP UX, Solaris (SunOS), IRIX, List of BSD operating systems
GNU/Linux List of Linux distributions, Comparison of Linux distributions
Microsoft Windows Windows 95, Windows 98, Windows NT, Windows 2000, Windows Me, Windows XP, Windows Vista, Windows 7, Windows 8, Windows 10
DOS 86 DOS (QDOS), IBM PC DOS, MS DOS, DR DOS, FreeDOS
Mac OS  Mac OS classic, Mac OS X
Embedded and real time  List of embedded operating systems
Experimental  Amoeba, Oberon/Bluebottle, Plan 9 from Bell Labs
Library Multimedia  DirectX, OpenGL, OpenAL, Vulkan_(API)
Programming library C standard library, Standard Template Library
Data  Protocol  TCP/IP, Kermit, FTP, HTTP, SMTP
File format HTML, XML, JPEG, MPEG, PNG
User interface  Graphical user interface (WIMP) Microsoft Windows, GNOME, KDE, QNX Photon, CDE, GEM, Aqua
Text based user interface Command line interface, Text user interface
Application Software  Office suite  Word processing, Desktop publishing, Presentation program, Database management system, Scheduling & Time management, Spreadsheet, Accounting software
Internet Access Browser, E mail client, Web server, Mail transfer agent, Instant messaging
Design and manufacturing  Computer aided design, Computer aided manufacturing, Plant management, Robotic manufacturing, Supply chain management
Graphics  Raster graphics editor, Vector graphics editor, 3D modeler, Animation editor, 3D computer graphics, Video editing, Image processing
Audio Digital audio editor, Audio playback, Mixing, Audio synthesis, Computer music
Software engineering  Compiler, Assembler, Interpreter, Debugger, Text editor, Integrated development environment, Software performance analysis, Revision control, Software configuration management
Educational Edutainment, Educational game, Serious game, Flight simulator
Games Strategy, Arcade, Puzzle, Simulation, First person shooter, Platform, Massively multiplayer, Interactive fiction
Misc  Artificial intelligence, Antivirus software, Malware scanner, Installer/Package management systems, File manager
Languages
There are thousands of different programming languages some intended to be general purpose, others useful only for highly specialized applications.

Programming languages
Lists of programming languages  Timeline of programming languages, List of programming languages by category, Generational list of programming languages, List of programming languages, Non English based programming languages
Commonly used assembly languages  ARM, MIPS, x86
Commonly used high level programming languages  Ada, BASIC, C, C++, C#, COBOL, Fortran, PL/1, REXX, Java, Lisp, Pascal, Object Pascal
Commonly used scripting languages Bourne script, JavaScript, Python, Ruby, PHP, Perl
Firmware
Firmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it.

Types of computers
Computers are typically classified based on their uses:

Based on uses
Analog computer
Digital computer
Hybrid computer
Based on sizes
Micro computer
Personal computer
Mini Computer
Mainframe computer
Super computer
Input Devices
When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand operated or automated. The act of processing is mainly regulated by the CPU. Some examples of hand operated input devices are:

Overlay keyboard
Trackball
Joystick
Digital camera
Microphone
Touchscreen
Digital video
Image scanner
Graphics tablet
Computer keyboard
Mouse
Output Devices
The means through which computer gives output are known as output devices. Some examples of output devices are:

Computer monitor
Printer
Projector
Sound card
PC speaker
Video card
Professions and organizations
As the use of computers has spread throughout society, there are an increasing number of careers involving computers.

Computer related professions
Hardware related  Electrical engineering, Electronic engineering, Computer engineering, Telecommunications engineering, Optical engineering, Nanoengineering
Software related  Computer science, Computer engineering, Desktop publishing, Human computer interaction, Information technology, Information systems, Computational science, Software engineering, Video game industry, Web design

mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[4] He 
started developing this machine in 1834 and "in less than two years he had sketched out many of the salient features of the modern computer".[5] "A crucial step was the 
adoption of a punched card system derived from the Jacquard loom"[5] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the 
Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first 
computer program.[6] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became 
part of IBM. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and 
was also in the calculator business[7] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used 
cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[8]
During the 1940s, as new and more powerful computing machines were developed, the term computer came to refer to the machines rather than their human 
predecessors.[9] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study 
computation in general. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[10][11] The world's first computer 
science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science 
degree program in the United States was formed at Purdue University in 1962.[12] Since practical computers became available, many applications of computing have 
become distinct areas of study in their own rights.
Although many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted 
among the greater academic population.[13][14] It is the now well known IBM brand that formed part of the computer science revolution during this time. IBM (short for 
International Business Machines) released the IBM 704[15] and later the IBM 709[16] computers, which were widely used during the exploration period of such devices. 
"Still, working with the IBM [computer] was frustrating […] if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to 
start the whole process over again".[13] During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were 
commonplace.[14]
Time has seen significant improvements in the usability and effectiveness of computing technology.[17] Modern society has seen a significant shift in the users of 
computer technology, from usage only by experts and professionals, to a near ubiquitous user base. Initially, computers were quite costly, and some degree of human aid 
was needed for efficient use,in part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was 
needed for common usage.
Despite its short history as a formal academic discipline, computer science has made a number of fundamental contributions to science and society,in fact, along with 
electronics, it is a founding science of the current epoch of human history called the Information Age and a driver of the Information Revolution, seen as the third major 
leap in human technological progress after the Industrial Revolution (1750 1850 CE) and the Agricultural Revolution (8000 5000 BC).
These contributions include:
The start of the "digital revolution", which includes the current Information Age and the Internet.[19]
A formal definition of computation and computability, and proof that there are computationally unsolvable and intractable problems.[20]
The concept of a programming language, a tool for the precise expression of methodological information at various levels of abstraction.[21]
In cryptography, breaking the Enigma code was an important factor contributing to the Allied victory in World War II.[18]
Scientific computing enabled practical evaluation of processes and situations of great complexity, as well as experimentation entirely by s oftware. It also enabled 
advanced study of the mind, and mapping of the human genome became possible with the Human Genome Project.[19] Distributed computing projects such as 
Folding@home explore protein folding.
Algorithmic trading has increased the efficiency and liquidity of financial markets by using artificial intelligence, machine learning, and other statistical and numerical 
techniques on a large scale.[22] High frequency algorithmic trading can also exacerbate volatility.[23]
Computer graphics and computer generated imagery have become ubiquitous in modern entertainment, particularly in television, cinema, advertising, animation and video 
games. Even films that feature no explicit CGI are usually "filmed" now on digital cameras, or edited or post processed using a digital video editor.[24][25]
Simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations 
(notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and 
electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated 
circuits.[citation needed]
Artificial intelligence is becoming increasingly important as it gets more efficient and complex. There are many applications of AI, some of which can be seen at home, 
such as robotic vacuum cleaners. It is also present in video games and on the modern battlefield in drones, anti missile systems, and squad support robots.
Main article: Philosophy of computer science
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are 
science, technology, and mathematics.[26] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[27] Amnon H. Eden described 
them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs 
deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific 
paradigm" (which approaches computer related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).[28]
Name of the field[edit]
Although first proposed in 1956,[14] the term "computer science" appears in a 1959 article in Communications of the ACM,[29] in which Louis Fein argues for the creation 
of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921,[30] justifying the name by arguing that, like management 
science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[29] His efforts, and those of others such 
as numerical analyst George Forsythe, were rewarded: universities went on to create such programs, starting with Purdue in 1962.[31] Despite its name, a significant 
amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[32] Certain departments 
of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[33] to reflect the 
fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the 
Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the 
Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a distinct field of data analysis, including statistics and 
databases.
Also, in the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM,turingineer, 
turologist, flow charts man, applied meta mathematician, and applied epistemologist.[34] Three months later in the same journal, comptologist was suggested, followed next 
year by hypologist.[35] The term computics has also been suggested.[36] In Europe, terms derived from contracted translations of the expression "automatic information" 
(e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), 
inform tica (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (           , which means informatics) in Greek. Similar words have also 
been adopted in the UK (as in the School of Informatics of the University of Edinburgh).[37]
A folkloric quotation, often attributed to,but almost certainly not first formulated by,Edsger Dijkstra, states that "computer science is no more about computers than 
astronomy is about telescopes."[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than 
computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems 
and their deployment is often called information technology or information systems. However, there has been much cross fertilization of ideas between the various 
computer related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, 
biology, statistics, and logic.
Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that 
computing is a mathematical science.[10] Early computer science was strongly influenced by the work of mathematicians such as Kurt G del and Alan Turing, and there 
continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[14]
The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term "software 
engineering" means, and how computer science is defined.[38] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has 
claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of 
specific computations to achieve practical goals, making the two separate but complementary disciplines.[39]
The academic, political, and funding aspects of computer science tend to depend on whether a department formed with a mathematical emphasis or with an engineering 
emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of 
departments tend to make efforts to bridge the field educationally if not across all research.
Areas of computer science[edit]
Further information: Outline of computer science
As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing 
computing systems in hardware and software.[40][41] CSAB, formerly called Computing Sciences Accreditation Board,which is made up of representatives of the 
Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[42],identifies four areas that it considers crucial to the discipline of computer 
science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these 
four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel 
computation, distributed computation, human computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important 
areas of computer science.[40]
Theoretical computer science[edit]
Main article: Theoretical computer science
The broader field of theoretical computer science encompasses both the classical theory of computation and a wide range of other topics that focus on the more abstract, 
logical, and mathematical aspects of computing.
Theory of computation[edit]
Main article: Theory of computation
According to Peter Denning, the fundamental question underlying computer science is, "What can be (efficiently) automated "[10] Theory of computation is focused on 
answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first 
question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by 
computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP  problem, one of the Millennium Prize Problems,[43] is an open problem in the theory of computation.
Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable computer programs. 
Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness 
and resources consumption, and implementation (commonly referred to as coding[1][2]) of algorithms in a target programming language. Source code is written in one or 
more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem. 
The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms and formal 
logic.

Related tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine 
code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the 
term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software 
development practices.
Within software engineering, programming (the implementation) is regarded as one phase in a software development process.

There is an ongoing debate on the extent to which the writing of programs is an art form, a craft, or an engineering discipline.[3] In general, good programming is 
considered to be the measured application of all three, with the goal of producing an efficient and evolvable software solution (the criteria for "efficient" and "evolvable" 
vary considerably). The discipline differs from many other technical professions in that programmers, in general, do not need to be licensed or pass any standardized (or 
governmentally regulated) certification tests in order to call themselves "programmers" or even "software engineers." Because the discipline covers many areas, which 
may or may not include critical applications, it is debatable whether licensing is required for the profession as a whole. In most cases, the discipline is self governed by 
the entities which require the programming, and sometimes very strict environments are defined (e.g. United States Air Force use of AdaCore and security clearance). 
However, representing oneself as a "professional software engineer" without a license from an accredited institution is illegal in many parts of the world.

Another ongoing debate is the extent to which the programming language used in writing computer programs affects the form that the final program takes.[citation needed] 
This debate is analogous to that surrounding the Sapir,Whorf hypothesis[4] in linguistics and cognitive science, which postulates that a particular spoken language's 
nature influences the habitual thought of its speakers. Different language patterns yield different patterns of thought. This idea challenges the possibility of representing the 
world perfectly with language, because it acknowledges that the mechanisms of any language condition the thoughts of its speaker community.
Ancient cultures seemed to have no conception of computing beyond arithmetic, algebra, and geometry, occasionally devising computational systems with elements of 
calculus (e.g. the method of exhaustion). The only mechanical device that existed for numerical computation at the beginning of human history was the abacus, invented in 
Sumeria circa 2500 BC. Later, the Antikythera mechanism, invented some time around 100 BC in ancient Greece, is the first known mechanical calculator utilizing gears 
of various sizes and configuration to perform calculations,[5] which tracked the metonic cycle still used in lunar to solar calendars, and which is consistent for calculating 
the dates of the Olympiads.[6]

The Kurdish medieval scientist Al Jazari built programmable automata in 1206 AD. One system employed in these devices was the use of pegs and cams placed into a 
wooden drum at specific locations, which would sequentially trigger levers that in turn operated percussion instruments. The output of this device was a small drummer 
playing various rhythms and drum patterns.[7] The Jacquard loom, which Joseph Marie Jacquard developed in 1801, uses a series of pasteboard cards with holes 
punched in them. The hole pattern represented the pattern that the loom had to follow in weaving cloth. The loom could produce entirely different weaves using different 
sets of cards.

Charles Babbage adopted the use of punched cards around 1830 to control his Analytical Engine. Mathematician Ada Lovelace, a friend of Babbage, between 1842 and 
1843 translated an article by Italian military engineer Luigi Menabrea on the engine,[8] which she supplemented with a set of notes, simply called Notes. These notes 
include algorithm to calculate a sequence of Bernoulli numbers,[9] intended to be carried out by a machine. Despite controversy over scope of her contribution, many 
consider this algorithm to be the first computer program.[8]


Data and instructions were once stored on external punched cards, which were kept in order and arranged in program decks.
In the 1880s, Herman Hollerith invented the recording of data on a medium that could then be read by a machine. Prior uses of machine readable media, above, had been 
for lists of instructions (not data) to drive programmed machines such as Jacquard looms and mechanized musical instruments. "After some initial trials with paper tape, 
he settled on punched cards..."[10] To process these punched cards, first known as "Hollerith cards" he invented the keypunch, sorter, and tabulator unit record 
machines.[11] These inventions were the foundation of the data processing industry. In 1896 he founded the Tabulating Machine Company (which later became the core 
of IBM). The addition of a control panel (plugboard) to his 1906 Type I Tabulator allowed it to do different jobs without having to be physically rebuilt. By the late 1940s, 
there were several unit record calculators, such as the IBM 602 and IBM 604, whose control panels specified a sequence (list) of operations and thus were programmable 
machines.

The invention of the von Neumann architecture allowed computer programs to be stored in computer memory. Early programs had to be painstakingly crafted using the 
instructions (elementary operations) of the particular machine, often in binary notation. Every model of computer would likely use different instructions (machine language) 
to do the same task. Later, assembly languages were developed that let the programmer specify each instruction in a text format, entering abbreviations for each operation 
code instead of a number and specifying addresses in symbolic form (e.g., ADD X, TOTAL). Entering a program in assembly language is usually more convenient, faster, 
and less prone to human error than using machine language, but because an assembly language is little more than a different notation for a machine language, any two 
machines with different instruction sets also have different assembly languages.


Wired control panel for an IBM 402 Accounting Machine
The synthesis of numerical calculation, predetermined operation and output, along with a way to organize and input instructions in a manner relatively easy for humans to 
conceive and produce, led to the modern development of computer programming. In 1954, FORTRAN was invented; it was the first widely used high level programming 
language to have a functional implementation, as opposed to just a design on paper.[12][13] (A high level language is, in very general terms, any programming language 
that allows the programmer to write programs in terms that are more abstract than assembly language instructions, i.e. at a level of abstraction "higher" than that of an 
assembly language.) It allowed programmers to specify calculations by entering a formula directly (e.g. Y = X*2 + 5*X + 9). The program text, or source, is converted into 
machine instructions using a special program called a compiler, which translates the FORTRAN program into machine language. In fact, the name FORTRAN stands for 
"Formula Translation". Many other languages were developed, including some for commercial programming, such as COBOL. Programs were mostly still entered using 
punched cards or paper tape. (See computer programming in the punch card era). By the late 1960s, data storage devices and computer terminals became inexpensive 
enough that programs could be created by typing directly into the computers. Text editors were developed that allowed changes and corrections to be made much more 
easily than with punched cards. (Usually, an error in punching a card meant that the card had to be discarded and a new one punched to replace it.)

As time has progressed, computers have made giant leaps in processing power, which have allowed the development of programming languages that are more abstracted 
from the underlying hardware. Popular programming languages of the modern era include ActionScript, C, C++, C#, Haskell, Java, JavaScript, Objective C, Perl, PHP, 
Python, Ruby, Smalltalk, SQL, Visual Basic, and dozens more.[14] Although these high level languages usually incur greater overhead, the increase in speed of modern 
computers has made the use of these languages much more practical than in the past. These increasingly abstracted languages are typically easier to learn and allow the 
programmer to develop applications much more efficiently and with less source code. However, high level languages are still impractical for a few programs, such as those 
where low level hardware control is necessary or where maximum processing speed is vital. Computer programming has become a popular career in the developed world, 
particularly in the United States, Europe, and Japan. Due to the high labor cost of programmers in these countries, some forms of programming have been increasingly 
subject to outsourcing (importing software and services from other countries, usually at a lower wage), making programming career decisions in developed countries more 
complicated, while increasing economic opportunities for programmers in less developed areas, particularly China and India.Quality requirements[edit]
Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:

Reliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms, and minimization of programming mistakes, such as 
mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off by one errors).
Robustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of 
needed resources such as memory, operating system services and network connections, user error, and unexpected power outages.
Usability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such 
issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical and sometimes hardware elements that improve the 
clarity, intuitiveness, cohesiveness and completeness of a program's user interface.
Portability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends 
on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware 
and operating system, and availability of platform specific compilers (and sometimes libraries) for the language of the source code.
Maintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or customizations, fix bugs and security 
holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end 
user but it can significantly affect the fate of a program over the long term.
Efficiency/performance: the amount of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to 
some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating 
memory leaks.Readability of source code[edit]
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects 
the aspects of quality above, including portability, usability and most importantly maintainability.

Readability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new 
source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study[15] found that a few simple readability transformations made code shorter 
and drastically reduced the time to understand it.

Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with 
the ability of the computer to efficiently compile and execute the code, contribute to readability.[16] Some of these factors include:

Different indentation styles (whitespace)
Comments
Decomposition
Naming conventions for objects (such as variables, classes, procedures, etc.)
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non traditional approaches to code structure 
and display. Techniques like Code refactoring can enhance readability.

Algorithmic complexity[edit]
The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for 
a given class of problem. For this purpose, algorithms are classified into orders using so called Big O notation, which expresses resource use, such as execution time or 
memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well established algorithms and their respective complexities and 
use this knowledge to choose algorithms that are best suited to the circumstances..

Methodologies[edit]
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure 
elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many 
programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a 
few weeks rather than years. There are many approaches to the Software development process.

Popular modeling techniques include Object Oriented Analysis and Design (OOAD) and Model Driven Architecture (MDA). The Unified Modeling Language (UML) is a 
notation used for both the OOAD and MDA.

A similar technique used for database design is Entity Relationship Modeling (ER Modeling).

Implementation techniques include imperative languages (object oriented or procedural), functional languages, and logic languages.

Measuring language usage[edit]
Main article: Measuring programming language popularity
It is very difficult to determine what are the most popular of modern programming languages. Methods of measuring programming language popularity include: counting 
the number of job advertisements that mention the language,[17] the number of books sold and courses teaching the language (this overestimates the importance of newer 
languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as 
COBOL).

Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, 
COBOL is still strong in corporate data centers[18] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and 
C in embedded software. Many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a 
prior language with new functionality added, (for example C++ adds object orientation to C, and Java adds memory management and bytecode to C++, but as a result, 
loses efficiency and the ability for low level manipulation).

Debugging[edit]

The bug from 1947 which is at the origin of a popular (but incorrect) etymology for the common term for a software defect.
Main article: Debugging
Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some 
languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static 
code analysis tool can help detect some possible problems.

Debugging is often done with IDEs like Eclipse, Visual Studio, Kdevelop, NetBeans and Code::Blocks. Standalone debuggers like gdb are also used, and these often 
provide less of a visual environment, usually using a command line.

Programming languages[edit]
Main articles: Programming language and List of programming languages
Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many 
considerations, such as company policy, suitability to task, availability of third party packages, or individual preference. Ideally, the programming language best suited for 
the task at hand will be selected. Trade offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that 
language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low level" to "high level"; "low level" 
languages are typically more machine oriented and faster to execute, whereas "high level" languages are more abstract and easier to use but execute less quickly. It is 
usually easier to code in "high level" languages than in "low level" ones.

Allen Downey, in his book How To Think Like A Computer Scientist, writes:

The details look different in different languages, but a few basic instructions appear in just about every language:
Input: Gather data from the keyboard, a file, or some other device.
Output: Display data on the screen or send data to a file or other device.
Arithmetic: Perform basic arithmetical operations like addition and multiplication.
Conditional Execution: Check for certain conditions and execute the appropriate sequence of statements.In mathematics and computer science, an algorithm 
is a self contained step by step set of operations to be performed. Algorithms exist that perform calculation, data processing, and automated reasoning.

The words 'algorithm' and 'algorism' come from the name al Khw rizm . Al Khw rizm  (Persian:          , c. 780 850) was a Persian mathematician, astronomer, 
geographer, and scholar.

An algorithm is an effective method that can be expressed within a finite amount of space and time[1] and in a well defined formal language[2] for calculating a function.[3] 
Starting from an initial state and initial input (perhaps empty),[4] the instructions describe a computation that, when executed, proceeds through a finite[5] number of well 
defined successive states, eventually producing "output"[6] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; 
some algorithms, known as randomized algorithms, incorporate random input.[7]

The concept of algorithm has existed for centuries, however a partial formalization of what would become the modern algorithm began with attempts to solve the 
Entscheidungsproblem (the "decision problem") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define "effective calculability"[8] or 
"effective method";[9] those formalizations included the G del,,Herbrand,Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, 
Emil Post's "Formulation 1" of 1936, and Alan Turing's Turing machines of 1936,7 and 1939. Giving a formal definition of algorithms, corresponding to the intuitive notion, 
remains a challenging problem.[10]
Informal definition[edit]
For a detailed presentation of the various points of view on the definition of "algorithm", see Algorithm characterizations.
An informal definition could be "a set of rules that precisely defines a sequence of operations."[11] which would include all computer programs, including programs that do 
not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.[12]

A prototypical example of an algorithm is Euclid's algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the 
flow chart above and as an example in a later section.

Boolos & Jeffrey (1974, 1999) offer an informal meaning of the word in the following quotation:

No human being can write fast enough, or long enough, or small enough  (  "smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on 
electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in 
the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be 
given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on 
symbols.[13]

An "enumerably infinite set" is one whose elements can be put into one to one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm 
implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can 
be an algebraic equation such as y = m + n,two arbitrary "input variables" m and n that produce an output y. But various authors' attempts to define the notion indicate
that the word implies much more than this, something on the order of (for the addition example):

Precise instructions (in language understood by "the computer")[14] for a fast, efficient, "good"[15] process that specifies the "moves" of "the computer" (machine or 
human, equipped with the necessary internally contained information and capabilities)[16] to find, decode, and then process arbitrary input integers/symbols m and n, 
symbols + and = ... and "effectively"[17] produce, in a "reasonable" time,[18] output integer y at a specified place and in a specified format.
The concept of algorithm is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set 
of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related with our customary physical dimension. 
From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage 
of the term.

Formalization[edit]
Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform 
(in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to 
be any sequence of operations that can be simulated by a Turing complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich 
(2000):

Minsky: "But we will also maintain, with Turing . . . that any procedure which could "naturally" be called effective, can in fact be realized by a (simple) machine. Although 
this may seem extreme, the arguments . . . in its favor are hard to refute".[19]

Gurevich: "...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine ... according to Savage 
[1987], an algorithm is a computational process defined by a Turing machine".[20]

Typically, when an algorithm is associated with processing information, data are read from an input source, written to an output device, and/or stored for further 
processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.

For some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any 
conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).

Because an algorithm is a precise list of precise steps, the order of computation is always critical to the functioning of the algorithm. Instructions are usually assumed to 
be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by flow of control.

So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to 
describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives 
from the intuition of "memory" as a scratchpad. There is an example below of such an assignment.

For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.

Expressing algorithms[edit]
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon charts, programming languages or control tables 
(processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. 
Pseudocode, flowcharts, drakon charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language 
statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define 
or document algorithms.

There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite state 
machine, state transition table and control table), as flowcharts and drakon charts (see more at state diagram), or as a form of rudimentary machine code or assembly 
code called "sets of quadruples" (see more at Turing machine).

Representations of algorithms can be classed into three accepted levels of Turing machine description:[21]

1 High level description
"...prose to describe an algorithm, ignoring the implementation details. At this level we do not need to mention how the machine manages its tape or head."
2 Implementation description
"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level we do not give details of states or transition 
function."
3 Formal description
Most detailed, "lowest level", gives the Turing machine's "state table".
For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.

Implementation[edit]

Logical NAND algorithm implemented electronically in 7400 chip
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network 
(for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.

Computer algorithms[edit]

Flowchart examples of the canonical B hm Jacopini structures: the SEQUENCE (rectangles descending the page), the WHILE DO and the IF THEN ELSE. The three 
structures are made of the primitive conditional GOTO (IF test=true THEN GOTO step xxx) (a diamond), the unconditional GOTO (rectangle), various assignment operators 
(rectangle), and HALT (rectangle). Nesting of these structures inside assignment blocks result in complex diagrams (cf Tausworthe 1977:100,114).
In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended "target" computer(s) to 
produce output from given input (perhaps null). An optimal algorithm, even running in old hardware, would produce faster results than a non optimal (higher time 
complexity) algorithm for the same purpose, running in more efficient hardware; that is why the algorithms, like computer hardware, are considered technology.

"Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:

Knuth: ". . .we want good algorithms in some loosely defined aesthetic sense. One criterion . . . is the length of time taken to perform the algorithm . . .. Other criteria are 
adaptability of the algorithm to computers, its simplicity and elegance, etc"[22]
Chaitin: " . . . a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does"[23]
Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant'",such a proof would solve the Halting problem (ibid).

Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set 
available to the programmer. Rogers observes that "It is . . . important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable 
by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".[24]

Unfortunately there may be a tradeoff between goodness (speed) and elegance (compactness),an elegant program may take more steps to complete a computation than
one less elegant. An example that uses Euclid's algorithm appears below.

Computers (and computors), models of computation: A computer (or human "computor"[25]) is a restricted type of machine, a "discrete deterministic mechanical 
device"[26] that blindly follows its instructions.[27] Melzak's and Lambek's primitive models[28] reduced this notion to four elements: (i) discrete, distinguishable locations, 
(ii) discrete, indistinguishable counters[29] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[30]

Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability".[31] Minsky's machine proceeds sequentially 
through its five (or six depending on how one counts) instructions unless either a conditional IF,THEN GOTO or an unconditional GOTO changes program flow out of 
sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution)[32] operations: ZERO (e.g. the contents of location replaced by 0: L   
0), SUCCESSOR (e.g. L   L+1), and DECREMENT (e.g. L   L   1).[33] Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows 
(as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, 
assignment/replacement/substitution, and HALT.[34]

Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and 
paper and work through an example".[35] But what about a simulation or execution of the real thing  The programmer must translate the algorithm into a language that the 
simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to 
take a square root. If they don't then for the algorithm to be effective it must provide a set of rules for extracting a square root.[36]

This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).

But what model should be used for the simulation  Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of 
the choice of a model remains. It is at this point that the notion of simulation enters".[37] When speed is being measured, the instruction set matters. For example, the 
subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" (division) instruction available rather than just 
subtraction (or worse: just Minsky's "decrement").

Structured programming, canonical structures: Per the Church,Turing thesis any algorithm can be computed by a model known to be Turing complete, and per Minsky's 
demonstrations Turing completeness requires only four instruction types,conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that
while "undisciplined" use of unconditional GOTOs and conditional IF THEN GOTOs can result in "spaghetti code" a programmer can write structured programs using these 
instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language".[38] Tausworthe augments the three 
B hm Jacopini canonical structures:[39] SEQUENCE, IF THEN ELSE, and WHILE DO, with two more: DO WHILE and CASE.[40] An additional benefit of a structured 
program is that it lends itself to proofs of correctness using mathematical induction.[41]

Canonical flowchart symbols[42]: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program of one). Like 
program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only 4: the directed arrow showing program 
flow, the rectangle (SEQUENCE, GOTO), the diamond (IF THEN ELSE), and the dot (OR tie). The B hm Jacopini canonical structures are made of these primitive 
shapes. Sub structures can "nest" in rectangles but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are 
shown in the diagram.

Examples[edit]
Further information: List of algorithms
Algorithm example[edit]

An animation of the quicksort algorithm sorting an array of randomized values. The red bars mark the pivot element; at the start of the animation, the element farthest to the 
right hand side is chosen as the pivot.
One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding solution requires looking at every number in the list. From this 
follows a simple algorithm, which can be stated in a high level description English prose, as:

High level description:

If there are no numbers in the set then there is no highest number.
Assume the first number in the set is the largest number in the set.
For each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.
When there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.
(Quasi )formal description: Written in prose but much closer to the high level language of a computer program, the following is the more formal coding of the algorithm in 
pseudocode or pidgin code:

Algorithm LargestNumber
  Input: A list of numbers L.
  Output: The largest number in the list L.
  if L.size = 0 return null
  largest   L[0]
  for each item in L, do
    if item > largest, then
      largest   item
  return largest
" " is a shorthand for "changes to". For instance, "largest   item" means that the value of largest changes to the value of item.
"return" terminates the algorithm and outputs the value that follows.
Euclid s algorithm[edit]
Further information: Euclid algorithm

The example diagram of Euclid's algorithm from T.L. Heath 1908 with more detail added. Euclid does not go beyond a third measuring and gives no numerical examples. 
Nicomachus gives the example of 49 and 21: "I subtract the less from the greater; 28 is left; then again I subtract from this the same 21 (for this is possible); 7 is left; I 
subtract this from 21, 14 is left; from which I again subtract 7 (for this is possible); 7 is left, but 7 cannot be subtracted from 7." Heath comments that, "The last phrase is 
curious, but the meaning of it is obvious enough, as also the meaning of the phrase about ending 'at one and the same number'."(Heath 1908:300).
Euclid s algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his Elements.
[43] Euclid poses the problem: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed 
of units": a counting number, a positive integer not including 0. And to "measure" is to place a shorter measuring length s successively (q times) along longer length l until 
the remaining portion r is less than the shorter length s.[44] In modern words, remainder r = l   q*s, q being the quotient, or remainder r is the "modulus", the integer 
fractional part left over after the division.[45]

For Euclid s method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be 0, AND (ii) the subtraction must be  proper , a test must 
guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields 0).

Euclid's original proof adds a third: the two lengths are not prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two 
numbers' common measure is in fact the greatest.[46] While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another it yields the 
number "1" for their common measure. So to be precise the following is really Nicomachus' algorithm.


A graphical expression of Euclid's algorithm to find the greatest common divisor for 1599 and 650.
 1599 = 650*2 + 299
 650 = 299*2 + 52
 299 = 52*5 + 39
 52 = 39*1 + 13
 39 = 13*3 + 0

Computer language for Euclid's algorithm[edit]
Only a few instruction types are required to execute Euclid's algorithm,some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and
subtraction.

A location is symbolized by upper case letter(s), e.g. S, A, etc.
The varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might 
contain the number l = 3009.
An inelegant program for Euclid's algorithm[edit]

"Inelegant" is a translation of Knuth's version of the algorithm with a subtraction based remainder loop replacing his use of division (or a "modulus" instruction). Derived 
from Knuth 1973:2,4. Depending on the two numbers "Inelegant" may compute the g.c.d. in fewer steps than "Elegant".
The following algorithm is framed as Knuth's 4 step version of Euclid's and Nicomachus', but rather than using division to find the remainder it uses successive 
subtractions of the shorter length s from the remaining length r until r is less than s. The high level description, shown in boldface, is adapted from Knuth 1973:2,4:

INPUT:

1 [Into two locations L and S put the numbers l and s that represent the two lengths]:
  INPUT L, S
2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:
  R   L
E0: [Ensure r   s.]

3 [Ensure the smaller of the two numbers is in S and the larger in R]: 
  IF R > S THEN 
    the contents of L is the larger number so skip over the exchange steps 4, 5 and 6: 
    GOTO step 6 
  ELSE 
    swap the contents of R and S.
4   L   R (this first step is redundant, but is useful for later discussion).
5   R   S
6   S   L
E1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in 
R.

7 IF S > R THEN 
    done measuring so 
    GOTO 10 
  ELSE 
    measure again,
8   R   R   S
9   [Remainder loop]: 
    GOTO 7.
E2: [Is the remainder 0 ]: EITHER (i) the last measure was exact and the remainder in R is 0 program can halt, OR (ii) the algorithm must continue: the last measure left a 
remainder in R less than measuring number in S.

10 IF R = 0 THEN 
     done so 
     GOTO step 15 
   ELSE 
     CONTINUE TO step 11,
E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s:; L serves as a temporary location.

11  L   R
12  R   S
13  S   L
14  [Repeat the measuring process]: 
    GOTO 7
OUTPUT:

15 [Done. S contains the greatest common divisor]: 
   PRINT S
DONE:

16 HALT, END, STOP.
An elegant program for Euclid's algorithm[edit]
The following version of Euclid's algorithm requires only 6 core instructions to do what 13 are required to do by "Inelegant"; worse, "Inelegant" requires more types of 
instructions. The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language the steps are numbered, and the instruction LET [] = [] 
is the assignment instruction symbolized by  .

  5 REM Euclid's algorithm for greatest common divisor
  6 PRINT "Type two integers greater than 0"
  10 INPUT A,B
  20 IF B=0 THEN GOTO 80
  30 IF A > B THEN GOTO 60
  40 LET B=B A
  50 GOTO 20
  60 LET A=A B
  70 GOTO 20
  80 PRINT A
  90 END
How "Elegant" works: In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co loops", an A > B loop that computes A   A   B, and a B   A loop 
that computes B   B   A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend   Subtrahend), the minuend 
can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the "sense" of the subtraction reverses.

Testing the Euclid algorithms[edit]
Does an algorithm do what its author wants it to do  A few test cases usually suffice to confirm core functionality. One source[47] uses 3009 and 884. Knuth suggested 
40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.

But exceptional cases must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S  Ditto for "Elegant": B > A, A > B, A = B  (Yes to all). 
What happens when one number is zero, both numbers are zero  ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if 
negative numbers are entered  Fractional numbers  If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive 
integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable 
failure due to exceptions is the Ariane V rocket failure.

Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's 
algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".[48] Tausworthe proposes that a measure of the complexity of a program 
be the length of its correctness proof.[49]

Measuring and improving the Euclid algorithms[edit]
Elegance (compactness) versus goodness (speed): With only 6 core instructions, "Elegant" is the clear winner compared to "Inelegant" at 13 instructions. However, 
"Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis[50] indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, 
whereas "Inelegant" only does one. As the algorithm (usually) requires many loop throughs, on average much time is wasted doing a "B = 0 " test that is needed only after 
the remainder is computed.

Can the algorithms be improved : Once the programmer judges a program "fit" and "effective",that is, it computes the function intended by its author,then the question
becomes, can it be improved 

The compactness of "Inelegant" can be improved by the elimination of 5 steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized 
algorithm;[51] rather, it can only be done heuristically, i.e. by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of 
inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps together with steps 
2 and 3 can be eliminated. This reduces the number of core instructions from 13 to 8, which makes it "more elegant" than "Elegant" at 9 steps.

The speed of "Elegant" can be improved by moving the B=0  test outside of the two subtraction loops. This change calls for the addition of 3 instructions (B=0 , A=0 , 
GOTO). Now "Elegant" computes the example numbers faster; whether for any given A, B and R, S this is always the case would require a detailed analysis.

Algorithmic analysis[edit]
Main article: Analysis of algorithms
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed 
for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O 
notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input 
list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.

Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search 
algorithm usually outperforms a brute force sequential search when used for table lookups on sorted lists.

Formal versus empirical[edit]
Main articles: Empirical algorithmics, Profiling (computer programming) and Program optimization
The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or 
implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the 
specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most 
algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution 
of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast 
interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.

Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential 
improvements to an algorithm after program optimization.

Execution efficiency[edit]
Main article: Algorithmic efficiency
To illustrate the potential improvements possible even in well established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of 
image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[52] In general, speed improvements depend on special 
properties of the problem, which are very common in practical applications.[53] Speedups of this magnitude enable computing devices that make extensive use of image 
processing (like digital cameras and medical equipment) to consume less power.

Classification[edit]
There are various ways to classify algorithms, each with its own merits.

By implementation[edit]
One way to classify algorithms is by implementation means.

Recursion
A recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method 
common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given 
problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every 
recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.
Logical
An algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control.[54] The logic component expresses the axioms 
that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic 
programming paradigm. In pure logic programming languages the control component is fixed and algorithms are specified by supplying only the logic component. The 
appeal of this approach is the elegant semantics: a change in the axioms has a well defined change in the algorithm.
Serial, parallel or distributed
Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial 
computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms take 
advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines 
connected with a network. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. 
The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some 
sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no 
parallel algorithms, and are called inherently serial problems.
Deterministic or non deterministic
Deterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non deterministic algorithms solve problems via guessing although 
typical guesses are made more accurate through the use of heuristics.
Exact or approximate
While many algorithms reach an exact solution, approximation algorithms seek an approximation that is close to the true solution. Approximation may use either a 
deterministic or a random strategy. Such algorithms have practical value for many hard problems.
Quantum algorithm
They run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of 
quantum computation such as quantum superposition or quantum entanglement.
By design paradigm[edit]
Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, 
each of these categories include many different types of algorithms. Some common paradigms are:

Brute force or exhaustive search
This is the naive method of trying every possible solution to see which is best.[55]
Divide and conquer
A divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances 
are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into 
segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease and 
conquer algorithm, that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into 
multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of decrease and conquer algorithm is the binary 
search algorithm.
Search and enumeration
Many problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for 
such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.
Randomized algorithm
Such algorithms make some choices randomly (or pseudo randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions 
can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness.[56] Whether 
randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are 
two large classes of such algorithms:
Monte Carlo algorithms return a correct answer with high probability. E.g. RP is the subclass of these that run in polynomial time.
Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.
Reduction of complexity
This technique involves solving a difficult problem by transforming it into a better known problem for which we have (hopefully) asymptotically optimal algorithms. The goal 
is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an 
unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known 
as transform and conquer.
Optimization problems[edit]
For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories 
described above as well as into one of the following:

Linear programming
When searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in 
producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm.[57] Problems that can be solved 
with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer 
then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are 
superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, 
depending on the difficulty of the problem.
Dynamic programming
When a problem shows optimal substructures , meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems , and overlapping
subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing 
solutions that have already been computed. For example, Floyd,Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using 
the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and 
divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference 
between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no 
repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of 
subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.
The greedy method
A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such 
algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they 
can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular 
use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are 
greedy algorithms that can solve this optimization problem.
The heuristic method
In optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These 
algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. 
Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated 
annealing, and genetic algorithms. Some of them, like simulated annealing, are non deterministic algorithms while others, like tabu search, are deterministic. When a 
bound on the error of the non optimal solution is known, the algorithm is further categorized as an approximation algorithm.
By field of study[edit]
See also: List of algorithms
Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search 
algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, 
medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.

Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic 
programming was invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.

By complexity[edit]
See also: Complexity class and Parameterized complexity
Algorithms can be classified by the amount of time they need to complete compared to their input size. There is a wide variety: some algorithms complete in linear time 
relative to input size, some do so in an exponential amount of time or even worse, and some never halt. Additionally, some problems may have multiple algorithms of 
differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. 
Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best 
possible algorithms for them.

Burgin (2005, p. 24) uses a generalized definition of algorithms that relaxes the common requirement that the output of the algorithm that computes a function must be 
determined after a finite number of steps. He defines a super recursive class of algorithms as "a class of algorithms in which it is possible to compute functions not 
computable by any Turing machine" (Burgin 2005, p. 107). This is closely related to the study of methods of hypercomputation.

Continuous algorithms[edit]
The adjective "continuous" when applied to the word "algorithm" can mean:

An algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations,such algorithms are studied in
numerical analysis; or
An algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.[58]
Legal issues[edit]
See also: Software patents for a general overview of the patentability of software, including computer implemented algorithms.
Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals 
does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However, practical applications of algorithms are 
sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. 
The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW 
patent.

Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).

Etymology[edit]
The words 'algorithm' and 'algorism' come from the name al Khw rizm . Al Khw rizm  (Persian:          , c. 780 850) was a Persian mathematician, astronomer, 
geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarezm', a region that was part of Greater Iran and is now in 
Uzbekistan.[59][60] About 825, he wrote a treatise in the Arabic language, which was translated into Latin in the 12th century under the title Algoritmi de numero Indorum. 
This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al Khwarizmi's name.[61] Al Khwarizmi was the most widely 
read mathematician in Europe in the late Middle Ages, primarily through his other book, the Algebra.[62] In late medieval Latin, algorismus, English 'algorism', the 
corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word         'number' (cf. 'arithmetic'), the Latin 
word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.
[63]

History: Development of the notion of "algorithm"[edit]
Ancient Near East[edit]
Algorithms were used in ancient Greece. Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus,[64][65]:Ch 9.2 
and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).[65]:Ch 9.1 Babylonian clay tablets describe and employ algorithmic procedures 
to compute the time and place of significant astronomical events.[66]

Discrete and distinguishable symbols[edit]
Tally marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks, or making 
discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16,41). Tally 
marks appear prominently in unary numeral system arithmetic used in Turing machine and Post,Turing machine computations.

Manipulation of symbols as "place holders" for numbers: algebra[edit]
The work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al Khwarizmi (from whose name 
the terms "algorism" and "algorithm" are derived), and Western European mathematicians culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):

A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner 
that ordinary algebra specifies the rules for manipulating numbers.[67]

Mechanical contrivances with discrete states[edit]
The clock: Bolter credits the invention of the weight driven clock as "The key invention [of Europe in the Middle Ages]", in particular the verge escapement[68] that 
provides us with the tick and tock of a mechanical clock. "The accurate automatic machine"[69] led immediately to "mechanical automata" beginning in the 13th century 
and finally to "computational machines",the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid 19th century.[70] Lovelace is
credited with the first creation of an algorithm intended for processing on a computer   Babbage's analytical engine, the first device considered a real Turing complete 
computer instead of just a calculator   and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not 
be realized until decades after her lifetime.

Logical machines 1870,Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form
similar to what are now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class 
of the [logical] combinations can be picked out mechanically . . . More recently however I have reduced the system to a completely mechanical form, and have thus 
embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and 
"at the foot are 21 keys like those of a piano [etc] . . .". With this machine he could analyze a "syllogism or any other simple logical argument".[71]

This machine he displayed in 1870 before the Fellows of the Royal Society.[72] Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye 
to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances 
at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented 
"a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be 
described. I prefer to call it merely a logical diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".
[73]

Jacquard loom, Hollerith punch cards, telegraphy and telephony,the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor
to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers.[74] By the mid 
19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a 
common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 
1910) with its punched paper use of Baudot code on tape.

Telephone switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he 
worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... 
When the tinkering was over, Stibitz had constructed a binary adding device".[75]

Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):

It was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had 
envisioned."[76]
Mathematics during the 19th century up to the mid 20th century[edit]
Symbols and rules: In rapid succession the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888,1889) reduced arithmetic to a 
sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was "the first attempt at an axiomatization of 
mathematics in a symbolic language".[77]

But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a " 'formula language', that is a 
lingua characterica, a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are 
manipulated according to definite rules".[78] The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia 
Mathematica (1910,1913).

The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular the Burali Forti paradox (1897), the Russell paradox (1902,03), 
and the Richard Paradox.[79] The resultant considerations led to Kurt G del's paper (1931),he specifically cites the paradox of the liar,that completely reduces rules of
recursion to numbers.

Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an 
"effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, 
Stephen Kleene and J.B. Rosser's   calculus[80] a finely honed definition of "general recursion" from the work of G del acting on suggestions of Jacques Herbrand (cf. 
G del's Princeton lectures of 1934) and subsequent simplifications by Kleene.[81] Church's proof[82] that the Entscheidungsproblem was unsolvable, Emil Post's definition 
of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a 
paper or observe the paper and make a yes no decision about the next instruction.[83] Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his 
"a  [automatic ] machine"[84],in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine".[85] S. C.
Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I",[86] and a few years later Kleene's renaming his Thesis "Church's Thesis"[87] and proposing 
"Turing's Thesis".[88]

Emil Post (1936) and Alan Turing (1936,37, 1939)[edit]
Here is a remarkable coincidence of two men not knowing each other but describing a process of men as computers working on computations,and they yield virtually
identical definitions.

Emil Post (1936) described the actions of a "computer" (human being) as follows:

"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.
His symbol space would be

"a two way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but 
one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.
"One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with 
a stroke. Likewise the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes....
"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to 
the direction of type (C ) [i.e., STOP]".[89] See more at Post,Turing machine

Alan Turing's statue at Bletchley Park.
Alan Turing's work[90] preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a 
typewriter like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter; and he could well have begun by 
asking himself what was meant by calling a typewriter 'mechanical'".[91] Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we 
might conjecture that all were influences.

Turing,his model of computation is now called a Turing machine,begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of
basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.[92]

"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book....I assume then that 
the computation is carried out on one dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is 
finite....
"The behaviour of the computer at any moment is determined by the symbols which he is observing, and his "state of mind" at that moment. We may suppose that there is 
a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We 
will also suppose that the number of states of mind which need be taken into account is finite...
"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further 
divided."[93]
Turing's reduction yields the following:

"The simple operations must therefore include:
"(a) Changes of the symbol on one of the observed squares
"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.
"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must therefore be taken to be one of the following:

"(A) A possible change (a) of symbol together with a possible change of state of mind.
"(B) A possible change (b) of observed squares, together with a possible change of state of mind"
"We may now construct a machine to do the work of this computer."[93]
A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:

"A function is said to be "effectively calculable" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, 
it is nevertheless desirable to have some more definite, mathematical expressible definition . . . [he discusses the history of the definition pretty much as presented above 
with respect to G del, Herbrand, Kleene, Church, Turing and Post] . . . We may take this statement literally, understanding by a purely mechanical process one which 
could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of 
these ideas leads to the author's definition of a computable function, and to an identification of computability   with effective calculability . . . .
"  We shall use the expression "computable function" to mean a function calculable by a machine, and we let "effectively calculable" refer to the intuitive idea without 
particular identification with any one of these definitions".[94]
J. B. Rosser (1939) and S. C. Kleene (1943)[edit]
J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):

"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite 
number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest 
of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then 
solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't 
matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one." (Rosser 1939:225,6)
Rosser's footnote #5 references the work of (1) Church and Kleene and their definition of   definability, in particular Church's use of it in his An Unsolvable Problem of 
Elementary Number Theory (1936); (2) Herbrand and G del and their use of recursion in particular G del's use in his famous paper On Formally Undecidable 
Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936,7) in their mechanism models of computation.

Stephen C. Kleene defined as his now famous "Thesis I" known as the Church Turing thesis. But he did this in the following context (boldface in original):

"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent 
variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, "yes" or "no," to the question, "is the 
predicate value true "" (Kleene 1943:273)
History after 1950[edit]
A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on going because of issues surrounding, in particular, 
foundations of mathematics (especially the Church,,Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm 
characterizations.
Repetition: Perform some action repeatedly, usually with some variation.
Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run time 
conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Programmers[edit]
Main article: Programmer
See also: Software developer and Software engineer
Computer programmers are those who write computer software. Their jobs usually involve:
Coding
Debugging
Documentation
Integration
Maintenance
Requirements analysis
Software architecture
Software testing
Specification
Raspberry Pi
From Wikipedia, the free encyclopedia
"RPi" redirects here. For other uses, see RPI.

Raspberry Pi logo
Raspberry Pi 1
Raspberry Pi B+ top.jpg
Raspberry Pi 1 model B+
Release date	February 2012; 4 years ago
Introductory price	US$25 (model A, B+[1]), US$20 (model A+), US$35 (RPi 1 model B, RPi 2 model B, RPi 3), US$30 (CM)
Operating system	Linux (e.g. Raspbian), RISC OS, FreeBSD, NetBSD, Plan 9, Inferno, AROS
CPU	700 MHz single-core ARM1176JZF-S (model A, A+, B, B+, CM)[2]
Memory	256 MB[3] (model A, A+ rev 1, B rev 1)
512 MB (model A+ rev 2,[4] B rev 2, B+, CM)
Storage	SDHC slot (model A and B), MicroSDHC slot (model A+ and B+), 4 GB eMMC IC chip (model CM)
Graphics	Broadcom VideoCore IV[2]
Power	1.5 W (model A), 1.0 W (model A+), 3.5 W (model B), 3.0 W (model B+) or 0.8 W (model Zero)
Raspberry Pi 2
Raspberry Pi 2 Model B v1.1 top new (bg cut out).jpg
Raspberry Pi 2 model B
Release date	February 2015; 1 year ago
Introductory price	US$35
Operating system	Same as for Raspberry Pi 1 plus Windows 10 IoT Core[5] and additional distributions of Linux such as Raspbian
CPU	900 MHz quad-core ARM Cortex-A7
Memory	1 GB RAM
Storage	MicroSDHC slot
Graphics	Broadcom VideoCore IV
Power	4.0 W
Raspberry Pi 3
Raspberry Pi 3 Model B.png
Raspberry Pi 3 model B
Release date	29 February 2016; 2 months ago
Introductory price	US$35
Operating system	Raspbian
Ubuntu MATE
Snappy Ubuntu Core
Windows 10 IoT Core[5]
RISC OS
Debian
Arch Linux ARM
CPU	1200 MHz quad-core ARM Cortex-A53
Memory	1 GB RAM
Storage	MicroSDHC slot
Graphics	Broadcom VideoCore IV at higher clock frequencies than previous that run at 250 MHz
Power	4.0 W
Raspberry Pi Zero
Pi Zero.png
Raspberry Pi Zero
Release date	November 2015; 6 months ago
Introductory price	US$5
Operating system	Linux (Raspbian[6]) or the same as for Raspberry Pi 1
CPU	1000 MHz single-core ARM1176JZF-S
Memory	512 MB RAM
Storage	MicroSDHC slot
Power	0.8 W
The Raspberry Pi is a series of credit card–sized single-board computers developed in the United Kingdom by the Raspberry Pi Foundation with the intent to promote the teaching of basic computer science in schools and developing countries.[7][8][9] The original Raspberry Pi and Raspberry Pi 2 are manufactured in several board configurations through licensed manufacturing agreements with Newark element14 (Premier Farnell), RS Components and Egoman.[10] The hardware is the same across all manufacturers.

Several generations of Raspberry Pi's have been released. The first generation (Pi 1) was released in February 2012 in basic model A and a higher specification model B. A+ and B+ models were released a year later. Raspberry Pi 2 model B was released in February 2015 and Raspberry Pi 3 model B in February 2016. These boards are priced between US$20 and US$35. A cut down compute model was released in April 2014 and a Pi Zero with smaller footprint and limited IO (GPIO) capabilities released in November 2015 for US$5.

All models feature a Broadcom system on a chip (SOC) which include an ARM compatible CPU and an on chip graphics processing unit GPU (a VideoCore IV). CPU speed range from 700 MHz to 1.2 GHz for the Pi 3 and on board memory range from 256 MB to 1 GB RAM. Secure Digital SD cards are used to store the operating system and program memory in either the SDHC or MicroSDHC sizes. Most boards have between one and four USB slots, HDMI and composite video output, and a 3.5 mm phono jack for audio. Lower level output is provided by a number of GPIO pins which support common protocols like I2C. Some models have an RJ45 Ethernet port and the Pi 3 has on board WiFi 802.11n and Bluetooth.

The Foundation provides Debian and Arch Linux ARM distributions for download,[11] and promotes Python as the main programming language, with support for BBC BASIC[12] (via the RISC OS image or the Brandy Basic clone for Linux),[13] C, C++, Java,[14] Perl, Ruby,[15] Squeak Smalltalk and more also available.

In February 2016, the Raspberry Pi Foundation announced that they had sold eight million devices, making it the best selling UK personal computer, ahead of the Amstrad PCW.[16][17]

Contents  [hide] 
1	Hardware
1.1	Processor
1.1.1	Performance of first generation models
1.1.2	Overclocking
1.2	RAM
1.3	Networking
1.4	Peripherals
1.5	Video
1.6	Real-time clock
1.7	Specifications
1.8	Connectors
1.9	General purpose input-output (GPIO) connector
1.10	Accessories
2	Software
2.1	Operating systems
2.2	Driver APIs
2.3	Third party application software
2.4	Software development tools
2.5	Tracking Raspberry Pi online on a global map
3	Reception and use
3.1	Community
3.2	Use in education
4	Astro Pi
5	Reviews
6	History
6.1	Pre-launch
6.2	Launch
6.3	Post-launch
7	See also
8	References
9	Further reading
10	External links
Hardware[edit]
The Raspberry Pi hardware has evolved through several versions that feature variations in memory capacity and peripheral-device support.

Raspberrypi block function v01.svg
This block diagram depicts models A, B, A+, and B+. Model A, A+, and Zero lack the Ethernet and USB hub components. The Ethernet adapter is connected to an additional USB port. In model A and A+ the USB port is connected directly to the SoC. On model B+ and later models the USB/Ethernet chip contains a five-point USB hub, of which four ports are available, while model B only provides two. On the model Zero, the USB port is also connected directly to the SoC, but it uses a micro USB (OTG) port.

Processor[edit]
The system on a chip (SoC) used in the first generation Raspberry Pi is somewhat equivalent to the chip used in older smartphones (such as iPhone, 3G, 3GS). The Raspberry Pi is based on the Broadcom BCM2835 SoC,[2] which includes an 700 MHz ARM1176JZF-S processor, VideoCore IV graphics processing unit (GPU),[18] and RAM. It has a Level 1 cache of 16 KB and a Level 2 cache of 128 KB. The Level 2 cache is used primarily by the GPU. The SoC is stacked underneath the RAM chip, so only its edge is visible.

The Raspberry Pi 2 uses a Broadcom BCM2836 SoC with a 900 MHz 32-bit quad-core ARM Cortex-A7 processor, with 256 KB shared L2 cache.

The Raspberry Pi 3 uses a Broadcom BCM2837 SoC with a 1.2 GHz 64-bit quad-core ARM Cortex-A53 processor, with 512 KB shared L2 cache.[19]

Performance of first generation models[edit]
While operating at 700 MHz by default, the first generation Raspberry Pi provided a real-world performance roughly equivalent to 0.041 GFLOPS.[20][21] On the CPU level the performance is similar to a 300 MHz Pentium II of 1997–99. The GPU provides 1 Gpixel/s or 1.5 Gtexel/s of graphics processing or 24 GFLOPS of general purpose computing performance. The graphics capabilities of the Raspberry Pi are roughly equivalent to the level of performance of the Xbox of 2001.

The LINPACK single node compute benchmark results in a mean single precision performance of 0.065 GFLOPS and a mean double precision performance of 0.041 GFLOPS for one Raspberry Pi Model-B board.[22] A cluster of 64 Raspberry Pi Model-B computers, labeled "Iridis-pi", achieved a LINPACK HPL suite result of 1.14 GFLOPS (n=10240) at 216 watts for c. US$4,000.[22]

Raspberry Pi 2 is based on Broadcom BCM2836 SoC, which includes a quad-core Cortex-A7 CPU running at 900 MHz and 1 GB RAM. It is described as 4–6 times more powerful than its predecessor. The GPU is identical to the original.

Overclocking[edit]
The first generation Raspberry Pi chip operated at 700 MHz by default, and did not become hot enough to need a heat sink or special cooling unless the chip was overclocked. The second generation runs at 900 MHz by default; it also does not become hot enough to need a heatsink or special cooling, although overclocking may heat up the SoC more than usual.

Most Raspberry Pi chips could be overclocked to 800 MHz and some even higher to 1000 MHz. There are reports the second generation can be similarly overclocked, in extreme cases, even to 1500 MHz (discarding all safety features and over voltage limitations). In the Raspbian Linux distro the overclocking options on boot can be done by a software command running "sudo raspi-config" without voiding the warranty.[23] In those cases the Pi automatically shuts the overclocking down in case the chip reaches 85 ?C (185 ?F), but it is possible to overrule automatic over voltage and overclocking settings (voiding the warranty). In that case, an appropriately sized heatsink is needed to keep the chip from heating up far above 85 ?C.

Newer versions of the firmware contain the option to choose between five overclock ("turbo") presets that when turned on try to get the most performance out of the SoC without impairing the lifetime of the Pi. This is done by monitoring the core temperature of the chip, and the CPU load, and dynamically adjusting clock speeds and the core voltage. When the demand is low on the CPU, or it is running too hot, the performance is throttled, but if the CPU has much to do, and the chip's temperature is acceptable, performance is temporarily increased, with clock speeds of up to 1 GHz, depending on the individual board, and on which of the turbo settings is used. The seven settings are:

none; 700 MHz ARM, 250 MHz core, 400 MHz SDRAM, 0 overvolt,
modest; 800 MHz ARM, 250 MHz core, 400 MHz SDRAM, 0 overvolt,
medium; 900 MHz ARM, 250 MHz core, 450 MHz SDRAM, 2 overvolt,
high; 950 MHz ARM, 250 MHz core, 450 MHz SDRAM, 6 overvolt,
turbo; 1000 MHz ARM, 500 MHz core, 600 MHz SDRAM, 6 overvolt,
Pi2; 1000 MHz ARM, 500 MHz core, 500 MHz SDRAM, 2 overvolt,
Pi3; 1100 MHz ARM, 550 MHz core, 500 MHz SDRAM, 6 overvolt. In system information CPU speed will appear as 1200 MHz. When in idle speed lowers to 600 MHz.
[24][25]

In the highest (turbo) preset the SDRAM clock was originally 500 MHz, but this was later changed to 600 MHz because 500 MHz sometimes causes SD card corruption. Simultaneously in high mode the core clock speed was lowered from 450 to 250 MHz, and in medium mode from 333 to 250 MHz.

The Raspberry Pi Zero runs at 1 GHz.

RAM[edit]
On the older beta model B boards, 128 MB was allocated by default to the GPU, leaving 128 MB for the CPU.[26] On the first 256 MB release model B (and model A), three different splits were possible. The default split was 192 MB (RAM for CPU), which should be sufficient for standalone 1080p video decoding, or for simple 3D, but probably not for both together. 224 MB was for Linux only, with only a 1080p framebuffer, and was likely to fail for any video or 3D. 128 MB was for heavy 3D, possibly also with video decoding (e.g. XBMC).[27] Comparatively the Nokia 701 uses 128 MB for the Broadcom VideoCore IV.[28] For the new model B with 512 MB RAM initially there were new standard memory split files released( arm256_start.elf, arm384_start.elf, arm496_start.elf) for 256 MB, 384 MB and 496 MB CPU RAM (and 256 MB, 128 MB and 16 MB video RAM). But a week or so later the RPF released a new version of start.elf that could read a new entry in config.txt (gpu_mem=xx) and could dynamically assign an amount of RAM (from 16 to 256 MB in 8 MB steps) to the GPU, so the older method of memory splits became obsolete, and a single start.elf worked the same for 256 and 512 MB Raspberry Pis.[29]

The Raspberry Pi 2 and the Raspberry Pi 3 have 1 GB of RAM. The Raspberry PI Zero has 512 MB of RAM.

Networking[edit]
Though the model A and A+ and Zero do not have an 8P8C ("RJ45") Ethernet port, they can be connected to a network using an external user-supplied USB Ethernet or Wi-Fi adapter. On the model B and B+ the Ethernet port is provided by a built-in USB Ethernet adapter using the SMSC LAN9514 chip.[30] The Raspberry Pi 3 is equipped with 2.4 GHz WiFi 802.11n (600 Mbit/s) and Bluetooth 4.1 (24 Mbit/s) in addition to the 10/100 Ethernet port.

Peripherals[edit]
The Raspberry Pi may be operated with any generic USB computer keyboard and mouse.[31]

Video[edit]
The video controller is capable of standard modern TV resolutions, such as HD and Full HD, and higher or lower monitor resolutions and older standard CRT TV resolutions. As shipped (i.e. without custom overclocking) it is capable of the following: 640?350 EGA; 640?480 VGA; 800?600 SVGA; 1024?768 XGA; 1280?720 720p HDTV; 1280?768 WXGA variant; 1280?800 WXGA variant; 1280?1024 SXGA; 1366?768 WXGA variant; 1400?1050 SXGA+; 1600?1200 UXGA; 1680?1050 WXGA+; 1920?1080 1080p HDTV; 1920?1200 WUXGA.[32]

Higher resolutions, such as, up to 2048?1152, may work[33][34] or even 3840?2160 at 15 Hz (too low a framerate for convincing video).[35] Note also that allowing the highest resolutions does not imply that the GPU can decode video formats at those; in fact, the Pis are known to not work reliably for H.265 (at those high resolution, at least), commonly used for very high resolutions (most formats, commonly used, up to full HD, do work).

Although the new Raspberry Pi 3 does not have H.265 decoding hardware, the OSMC project has this to say on February launch date on the possibility of decoding more videos encoded in that way using software, due to the more advanced CPU architecture:

The new BCM2837 is based on 64-bit ARMv8 architecture is backwards compatible with the Raspberry Pi 2 as well as the original. While the new CPU is 64-bit, the Pi retains the original VideoCore IV GPU which has a 32-bit design. It will be a few months before work is done to establish 64-bit pointer interfacing from the kernel and userland on the ARM to the 32-bit GPU. As such, for the time being, we will be offering a single Raspberry Pi image for Raspberry Pi 2 and the new Raspberry Pi 3. Only when 64-bit support is ready, and beneficial to OSMC users, will we offer a separate image. The new quad core CPU will bring smoother GUI performance. There have also been recent improvements to H265 decoding. While not hardware accelerated on the Raspberry Pi, the new CPU will enable more H265 content to be played back on the Raspberry Pi than before.

—?Raspberry Pi 3 announced with OSMC support[36]
The Pi 3's GPU has higher clock frequencies 300 MHz and 400 MHz of different parts that in previous versions ran at 250 MHz.[37][38]

The Pis can also generate 576i and 480i composite video signals, as used on old-style (CRT) TV screens, (through non-standard connectors, different kind depending on models) for PAL-BGHID, PAL-M, PAL-N, NTSC and NTSC-J.[39]

Real-time clock[edit]
The Raspberry Pi does not come with a real-time clock, which means it cannot keep track of the time of day while it is not powered on.

As alternatives, a program running on the Pi can get the time from a network time server or user input at boot time.

A real-time clock (such as the DS1307, which is fully binary coded) with battery backup may be added (often via the I?C interface).

Specifications[edit]
Raspberry Pi 1
Model A	Raspberry Pi 1
Model A+	Raspberry Pi 1
Model B	Raspberry Pi 1
Model B+	Raspberry Pi 2
Model B	Raspberry Pi 3
Model B	Compute Module*	Raspberry Pi Zero
Release date	February 2013	November 2014[40]	April–June 2012	July 2014[41]	February 2015[42]	February 2016[43]	April 2014[44]	November 2015[45]
Target price	US$25	US$20[46]	US$35[47]	US$25	US$35	US$35	US$30 (in batches of 100)[44]	US$5[45]
SoC	Broadcom BCM2835[2]	Broadcom BCM2836	Broadcom BCM2837	Broadcom BCM2835[44]
CPU	700 MHz single-core ARM1176JZF-S[2]	900 MHz 32-bit quad-core ARM Cortex-A7	1.2 GHz 64-bit quad-core ARM Cortex-A53	700 MHz single-core ARM1176JZF-S	1 GHz ARM1176JZF-S single-core[45]
GPU	Broadcom VideoCore IV @ 250 MHz (BCM2837: 3D part of GPU @ 300 MHz, video part of GPU @ 400 MHz)[48][49]
OpenGL ES 2.0 (BCM2835, BCM2836: 24 GFLOPS / BCM2837: 28.8 GFLOPS)
MPEG-2 and VC-1 (with license),[50] 1080p30 H.264/MPEG-4 AVC high-profile decoder and encoder[2] (BCM2837: 1080p60)
Memory (SDRAM)	256 MB (shared with GPU)	512 MB (shared with GPU) as of 4 May 2016. Older boards had 256 MB (shared with GPU)[4]	1 GB (shared with GPU)	512 MB (shared with GPU)
USB 2.0 ports[31]	1 (direct from BCM2835 chip)	2 (via the on-board 3-port USB hub)[51]	4 (via the on-board 5-port USB hub)[41][30]	1 (direct from BCM2835 chip)	1 Micro-USB (direct from BCM2835 chip)
Video input	15-pin MIPI camera interface (CSI) connector, used with the Raspberry Pi camera or Raspberry Pi NoIR camera[52]	2? MIPI camera interface (CSI)[44][53][54]	
Video outputs	HDMI (rev 1.3 & 1.4),[32][55] composite video (RCA jack)	HDMI (rev 1.3 & 1.4), composite video (3.5 mm TRRS jack)	HDMI (rev 1.3 & 1.4), composite video (RCA jack)	HDMI (rev 1.3 & 1.4), composite video (3.5 mm TRRS jack)	HDMI, 2? MIPI display interface (DSI) for raw LCD panels,[44][54][56][57] composite video[53][58]	Mini-HDMI, 1080p60,[45] composite video via GPIO[59]
Audio inputs	As of revision 2 boards via I?S[60]
Audio outputs	Analog via 3.5 mm phone jack; digital via HDMI and, as of revision 2 boards, I?S	Analog, HDMI, I?S	Mini-HDMI, stereo audio through PWM on GPIO
On-board storage[31]	SD / MMC / SDIO card slot (3.3 V with card power only)	MicroSDHC slot[41]	SD / MMC / SDIO card slot	MicroSDHC slot	4 GB eMMC flash memory chip;[44]	MicroSDHC
On-board network[31]	None[61]	10/100 Mbit/s Ethernet (8P8C) USB adapter on the USB hub[51]	10/100 Mbit/s Ethernet
802.11n wireless
Bluetooth 4.1		None
Low-level peripherals	8? GPIO[62] plus the following, which can also be used as GPIO: UART, I?C bus, SPI bus with two chip selects, I?S audio[63] +3.3 V, +5 V, ground[48][64]
17? GPIO plus the same specific functions, and HAT ID bus	8? GPIO plus the following, which can also be used as GPIO: UART, I?C bus, SPI bus with two chip selects, I?S audio +3.3 V, +5 V, ground.
An additional 4? GPIO are available on the P5 pad if the user is willing to make solder connections

17? GPIO plus the same specific functions, and HAT ID bus	46? GPIO, some of which can be used for specific functions including I?C, SPI, UART, PCM, PWM[65]	40? GPIO ("unpopulated header")[45]
Power ratings	300 mA (1.5 W)[66]	200 mA (1 W)[67]	700 mA (3.5 W)	600 mA (3.0 W)[41]	800 mA[68] (4.0 W)[69]	200 mA (1 W)	~160 mA[45] (0.8 W)
Power source	5 V via MicroUSB or GPIO header
Size	85.60 mm ? 56.5 mm (3.370 in ? 2.224 in), not including protruding connectors	65 mm ? 56.5 mm ? 10 mm (2.56 in ? 2.22 in ? 0.39 in), same as HAT board	85.60 mm ? 56.5 mm (3.370 in ? 2.224 in), not including protruding connectors	67.6 mm ? 30 mm (2.66 in ? 1.18 in)	65 mm ? 30 mm ? 5 mm (2.56 in ? 1.18 in ? 0.20 in)
Weight	45 g (1.6 oz)	23 g (0.81 oz)	45 g (1.6 oz)	7 g (0.25 oz)[70]	9 g (0.32 oz)[71]
Console	Micro-USB cable[61] or a serial cable with optional GPIO power connector[72]	
Model A	Model A+	Model B	Model B+	Raspberry Pi 2
Model B	Raspberry Pi 3
Model B	Compute Module
Zero
* - all interfaces are via 200-pin DDR2 SO-DIMM connector.

Connectors[edit]

Location of connectors and main ICs on Raspberry Pi Zero
 

Location of connectors and main ICs on Raspberry Pi 1 model A+ revision 1.1
 

Location of connectors and main ICs on Raspberry Pi 1 model B revision 2
 

Location of connectors and main ICs on Raspberry Pi 1 model B+ revision 1.2, Raspberry Pi 2 model B and Raspberry Pi 3 model B
General purpose input-output (GPIO) connector[edit]
RPi A+, B+, 2B and Zero GPIO J8 40-pin pinout.,[73] Model 3 has 40 pins as well, but someone will need to confirm that the pin layout is the same as its predecessor. Models A and B have only the first 26 pins.

GPIO#	2nd func.	Pin#		Pin#	2nd func.	GPIO#
+3.3 V	1		2	+5 V	
2	SDA1 (I2C)	3		4	+5 V	
3	SCL1 (I2C)	5		6	GND	
4	GCLK	7		8	TXD0 (UART)	14
GND	9		10	RXD0 (UART)	15
17	GEN0	11		12	GEN1	18
27	GEN2	13		14	GND	
22	GEN3	15		16	GEN4	23
+3.3 V	17		18	GEN5	24
10	MOSI (SPI)	19		20	GND	
9	MISO (SPI)	21		22	GEN6	25
11	SCLK (SPI)	23		24	CE0_N (SPI)	8
GND	25		26	CE1_N (SPI)	7
(RPi 1 Models A and B stop here)
EEPROM	ID_SD	27		28	ID_SC	EEPROM
5	N/A	29		30	GND	
6	N/A	31		32		12
13	N/A	33		34	GND	
19	N/A	35		36	N/A	16
26	N/A	37		38	Digital IN	20
GND	39		40	Digital OUT	21
Model B rev. 2 also has a pad (called P5 on the board and P6 on the schematics) of 8 pins offering access to an additional 4 GPIO connections.[74]

Function	2nd func.	Pin#		Pin#	2nd func.	Function
N/A	+5 V	1		2	+3.3 V	N/A
GPIO28	GPIO_GEN7	3		4	GPIO_GEN8	GPIO29
GPIO30	GPIO_GEN9	5		6	GPIO_GEN10	GPIO31
N/A	GND	7		8	GND	N/A
Models A and B provide GPIO access to the ACT status LED using GPIO 16. Models A+ and B+ provide GPIO access to the ACT status LED using GPIO 47, and the power status LED using GPIO 35.

Accessories[edit]
Camera – On 14 May 2013, the foundation and the distributors RS Components & Premier Farnell/Element 14 launched the Raspberry Pi camera board with a firmware update to accommodate it.[75] The camera board is shipped with a flexible flat cable that plugs into the CSI connector located between the Ethernet and HDMI ports. In Raspbian, one enables the system to use the camera board by the installing or upgrading to the latest version of the operating system (OS) and then running Raspi-config and selecting the camera option. The cost of the camera module is €20 in Europe (9 September 2013).[76] It can produce 1080p, 720p and 640x480p video. The dimensions are 25 mm x 20 mm x 9 mm.[76]
Gertboard – A Raspberry Pi Foundation sanctioned device, designed for educational purposes, that expands the Raspberry Pi's GPIO pins to allow interface with and control of LEDs, switches, analog signals, sensors and other devices. It also includes an optional Arduino compatible controller to interface with the Pi.[77]
Infrared Camera – In October 2013, the foundation announced that they would begin producing a camera module without an infrared filter, called the Pi NoIR.[78]
HAT (Hardware Attached on Top) expansion boards – Together with the model B+, inspired by the Arduino shield boards, the interface for HAT boards was devised by the Raspberry Pi Foundation. Each HAT board carries a small EEPROM (typically a CAT24C32WI-GT3)[79] containing the relevant details of the board,[80] so that the Raspberry Pi's OS is informed of the HAT, and the technical details of it, relevant to the OS using the HAT.[81] Mechanical details of a HAT board, that use the four mounting holes in their rectangular formation.[82][83]
Software[edit]
Operating systems[edit]
The Raspberry Pi primarily uses Linux-kernel-based operating systems.

The ARM11 chip at the heart of the Pi (first generation models) is based on version 6 of the ARM. The primary supported operating system is Raspbian,[84] although it is compatible with many others. The current release of Ubuntu supports the Raspberry Pi 2,[85] while Ubuntu, and several popular versions of Linux, do not support the older[86] Raspberry Pi 1 that runs on the ARM11. Raspberry Pi 2 can also run the Windows 10 IoT Core operating system,[87] while no version of the Pi can run traditional Windows.[88] The Raspberry Pi 2 currently also supports OpenELEC and RISC OS.[89]

The install manager for the Raspberry Pi is NOOBS. The operating systems included with NOOBS are:

Arch Linux ARM
OpenELEC[90]
OSMC[91] (formerly Raspbmc[92]) and the Kodi open source digital media center[93]
Pidora (Fedora Remix)
Puppy Linux[94]
RISC OS[95] – is the operating system of the first ARM-based computer.
Raspbian (recommended for Raspberry Pi 1)[96] – is maintained independently of the Foundation;[97] based on the Debian ARM hard-float (armhf) architecture port originally designed for ARMv7 and later processors (with Jazelle RCT/ThumbEE and VFPv3), compiled for the more limited ARMv6 instruction set of the Raspberry Pi 1. A minimum size of 4 GB SD card is required for the Raspbian images provided by the Raspberry Pi Foundation. There is a Pi Store for exchanging programs.[98][99]
The Raspbian Server Edition is a stripped version with fewer software packages bundled as compared to the usual desktop computer oriented Raspbian.[100][101]
The Wayland display server protocol enables efficient use of the GPU for hardware accelerated GUI drawing functions.[102] On 16 April 2014, a GUI shell for Weston called Maynard was released.
PiBang Linux – is derived from Raspbian.[103]
Raspbian for Robots – is a fork of Raspbian for robotics projects with Lego, Grove, and Arduino.[104]
Other operating systems
Xbian[105] – using the Kodi (formerly XBMC) open source digital media center
openSUSE[106]
Raspberry Pi Fedora Remix[107]
Gentoo Linux[108]
Ubuntu MATE
CentOS for Raspberry Pi 2 and later
RedSleeve (a RHEL port) for Raspberry Pi 1
Slackware ARM – version 13.37 and later runs on the Raspberry Pi without modification.[109][110][111][112] The 128–496 MB of available memory on the Raspberry Pi is at least twice the minimum requirement of 64 MB needed to run Slackware Linux on an ARM or i386 system.[113] (Whereas the majority of Linux systems boot into a graphical user interface, Slackware's default user environment is the textual shell / command line interface.[114]) The Fluxbox window manager running under the X Window System requires an additional 48 MB of RAM.[115]
FreeBSD[116]
NetBSD.[117][118]
Plan 9 from Bell Labs[119][120] and Inferno[121] (in beta)
Moebius[122] – is a light ARM HF distribution based on Debian. It uses Raspbian repository, but it fits in a 128 MB SD card.[123] It has only minimal services and its memory use is optimized to be small.
OpenWrt – is primarily used on embedded devices to route network traffic.
Kali Linux – is a Debian-derived distro designed for digital forensics and penetration testing.
Pardus ARM[124] – is a Debian-based operating system which is the light version of the Pardus (operating system).
Instant WebKiosk – is an operating system for digital signage purposes (web and media views).
Ark OS – is designed for website and email self-hosting.
ROKOS[125] – is a Raspbian-based operating system with integrated clients for the Bitcoin and OKCash cryptocurrencies.
MinePeon – is a dedicated operating system for mining cryptocurrency.
Kano OS[126]
Nard SDK[127] – is a software development kit (SDK) for industrial embedded systems.
Sailfish OS with Raspberry Pi 2 (due to use ARM Cortex-A7 CPU; Raspberry Pi 1 uses different ARMv6 architecture and Sailfish requires ARMv7.)[128]
Tiny Core Linux – a minimal Linux operating system focused on providing a base system using BusyBox and FLTK. Designed to run primarily in RAM.
Windows 10 IoT Core – a free edition of Windows 10 offered by Microsoft that runs natively on the Raspberry Pi 2.[129]
WTware for Raspberry Pi 2[130] – is a free operating system for creating Windows thin client from Pi 2.
IPFire – is a dedicated firewall/router distribution for the protection of a SOHO LAN; runs only on a Raspberry Pi 1; porting to the Raspberry Pi 2 is not planned for now.[131]
xv6[132] – is a modern reimplementation of Sixth Edition Unix OS for teaching purposes; it is ported to Raspberry Pi from MIT xv6; this xv6 port can boot from NOOBS.
Alpine Linux – is a Linux distribution based on musl and BusyBox, primarily designed for "power users who appreciate security, simplicity and resource efficiency".
Void Linux – a rolling release Linux distribution which was designed and implemented from scratch, provides images based on musl or glibc.
Tingbot OS[133] – based on Raspbian, primarily designed for use with the Tingbot addon and running Tide apps.[134]
Media center operating systems:
OSMC – https://osmc.tv
OpenELEC – http://openelec.tv
Xbian – http://www.xbian.org
Rasplex – http://www.rasplex.com)
Audio operating systems :
Volumio – https://volumio.org
Pimusicbox – http://www.pimusicbox.com
Runeaudio – http://www.runeaudio.com
moOdeaudio – http://www.moodeaudio.org
Retrogaming operating systems:
Retropie – http://blog.petrockblock.com/2015/08/11/retropie-3-0-is-released/
Recalbox – http://www.recalbox.com/
Happi game center – http://happi-game-center.com/
Lakka – http://www.lakka.tv/
ChameleonPi
Piplay – http://piplay.org/
Planned operating systems
Haiku – This open source BeOS clone has been targeted for the Raspberry Pi and several other ARM boards.[135] Work began in 2011 on model 1, but only the model 2 will be supported.
Driver APIs[edit]

Scheme of the implemented APIs: OpenMAX, OpenGL ES and OpenVG
Raspberry Pi can use a VideoCore IV GPU via a binary blob, which is loaded into the GPU at boot time from the SD-card, and additional software, that initially was closed source.[136] This part of the driver code was later released,[137] however much of the actual driver work is done using the closed source GPU code. Application software use calls to closed source run-time libraries (OpenMax, OpenGL ES or OpenVG) which in turn calls an open source driver inside the Linux kernel, which then calls the closed source VideoCore IV GPU driver code. The API of the kernel driver is specific for these closed libraries. Video applications use OpenMAX, 3D applications use OpenGL ES and 2D applications use OpenVG which both in turn use EGL. OpenMAX and EGL use the open source kernel driver in turn.[138]

Third party application software[edit]
AstroPrint – Since August 2014, AstroPrint's Wireless 3D Printing software can be run on the Pi 2[139]
Mathematica & the Wolfram Language – Since 21 November 2013, Raspbian includes a full installation of this proprietary software for free.[140][141][142] As of 24 August 2015, the version is Mathematica 10.2.[143] Programs can be run either from a command line interface or from a Notebook interface. There are Wolfram Language functions for accessing connected devices.[144] There is also a Wolfram Language desktop development kit allowing development for Raspberry Pi in Mathematica from desktop machines.[145]
Minecraft – Released 11 February 2013, a version for the Raspberry Pi, in which you can modify the game world with code, the only official version of the game in which you can do so.[146]
UserGate Web Filter – On 20 September 2013, Florida-based security vendor Entensys announced porting UserGate Web Filter to Raspberry Pi platform.[147]
Software development tools[edit]
AlgoIDE – Learn programming for kids and beginners.
BlueJ – For teaching Java to beginners.
Fawlty Language – A freely usable IDL (programming language) clone for Pi 2.
Greenfoot – Greenfoot teaches object orientation with Java. Create 'actors' which live in 'worlds' to build games, simulations, and other graphical programs.
Julia – Since May 2015, the interactive and cross-platform programming language/environment, Julia, runs on the Pi 2 (and the original).[148]
Lazarus – The professional Free Pascal RAD IDE.
LiveCode – educational RAD IDE descended from HyperCard using English-like language to write event-handlers for WYSIWYG widgets runnable on desktop, mobile and Raspberry Pi platforms
Object Pascal[149]
Ninja-IDE – A cross-platform integrated development environment (IDE) for Python.
Xojo – A cross-platform, professional RAD tool that can create desktop, web and console apps for Pi 2.
V-Play Game Engine – A cross-platform development framework that supports mobile game and app development with the V-Play Game Engine, V-Play Apps and V-Play Plugins.
Tracking Raspberry Pi online on a global map[edit]
Ryan Walmsley, a UK school student, created a site in 2012 to register and track any Raspberry Pi across the globe.[150] It became very popular soon after its launch.[151] The current site is powered by Google Maps and Digital Ocean and is free. It has a limitation of registering only one Raspberry Pi per unique email id. It uses IP based basic location tracking and is fairly accurate up to Locale or City level.

Reception and use[edit]
Technology writer Glyn Moody described the project in May 2011 as a "potential BBC Micro 2.0", not by replacing PC compatible machines but by supplementing them.[152] In March 2012 Stephen Pritchard echoed the BBC Micro successor sentiment in ITPRO.[153] Alex Hope, co-author of the Next Gen report, is hopeful that the computer will engage children with the excitement of programming.[154] Co-author Ian Livingstone suggested that the BBC could be involved in building support for the device, possibly branding it as the BBC Nano.[98] Chris Williams, writing in The Register sees the inclusion of programming languages such as Kids Ruby, Scratch and BASIC as a "good start" to equip kids with the skills needed in the future – although it remains to be seen how effective their use will be.[155] The Centre for Computing History strongly supports the Raspberry Pi project, feeling that it could "usher in a new era".[156] Before release, the board was showcased by ARM's CEO Warren East at an event in Cambridge outlining Google's ideas to improve UK science and technology education.[157]

Harry Fairhead, however, suggests that more emphasis should be put on improving the educational software available on existing hardware, using tools such as Google App Inventor to return programming to schools, rather than adding new hardware choices.[158] Simon Rockman, writing in a ZDNet blog, was of the opinion that teens will have "better things to do", despite what happened in the 1980s.[159]

In October 2012, the Raspberry Pi won T3's Innovation of the Year award,[160] and futurist Mark Pesce cited a (borrowed) Raspberry Pi as the inspiration for his ambient device project MooresCloud.[161] In October 2012, the British Computer Society reacted to the announcement of enhanced specifications by stating, "it's definitely something we'll want to sink our teeth into."[162]

In February 2015, a switched-mode power supply chip, designated U16, of the Raspberry Pi 2 model B version 1.1 (the initially released version) was found to be vulnerable to flashes of light,[163] particularly the light from xenon camera flashes and green[164] and red laser pointers. However, other bright lights, particularly ones that are on continuously, were found to have no effect. The symptom was the Raspberry Pi 2 spontaneously rebooting or turning off when these lights were flashed at the chip. Initially, some users and commenters suspected that the electromagnetic pulse from the xenon flash tube was causing the problem by interfering with the computer's digital circuitry, but this was ruled out by tests where the light was either blocked by a card or aimed at the other side of the Raspberry Pi 2, both of which did not cause a problem. The problem was narrowed down to the U16 chip by covering first the system on a chip (main processor) and then U16 with opaque poster mounting compound. Light being the sole culprit, instead of EMP, was further confirmed by the laser pointer tests,[164] where it was also found that less opaque covering was needed to shield against the laser pointers than to shield against the xenon flashes.[163] The U16 chip seems to be bare silicon without a plastic cover (i.e. a chip-scale package or wafer-level package), which would, if present, block the light. Based on the facts that the chip, like all semiconductors, is light-sensitive (photovoltaic effect), that silicon is transparent to infrared light, and that xenon flashes emit more infrared light than laser pointers (therefore requiring more light shielding),[163] it is currently thought that this combination of factors allows the sudden bright infrared light to cause an instability in the output voltage of the power supply, triggering shutdown or restart of the Raspberry Pi 2. Unofficial workarounds include covering U16 with opaque material (such as electrical tape,[163][164] lacquer, poster mounting compound, or even balled-up bread[163]), putting the Raspberry Pi 2 in a case,[164] and avoiding taking photos of the top side of the board with a xenon flash. This issue was not caught before the release of the Raspberry Pi 2 because while commercial electronic devices are routinely subjected to tests of susceptibility to radio interference, it is not standard or common practice to test their susceptibility to optical interference.[163]

Community[edit]
The Raspberry Pi community was described by Jamie Ayre of FLOSS software company AdaCore as one of the most exciting parts of the project.[165] Community blogger Russell Davis said that the community strength allows the Foundation to concentrate on documentation and teaching.[165] The community developed a fanzine around the platform called The MagPi[166] which in 2015, was handed over to the Raspberry Pi Foundation by its volunteers to be continued in-house.[167] A series of community Raspberry Jam events have been held across the UK and around the world.[168]

Use in education[edit]
As of January 2012, enquiries about the board in the United Kingdom have been received from schools in both the state and private sectors, with around five times as much interest from the latter. It is hoped that businesses will sponsor purchases for less advantaged schools.[169] The CEO of Premier Farnell said that the government of a country in the Middle East has expressed interest in providing a board to every schoolgirl, in order to enhance her employment prospects.[170][171]

In 2014, the Raspberry Pi Foundation hired a number of its community members including ex-teachers and software developers to launch a set of free learning resources for its website.[172] The resources are freely licensed under Creative Commons, and contributions and collaborations are encouraged on social coding platform GitHub.

The Foundation also started a teacher training course called Picademy with the aim of helping teachers prepare for teaching the new computing curriculum using the Raspberry Pi in the classroom.[173] The continued professional development course is provided free for teachers and is run by the Foundation's education team.

Astro Pi[edit]
A project was launched in December 2014 at an event held by the UK Space Agency. The Astro Pi competition was officially opened in January and was opened to all primary and secondary school aged children who were residents of the United Kingdom. During his mission, British ESA Astronaut Tim Peake plans to deploy the computers on board the International Space Station. He will then load up the winning code while in orbit, collect the data generated and then send this to Earth where it will be distributed to the winning teams. The themes of Spacecraft Sensors, Satellite Imaging, Space Measurements, Data Fusion and Space Radiation were devised to stimulate creative and scientific thinking.

The organisations involved in the Astro Pi competition include the UK Space Agency, UKspace, Raspberry Pi, ESERO-UK and ESA.

Reviews[edit]
Raspberry Pi model B rev. 1 was rated 4/5 by PCMag, while Raspberry Pi model B rev. 2 was rated 4.1/5 by Board-DB.org.

History[edit]

This section is in a list format that may be better presented using prose. You can help by converting this section to prose, if appropriate. Editing help is available. (February 2015)

An early alpha-test board in operation using different layout from later beta and production boards
In 2006, early concepts of the Raspberry Pi were based on the Atmel ATmega644 microcontroller. Its schematics and PCB layout are publicly available.[174] Foundation trustee Eben Upton assembled a group of teachers, academics and computer enthusiasts to devise a computer to inspire children.[169] The computer is inspired by Acorn's BBC Micro of 1981.[175][176] Pi's model A, model B and model B+ are references to the original models of the British educational BBC Micro computer, developed by Acorn Computers.[155] The first ARM prototype version of the computer was mounted in a package the same size as a USB memory stick.[177] It had a USB port on one end and an HDMI port on the other.

The Foundation's goal was to offer two versions, priced at US$25 and US$35. They started accepting orders for the higher priced model B on 29 February 2012,[178] the lower cost model A on 4 February 2013.[179] and the even lower cost (US$20) A+ on 10 November 2014.[46] On November 26, 2015, the cheapest Raspberry PI yet, the Raspberry PI Zero was launched at US$5 or ?4.[180]

Pre-launch[edit]
July 2011: Trustee Eben Upton publicly approached the RISC OS Open community in July 2011 to enquire about assistance with a port.[181] Adrian Lees at Broadcom has since worked on the port,[182][183] with his work being cited in a discussion regarding the graphics drivers.[184] This port is now included in NOOBS.
August 2011 – 50 alpha boards are manufactured. These boards were functionally identical to the planned model B,[185] but they were physically larger to accommodate debug headers. Demonstrations of the board showed it running the LXDE desktop on Debian, Quake 3 at 1080p,[186] and Full HD MPEG-4 video over HDMI.[187]
October 2011 – A version of RISC OS 5 was demonstrated in public, and following a year of development the port was released for general consumption in November 2012.[95][188][189][190]
December 2011 – Twenty-five model B Beta boards were assembled and tested[191] from one hundred unpopulated PCBs.[192] The component layout of the Beta boards was the same as on production boards. A single error was discovered in the board design where some pins on the CPU were not held high; it was fixed for the first production run.[193] The Beta boards were demonstrated booting Linux, playing a 1080p movie trailer and the Rightware Samurai OpenGL ES benchmark.[194]
Early 2012 – During the first week of the year, the first 10 boards were put up for auction on eBay.[195][196] One was bought anonymously and donated to the museum at The Centre for Computing History in Cambridge, England.[156][197] The ten boards (with a total retail price of ?220) together raised over ?16,000,[198] with the last to be auctioned, serial number No. 01, raising ?3,500.[199] In advance of the anticipated launch at the end of February 2012, the Foundation's servers struggled to cope with the load placed by watchers repeatedly refreshing their browsers.[200]
Launch[edit]

Raspberry Pi model A
19 February 2012 – The first proof of concept SD card image that could be loaded onto an SD card to produce a preliminary operating system is released. The image was based on Debian 6.0 (Squeeze), with the LXDE desktop and the Midori browser, plus various programming tools. The image also runs on QEMU allowing the Raspberry Pi to be emulated on various other platforms.[201][202]
29 February 2012 – Initial sales commence 29 February 2012[203] at 06:00 UTC;. At the same time, it was announced that the model A, originally to have had 128 MB of RAM, was to be upgraded to 256 MB before release.[178] The Foundation's website also announced: "Six years after the project's inception, we're nearly at the end of our first run of development – although it's just the beginning of the Raspberry Pi story."[204] The web-shops of the two licensed manufacturers selling Raspberry Pi's within the United Kingdom, Premier Farnell and RS Components, had their websites stalled by heavy web traffic immediately after the launch (RS Components briefly going down completely).[205][206] Unconfirmed reports suggested that there were over two million expressions of interest or pre-orders.[207] The official Raspberry Pi Twitter account reported that Premier Farnell sold out within a few minutes of the initial launch, while RS Components took over 100,000 pre orders on day one.[178] Manufacturers were reported in March 2012 to be taking a "healthy number" of pre-orders.[165]
March 2012 – Shipping delays for the first batch were announced in March 2012, as the result of installation of an incorrect Ethernet port,[208][209] but the Foundation expected that manufacturing quantities of future batches could be increased with little difficulty if required.[210] "We have ensured we can get them [the Ethernet connectors with magnetics] in large numbers and Premier Farnell and RS Components [the two distributors] have been fantastic at helping to source components," Upton said. The first batch of 10,000 boards was manufactured in Taiwan and China.[211][212]
8 March 2012 – Release Raspberry Pi Fedora Remix, the recommended Linux distribution,[213] developed at Seneca College in Canada.[214]
March 2012 – The Debian port is initiated by Mike Thompson, former CTO of Atomz. The effort was largely carried out by Thompson and Peter Green, a volunteer Debian developer, with some support from the Foundation, who tested the resulting binaries that the two produced during the early stages (neither Thompson nor Green had physical access to the hardware, as boards were not widely accessible at the time due to demand).[215] While the preliminary proof of concept image distributed by the Foundation before launch was also Debian-based, it differed from Thompson and Green's Raspbian effort in a couple of ways. The POC image was based on then-stable Debian Squeeze, while Raspbian aimed to track then-upcoming Debian Wheezy packages.[202] Aside from the updated packages that would come with the new release, Wheezy was also set to introduce the armhf architecture,[216] which became the raison d'?tre for the Raspbian effort. The Squeeze-based POC image was limited to the armel architecture, which was, at the time of Squeeze's release, the latest attempt by the Debian project to have Debian run on the newest ARM embedded-application binary interface (EABI).[217] The armhf architecture in Wheezy intended to make Debian run on the ARM VFP hardware floating-point unit, while armel was limited to emulating floating point operations in software.[218][219] Since the Raspberry Pi included a VFP, being able to make use of the hardware unit would result in performance gains and reduced power use for floating point operations.[215] The armhf effort in mainline Debian, however, was orthogonal to the work surrounding the Pi and only intended to allow Debian to run on ARMv7 at a minimum, which would mean the Pi, an ARMv6 device, would not benefit.[216] As a result, Thompson and Green set out to build the 19,000 Debian packages for the device using a custom build cluster.[215]
Post-launch[edit]
16 April 2012 – Reports appear from the first buyers who had received their Raspberry Pi.[220][221]
20 April 2012 – The schematics for the model A and model B are released.[222]
18 May 2012 – The Foundation reported on its blog about a prototype camera module they had tested.[223] The prototype used a 14-megapixel module.
22 May 2012 – Over 20,000 units had been shipped.[150]
16 July 2012 – It was announced that 4,000 units were being manufactured per day, allowing Raspberry Pis to be bought in bulk.[224][225]
24 August 2012 – Hardware accelerated video (H.264) encoding becomes available after it became known that the existing license also covered encoding. Formerly it was thought that encoding would be added with the release of the announced camera module.[226][227] However, no stable software exists for hardware H.264 encoding.[228] At the same time the Foundation released two additional codecs that can be bought separately, MPEG-2 and Microsoft's VC-1. Also it was announced that the Pi will implement CEC, enabling it to be controlled with the television's remote control.[50]
July 2012 – Release of Raspbian.[229]
5 September 2012 – The Foundation announced a second revision of the Raspberry Pi Model B.[230] A revision 2.0 board is announced, with a number of minor corrections and improvements.[231]
6 September 2012 – Announcement that in future the bulk of Raspberry Pi units would be manufactured in the UK, at Sony's manufacturing facility in Pencoed, Wales. The Foundation estimated that the plant would produce 30,000 units per month, and would create about 30 new jobs.[232][233]
15 October 2012 – It is announced that new Raspberry Pi Model Bs are to be fitted with 512 MB instead of 256 MB RAM.[234]
24 October 2012 – The Foundation announces that "all of the VideoCore driver code which runs on the ARM" had been released as free software under a BSD-style license, making it "the first ARM-based multimedia SoC with fully-functional, vendor-provided (as opposed to partial, reverse engineered) fully open-source drivers", although this claim has not been universally accepted.[137] On 28 February 2014, they also announced the release of full documentation for the VideoCore IV graphics core, and a complete source release of the graphics stack under a 3-clause BSD license[235][236]
October 2012 – It was reported that some customers of one of the two main distributors had been waiting more than six months for their orders. This was reported to be due to difficulties in sourcing the CPU and conservative sales forecasting by this distributor.[237]
17 December 2012 – The Foundation, in collaboration with IndieCity and Velocix, opens the Pi Store, as a "one-stop shop for all your Raspberry Pi (software) needs". Using an application included in Raspbian, users can browse through several categories and download what they want. Software can also be uploaded for moderation and release.[238]
3 June 2013 – 'New Out Of Box Software or NOOBS is introduced. This makes the Raspberry Pi easier to use by simplifying the installation of an operating system. Instead of using specific software to prepare an SD card, a file is unzipped and the contents copied over to a FAT formatted (4 GB or bigger) SD card. That card can then be booted on the Raspberry Pi and a choice of six operating systems is presented for installation on the card. The system also contains a recovery partition that allows for the quick restoration of the installed OS, tools to modify the config.txt and an online help button and web browser which directs to the Raspberry Pi Forums.[239]
October 2013 – The Foundation announces that the one millionth Pi had been manufactured in the United Kingdom.[240]
November 2013: they announce that the two millionth Pi shipped between 24 and 31 October.[241]
28 February 2014 – On the day of the second anniversary of the Raspberry Pi, Broadcom, together with the Raspberry PI foundation, announced the release of full documentation for the VideoCore IV graphics core[clarification needed], and a complete source release of the graphics stack under a 3-clause BSD license.[235][236]

Raspberry Pi Compute Module

Raspberry Pi Model B
7 April 2014 – The official Raspberry Pi blog announced the Raspberry Pi Compute Module, a device in a 200-pin DDR2 SO-DIMM-configured memory module (though not in any way compatible with such RAM), intended for consumer electronics designers to use as the core of their own products.[44]
June 2014 – The official Raspberry Pi blog mentioned that the three millionth Pi shipped in early May 2014.[242]
14 July 2014 – The official Raspberry Pi blog announced the Raspberry Pi Model B+, "the final evolution of the original Raspberry Pi. For the same price as the original Raspberry Pi model B, but incorporating numerous small improvements people have been asking for".[41]
10 November 2014 – The official Raspberry Pi blog announced the Raspberry Pi Model A+.[46] It is the smallest and cheapest (US$20) Raspberry Pi so far and has the same processor and RAM as the model A. Like the A, it has no Ethernet port, and only one USB port, but does have the other innovations of the B+, like lower power, micro-SD-card slot, and 40-pin HAT compatible GPIO.
2 February 2015 – The official Raspberry Pi blog announced the Raspberry Pi 2. Looking like a Model B+, it has a 900 MHz quad-core ARMv7 Cortex-A7 CPU, twice the memory (for a total of 1 GB) and complete compatibility with the original generation of Raspberry Pis.[243]
14 May 2015 – The price of Model B+ was decreased from US$35 to US$25, purportedly as a "side effect of the production optimizations" from the Pi 2 development.[244] Industry observers have skeptically noted, however, that the price drop appeared to be a direct response to the "C.H.I.P.", a lower-priced competitor.[245]
26 November 2015 – The Raspberry Pi Foundation launched the Raspberry Pi Zero, the smallest and cheapest member of the Raspberry Pi family yet, at 65 mm ? 30 mm, and US$5. The Zero is similar to the model A+ without camera and LCD connectors, while smaller and uses less power. It was given away with the Raspberry PI magazine Magpi #40 that was distributed in the UK and USA that day – the MagPi was sold out at almost every retailer internationally due to the freebie.[45]
29 February 2016 – Raspberry Pi 3 with a BCM2837 1.2 GHz 64-bit quad processor based on the ARMv8 Cortex A53, with built-in Wi-Fi BCM43438 802.11n 2.4 GHz and Bluetooth 4.1 Low Energy (BLE). Starting with a 32-bit Raspbian version.[246]
25 April 2016 - Raspberry Pi Camera v2.1 announced with 8 Mpixels, in normal and NoIR (can receive IR) versions. The camera uses the Sony IMX219 chip. The specific resolution is 3280 ? 2464 to make use of the new resolution the software has to be updated.[247]
Programmable logic device
From Wikipedia, the free encyclopedia

It has been suggested that Simple programmable logic device be merged into this article. (Discuss) Proposed since September 2013.

This article's lead section may not adequately summarize key points of its contents. Please consider expanding the lead to provide an accessible overview of all important aspects of the article. Please discuss this issue on the article's talk page. (January 2013)

A simplified PAL device. The programmable elements (shown as a fuse) connect both the true and complemented inputs to the AND gates. These AND gates, also known as product terms, are ORed together to form a sum-of-products logic array.
A programmable logic device (PLD) is an electronic component used to build reconfigurable digital circuits. Unlike a logic gate, which has a fixed function, a PLD has an undefined function at the time of manufacture. Before the PLD can be used in a circuit it must be programmed, that is, reconfigured.

Contents  [hide] 
1	Using a ROM as a PLD
2	Early programmable logic
3	PLA
4	PAL
5	GALs
6	CPLDs
7	FPGAs
8	Other variants
9	How PLDs retain their configuration
10	PLD programming languages
11	PLD programming devices
12	See also
13	References
14	External links
Using a ROM as a PLD[edit]

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2013)
Before PLDs were invented, read-only memory (ROM) chips were used to create arbitrary combinational logic functions of a number of inputs. Consider a ROM with m inputs (the address lines) and n outputs (the data lines). When used as a memory, the ROM contains 2m words of n bits each.

Now imagine that the inputs are driven not by an m-bit address, but by m independent logic signals. Theoretically, there are 22m possible Boolean functions of these m input signals. By Boolean function in this context is meant a single function that maps each of the 2m possible combinations of the m Boolean inputs to a single Boolean output. There are 22m possible distinct ways to map each of 2m inputs to a Boolean value, which explains why there are 22m such Boolean functions of m inputs.

Now, consider that each of the n output pins acts, independently, as a logic device that is specially selected to sample just one of the possible 22m such functions. At any given time, only one of the 2m possible input values can be present on the ROM, but over time, as the input values span their full possible domain, each output pin will map out its particular function of the 2m possible input values, from among the 22m possible such functions. Note that the structure of the ROM allows just n of the 22m possible such Boolean functions to be produced at the output pins. The ROM therefore becomes equivalent to n separate logic circuits, each of which generates a chosen function of the m inputs.

The advantage of using a ROM in this way is that any conceivable function of all possible combinations of the m inputs can be made to appear at any of the n outputs, making this the most general-purpose combinational logic device available for m input pins and n output pins.

Also, PROMs (programmable ROMs), EPROMs (ultraviolet-erasable PROMs) and EEPROMs (electrically erasable PROMs) are available that can be programmed using a standard PROM programmer without requiring specialised hardware or software. However, there are several disadvantages:

they are usually much slower than dedicated logic circuits,
they cannot necessarily provide safe "covers" for asynchronous logic transitions so the PROM's outputs may glitch as the inputs switch,
they consume more power[clarification needed],
they are often more expensive than programmable logic, especially if high speed is required.
Since most ROMs do not have input or output registers, they cannot be used stand-alone for sequential logic. An external TTL register was often used for sequential designs such as state machines. Common EPROMs, for example the 2716, are still sometimes used in this way by hobby circuit designers, who often have some lying around. This use is sometimes called a 'poor man's PAL'.

Early programmable logic[edit]
In 1969, Motorola offered the XC157, a mask-programmed gate array with 12 gates and 30 uncommitted input/output pins.[1]

In 1970, Texas Instruments developed a mask-programmable IC based on the IBM read-only associative memory or ROAM. This device, the TMS2000, was programmed by altering the metal layer during the production of the IC. The TMS2000 had up to 17 inputs and 18 outputs with 8 JK flip flop for memory. TI coined the term programmable logic array for this device.[2]

In 1971, General Electric Company (GE) was developing a programmable logic device based on the new PROM technology. This experimental device improved on IBM's ROAM by allowing multilevel logic. Intel had just introduced the floating-gate UV erasable PROM so the researcher at GE incorporated that technology. The GE device was the first erasable PLD ever developed, predating the Altera EPLD by over a decade. GE obtained several early patents on programmable logic devices.[3][4][5]

In 1973 National Semiconductor introduced a mask-programmable PLA device (DM7575) with 14 inputs and 8 outputs with no memory registers. This was more popular than the TI part but cost of making the metal mask limited its use. The device is significant because it was the basis for the field programmable logic array produced by Signetics in 1975, the 82S100. (Intersil actually beat Signetics to market but poor yield doomed their part.)[6][7]

In 1974 GE entered into an agreement with Monolithic Memories to develop a mask- programmable logic device incorporating the GE innovations. The device was named the 'Programmable Associative Logic Array' or PALA. The MMI 5760 was completed in 1976 and could implement multilevel or sequential circuits of over 100 gates. The device was supported by a GE design environment where Boolean equations would be converted to mask patterns for configuring the device. The part was never brought to market.[8]

PLA[edit]
Main article: Programmable logic array
In 1970, Texas Instruments developed a mask-programmable IC based on the IBM read-only associative memory or ROAM. This device, the TMS2000, was programmed by altering the metal layer during the production of the IC. The TMS2000 had up to 17 inputs and 18 outputs with 8 JK flip flop for memory. TI coined the term programmable logic array for this device.[2]

A programmable logic array (PLA) has a programmable AND gate array, which links to a programmable OR gate array, which can then be conditionally complemented to produce an output.

PAL[edit]
Main article: Programmable array logic
PAL devices have arrays of transistor cells arranged in a "fixed-OR, programmable-AND" plane used to implement "sum-of-products" binary logic equations for each of the outputs in terms of the inputs and either synchronous or asynchronous feedback from the outputs.

MMI introduced a breakthrough device in 1978, the programmable array logic or PAL. The architecture was simpler than that of Signetics FPLA because it omitted the programmable OR array. This made the parts faster, smaller and cheaper. They were available in 20 pin 300 mil DIP packages while the FPLAs came in 28 pin 600 mil packages. The PAL Handbook demystified the design process. The PALASM design software (PAL assembler) converted the engineers' Boolean equations into the fuse pattern required to program the part. The PAL devices were soon second-sourced by National Semiconductor, Texas Instruments and AMD.

After MMI succeeded with the 20-pin PAL parts, AMD introduced the 24-pin 22V10 PAL with additional features. After buying out MMI (1987), AMD spun off a consolidated operation as Vantis, and that business was acquired by Lattice Semiconductor in 1999.

GALs[edit]
Main article: Generic array logic

Lattice GAL 16V8 and 20V8
An improvement on the PAL was the generic array logic device, or GAL, invented by Lattice Semiconductor in 1985. This device has the same logical properties as the PAL but can be erased and reprogrammed. The GAL is very useful in the prototyping stage of a design, when any bugs in the logic can be corrected by reprogramming. GALs are programmed and reprogrammed using a PAL programmer, or by using the in-circuit programming technique on supporting chips.

Lattice GALs combine CMOS and electrically erasable (E2) floating gate technology for a high-speed, low-power logic device.

A similar device called a PEEL (programmable electrically erasable logic) was introduced by the International CMOS Technology (ICT) corporation.

CPLDs[edit]
Main article: Complex programmable logic device
PALs and GALs are available only in small sizes, equivalent to a few hundred logic gates. For bigger logic circuits, complex PLDs or CPLDs can be used. These contain the equivalent of several PALs linked by programmable interconnections, all in one integrated circuit. CPLDs can replace thousands, or even hundreds of thousands, of logic gates.

Some CPLDs are programmed using a PAL programmer, but this method becomes inconvenient for devices with hundreds of pins. A second method of programming is to solder the device to its printed circuit board, then feed it with a serial data stream from a personal computer. The CPLD contains a circuit that decodes the data stream and configures the CPLD to perform its specified logic function. Some manufacturers (including Altera and Microsemi) use JTAG to program CPLD's in-circuit from .JAM files.

FPGAs[edit]
Main article: Field-programmable gate array
While PALs were busy developing into GALs and CPLDs (all discussed above), a separate stream of development was happening. This type of device is based on gate array technology and is called the field-programmable gate array (FPGA). Early examples of FPGAs are the 82s100 array, and 82S105 sequencer, by Signetics, introduced in the late 1970s. The 82S100 was an array of AND terms. The 82S105 also had flip flop functions.

FPGAs use a grid of logic gates, and once stored, the data doesn't change, similar to that of an ordinary gate array. The term "field-programmable" means the device is programmed by the customer, not the manufacturer.

FPGAs are usually programmed after being soldered down to the circuit board, in a manner similar to that of larger CPLDs. In most larger FPGAs, the configuration is volatile and must be re-loaded into the device whenever power is applied or different functionality is required. Configuration is typically stored in a configuration PROM or EEPROM. EEPROM versions may be in-system programmable (typically via JTAG).

The difference between FPGAs and CPLDs is that FPGAs are internally based on Look-up tables (LUTs) whereas CPLDs form the logic functions with sea-of-gates (e.g. sum of products). CPLDs are meant for simpler designs while FPGAs are meant for more complex designs. In general, CPLDs are a good choice for wide combinational logic applications, whereas FPGAs are more suitable for large state machines (i.e. microprocessors).

Other variants[edit]
At present, much interest exists in reconfigurable systems. These are microprocessor circuits that contain some fixed functions and other functions that can be altered by code running on the processor. Designing self-altering systems requires engineers to learn new methods, and that new software tools be developed.

PLDs are being sold now that contain a microprocessor with a fixed function (the so-called core) surrounded by programmable logic. These devices let designers concentrate on adding new features to designs without having to worry about making the microprocessor work.

How PLDs retain their configuration[edit]
A PLD is a combination of a logic device and a memory device. The memory is used to store the pattern that was given to the chip during programming. Most of the methods for storing data in an integrated circuit have been adapted for use in PLDs. These include:

Silicon antifuses
SRAM
EPROM or EEPROM cells
Flash memory
Silicon antifuses are connections that are made by applying a voltage across a modified area of silicon inside the chip. They are called antifuses because they work in the opposite way to normal fuses, which begin life as connections until they are broken by an electric current.

SRAM, or static RAM, is a volatile type of memory, meaning that its contents are lost each time the power is switched off. SRAM-based PLDs therefore have to be programmed every time the circuit is switched on. This is usually done automatically by another part of the circuit.

An EPROM cell is a MOS (metal-oxide-semiconductor) transistor that can be switched on by trapping an electric charge permanently on its gate electrode. This is done by a PAL programmer. The charge remains for many years and can only be removed by exposing the chip to strong ultraviolet light in a device called an EPROM eraser.

Flash memory is non-volatile, retaining its contents even when the power is switched off. It can be erased and reprogrammed as required. This makes it useful for PLD memory.

As of 2005, most CPLDs are electrically programmable and erasable, and non-volatile. This is because they are too small to justify the inconvenience of programming internal SRAM cells every time they start up, and EPROM cells are more expensive due to their ceramic package with a quartz window.

PLD programming languages[edit]
Many PAL programming devices accept input in a standard file format, commonly referred to as 'JEDEC files'.They are analogous to software compilers. The languages used as source code for logic compilers are called hardware description languages, or HDLs.

PALASM, ABEL and CUPL are frequently used for low-complexity devices, while Verilog and VHDL are popular higher-level description languages for more complex devices. The more limited ABEL is often used for historical reasons, but for new designs VHDL is more popular, even for low-complexity designs.

For modern PLD programming languages, design flows, and tools, see FPGA and Reconfigurable computing.

PLD programming devices[edit]
A device programmer is used to transfer the boolean logic pattern into the programmable device. In the early days of programmable logic, every PLD manufacturer also produced a specialized device programmer for its family of logic devices. Later, universal device programmers came onto the market that supported several logic device families from different manufacturers. Today's device programmers usually can program common PLDs (mostly PAL/GAL equivalents) from all existing manufacturers. Common file formats used to store the boolean logic pattern (fuses) are JEDEC, Altera POF (programmable object file), or Xilinx BITstream.[9]

See also[edit]
Macrocell array
Programmable logic array (PLA)
Programmable array logic (PAL)
Field-programmable gate array (FPGA)
Complex programmable logic device (CPLD)
Erasable programmable logic device (EPLD)
Application-specific integrated circuit (ASIC)
Programmable logic controller (PLC)

Macrocell array
From Wikipedia, the free encyclopedia

Each OR gate drives an output logic macrocell. Each macrocell in a PLD consists of a D flip-flop.

This article does not cite any sources. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2009) (Learn how and when to remove this template message)
For other uses, see Macrocell (disambiguation).
Macrocell arrays in PLD[edit]
Programmable logic devices, such as programmable array logic and complex programmable logic devices, typically have a macrocell on every output pin.

Macrocell arrays in ASIC[edit]
A macrocell array is an approach to the design and manufacture of ASICs. Essentially, it is a small step up from the otherwise similar gate array, but rather than being a prefabricated array of simple logic gates, the macrocell array is a prefabricated array of higher-level logic functions such as flip-flops, ALU functions, registers, and the like. These logic functions are simply placed at regular predefined positions and manufactured on a wafer, usually called master slice. Creation of a circuit with a specified function is accomplished by adding metal interconnects to the chips on the master slice late in the manufacturing process, allowing the function of the chip to be customised as desired.

Macrocell array master slices are usually prefabricated and stockpiled in large quantities regardless of customer orders. The fabrication according to the individual customer specifications may be finished in a shorter time compared with standard cell or full custom design. The macrocell array approach reduces the mask costs since fewer custom masks need to be produced. In addition manufacturing test tooling lead time and costs are reduced since the same test fixtures may be used for all macrocell array products manufactured on the same die size.

Drawbacks are somewhat low density and performance than other approaches to ASIC design. However this style is often a viable approach for low production volumes.

A standard cell library is sometimes called a "macrocell library".[1][2]

Flip-flop (electronics)
From Wikipedia, the free encyclopedia

This article includes a list of references, but its sources remain unclear because it has insufficient inline citations. Please help to improve this article by introducing more precise citations. (May 2013)

An animated interactive SR latch (R1, R2 = 1 k? R3, R4 = 10 k?).

An SR latch, constructed from a pair of cross-coupled NOR gates.
In electronics, a flip-flop or latch is a circuit that has two stable states and can be used to store state information. A flip-flop is a bistable multivibrator. The circuit can be made to change state by signals applied to one or more control inputs and will have one or two outputs. It is the basic storage element in sequential logic. Flip-flops and latches are fundamental building blocks of digital electronics systems used in computers, communications, and many other types of systems.

Flip-flops and latches are used as data storage elements. A flip-flop stores a single bit (binary digit) of data; one of its two states represents a "one" and the other represents a "zero". Such data storage can be used for storage of state, and such a circuit is described as sequential logic. When used in a finite-state machine, the output and next state depend not only on its current input, but also on its current state (and hence, previous inputs). It can also be used for counting of pulses, and for synchronizing variably-timed input signals to some reference timing signal.

Flip-flops can be either simple (transparent or opaque) or clocked (synchronous or edge-triggered). Although the term flip-flop has historically referred generically to both simple and clocked circuits, in modern usage it is common to reserve the term flip-flop exclusively for discussing clocked circuits; the simple ones are commonly called latches.[1][2]

Using this terminology, a latch is level-sensitive, whereas a flip-flop is edge-sensitive. That is, when a latch is enabled it becomes transparent, while a flip flop's output only changes on a single type (positive going or negative going) of clock edge.

Contents  [hide] 
1	History
2	Implementation
3	Flip-flop types
3.1	Simple set-reset latches
3.1.1	SR NOR latch
3.1.2	SR NAND latch
3.1.3	SR AND-OR latch
3.1.4	JK latch
3.2	Gated latches and conditional transparency
3.2.1	Gated SR latch
3.2.2	Gated D latch
3.2.3	Earle latch
3.3	D flip-flop
3.3.1	Classical positive-edge-triggered D flip-flop
3.3.2	Master–slave edge-triggered D flip-flop
3.3.3	Edge-triggered dynamic D storage element
3.4	T flip-flop
3.5	JK flip-flop
4	Timing considerations
4.1	Timing parameters
4.2	Metastability
4.3	Propagation delay
5	Generalizations
6	See also
7	References
8	External links
History[edit]

Flip-flop schematics from the Eccles and Jordan patent filed 1918, one drawn as a cascade of amplifiers with a positive feedback path, and the other as a symmetric cross-coupled pair
The first electronic flip-flop was invented in 1918 by the British physicists William Eccles and F. W. Jordan.[3][4] It was initially called the Eccles–Jordan trigger circuit and consisted of two active elements (vacuum tubes).[5] The design was used in the 1943 British Colossus codebreaking computer[6] and such circuits and their transistorized versions were common in computers even after the introduction of integrated circuits, though flip-flops made from logic gates are also common now.[7][8] Early flip-flops were known variously as trigger circuits or multivibrators.

According to P. L. Lindley, an engineer at the US Jet Propulsion Laboratory, the flip-flop types detailed below (RS, D, T, JK) were first discussed in a 1954 UCLA course on computer design by Montgomery Phister, and then appeared in his book Logical Design of Digital Computers.[9][10] Lindley was at the time working at Hughes Aircraft under Eldred Nelson, who had coined the term JK for a flip-flop which changed states when both inputs were on (a logical "one"). The other names were coined by Phister. They differ slightly from some of the definitions given below. Lindley explains that he heard the story of the JK flip-flop from Eldred Nelson, who is responsible for coining the term while working at Hughes Aircraft. Flip-flops in use at Hughes at the time were all of the type that came to be known as J-K. In designing a logical system, Nelson assigned letters to flip-flop inputs as follows: #1: A & B, #2: C & D, #3: E & F, #4: G & H, #5: J & K. Nelson used the notations "j-input" and "k-input" in a patent application filed in 1953.[11]

Implementation[edit]

A traditional flip-flop circuit based on bipolar junction transistors
Flip-flops can be either simple (transparent or asynchronous) or clocked (synchronous); the transparent ones are commonly called latches.[1] The word latch is mainly used for storage elements, while clocked devices are described as flip-flops.[2]

Simple flip-flops can be built around a pair of cross-coupled inverting elements: vacuum tubes, bipolar transistors, field effect transistors, inverters, and inverting logic gates have all been used in practical circuits. Clocked devices are specially designed for synchronous systems; such devices ignore their inputs except at the transition of a dedicated clock signal (known as clocking, pulsing, or strobing). Clocking causes the flip-flop to either change or retain its output signal based upon the values of the input signals at the transition. Some flip-flops change output on the rising edge of the clock, others on the falling edge.

Since the elementary amplifying stages are inverting, two stages can be connected in succession (as a cascade) to form the needed non-inverting amplifier. In this configuration, each amplifier may be considered as an active inverting feedback network for the other inverting amplifier. Thus the two stages are connected in a non-inverting loop although the circuit diagram is usually drawn as a symmetric cross-coupled pair (both the drawings are initially introduced in the Eccles–Jordan patent).

Flip-flop types[edit]
Flip-flops can be divided into common types: the SR ("set-reset"), D ("data" or "delay"[12]), T ("toggle"), and JK types are the common ones. The behavior of a particular type can be described by what is termed the characteristic equation, which derives the "next" (i.e., after the next clock pulse) output, Qnext in terms of the input signal(s) and/or the current output, Q.

Simple set-reset latches[edit]
SR NOR latch[edit]

A SR latch, constructed from a pair of cross-coupled NOR gates (an animated picture). Red and black mean logical '1' and '0', respectively.

An animated SR latch. Black and white mean logical '1' and '0', respectively.
(A) S = 1, R = 0: set
(B) S = 0, R = 0: hold
(C) S = 0, R = 1: reset
(D) S = 1, R = 1: not allowed
The restricted combination (D) leads to an unstable state.
When using static gates as building blocks, the most fundamental latch is the simple SR latch, where S and R stand for set and reset. It can be constructed from a pair of cross-coupled NOR logic gates. The stored bit is present on the output marked Q.

While the R and S inputs are both low, feedback maintains the Q and Q outputs in a constant state, with Q the complement of Q. If S (Set) is pulsed high while R (Reset) is held low, then the Q output is forced high, and stays high when S returns to low; similarly, if R is pulsed high while S is held low, then the Q output is forced low, and stays low when R returns to low.

SR latch operation[13]
Characteristic table	Excitation table
S	R	Qnext	Action	Q	Qnext	S	R
0	0	Q	hold state	0	0	0	X
0	1	0	reset	0	1	1	0
1	0	1	set	1	0	0	1
1	1	X	not allowed	1	1	X	0
Note: X means don't care, that is, either 0 or 1 is a valid value.

The R = S = 1 combination is called a restricted combination or a forbidden state because, as both NOR gates then output zeros, it breaks the logical equation Q = not Q. The combination is also inappropriate in circuits where both inputs may go low simultaneously (i.e. a transition from restricted to keep). The output would lock at either 1 or 0 depending on the propagation time relations between the gates (a race condition).

To overcome the restricted combination, one can add gates to the inputs that would convert (S,R) = (1,1) to one of the non-restricted combinations. That can be:

Q = 1 (1,0) – referred to as an S (dominated)-latch
Q = 0 (0,1) – referred to as an R (dominated)-latch
This is done in nearly every programmable logic controller.

Keep state (0,0) – referred to as an E-latch
Alternatively, the restricted combination can be made to toggle the output. The result is the JK latch.

Characteristic: Q+ = R'Q + R'S or Q+ = R'Q + S.[14]

SR NAND latch[edit]

An SR latch
This is an alternate model of the simple SR latch which is built with NAND logic gates. Set and reset now become active low signals, denoted S and R respectively. Otherwise, operation is identical to that of the SR latch. Historically, SR-latches have been predominant despite the notational inconvenience of active-low inputs.[citation needed]

SR latch operation
S	R	Action
0	0	Not allowed
0	1	Q = 1
1	0	Q = 0
1	1	No change

Symbol for an SR NAND latch
SR AND-OR latch[edit]

An SR AND-OR latch. Light green means logical '1' and dark green means logical '0'. The latch is currently in hold mode (no change).
From the teaching point of view, SR latches realised as a pair of cross-coupled components (transistors, gates, tubes, etc.) are rather hard to understand for beginners. A didactically easier to understand model uses a feedback loop instead of the cross-coupling. The following is an SR latch built with an AND gate with one inverted input and an OR gate.

SR AND-OR latch operation
S	R	Action
0	0	No change
1	0	Q = 1
X	1	Q = 0
JK latch[edit]
The JK latch is much less frequently used than the JK flip-flop. The JK latch follows the following state table:

JK latch truth table
J	K	Qnext	Comment
0	0	Q	No change
0	1	0	Reset
1	0	1	Set
1	1	Q	Toggle
Hence, the JK latch is an SR latch that is made to toggle its output (oscillate between 0 and 1) when passed the input combination of 11.[15] Unlike the JK flip-flop, the 11 input combination for the JK latch is not very useful because there is no clock that directs toggling.[16]

Gated latches and conditional transparency[edit]
Latches are designed to be transparent. That is, input signal changes cause immediate changes in output; when several transparent latches follow each other, using the same enable signal, signals can propagate through all of them at once. Alternatively, additional logic can be added to a simple transparent latch to make it non-transparent or opaque when another input (an "enable" input) is not asserted. By following a transparent-high latch with a transparent-low (or opaque-high) latch, a master–slave flip-flop is implemented.

Gated SR latch[edit]

A gated SR latch circuit diagram constructed from NOR gates.
A synchronous SR latch (sometimes clocked SR flip-flop) can be made by adding a second level of NAND gates to the inverted SR latch (or a second level of AND gates to the direct SR latch). The extra NAND gates further invert the inputs so the simple SR latch becomes a gated SR latch (and a simple SR latch would transform into a gated SR latch with inverted enable).

With E high (enable true), the signals can pass through the input gates to the encapsulated latch; all signal combinations except for (0,0) = hold then immediately reproduce on the (Q,Q) output, i.e. the latch is transparent.

With E low (enable false) the latch is closed (opaque) and remains in the state it was left the last time E was high.

The enable input is sometimes a clock signal, but more often a read or write strobe.

Gated SR latch operation
E/C	Action
0	No action (keep state)
1	The same as non-clocked SR latch

Symbol for a gated SR latch
Gated D latch[edit]
Schematic diagram
A gated D latch based on an SR NAND latch

A gated D latch based on an SR NOR latch

An animated gated D latch.
(A) D = 1, E = 1: set
(B) D = 1, E = 0: hold
(C) D = 0, E = 0: hold
(D) D = 0, E = 1: reset

A gated D latch in pass transistor logic, similar to the ones in the CD4042 or the CD74HC75 integrated circuits.
This latch exploits the fact that, in the two active input combinations (01 and 10) of a gated SR latch, R is the complement of S. The input NAND stage converts the two D input states (0 and 1) to these two input combinations for the next SR latch by inverting the data input signal. The low state of the enable signal produces the inactive "11" combination. Thus a gated D-latch may be considered as a one-input synchronous SR latch. This configuration prevents application of the restricted input combination. It is also known as transparent latch, data latch, or simply gated latch. It has a data input and an enable signal (sometimes named clock, or control). The word transparent comes from the fact that, when the enable input is on, the signal propagates directly through the circuit, from the input D to the output Q.

Transparent latches are typically used as I/O ports or in asynchronous systems, or in synchronous two-phase systems (synchronous systems that use a two-phase clock), where two latches operating on different clock phases prevent data transparency as in a master–slave flip-flop.

Latches are available as integrated circuits, usually with multiple latches per chip. For example, 74HC75 is a quadruple transparent latch in the 7400 series.

Gated D latch truth table
E/C	D		Q	Q	Comment
0	X	Qprev	Qprev	No change
1	0	0	1	Reset
1	1	1	0	Set

Symbol for a gated D latch
The truth table shows that when the enable/clock input is 0, the D input has no effect on the output. When E/C is high, the output equals D.

Earle latch[edit]

Earle latch uses complementary enable inputs: enable active low (E_L) and enable active high (E_H)

An animated Earle latch.
(A) D = 1, E_H = 1: set
(B) D = 0, E_H = 1: reset
(C) D = 1, E_H = 0: hold
(D) D = 1, E_H = 1: set
The classic gated latch designs have some undesirable characteristics.[17] They require double-rail logic or an inverter. The input-to-output propagation may take up to three gate delays. The input-to-output propagation is not constant – some outputs take two gate delays while others take three.

Designers looked for alternatives.[18] A successful alternative is the Earle latch. It requires only a single data input, and its output takes a constant two gate delays. In addition, the two gate levels of the Earle latch can, in some cases, be merged with the last two gate levels of the circuits driving the latch because many common computational circuits have an OR layer followed by an AND layer as their last two levels. Merging the latch function can implement the latch with no additional gate delays.[17] The merge is commonly exploited in the design of pipelined computers, and, in fact, was originally developed by J. G. Earle to be used in the IBM System/360 Model 91 for that purpose.[19]

The Earle latch is hazard free.[20] If the middle NAND gate is omitted, then one gets the polarity hold latch, which is commonly used because it demands less logic.[20][21] However, it is susceptible to logic hazard. Intentionally skewing the clock signal can avoid the hazard.[21]

D flip-flop[edit]

D flip-flop symbol
The D ?ip-?op is widely used. It is also known as a "data" or "delay" flip-flop.

The D flip-flop captures the value of the D-input at a definite portion of the clock cycle (such as the rising edge of the clock). That captured value becomes the Q output. At other times, the output Q does not change.[22][23] The D flip-flop can be viewed as a memory cell, a zero-order hold, or a delay line.[24]

Truth table:

Clock	D	Qnext
Rising edge	0	0
Rising edge	1	1
Non-Rising	X	Q
('X' denotes a Don't care condition, meaning the signal is irrelevant)

Most D-type flip-flops in ICs have the capability to be forced to the set or reset state (which ignores the D and clock inputs), much like an SR flip-flop. Usually, the illegal S = R = 1 condition is resolved in D-type flip-flops. By setting S = R = 0, the flip-flop can be used as described above. Here is the truth table for the others S and R possible configurations:

Inputs	Outputs
S	R	D	>	Q	Q'
0	1	X	X	0	1
1	0	X	X	1	0
1	1	X	X	1	1

4-bit serial-in, parallel-out (SIPO) shift register
These flip-flops are very useful, as they form the basis for shift registers, which are an essential part of many electronic devices. The advantage of the D flip-flop over the D-type "transparent latch" is that the signal on the D input pin is captured the moment the flip-flop is clocked, and subsequent changes on the D input will be ignored until the next clock event. An exception is that some flip-flops have a "reset" signal input, which will reset Q (to zero), and may be either asynchronous or synchronous with the clock.

The above circuit shifts the contents of the register to the right, one bit position on each active transition of the clock. The input X is shifted into the leftmost bit position.

Classical positive-edge-triggered D flip-flop[edit]

A positive-edge-triggered D flip-flop
This circuit[25] consists of two stages implemented by SR NAND latches. The input stage (the two latches on the left) processes the clock and data signals to ensure correct input signals for the output stage (the single latch on the right). If the clock is low, both the output signals of the input stage are high regardless of the data input; the output latch is unaffected and it stores the previous state. When the clock signal changes from low to high, only one of the output voltages (depending on the data signal) goes low and sets/resets the output latch: if D = 0, the lower output becomes low; if D = 1, the upper output becomes low. If the clock signal continues staying high, the outputs keep their states regardless of the data input and force the output latch to stay in the corresponding state as the input logical zero (of the output stage) remains active while the clock is high. Hence the role of the output latch is to store the data only while the clock is low.

The circuit is closely related to the gated D latch as both the circuits convert the two D input states (0 and 1) to two input combinations (01 and 10) for the output SR latch by inverting the data input signal (both the circuits split the single D signal in two complementary S and R signals). The difference is that in the gated D latch simple NAND logical gates are used while in the positive-edge-triggered D flip-flop SR NAND latches are used for this purpose. The role of these latches is to "lock" the active output producing low voltage (a logical zero); thus the positive-edge-triggered D flip-flop can also be thought of as a gated D latch with latched input gates.

Master–slave edge-triggered D flip-flop[edit]

A master–slave D flip-flop. It responds on the falling edge of the enable input (usually a clock)

An implementation of a master–slave D flip-flop that is triggered on the rising edge of the clock
A master–slave D flip-flop is created by connecting two gated D latches in series, and inverting the enable input to one of them. It is called master–slave because the second latch in the series only changes in response to a change in the first (master) latch.

For a positive-edge triggered master–slave D flip-flop, when the clock signal is low (logical 0) the "enable" seen by the first or "master" D latch (the inverted clock signal) is high (logical 1). This allows the "master" latch to store the input value when the clock signal transitions from low to high. As the clock signal goes high (0 to 1) the inverted "enable" of the first latch goes low (1 to 0) and the value seen at the input to the master latch is "locked". Nearly simultaneously, the twice inverted "enable" of the second or "slave" D latch transitions from low to high (0 to 1) with the clock signal. This allows the signal captured at the rising edge of the clock by the now "locked" master latch to pass through the "slave" latch. When the clock signal returns to low (1 to 0), the output of the "slave" latch is "locked", and the value seen at the last rising edge of the clock is held while the "master" latch begins to accept new values in preparation for the next rising clock edge.

By removing the leftmost inverter in the circuit at side, a D-type flip-flop that strobes on the falling edge of a clock signal can be obtained. This has a truth table like this:

D	Q	>	Qnext
0	X	Falling	0
1	X	Falling	1

A CMOS IC implementation of a "true single-phase edge-triggered flip-flop with reset"
Edge-triggered dynamic D storage element[edit]
An efficient functional alternative to a D flip-flop can be made with dynamic circuits (where information is stored in a capacitance) as long as it is clocked often enough; while not a true flip-flop, it is still called a flip-flop for its functional role. While the master–slave D element is triggered on the edge of a clock, its components are each triggered by clock levels. The "edge-triggered D flip-flop", as it is called even though it is not a true flip-flop, does not have the master–slave properties.

Edge-triggered D flip-flops are often implemented in integrated high-speed operations using dynamic logic. This means that the digital output is stored on parasitic device capacitance while the device is not transitioning. This design of dynamic flip flops also enables simple resetting since the reset operation can be performed by simply discharging one or more internal nodes. A common dynamic flip-flop variety is the true single-phase clock (TSPC) type which performs the flip-flop operation with little power and at high speeds. However, dynamic flip-flops will typically not work at static or low clock speeds: given enough time, leakage paths may discharge the parasitic capacitance enough to cause the flip-flop to enter invalid states.

T flip-flop[edit]

A circuit symbol for a T-type flip-flop
If the T input is high, the T flip-flop changes state ("toggles") whenever the clock input is strobed. If the T input is low, the flip-flop holds the previous value. This behavior is described by the characteristic equation:

Q_{\rm next} = T \oplus Q = T\overline{Q} + \overline{T}Q (expanding the XOR operator)
and can be described in a truth table:

T flip-flop operation[26]
Characteristic table	Excitation table
T	Q	Q_{\rm next}	Comment	Q	Q_{\rm next}	T	Comment
0	0	0	hold state (no clk)	0	0	0	No change
0	1	1	hold state (no clk)	1	1	0	No change
1	0	1	toggle	0	1	1	Complement
1	1	0	toggle	1	0	1	Complement
When T is held high, the toggle flip-flop divides the clock frequency by two; that is, if clock frequency is 4 MHz, the output frequency obtained from the flip-flop will be 2 MHz. This "divide by" feature has application in various types of digital counters. A T flip-flop can also be built using a JK flip-flop (J & K pins are connected together and act as T) or a D flip-flop (T input XOR Qprevious drives the D input).

JK flip-flop[edit]

A circuit symbol for a positive-edge-triggered JK flip-flop

A JK flip-flop made of NAND gates

JK flip-flop timing diagram
The JK flip-flop augments the behavior of the SR flip-flop (J=Set, K=Reset) by interpreting the J = K = 1 condition as a "flip" or toggle command. Specifically, the combination J = 1, K = 0 is a command to set the flip-flop; the combination J = 0, K = 1 is a command to reset the flip-flop; and the combination J = K = 1 is a command to toggle the flip-flop, i.e., change its output to the logical complement of its current value. Setting J = K = 0 maintains the current state. To synthesize a D flip-flop, simply set K equal to the complement of J. Similarly, to synthesize a T flip-flop, set K equal to J. The JK flip-flop is therefore a universal flip-flop, because it can be configured to work as an SR flip-flop, a D flip-flop, or a T flip-flop.

The characteristic equation of the JK flip-flop is:

Q_{\rm next} = J\overline Q + \overline KQ

and the corresponding truth table is:

JK flip-flop operation[26]
Characteristic table	Excitation table
J	K	Comment	Qnext	Q	Qnext	Comment	J	K
0	0	hold state	Q	0	0	No Change	0	X
0	1	reset	0	0	1	Set	1	X
1	0	set	1	1	0	Reset	X	1
1	1	toggle	Q	1	1	No Change	X	0
Timing considerations[edit]
Timing parameters[edit]

Flip-flop setup, hold and clock-to-output timing parameters
The input must be held steady in a period around the rising edge of the clock known as the aperture. Imagine taking a picture of a frog on a lily-pad.[27] Suppose the frog then jumps into the water. If you take a picture of the frog as it jumps into the water, you will get a blurry picture of the frog jumping into the water—it's not clear which state the frog was in. But if you take a picture while the frog sits steadily on the pad (or is steadily in the water), you will get a clear picture. In the same way, the input to a flip-flop must be held steady during the aperture of the flip-flop.

More precisely, Setup time is the minimum amount of time the data signal should be held steady before the clock event so that the data is reliably sampled by the clock.

Hold time is the minimum amount of time the data signal should be held steady after the clock event so that the data is reliably sampled.

Aperture is the time from the setup through the hold time, relative to the clock signal. Input signals (such as data) should be held steady throughout this time period.[27]

Recovery time is like setup time for asynchronous ports (set, reset). It is the time available between the asynchronous signals going inactive and the active clock edge.

Removal time is like hold time for asynchronous ports (set, reset). It is the time between active clock edge and asynchronous signal going inactive.[28]

Short impulses applied to asynchronous inputs (set, reset) should not be applied completely within the recovery-removal period, or else it becomes entirely indeterminable whether the flip-flop will transition to the appropriate state. In another case, where an asynchronous signal simply makes one transition that happens to fall between the recovery/removal time, eventually the flip-flop will transition to the appropriate state, but a very short glitch may or may not appear on the output, dependent on the synchronous input signal. This second situation may or may not have significance to a circuit design.

Set and Reset (and other) signals may be either synchronous or asynchronous and therefore may be characterized with either Setup/Hold or Recovery/Removal times, and synchronicity is very dependent on the TTL design of the flip-flop.

Differentiation between Setup/Hold and Recovery/Removal times is often necessary when verifying the timing of larger circuits because asynchronous signals may be found to be less critical than synchronous signals. The differentiation offers circuit designers the ability to define the verification conditions for these types of signals independently.

Metastability[edit]
Main article: metastability in electronics
Flip-flops are subject to a problem called metastability, which can happen when two inputs, such as data and clock or clock and reset, are changing at about the same time. When the order is not clear, within appropriate timing constraints, the result is that the output may behave unpredictably, taking many times longer than normal to settle to one state or the other, or even oscillating several times before settling. Theoretically, the time to settle down is not bounded. In a computer system, this metastability can cause corruption of data or a program crash if the state is not stable before another circuit uses its value; in particular, if two different logical paths use the output of a flip-flop, one path can interpret it as a 0 and the other as a 1 when it has not resolved to stable state, putting the machine into an inconsistent state.[29]

The metastability in flip-flops can be avoided by ensuring that the data and control inputs are held valid and constant for specified periods before and after the clock pulse, called the setup time (tsu) and the hold time (th) respectively. These times are specified in the data sheet for the device, and are typically between a few nanoseconds and a few hundred picoseconds for modern devices. Depending upon the flip-flop's internal organization, it is possible to build a device with a zero (or even negative) setup or hold time requirement but not both simultaneously.

Unfortunately, it is not always possible to meet the setup and hold criteria, because the flip-flop may be connected to a real-time signal that could change at any time, outside the control of the designer. In this case, the best the designer can do is to reduce the probability of error to a certain level, depending on the required reliability of the circuit. One technique for suppressing metastability is to connect two or more flip-flops in a chain, so that the output of each one feeds the data input of the next, and all devices share a common clock. With this method, the probability of a metastable event can be reduced to a negligible value, but never to zero. The probability of metastability gets closer and closer to zero as the number of flip-flops connected in series is increased. The number of flip-flops being cascaded is referred to as the "ranking"; "dual-ranked" flip flops (two flip-flops in series) is a common situation.

So-called metastable-hardened flip-flops are available, which work by reducing the setup and hold times as much as possible, but even these cannot eliminate the problem entirely. This is because metastability is more than simply a matter of circuit design. When the transitions in the clock and the data are close together in time, the flip-flop is forced to decide which event happened first. However fast we make the device, there is always the possibility that the input events will be so close together that it cannot detect which one happened first. It is therefore logically impossible to build a perfectly metastable-proof flip-flop. Flip-flops are sometimes characterized for a maximum settling time (the maximum time they will remain metastable under specified conditions). In this case, dual-ranked flip-flops that are clocked slower than the maximum allowed metastability time will provide proper conditioning for asynchronous (e.g., external) signals.

Propagation delay[edit]
Another important timing value for a flip-flop is the clock-to-output delay (common symbol in data sheets: tCO) or propagation delay (tP), which is the time a flip-flop takes to change its output after the clock edge. The time for a high-to-low transition (tPHL) is sometimes different from the time for a low-to-high transition (tPLH).

When cascading flip-flops which share the same clock (as in a shift register), it is important to ensure that the tCO of a preceding flip-flop is longer than the hold time (th) of the following flip-flop, so data present at the input of the succeeding flip-flop is properly "shifted in" following the active edge of the clock. This relationship between tCO and th is normally guaranteed if the flip-flops are physically identical. Furthermore, for correct operation, it is easy to verify that the clock period has to be greater than the sum tsu + th.

Generalizations[edit]
Flip-flops can be generalized in at least two ways: by making them 1-of-N instead of 1-of-2, and by adapting them to logic with more than two states. In the special cases of 1-of-3 encoding, or multi-valued ternary logic, these elements may be referred to as flip-flap-flops.[30]

In a conventional flip-flop, exactly one of the two complementary outputs is high. This can be generalized to a memory element with N outputs, exactly one of which is high (alternatively, where exactly one of N is low). The output is therefore always a one-hot (respectively one-cold) representation. The construction is similar to a conventional cross-coupled flip-flop; each output, when high, inhibits all the other outputs.[31] Alternatively, more or less conventional flip-flops can be used, one per output, with additional circuitry to make sure only one at a time can be true.[32]

Another generalization of the conventional flip-flop is a memory element for multi-valued logic. In this case the memory element retains exactly one of the logic states until the control inputs induce a change.[33] In addition, a multiple-valued clock can also be used, leading to new possible clock transitions.[34]

Relay
From Wikipedia, the free encyclopedia
  (Redirected from Latching relay)
This article is about the electrical component. For other uses, see Relay (disambiguation).

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (November 2009) (Learn how and when to remove this template message)

Automotive-style miniature relay, dust cover is taken off
A relay is an electrically operated switch. Many relays use an electromagnet to mechanically operate a switch, but other operating principles are also used, such as solid-state relays. Relays are used where it is necessary to control a circuit by a low-power signal (with complete electrical isolation between control and controlled circuits), or where several circuits must be controlled by one signal. The first relays were used in long distance telegraph circuits as amplifiers: they repeated the signal coming in from one circuit and re-transmitted it on another circuit. Relays were used extensively in telephone exchanges and early computers to perform logical operations.

A type of relay that can handle the high power required to directly control an electric motor or other loads is called a contactor. Solid-state relays control power circuits with no moving parts, instead using a semiconductor device to perform switching. Relays with calibrated operating characteristics and sometimes multiple operating coils are used to protect electrical circuits from overload or faults; in modern electric power systems these functions are performed by digital instruments still called "protective relays".

Magnetic latching relays require one pulse of coil power to move their contacts in one direction, and another, redirected pulse to move them back. Repeated pulses from the same input have no effect. Magnetic latching relays are useful in applications where interrupted power should not be able to transition the contacts.

Magnetic latching relays can have either single or dual coils. On a single coil device, the relay will operate in one direction when power is applied with one polarity, and will reset when the polarity is reversed. On a dual coil device, when polarized voltage is applied to the reset coil the contacts will transition. AC controlled magnetic latch relays have single coils that employ steering diodes to differentiate between operate and reset commands.

Contents  [hide] 
1	History
2	Basic design and operation
3	Types
3.1	Latching relay
3.2	Reed relay
3.3	Mercury-wetted relay
3.4	Mercury relay
3.5	Polarized relay
3.6	Machine tool relay
3.7	Coaxial relay
3.8	Time delay relay
3.9	Contactor
3.10	Solid-state relay
3.11	Solid state contactor relay
3.12	Buchholz relay
3.13	Force-guided contacts relay
3.14	Overload protection relay
3.15	Vacuum relays
3.16	Safety relays
3.17	Multi-voltage relays
4	Pole and throw
5	Applications
6	Relay application considerations
6.1	Derating factors
6.2	Undesired arcing
7	Protective relays
8	Railway signalling
9	See also
10	References
11	External links
History[edit]
The American scientist Joseph Henry is often claimed to have invented a relay in 1835 in order to improve his version of the electrical telegraph, developed earlier in 1831.[1][2][3][4] However, there is little in the way of official documentation to suggest he had made the discovery prior to 1837.[5]

It is claimed that the English inventor Edward Davy "certainly invented the electric relay"[6] in his electric telegraph c.1835.

A simple device, which we now call a relay, was included in the original 1840 telegraph patent[7] of Samuel Morse. The mechanism described acted as a digital amplifier, repeating the telegraph signal, and thus allowing signals to be propagated as far as desired. This overcame the problem of limited range of earlier telegraphy schemes.[citation needed]

The word relay appears in the context of electromagnetic operations from 1860.[8]

Basic design and operation[edit]

Simple electromechanical relay.

Small "cradle" relay often used in electronics. The "cradle" term refers to the shape of the relay's armature.
A simple electromagnetic relay consists of a coil of wire wrapped around a soft iron core, an iron yoke which provides a low reluctance path for magnetic flux, a movable iron armature, and one or more sets of contacts (there are two contacts in the relay pictured). The armature is hinged to the yoke and mechanically linked to one or more sets of moving contacts. It is held in place by a spring so that when the relay is de-energized there is an air gap in the magnetic circuit. In this condition, one of the two sets of contacts in the relay pictured is closed, and the other set is open. Other relays may have more or fewer sets of contacts depending on their function. The relay in the picture also has a wire connecting the armature to the yoke. This ensures continuity of the circuit between the moving contacts on the armature, and the circuit track on the printed circuit board (PCB) via the yoke, which is soldered to the PCB.

When an electric current is passed through the coil it generates a magnetic field that activates the armature, and the consequent movement of the movable contact(s) either makes or breaks (depending upon construction) a connection with a fixed contact. If the set of contacts was closed when the relay was de-energized, then the movement opens the contacts and breaks the connection, and vice versa if the contacts were open. When the current to the coil is switched off, the armature is returned by a force, approximately half as strong as the magnetic force, to its relaxed position. Usually this force is provided by a spring, but gravity is also used commonly in industrial motor starters. Most relays are manufactured to operate quickly. In a low-voltage application this reduces noise; in a high voltage or current application it reduces arcing.

When the coil is energized with direct current, a diode is often placed across the coil to dissipate the energy from the collapsing magnetic field at deactivation, which would otherwise generate a voltage spike dangerous to semiconductor circuit components. Such diodes were not widely used before the application of transistors as relay drivers, but soon became ubiquitous as early germanium transistors were easily destroyed by this surge. Some automotive relays include a diode inside the relay case.

If the relay is driving a large, or especially a reactive load, there may be a similar problem of surge currents around the relay output contacts. In this case a snubber circuit (a capacitor and resistor in series) across the contacts may absorb the surge. Suitably rated capacitors and the associated resistor are sold as a single packaged component for this commonplace use.

If the coil is designed to be energized with alternating current (AC), some method is used to split the flux into two out-of-phase components which add together, increasing the minimum pull on the armature during the AC cycle. Typically this is done with a small copper "shading ring" crimped around a portion of the core that creates the delayed, out-of-phase component,[9] which holds the contacts during the zero crossings of the control voltage.

Types[edit]
Latching relay[edit]

Latching relay with permanent magnet
A latching relay (also called "impulse", "keep", or "stay" relays) maintains either contact position indefinitely without power applied to the coil. The advantage is that one coil consumes power only for an instant while the relay is being switched, and the relay contacts retain this setting across a power outage. A latching relay allows remote control of building lighting without the hum that may be produced from a continuously (AC) energized coil.

In one mechanism, two opposing coils with an over-center spring or permanent magnet hold the contacts in position after the coil is de-energized. A pulse to one coil turns the relay on and a pulse to the opposite coil turns the relay off. This type is widely used where control is from simple switches or single-ended outputs of a control system, and such relays are found in avionics and numerous industrial applications.

Another latching type has a remanent core that retains the contacts in the operated position by the remanent magnetism in the core. This type requires a current pulse of opposite polarity to release the contacts. A variation uses a permanent magnet that produces part of the force required to close the contact; the coil supplies sufficient force to move the contact open or closed by aiding or opposing the field of the permanent magnet.[10] A polarity controlled relay needs changeover switches or an H bridge drive circuit to control it. The relay may be less expensive than other types, but this is partly offset by the increased costs in the external circuit.

In another type, a ratchet relay has a ratchet mechanism that holds the contacts closed after the coil is momentarily energized. A second impulse, in the same or a separate coil, releases the contacts.[10] This type may be found in certain cars, for headlamp dipping and other functions where alternating operation on each switch actuation is needed.

A stepping relay is a specialized kind of multi-way latching relay designed for early automatic telephone exchanges.

An earth leakage circuit breaker includes a specialized latching relay.

Very early computers often stored bits in a magnetically latching relay, such as ferreed or the later memreed in the 1ESS switch.

Some early computers used ordinary relays as a kind of latch—they store bits in ordinary wire spring relays or reed relays by feeding an output wire back as an input, resulting in a feedback loop or sequential circuit. Such an electrically latching relay requires continuous power to maintain state, unlike magnetically latching relays or mechanically racheting relays.

In computer memories, latching relays and other relays were replaced by delay line memory, which in turn was replaced by a series of ever-faster and ever-smaller memory technologies.

Reed relay[edit]
Main article: reed relay

Top, middle: reed switches, bottom: reed relay
A reed relay is a reed switch enclosed in a solenoid. The switch has a set of contacts inside an evacuated or inert gas-filled glass tube which protects the contacts against atmospheric corrosion; the contacts are made of magnetic material that makes them move under the influence of the field of the enclosing solenoid or an external magnet.

Reed relays can switch faster than larger relays and require very little power from the control circuit. However, they have relatively low switching current and voltage ratings. Though rare, the reeds can become magnetized over time, which makes them stick 'on' even when no current is present; changing the orientation of the reeds with respect to the solenoid's magnetic field can resolve this problem.

Sealed contacts with mercury-wetted contacts have longer operating lives and less contact chatter than any other kind of relay.[11]

Mercury-wetted relay[edit]

A mercury-wetted reed relay that has AC/DC switching specifications of 100 W, 500 V, 2 A maximum
See also: mercury switch
A mercury-wetted reed relay is a form of reed relay in which the contacts are wetted with mercury. Such relays are used to switch low-voltage signals (one volt or less) where the mercury reduces the contact resistance and associated voltage drop, for low-current signals where surface contamination may make for a poor contact, or for high-speed applications where the mercury eliminates contact bounce. Mercury wetted relays are position-sensitive and must be mounted vertically to work properly. Because of the toxicity and expense of liquid mercury, these relays are now rarely used.

The mercury-wetted relay has one particular advantage, in that the contact closure appears to be virtually instantaneous, as the mercury globules on each contact coalesce. The current rise time through the contacts is generally considered to be a few picoseconds, however in a practical circuit it will be limited by the inductance of the contacts and wiring. It was quite common, before the restrictions on the use of mercury, to use a mercury-wetted relay in the laboratory as a convenient means of generating fast rise time pulses, however although the rise time may be picoseconds, the exact timing of the event is, like all other types of relay, subject to considerable jitter, possibly milliseconds, due to mechanical imperfections.

The same coalescence process causes another effect, which is a nuisance in some applications. The contact resistance is not stable immediately after contact closure, and drifts, mostly downwards, for several seconds after closure, the change perhaps being 0.5 ohm.

Mercury relay[edit]
A mercury relay is a relay that uses mercury as the switching element. They are used where contact erosion would be a problem for conventional relay contacts. Owing to environmental considerations about significant amount of mercury used and modern alternatives, they are now comparatively uncommon.

Polarized relay[edit]
A polarized relay places the armature between the poles of a permanent magnet to increase sensitivity. Polarized relays were used in middle 20th Century telephone exchanges to detect faint pulses and correct telegraphic distortion. The poles were on screws, so a technician could first adjust them for maximum sensitivity and then apply a bias spring to set the critical current that would operate the relay.

Machine tool relay[edit]
A machine tool relay is a type standardized for industrial control of machine tools, transfer machines, and other sequential control. They are characterized by a large number of contacts (sometimes extendable in the field) which are easily converted from normally open to normally closed status, easily replaceable coils, and a form factor that allows compactly installing many relays in a control panel. Although such relays once were the backbone of automation in such industries as automobile assembly, the programmable logic controller (PLC) mostly displaced the machine tool relay from sequential control applications.

A relay allows circuits to be switched by electrical equipment: for example, a timer circuit with a relay could switch power at a preset time. For many years relays were the standard method of controlling industrial electronic systems. A number of relays could be used together to carry out complex functions (relay logic). The principle of relay logic is based on relays which energize and de-energize associated contacts. Relay logic is the predecessor of ladder logic, which is commonly used in programmable logic controllers.

Coaxial relay[edit]
Where radio transmitters and receivers share one antenna, often a coaxial relay is used as a TR (transmit-receive) relay, which switches the antenna from the receiver to the transmitter. This protects the receiver from the high power of the transmitter. Such relays are often used in transceivers which combine transmitter and receiver in one unit. The relay contacts are designed not to reflect any radio frequency power back toward the source, and to provide very high isolation between receiver and transmitter terminals. The characteristic impedance of the relay is matched to the transmission line impedance of the system, for example, 50 ohms.[12]

Time delay relay[edit]
Timing relays are arranged for an intentional delay in operating their contacts. A very short (a fraction of a second) delay would use a copper disk between the armature and moving blade assembly. Current flowing in the disk maintains magnetic field for a short time, lengthening release time. For a slightly longer (up to a minute) delay, a dashpot is used. A dashpot is a piston filled with fluid that is allowed to escape slowly; both air-filled and oil-filled dashpots are used. The time period can be varied by increasing or decreasing the flow rate. For longer time periods, a mechanical clockwork timer is installed. Relays may be arranged for a fixed timing period, or may be field adjustable, or remotely set from a control panel. Modern microprocessor-based timing relays provide precision timing over a great range.

Some relays are constructed with a kind of "shock absorber" mechanism attached to the armature which prevents immediate, full motion when the coil is either energized or de-energized. This addition gives the relay the property of time-delay actuation. Time-delay relays can be constructed to delay armature motion on coil energization, de-energization, or both.

Time-delay relay contacts must be specified not only as either normally open or normally closed, but whether the delay operates in the direction of closing or in the direction of opening. The following is a description of the four basic types of time-delay relay contacts.

First we have the normally open, timed-closed (NOTC) contact. This type of contact is normally open when the coil is unpowered (de-energized). The contact is closed by the application of power to the relay coil, but only after the coil has been continuously powered for the specified amount of time. In other words, the direction of the contact's motion (either to close or to open) is identical to a regular NO contact, but there is a delay in closing direction. Because the delay occurs in the direction of coil energization, this type of contact is alternatively known as a normally open, on-delay:

Contactor[edit]
Main article: Contactor
A contactor is a heavy-duty relay used for switching electric motors and lighting loads, but contactors are not generally called relays. Continuous current ratings for common contactors range from 10 amps to several hundred amps. High-current contacts are made with alloys containing silver. The unavoidable arcing causes the contacts to oxidize; however, silver oxide is still a good conductor.[13] Contactors with overload protection devices are often used to start motors. Contactors can make loud sounds when they operate, so they may be unfit for use where noise is a chief concern.

A contactor is an electrically controlled switch used for switching a power circuit, similar to a relay except with higher current ratings.[14] A contactor is controlled by a circuit which has a much lower power level than the switched circuit.

Contactors come in many forms with varying capacities and features. Unlike a circuit breaker, a contactor is not intended to interrupt a short circuit current. Contactors range from those having a breaking current of several amperes to thousands of amperes and 24 V DC to many kilovolts. The physical size of contactors ranges from a device small enough to pick up with one hand, to large devices approximately a meter (yard) on a side.

Solid-state relay[edit]
Main article: Solid-state relay

Solid state relay with no moving parts

25 A or 40 A solid state contactors
A solid state relay or SSR is a solid state electronic component that provides a function similar to an electromechanical relay but does not have any moving components, increasing long-term reliability. A solid-state relay uses a thyristor, TRIAC or other solid-state switching device, activated by the control signal, to switch the controlled load, instead of a solenoid. An optocoupler (a light-emitting diode (LED) coupled with a photo transistor) can be used to isolate control and controlled circuits.

As every solid-state device has a small voltage drop across it, this voltage drop limits the amount of current a given SSR can handle. The minimum voltage drop for such a relay is a function of the material used to make the device. Solid-state relays rated to handle as much as 1,200 amperes have become commercially available. Compared to electromagnetic relays, they may be falsely triggered by transients and in general may be susceptible to damage by extreme cosmic ray and EMP episodes.[citation needed]

Solid state contactor relay[edit]
A solid state contactor is a heavy-duty solid state relay, including the necessary heat sink, used where frequent on/off cycles are required, such as with electric heaters, small electric motors, and lighting loads. There are no moving parts to wear out and there is no contact bounce due to vibration. They are activated by AC control signals or DC control signals from Programmable logic controller (PLCs), PCs, Transistor-transistor logic (TTL) sources, or other microprocessor and microcontroller controls.

Buchholz relay[edit]
Main article: Buchholz relay
A Buchholz relay is a safety device sensing the accumulation of gas in large oil-filled transformers, which will alarm on slow accumulation of gas or shut down the transformer if gas is produced rapidly in the transformer oil. The contacts are not operated by an electric current but by the pressure of accumulated gas or oil flow.

Force-guided contacts relay[edit]
A 'force-guided contacts relay' has relay contacts that are mechanically linked together, so that when the relay coil is energized or de-energized, all of the linked contacts move together. If one set of contacts in the relay becomes immobilized, no other contact of the same relay will be able to move. The function of force-guided contacts is to enable the safety circuit to check the status of the relay. Force-guided contacts are also known as "positive-guided contacts", "captive contacts", "locked contacts", "mechanically linked contacts", or "safety relays".

These safety relays have to follow design rules and manufacturing rules that are defined in one main machinery standard EN 50205 : Relays with forcibly guided (mechanically linked) contacts. These rules for the safety design are the one that are defined in type B standards such as EN 13849-2 as Basic safety principles and Well-tried safety principles for machinery that applies to all machines.

Force-guided contacts by themselves can not guarantee that all contacts are in the same state, however they do guarantee, subject to no gross mechanical fault, that no contacts are in opposite states. Otherwise, a relay with several normally open (NO) contacts may stick when energised, with some contacts closed and others still slightly open, due to mechanical tolerances. Similarly, a relay with several normally closed (NC) contacts may stick to the unenergised position, so that when energised, the circuit through one set of contacts is broken, with a marginal gap, while the other remains closed. By introducing both NO and NC contacts, or more commonly, changeover contacts, on the same relay, it then becomes possible to guarantee that if any NC contact is closed, all NO contacts are open, and conversely, if any NO contact is closed, all NC contacts are open. It is not possible to reliably ensure that any particular contact is closed, except by potentially intrusive and safety-degrading sensing of its circuit conditions, however in safety systems it is usually the NO state that is most important, and as explained above, this is reliably verifiable by detecting the closure of a contact of opposite sense.

Force-guided contact relays are made with different main contact sets, either NO, NC or changeover, and one or more auxiliary contact sets, often of reduced current or voltage rating, used for the monitoring system. Contacts may be all NO, all NC, changeover, or a mixture of these, for the monitoring contacts, so that the safety system designer can select the correct configuration for the particular application. Safety relays are used as part of an engineered safety system.

Overload protection relay[edit]
Electric motors need overcurrent protection to prevent damage from over-loading the motor, or to protect against short circuits in connecting cables or internal faults in the motor windings.[15] The overload sensing devices are a form of heat operated relay where a coil heats a bimetallic strip, or where a solder pot melts, releasing a spring to operate auxiliary contacts. These auxiliary contacts are in series with the coil. If the overload senses excess current in the load, the coil is de-energized.

This thermal protection operates relatively slowly allowing the motor to draw higher starting currents before the protection relay will trip. Where the overload relay is exposed to the same environment as the motor, a useful though crude compensation for motor ambient temperature is provided.

The other common overload protection system uses an electromagnet coil in series with the motor circuit that directly operates contacts. This is similar to a control relay but requires a rather high fault current to operate the contacts. To prevent short over current spikes from causing nuisance triggering the armature movement is damped with a dashpot. The thermal and magnetic overload detections are typically used together in a motor protection relay.

Electronic overload protection relays measure motor current and can estimate motor winding temperature using a "thermal model" of the motor armature system that can be set to provide more accurate motor protection. Some motor protection relays include temperature detector inputs for direct measurement from a thermocouple or resistance thermometer sensor embedded in the winding.

Vacuum relays[edit]
A sensitive relay having its contacts mounted in a highly evacuated glass housing, to permit handling radio-frequency voltages as high as 20,000 volts without flashover between contacts even though contact spacing is but a few hundredths of an inch when open.

Safety relays[edit]
Main article: Safety relay
Safety relays are devices which generally implement safety functions. In the event of a hazard, the task of such a safety function is to use appropriate measures to reduce the existing risk to an acceptable level.[16]

Multi-voltage relays[edit]
Multi-voltage relays are devices designed to work for wide voltage ranges such as 24 to 240 VAC/VDC and wide frequency ranges such as 0 to 300 Hz. They are indicated for use in installations that do not have stable supply voltages.

Pole and throw[edit]

Circuit symbols of relays. (C denotes the common terminal in SPDT and DPDT types.)
Since relays are switches, the terminology applied to switches is also applied to relays; a relay switches one or more poles, each of whose contacts can be thrown by energizing the coil.

Normally open (NO) contacts connect the circuit when the relay is activated; the circuit is disconnected when the relay is inactive. It is also called a "Form A" contact or "make" contact. NO contacts may also be distinguished as "early-make" or "NOEM", which means that the contacts close before the button or switch is fully engaged.

Normally closed (NC) contacts disconnect the circuit when the relay is activated; the circuit is connected when the relay is inactive. It is also called a "Form B" contact or "break" contact. NC contacts may also be distinguished as "late-break" or "NCLB", which means that the contacts stay closed until the button or switch is fully disengaged.

Change-over (CO), or double-throw (DT), contacts control two circuits: one normally open contact and one normally closed contact with a common terminal. It is also called a "Form C" contact or "transfer" contact ("break before make"). If this type of contact has a "make before break" action, then it is called a "Form D" contact.

The following designations are commonly encountered:

SPST – Single Pole Single Throw. These have two terminals which can be connected or disconnected. Including two for the coil, such a relay has four terminals in total. It is ambiguous whether the pole is normally open or normally closed. The terminology "SPNO" and "SPNC" is sometimes used to resolve the ambiguity.
SPDT – Single Pole Double Throw. A common terminal connects to either of two others. Including two for the coil, such a relay has five terminals in total.
DPST – Double Pole Single Throw. These have two pairs of terminals. Equivalent to two SPST switches or relays actuated by a single coil. Including two for the coil, such a relay has six terminals in total. The poles may be Form A or Form B (or one of each).
DPDT – Double Pole Double Throw. These have two rows of change-over terminals. Equivalent to two SPDT switches or relays actuated by a single coil. Such a relay has eight terminals, including the coil.
The "S" or "D" may be replaced with a number, indicating multiple switches connected to a single actuator. For example, 4PDT indicates a four pole double throw relay that has 12 switch terminals.

EN 50005 are among applicable standards for relay terminal numbering; a typical EN 50005-compliant SPDT relay's terminals would be numbered 11, 12, 14, A1 and A2 for the C, NC, NO, and coil connections, respectively.[17]

DIN 72552 defines contact numbers in relays for automotive use;

85 = relay coil -
86 = relay coil +
87 = common contact
87a = normally closed contact
87b = normally open contact
Applications[edit]

A DPDT AC coil relay with "ice cube" packaging
Relays are used wherever it is necessary to control a high power or high voltage circuit with a low power circuit, especially when galvanic isolation is desirable. The first application of relays was in long telegraph lines, where the weak signal received at an intermediate station could control a contact, regenerating the signal for further transmission. High-voltage or high-current devices can be controlled with small, low voltage wiring and pilots switches. Operators can be isolated from the high voltage circuit. Low power devices such as microprocessors can drive relays to control electrical loads beyond their direct drive capability. In an automobile, a starter relay allows the high current of the cranking motor to be controlled with small wiring and contacts in the ignition key.

Electromechanical switching systems including Strowger and Crossbar telephone exchanges made extensive use of relays in ancillary control circuits. The Relay Automatic Telephone Company also manufactured telephone exchanges based solely on relay switching techniques designed by Gotthilf Ansgarius Betulander. The first public relay based telephone exchange in the UK was installed in Fleetwood on 15 July 1922 and remained in service until 1959.[18][19]

The use of relays for the logical control of complex switching systems like telephone exchanges was studied by Claude Shannon, who formalized the application of Boolean algebra to relay circuit design in A Symbolic Analysis of Relay and Switching Circuits. Relays can perform the basic operations of Boolean combinatorial logic. For example, the boolean AND function is realised by connecting normally open relay contacts in series, the OR function by connecting normally open contacts in parallel. Inversion of a logical input can be done with a normally closed contact. Relays were used for control of automated systems for machine tools and production lines. The Ladder programming language is often used for designing relay logic networks.

Early electro-mechanical computers such as the ARRA, Harvard Mark II, Zuse Z2, and Zuse Z3 used relays for logic and working registers. However, electronic devices proved faster and easier to use.

Because relays are much more resistant than semiconductors to nuclear radiation, they are widely used in safety-critical logic, such as the control panels of radioactive waste-handling machinery. Electromechanical protective relays are used to detect overload and other faults on electrical lines by opening and closing circuit breakers.

Relay application considerations[edit]

Several 30-contact relays in "Connector" circuits in mid 20th century 1XB switch and 5XB switch telephone exchanges; cover removed on one
Selection of an appropriate relay for a particular application requires evaluation of many different factors:

Number and type of contacts – normally open, normally closed, (double-throw)
Contact sequence – "Make before Break" or "Break before Make". For example, the old style telephone exchanges required Make-before-break so that the connection didn't get dropped while dialing the number.
Contact current rating – small relays switch a few amperes, large contactors are rated for up to 3000 amperes, alternating or direct current
Contact voltage rating – typical control relays rated 300 VAC or 600 VAC, automotive types to 50 VDC, special high-voltage relays to about 15,000 V
Operating lifetime, useful life - the number of times the relay can be expected to operate reliably. There is both a mechanical life and a contact life. The contact life is affected by the type of load switched. Breaking load current causes undesired arcing between the contacts, eventually leading to contacts that weld shut or contacts that fail due erosion by the arc.[20]
Coil voltage – machine-tool relays usually 24 VDC, 120 or 250 VAC, relays for switchgear may have 125 V or 250 VDC coils,
Coil current - Minimum current required for reliable operation and minimum holding current, as well as, effects of power dissipation on coil temperature, at various duty cycles. "Sensitive" relays operate on a few milliamperes
Package/enclosure – open, touch-safe, double-voltage for isolation between circuits, explosion proof, outdoor, oil and splash resistant, washable for printed circuit board assembly
Operating environment - minimum and maximum operating temperature and other environmental considerations such as effects of humidity and salt
Assembly – Some relays feature a sticker that keeps the enclosure sealed to allow PCB post soldering cleaning, which is removed once assembly is complete.
Mounting – sockets, plug board, rail mount, panel mount, through-panel mount, enclosure for mounting on walls or equipment
Switching time – where high speed is required
"Dry" contacts – when switching very low level signals, special contact materials may be needed such as gold-plated contacts
Contact protection – suppress arcing in very inductive circuits
Coil protection – suppress the surge voltage produced when switching the coil current
Isolation between coil contacts
Aerospace or radiation-resistant testing, special quality assurance
Expected mechanical loads due to acceleration – some relays used in aerospace applications are designed to function in shock loads of 50 g or more
Size - smaller relays often resist mechanical vibration and shock better than larger relays, because of the lower inertia of the moving parts and the higher natural frequencies of smaller parts.[11] Larger relays often handle higher voltage and current than smaller relays.
Accessories such as timers, auxiliary contacts, pilot lamps, and test buttons
Regulatory approvals
Stray magnetic linkage between coils of adjacent relays on a printed circuit board.
There are many considerations involved in the correct selection of a control relay for a particular application. These considerations include factors such as speed of operation, sensitivity, and hysteresis. Although typical control relays operate in the 5 ms to 20 ms range, relays with switching speeds as fast as 100 us are available. Reed relays which are actuated by low currents and switch fast are suitable for controlling small currents.

As with any switch, the contact current (unrelated to the coil current) must not exceed a given value to avoid damage. In high-inductance circuits such as motors, other issues must be addressed. When an inductance is connected to a power source, an input surge current or electromotor starting current larger than the steady-state current exists. When the circuit is broken, the current cannot change instantaneously, which creates a potentially damaging arc across the separating contacts.

Consequently, for relays used to control inductive loads, we must specify the maximum current that may flow through the relay contacts when it actuates, the make rating; the continuous rating; and the break rating. The make rating may be several times larger than the continuous rating, which is itself larger than the break rating.

Derating factors[edit]
Type of load	% of rated value
Resistive	75
Inductive	35
Motor	20
Filament	10
Capacitive	75
Control relays should not be operated above rated temperature because of resulting increased degradation and fatigue. Common practice is to derate 20 degrees Celsius from the maximum rated temperature limit. Relays operating at rated load are affected by their environment. Oil vapor may greatly decrease the contact life, and dust or dirt may cause the contacts to burn before the end of normal operating life. Control relay life cycle varies from 50,000 to over one million cycles depending on the electrical loads on the contacts, duty cycle, application, and the extent to which the relay is derated. When a control relay is operating at its derated value, it is controlling a smaller value of current than its maximum make and break ratings. This is often done to extend the operating life of a control relay. The table lists the relay derating factors for typical industrial control applications.

Undesired arcing[edit]
Main article: Arc suppression
Switching while "wet" (under load) causes undesired arcing between the contacts, eventually leading to contacts that weld shut or contacts that fail due to a buildup of contact surface damage caused by the destructive arc energy.[20]

Inside the 1ESS switch matrix switch and certain other high-reliability designs, the reed switches are always switched "dry" to avoid that problem, leading to much longer contact life.[21]

Without adequate contact protection, the occurrence of electric current arcing causes significant degradation of the contacts, which suffer significant and visible damage. Every time a relay transitions either from a closed to an open state (break arc) or from an open to a closed state (make arc & bounce arc), under load, an electrical arc can occur between the two contact points (electrodes) of the relay. In many situations, the break arc is more energetic and thus more destructive, in particular with resistive-type loads. However, inductive loads can cause more destructive make arcs. For example, with standard electric motors, the start-up (inrush) current tends to be much greater than the running current. This translates into enormous make arcs.[citation needed]

During an arc event, the heat energy contained in the electrical arc is very high (tens of thousands of degrees Fahrenheit), causing the metal on the contact surfaces to melt, pool and migrate with the current. The extremely high temperature of the arc cracks the surrounding gas molecules creating ozone, carbon monoxide, and other compounds. The arc energy slowly destroys the contact metal, causing some material to escape into the air as fine particulate matter. This action causes the material in the contacts to degrade quickly, resulting in device failure. This contact degradation drastically limits the overall life of a relay to a range of about 10,000 to 100,000 operations, a level far below the mechanical life of the same device, which can be in excess of 20 million operations.[22]

Protective relays[edit]
Main article: protective relay
For protection of electrical apparatus and transmission lines, electromechanical relays with accurate operating characteristics were used to detect overload, short-circuits, and other faults. While many such relays remain in use, digital devices now provide equivalent protective functions.

Railway signalling[edit]

Part of a relay interlocking using UK Q-style miniature plug-in relays.

UK Q-style signalling relay and base.
Railway signalling relays are large considering the mostly small voltages (less than 120 V) and currents (perhaps 100 mA) that they switch. Contacts are widely spaced to prevent flashovers and short circuits over a lifetime that may exceed fifty years. BR930 series plug-in relays[23] are widely used on railways following British practice. These are 120 mm high, 180 mm deep and 56 mm wide and weigh about 1400 g, and can have up to 16 separate contacts, for example, 12 make and 4 break contacts. Many of these relays come in 12V, 24V and 50V versions.

The BR Q-type relay are available in a number of different configurations:

QN1 Neutral
QL1 Latched - see above
QNA1 AC-immune
QBA1 Biased AC-immune - see above
QNN1 Twin Neutral 2x4-4 or 2x6-2
QBCA1 Contactor for high current applications such as point motors. Also DC biased and AC immune.[24]
QTD4 - Slow to release timer [25]
QTD5 - Slow to pick up timer [26]
Since rail signal circuits must be highly reliable, special techniques are used to detect and prevent failures in the relay system. To protect against false feeds, double switching relay contacts are often used on both the positive and negative side of a circuit, so that two false feeds are needed to cause a false signal. Not all relay circuits can be proved so there is reliance on construction features such as carbon to silver contacts to resist lightning induced contact welding and to provide AC immunity.

Opto-isolators are also used in some instances with railway signalling, especially where only a single contact is to be switched.

Signalling relays, typical circuits, drawing symbols, abbreviations & nomenclature, etc. come in a number of schools, including the United States, France, Germany, and the United Kingdom.

See also[edit]
Contactor
Digital protective relay
Dry contact
Race condition
Stepping switch - a kind of multi-position relay
Wire spring relay
Analogue switch
Nanoelectromechanical relay

Analogue switch
From Wikipedia, the free encyclopedia
The analogue (or analog) switch, also called the bilateral switch, is an electronic component that behaves in a similar way to a relay, but has no moving parts. The switching element is normally a pair of MOSFET transistors, one an N-channel device, the other a P-channel device. The device can conduct analog or digital signals in either direction when on and isolates the switched terminals when off. Analogue switches are usually manufactured as integrated circuits in packages containing multiple switches (typically two, four or eight). These include the 4016 and 4066 from the 4000 series.

The control input to the device may be a signal that switches between the positive and negative supply voltages, with the more positive voltage switching the device on and the more negative switching the device off. Other circuits are designed to communicate through a serial port with a host controller in order to set switches on or off.

The signal being switched must remain within the bounds of the positive and negative supply rails which are connected to the P-MOS and N-MOS body terminals. The switch generally provides good isolation between the control signal and the input/output signals. They are not used for high voltage switching.

Important parameters of an analogue switch are:

on-resistance: the resistance when switched on. This commonly ranges from 5 ohms to a few hundred ohms.
off-resistance: the resistance when switched off. This is typically a number of megohms or gigaohms.
signal range: the minimum and maximum voltages allowed for the signal to be passed through. If these are exceeded, the switch may be destroyed by excessive currents. Older types of switches can even latch up, which means that they continue to conduct excessive currents even after the faulty signal is removed.
charge injection. This effect causes the switch to inject a small electric charge into the signal when it switches on, causing a small spike or glitch. The charge injection is specified in coulombs.
Analogue switches are available in both through-hole technology or by surface-mount technology packages.

Glitch
From Wikipedia, the free encyclopedia
For other uses, see Glitch (disambiguation).
A glitch is a short-lived fault in a system. It is often used to describe a transient fault that corrects itself, and is therefore difficult to troubleshoot. The term is particularly common in the computing and electronics industries, and in circuit bending, as well as among players of video games, although it is applied to all types of systems including human organizations and nature.

Contents  [hide] 
1	Etymology
2	Electronics glitch
3	Computer glitch
4	Video game glitches
5	TV glitch
6	Popular culture
7	See also
8	References
Etymology[edit]
	Look up glitch in Wiktionary, the free dictionary.
Some reference books, including Random House's American Slang, claim that the term comes from the German word glitschen ("to slip") and the Yiddish word gletshn ("to slide or skid"). Either way it is a relatively new term, as on July 23, 1965, Time Magazine felt it necessary to define it in an article: "Glitches—a spaceman's word for irritating disturbances." In relation to the reference by Time Magazine, the term has been believed to enter common usage during the American Space Race of the 1950s, where it was used to describe minor faults in the rocket hardware that were difficult to pinpoint.[1][2]

Electronics glitch[edit]
An electronics glitch is an undesired transition that occurs before the signal settles to its intended value. In other words, glitch is an electrical pulse of short duration that is usually the result of a fault or design error, particularly in a digital circuit. For example, many electronic components, such as flip-flops, are triggered by a pulse that must not be shorter than a specified minimum duration; otherwise, the component may malfunction. A pulse shorter than the specified minimum is called a glitch. A related concept is the runt pulse, a pulse whose amplitude is smaller than the minimum level specified for correct operation, and a spike, a short pulse similar to a glitch but often caused by ringing or crosstalk. A glitch can occur in the presence of race condition in a poorly designed digital logic circuit.

Computer glitch[edit]
A computer glitch is the failure of a system, usually containing a computing device, to complete its functions or to perform them properly.

In public declarations, glitch is used to suggest a minor fault which will soon be rectified and is therefore used as a euphemism for a bug, which is a factual statement that a programming fault is to blame for a system failure.

It frequently refers to an error which is not detected at the time it occurs but shows up later in data errors or incorrect human decisions. Situations which are frequently called computer glitches are incorrectly written software (software bugs), incorrect instructions given by the operator (operator errors, and a failure to account for this possibility might also be considered a software bug), undetected invalid input data (this might also be considered a software bug), undetected communications errors, computer viruses, Trojan attacks and computer exploiting (sometimes called "hacking").

Such glitches could produce problems such as keyboard malfunction, number key failures, screen abnormalities (turned left, right or upside-down), random program malfunctions, and abnormal program registering.

Examples of computer glitches causing disruption include an unexpected shutdown of a water filtration plant in New Canaan, 2010,[3] failures in the Computer Aided Dispatch system used by the police in Austin, resulting in unresponded 911 calls,[4] and an unexpected bit flip causing the Cassini spacecraft to enter "safe mode" in November 2010.[5] Glitches can also be costly: in 2015, a bank was unable to raise interest rates for weeks resulting in losses of more than a million dollars per day.[6]

Video game glitches[edit]
See also: Software bug and Glitching

The start-up screen of the Virtual Boy is affected by a visual glitch
Glitches/bugs are software errors that can cause drastic problems within the code, and typically go unnoticed or unsolved during the production of said software. These errors can be game caused or otherwise exploited until a developer/development team repairs them. Complex software is rarely bug-free or otherwise free from errors upon first release.

Texture/model glitches are a kind of bug or other error that causes any specific model or texture to either become distorted or otherwise to not look as intended by the developers. Bethesda's The Elder Scrolls V: Skyrim is notorious for texture glitches, as well as other errors that affect many of the company's popular titles.[7] Many games that uses ragdoll physics for their character models can have such glitches happen to them.[citation needed]

Physics glitches are errors in a game's physics engine that causes a specific entity, be it a physics object or an NPC (Non-Player Character), to be unintentionally moved to some degree. These kinds of errors can be exploited, unlike many. The chance of a physics error happening can either be entirely random or accidentally caused.

Sound glitches are in which there is an error with the game's sound. These can range from sounds playing when not intended to play or even not playing at all. Occasionally, a certain sound will loop or otherwise the player will be given the option to continuously play the sound when not intended. Often, games will play sounds incorrectly due to corrupt data altering the values predefined in the code. Examples include, but are not limited to, extremely high or low pitched sounds, volume being mute or too high to understand, and also rarely even playing in reverse order/playing reversed.[citation needed]

Glitches may include incorrectly displayed graphics, collision detection errors, game freezes/crashes, sound errors, and other issues. Graphical glitches are especially notorious in platforming games, where malformed textures can directly affect gameplay (for example, by displaying a ground texture where the code calls for an area that should damage the character, or by not displaying a wall texture where there should be one, resulting in an invisible wall.). Some glitches are potentially dangerous to the game's stored data.[8]

"Glitching" is the practice of players exploiting faults in a video game's programming to achieve tasks that give them an unfair advantage in the game, over NPC's or other players, such as running through walls or defying the game's physics. Glitches can be deliberately induced in certain home video game consoles by manipulating the game medium, such as tilting a ROM cartridge to disconnect one or more connections along the edge connector and interrupt part of the flow of data between the cartridge and the console.[9] This can result in graphic, music, or gameplay errors. Doing this, however, carries the risk of crashing the game or even causing permanent damage to the game medium.[10]

Heavy use of glitches are used in performing a speedrun of a video game.[11] One type of glitch often used for speedrunning is a Stack overflow, which is referred to "overflowing." Another type of speedrunning glitch, which is almost impossible to do by humans and is mostly made use of in tool assisted speedruns, is arbitrary code execution which will cause an object in a game to do something outside of its intended function.[12]

Part of the quality assurance process (as performed by game testers for video games) is locating and reproducing glitches, and then compiling reports on the glitches to be fed back to the programmers so that they can repair the bugs. Certain games have a cloud-type system for updates to the software that can be used to repair coding faults and other errors in the games.[8]

TV glitch[edit]
In broadcasting, a corrupted signal may glitch in the form of jagged lines on the screen, misplaced squares, static looking effects, freezing problems, or inverted colors. The glitches may affect the video and/or audio (usually audio dropout) or the transmission. These glitches may be caused by a variety of issues, interference from portable electronics or microwaves, damaged cables at the broadcasting center, or weather.[13]

Popular culture[edit]
The 1976 nonfiction book CB Bible includes glitch in its glossary of citizens band radio slang, meaning "an indefinable technical defect in CB equipment", indicating the term was already then in use on citizens band.[14] The 2008 short film The Glitch, opening film and best science fiction finalist at Dragon Con Independent Film Festival 2008, deals with the disorientation of late-night TV viewer Harry Owen (Scott Charles Blamphin), who experiences 'heavy brain-splitting digital breakdowns.'[15] In the 2012 animated film Wreck-It Ralph, Vanellope von Schweetz is a glitchy character in the video game Sugar Rush.

Software bug
From Wikipedia, the free encyclopedia
To report a MediaWiki error on Wikipedia, see Wikipedia:Bug reports.

[hide]This article has multiple issues. Please help improve it (see how) or discuss these issues on the talk page.
This article reads like an editorial or opinion piece. (May 2012)
This article needs additional citations for verification. (August 2015)
Software development process
Core activities
Requirements Design Construction Testing Debugging Deployment Maintenance
Paradigms and models
Software engineering Waterfall Prototyping Incremental V-Model Dual Vee Model Spiral IID Agile Lean DevOps
Methodologies and frameworks
Cleanroom TSP PSP RAD DSDM MSF Scrum Kanban UP XP TDD ATDD BDD FDD DDD MDD
Supporting disciplines
Configuration management Infrastructure as Code Documentation Software Quality assurance (SQA) Project management User experience
Tools
Compiler Debugger Profiler GUI designer Modeling IDE Build automation Release automation Testing
Standards and BOKs
CMMI IEEE standards ISO 9001 ISO/IEC standards SWEBOK PMBOK BABOK
v t e
A software bug is an error, flaw, failure or fault in a computer program or system that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. Most bugs arise from mistakes and errors made by people in either a program's source code or its design, or in frameworks and operating systems used by such programs, and a few are caused by compilers producing incorrect code. A program that contains a large number of bugs, and/or bugs that seriously interfere with its functionality, is said to be buggy or defective. Reports detailing bugs in a program are commonly known as bug reports, defect reports, fault reports, problem reports, trouble reports, change requests and so forth.

Bugs trigger errors that may in turn have a wide variety of ripple effects with varying levels of inconvenience to the user of the program. Some bugs have only a subtle effect on the program's functionality, and may thus lie undetected for a long time. More serious bugs may cause the program to crash or freeze. Others qualify as security bugs and might, for example, enable a malicious user to bypass access controls in order to obtain unauthorized privileges.

The results of bugs may be extremely serious. Bugs in the code controlling the Therac-25 radiation therapy machine were directly responsible for some patient deaths in the 1980s. In 1996, the European Space Agency's US$1 billion prototype Ariane 5 rocket had to be destroyed less than a minute after launch due to a bug in the on-board guidance computer program. In June 1994, a Royal Air Force Chinook helicopter crashed into the Mull of Kintyre, killing 29. This was initially dismissed as pilot error, but an investigation by Computer Weekly uncovered sufficient evidence to convince a House of Lords inquiry that it may have been caused by a software bug in the aircraft's engine control computer.[1]

In 2002, a study commissioned by the US Department of Commerce's National Institute of Standards and Technology concluded that "software bugs, or errors, are so prevalent and so detrimental that they cost the US economy an estimated $59 billion annually, or about 0.6 percent of the gross domestic product".[2]

Contents  [hide] 
1	Etymology
2	History
3	Prevalence
4	Mistake metamorphism
5	Prevention
6	Debugging
7	Bug management
7.1	Severity of a bug
7.2	Priority of a bug
7.3	Connection between priority and severity
7.4	Software releases
8	Security vulnerabilities
9	Common types of computer bugs
9.1	Arithmetic bugs
9.2	Logic bugs
9.3	Syntax bugs
9.4	Resource bugs
9.5	Multi-threading programming bugs
9.6	Interfacing bugs
9.7	Performance bugs
9.8	Teamworking bugs
10	Well-known bugs
11	In popular culture
12	See also
13	Notes
14	Further reading
15	External links
Etymology[edit]
Use of the term "bug" to describe inexplicable defects has been a part of engineering jargon for many decades and predates computers and computer software; it may have originally been used in hardware engineering to describe mechanical malfunctions. For instance, Thomas Edison wrote the following words in a letter to an associate in 1878:

It has been just so in all of my inventions. The first step is an intuition, and comes with a burst, then difficulties arise — this thing gives out and [it is] then that "Bugs" — as such little faults and difficulties are called — show themselves and months of intense watching, study and labor are requisite before commercial success or failure is certainly reached.[3]

The Middle English word bugge is the basis for the terms "bugbear" and "bugaboo", terms used for a monster.[4] Baffle Ball, the first mechanical pinball game, was advertised as being "free of bugs" in 1931.[5] Problems with military gear during World War II were referred to as bugs (or glitches).[6]


A page from the Harvard Mark II electromechanical computer's log, featuring a dead moth that was removed from the device
The term "bug" was used in an account by computer pioneer Grace Hopper, who publicized the cause of a malfunction in an early electromechanical computer.[7] A typical version of the story is given by this quote:[8]

In 1946, when Hopper was released from active duty, she joined the Harvard Faculty at the Computation Laboratory where she continued her work on the Mark II and Mark III. Operators traced an error in the Mark II to a moth trapped in a relay, coining the term bug. This bug was carefully removed and taped to the log book. Stemming from the first bug, today we call errors or glitches in a program a bug.

Hopper was not actually the one who found the insect, as she readily acknowledged. The date in the log book was September 9, 1947,[9][10] although sometimes erroneously reported as 1945.[11] The operators who did find it, including William "Bill" Burke, later of the Naval Weapons Laboratory, Dahlgren, Virginia,[12] were familiar with the engineering term and, amused, kept the insect with the notation "First actual case of bug being found." Hopper loved to recount the story.[13] This log book, complete with attached moth, is part of the collection of the Smithsonian National Museum of American History.[10]

The related term "debug" also appears to predate its usage in computing: the Oxford English Dictionary's etymology of the word contains an attestation from 1945, in the context of aircraft engines.[14]

History[edit]
The concept that software might contain errors dates back to Ada Lovelace's 1843 notes on the analytical engine, in which she speaks of the possibility of program "cards" for Charles Babbage's analytical engine being erroneous:

... an analysing process must equally have been performed in order to furnish the Analytical Engine with the necessary operative data; and that herein may also lie a possible source of error. Granted that the actual mechanism is unerring in its processes, the cards may give it wrong orders.

Prevalence[edit]
In software development projects, a "mistake" or "fault" may be introduced at any stage during development. Bugs are a consequence of the nature of human factors in the programming task. They arise from oversights or mutual misunderstandings made by a software team during specification, design, coding, data entry and documentation. For example, in creating a relatively simple program to sort a list of words into alphabetical order, one's design might fail to consider what should happen when a word contains a hyphen. Perhaps, when converting the abstract design into the chosen programming language, one might inadvertently create an off-by-one error and fail to sort the last word in the list. Finally, when typing the resulting program into the computer, one might accidentally type a "<" where a ">" was intended, perhaps resulting in the words being sorted into reverse alphabetical order.

Another category of bug is called a race condition that may occur when programs have multiple components executing at the same time, either on the same system or across multiple systems interacting across a network. If the components interact in a different order than the developers intended, it may break the logical flow of the program. These bugs may be difficult to detect or anticipate, since they may not occur during every execution of a program.

More complex bugs may arise from unintended interactions between different parts of a computer program. This frequently occurs because computer programs may be complex — millions of lines long in some cases — often having been programmed by many people over a great length of time, so that programmers are unable to mentally track every possible way in which parts may interact.

Mistake metamorphism[edit]
There is ongoing debate over the use of the term "bug" to describe software errors.[15] One argument is that the word "bug" is divorced from a sense that a human being caused the problem, and instead implies that the defect arose on its own, leading to a push to abandon the term "bug" in favor of terms such as "defect", with limited success.[citation needed]

In software engineering, mistake metamorphism (from Greek meta = "change", morph = "form") refers to the evolution of a defect in the final stage of software deployment. [clarification needed] Transformation of a "mistake" committed by an analyst in the early stages of the software development lifecycle, which leads to a "defect" in the final stage of the cycle has been called 'mistake metamorphism'.[16]

Different stages of a "mistake" in the entire cycle may be described as "mistakes", "anomalies", "faults", "failures", "errors", "exceptions", "crashes", "bugs", "defects", "incidents", or "side effects".[16]

Prevention[edit]
The software industry has put much effort into finding methods for preventing programmers from inadvertently introducing bugs while writing software.[17][18] These include:

Programming style
While typos in the program code are often caught by the compiler, a bug usually appears when the programmer makes a logic error. Various innovations in programming style and defensive programming are designed to make these bugs less likely, or easier to spot. In some programming languages, so-called typos, especially of symbols or logical/mathematical operators, actually represent logic errors, since the mistyped constructs are accepted by the compiler with a meaning other than that which the programmer intended.
Programming techniques
Bugs often create inconsistencies in the internal data of a running program. Programs can be written to check the consistency of their own internal data while running. If an inconsistency is encountered the program may immediately halt so that the bug may be located and fixed. Alternatively, the program may simply inform the user, attempt to correct the inconsistency and continue running.
Development methodologies
There are several schemes for managing programmer activity so that fewer bugs are produced. Many of these fall under the discipline of software engineering (which addresses software design issues as well). For example, formal program specifications are used to state the exact behavior of programs so that design bugs may be eliminated. Unfortunately, formal specifications are impractical or impossible[citation needed] for anything but the shortest programs, because of problems of combinatorial explosion and indeterminacy.
In modern times, popular approaches include automated unit testing and automated acceptance testing (sometimes going to the extreme of test-driven development), and agile software development (which is often combined with, or even in some cases mandates, automated testing). All of these approaches are supposed to catch bugs and poorly-specified requirements soon after they are introduced, which should make them easier and cheaper to fix, and to catch at least some of them before they enter into production use.
Programming language support
Programming languages often include features which help programmers prevent bugs, such as static type systems, restricted namespaces and modular programming, among others. For example, when a programmer writes (pseudocode) LET REAL_VALUE PI = "THREE AND A BIT", although this may be syntactically correct, the code fails a type check. Depending on the language and implementation, this may be caught by the compiler or at run-time. In addition, many recently invented languages have deliberately excluded features which may easily lead to bugs, at the expense of making code slower than it need be: the general principle being that, because of Moore's law, computers get faster and software engineers get slower; it is almost always better to write simpler, slower code than "clever", inscrutable code, especially considering that maintenance cost is substantial. For example, the Java programming language does not support pointer arithmetic; implementations of some languages such as Pascal and scripting languages often have runtime bounds checking of arrays, at least in a debugging build.
Code analysis
Tools for code analysis help developers by inspecting the program text beyond the compiler's capabilities to spot potential problems. Although in general the problem of finding all programming errors given a specification is not solvable (see halting problem), these tools exploit the fact that human programmers tend to make the same kinds of mistakes when writing software.
Instrumentation
Tools to monitor the performance of the software as it is running, either specifically to find problems such as bottlenecks or to give assurance as to correct working, may be embedded in the code explicitly (perhaps as simple as a statement saying PRINT "I AM HERE"), or provided as tools. It is often a surprise to find where most of the time is taken by a piece of code, and this removal of assumptions might cause the code to be rewritten.
Debugging[edit]

The typical bug history (GNU Classpath project data). A new bug submitted by the user is unconfirmed. Once it has been reproduced by a developer, it is a confirmed bug. The confirmed bugs are later fixed. Bugs belonging to other categories (unreproducible, will not be fixed, etc.) are usually in the minority
Main article: Debugging
Finding and fixing bugs, or "debugging", has always been a major part of computer programming. Maurice Wilkes, an early computing pioneer, described his realization in the late 1940s that much of the rest of his life would be spent finding mistakes in his own programs.[19] As computer programs grow more complex, bugs become more common and difficult to fix. Often programmers spend more time and effort finding and fixing bugs than writing new code. Software testers are professionals whose primary task is to find bugs, or write code to support testing. On some projects, more resources may be spent on testing than in developing the program.

Usually, the most difficult part of debugging is finding the bug in the source code. Once it is found, correcting it is usually relatively easy. Programs known as debuggers exist to help programmers locate bugs by executing code line by line, watching variable values, and other features to observe program behavior. Without a debugger, code may be added so that messages or values may be written to a console (for example with printf in the C programming language) or to a window or log file to trace program execution or show values.

However, even with the aid of a debugger, locating bugs is something of an art. It is not uncommon for a bug in one section of a program to cause failures in a completely different section,[citation needed] thus making it especially difficult to track (for example, an error in a graphics rendering routine causing a file I/O routine to fail), in an apparently unrelated part of the system.

Sometimes, a bug is not an isolated flaw, but represents an error of thinking or planning on the part of the programmer. Such logic errors require a section of the program to be overhauled or rewritten. As a part of Code review, stepping through the code modelling the execution process in one's head or on paper may often find these errors without ever needing to reproduce the bug as such, if it can be shown there is some faulty logic in its implementation.

But more typically, the first step in locating a bug is to reproduce it reliably. Once the bug is reproduced, the programmer may use a debugger or some other tool to monitor the execution of the program in the faulty region, and find the point at which the program went astray.

It is not always easy to reproduce bugs. Some are triggered by inputs to the program which may be difficult for the programmer to re-create. One cause of the Therac-25 radiation machine deaths was a bug (specifically, a race condition) that occurred only when the machine operator very rapidly entered a treatment plan; it took days of practice to become able to do this, so the bug did not manifest in testing or when the manufacturer attempted to duplicate it. Other bugs may disappear when the program is run with a debugger; these are heisenbugs (humorously named after the Heisenberg uncertainty principle).

Debugging is still a tedious task requiring considerable effort. Since the 1990s, particularly following the Ariane 5 Flight 501 disaster, there has been a renewed interest in the development of effective automated aids to debugging. For instance, methods of static code analysis by abstract interpretation have already made significant achievements, while still remaining much of a work in progress.

As with any creative act, sometimes a flash of inspiration will show a solution, but this is rare and, by definition, cannot be relied on.

There are also classes of bugs that have nothing to do with the code itself. If, for example, one relies on faulty documentation or hardware, the code may be written perfectly properly to what the documentation says, but the bug truly lies in the documentation or hardware, not the code. However, it is common to change the code instead of the other parts of the system, as the cost and time to change it is generally less. Embedded systems frequently have workarounds for hardware bugs, since to make a new version of a ROM is much cheaper than remanufacturing the hardware, especially if they are commodity items.

Bug management[edit]
Acap.svg	This section may require copy editing. (August 2015)
Bug management may be more than bug tracking alone but there is no industry-wide standard. Proposed changes to software – bugs as well as enhancement requests and even entire releases – are commonly tracked and managed using bug tracking systems or issue tracking systems. The items added may be called defects, tickets, issues, or, following the agile development paradigm, stories and epics. The systems allow or even require some type of categorization of each issue. Categories may be objective, subjective or a combination, such as version number, area of the software, severity and priority, as well as what type of issue it is, such as a feature request or a bug.

Severity of a bug[edit]
Given a bug is impairing a user scenario, one can easily see the impact the bug has. This impact may be tangible as tangible as of data loss, immediate losses in terms of money, or may be indirect – loss of goodwill or man hours, and eventually business. This impact is said to be the severity of a bug: "the impact a bug causes when encountered by users". Thus, severity, as a software metric does have a very precise meaning. Unfortunately, severity levels are not standardized in industry and are decided by each software producer, if they are even used. This is because impacts differ across the industry. A crash in a video game has a totally different impact than a crash in the browser, or real time monitoring system. Irrespective of that, crashes would be generally categorised high severity in the respective fields. For example, bug severity levels might be "crash or hang", "no workaround" (meaning there is no way the customer can accomplish a given task), "has workaround" (meaning there is a way for the user to recover and accomplish the task), "UI" or "visual defect" (for example, a missing image or displaced button or form element), or "documentation error". Some software publishers use more qualified severities such as "critical", "high", "low," "blocker," or "trivial".[20] The severity of a bug may be a separate category to its priority for fixing, and the two may be quantified and managed separately.

Priority of a bug[edit]
Given a bug, how fast it needs to get fixed is defined by the software metric priority. How the priority for fixing is used is decided internally by each software producer. Priorities are sometimes numerical and sometimes words, such "critical," "high," "low" or "deferred"; note that these may be similar or even identical to severity ratings when looking at different software producers. For example, a software company may decide that priority 1 bugs are always to be fixed for the next release, whereas "5" could mean its fix is put off – sometimes indefinitely.

Connection between priority and severity[edit]
If a flaw is found in an application which causes it to crash, yet the crash is so rare and takes, say, ten extremely unusual or unlikely steps to produce it, management may set its priority as "low" or even "will not fix." Thus it is easily seen that Priority is a function of probability of a bug to occur, and Severity (impact) of the bug. In specificity, priority is a strictly increasing function of both probability of the bug occurrence and severity. Given probability p = 1, the severity defines the priority. When p = 0, the bug, in all probability needs not to be fixed at all, however we may have a priority strictly proportional to the severity. In the same way, when Severity S=0 for a bug, we may have a priority strictly proportional to the probability.

One may axiomatize the priority function as any function having above characteristics, for example this very simplified function works : P(p,s) = B - \lceil kpS  \rceil   where p is the probability while S is the severity, with k a scaling constant, and to invert the value B is the base. The ceiling function is used to get the domain of priority to only integers. It also assigns priority-B to almost never occurring bugs. Industry standard practice is to use an inverted scale, so that highest priority are low numbers, example priority 0, priority 1, while lowest priority are bigger numbers, i.e. priority 3, priority 4... etc.

Software releases[edit]
It is common practice for software to be released with known bugs that are considered "non-critical" as defined by the software producer(s). While software products may, by definition, contain any number of unknown bugs, measurements during testing can provide an estimate of the number of likely bugs remaining; this becomes more reliable the longer a product is tested and developed. Most big software projects maintain two lists of "known bugs" – those known to the software team, and those to be told to users.[citation needed] The second list informs users about bugs that are not fixed in the current release, or not fixed at all, and a workaround may be offered.

A software publisher may opt not to fix a particular bug for a number of reasons, including:

A deadline must be met and priorities are such that only those above a certain severity are fixed for the current software release.
The bug is already fixed in an upcoming release, and it is not serious enough to warrant an immediate update or patch
The changes to the code required to fix the bug are too costly, will take too long for the current release, or affect too many other areas of the software.
Users may be relying on the undocumented, buggy behavior; it may introduce a breaking change.
The problem is in an area which will be obsolete with an upcoming release; fixing it is unnecessary.
It's "not a bug". A misunderstanding has arisen between expected and perceived behavior, when such misunderstanding is not due to confusion arising from design flaws, or faulty documentation.
The amount and type of damage a software bug may cause naturally affects decision-making, processes and policy regarding software quality. In applications such as manned space travel or automotive safety, since software flaws have the potential to cause human injury or even death, such software will have far more scrutiny and quality control than, for example, an online shopping website. In applications such as banking, where software flaws have the potential to cause serious financial damage to a bank or its customers, quality control is also more important than, say, a photo editing application. NASA's Software Assurance Technology Center managed to reduce the number of errors to fewer than 0.1 per 1000 lines of code (SLOC)[citation needed] but this was not felt to be feasible for projects in the business world.

A school of thought popularized by Eric S. Raymond as Linus's Law says that popular open-source software has more chance of having few or no bugs than other software, because "given enough eyeballs, all bugs are shallow".[21] This assertion has been disputed, however: computer security specialist Elias Levy wrote that "it is easy to hide vulnerabilities in complex, little understood and undocumented source code," because, "even if people are reviewing the code, that doesn't mean they're qualified to do so."[22]

Security vulnerabilities[edit]
Malicious software may attempt to exploit known vulnerabilities in a system–which may or may not be bugs. Viruses are not bugs in themselves–they are typically programs that are doing precisely what they were designed to do. However, viruses are occasionally referred to as such in the popular press.[citation needed] In addition, it is often a security bug in a computer program that allows viruses to work in the first place.

Common types of computer bugs[edit]

This section contains embedded lists that may be poorly defined, unverified or indiscriminate. Please help to clean it up to meet Wikipedia's quality standards. Where appropriate, incorporate items into the main body of the article. (August 2015)

This section is in a list format that may be better presented using prose. You can help by converting this section to prose, if appropriate. Editing help is available. (August 2015)
Conceptual error (code is syntactically correct, but the programmer or designer intended it to do something else).
Arithmetic bugs[edit]
Division by zero.
Arithmetic overflow or underflow.
Loss of arithmetic precision due to rounding or numerically unstable algorithms.
Logic bugs[edit]
Infinite loops and infinite recursion.
Off-by-one error, counting one too many or too few when looping.
Syntax bugs[edit]
Use of the wrong operator, such as performing assignment instead of equality test. For example, in some languages x=5 will set the value of x to 5 while x==5 will check whether x is currently 5 or some other number. In simple cases often the compiler may generate a warning. In many languages, the language syntax is deliberately designed to guard against this error.
Resource bugs[edit]
Null pointer dereference.
Using an uninitialized variable.
Using an otherwise valid instruction on the wrong data type (see packed decimal/binary coded decimal).
Access violations.
Resource leaks, where a finite system resource (such as memory or file handles) become exhausted by repeated allocation without release.
Buffer overflow, in which a program tries to store data past the end of allocated storage. This may or may not lead to an access violation or storage violation. These bugs may form a security vulnerability.
Excessive recursion which — though logically valid — causes stack overflow.
Use-after-free error, where a pointer is used after the system has freed the memory it references.
Double free error.
Multi-threading programming bugs[edit]
Deadlock, where task A can't continue until task B finishes, but at the same time, task B can't continue until task A finishes.
Race condition, where the computer does not perform tasks in the order the programmer intended.
Concurrency errors in critical sections, mutual exclusions and other features of concurrent processing. Time-of-check-to-time-of-use (TOCTOU) is a form of unprotected critical section.
Interfacing bugs[edit]
Incorrect API usage.
Incorrect protocol implementation.
Incorrect hardware handling.
Incorrect assumptions of a particular platform.
Incompatible systems. Often a proposed "new API" or new communications protocol may seem to work when both computers use the old version or both computers use the new version, but upgrading only the receiver exposes backward compatibility problems; in other cases upgrading only the transmitter exposes forward compatibility problems. Often it is not feasible to upgrade every computer simultaneously—in particular, in the telecommunication industry[23] or the internet.[24][25][26] Even when it is feasible to update every computer simultaneously, sometimes people accidentally forget to update every computer—the Knight Capital Group#2012 stock trading disruption involved one such incompatibility between the old API and a new API.
Performance bugs[edit]
Too high computational complexity of algorithm.
Random disk or memory access.
Teamworking bugs[edit]
Unpropagated updates; e.g. programmer changes "myAdd" but forgets to change "mySubtract", which uses the same algorithm. These errors are mitigated by the Don't Repeat Yourself philosophy.
Comments out of date or incorrect: many programmers assume the comments accurately describe the code.
Differences between documentation and the actual product.
Well-known bugs[edit]
Main article: List of software bugs
A number of software bugs have become well-known, usually due to their severity: examples include various space and military aircraft crashes. Possibly the most famous bug is the Year 2000 problem, also known as the Y2K bug, in which it was feared that worldwide economic collapse would happen at the start of the year 2000 as a result of computers thinking it was 1900. (In the end, no major problems occurred.)

In popular culture[edit]
In Robert A. Heinlein's 1966 novel The Moon Is a Harsh Mistress, computer technician Manuel Davis blames a real bug for a (non-existent) failure of supercomputer Mike, presenting a dead fly as evidence.
In the 1968 novel 2001: A Space Odyssey (and its corresponding 1968 film adaptation), a spaceship's onboard computer, HAL 9000, attempts to kill all its crew members. In the followup 1982 novel, 2010: Odyssey Two, and the accompanying 1984 film, 2010, it is revealed that this action was caused by the computer having been programmed with two conflicting objectives: to fully disclose all its information, and to keep the true purpose of the flight secret from the crew; this conflict caused HAL to become paranoid and eventually homicidal.
In the 1999 American comedy Office Space, the plot focuses on an attempt by three employees to exploit the company's preoccupation with fixing the Y2K computer bug by infecting the company's computer system with a virus that sends rounded off pennies to a separate bank account. The plan backfires as the virus itself has its own bug which sends large amounts of money to the account prematurely.
In 2000, Joe Trela correctly answered moth for the million dollar question: "What insect shorted out an early supercomputer and inspired the term Computer Bug." in the United States version of the game show Who Wants to be a Millionaire.
The 2004 novel The Bug, by Ellen Ullman, is about a programmer's attempt to find an elusive bug in a database application.[citation needed]
The 2008 Canadian film Control Alt Delete is about a computer programmer at the end of 1999 struggling to fix bugs at his company related to the year 2000 problem.
See also[edit]

This "see also" section may contain an excessive number of suggestions. Please ensure that only the most relevant suggestions are given and that they are not red links, and consider integrating suggestions into the article itself. (August 2015)
Portal icon	Software Testing portal
Anti-pattern
Software rot
Bug bounty program
Glitch removal
ISO/IEC 9126, which classifies a bug as either a defect or a nonconformity
Orthogonal Defect Classification
Racetrack problem
RISKS Digest
Software defect indicator
Software regression

Bug bounty program
From Wikipedia, the free encyclopedia
A bug bounty program is a deal offered by many websites and software developers by which individuals can receive recognition and compensation for reporting bugs, especially those pertaining to exploits and vulnerabilities. These programs allow the developers to discover and resolve bugs before the general public is aware of them, preventing incidents of widespread abuse. Bug bounty programs have been implemented by Facebook,[1] Yahoo!,[2] Google,[3] Reddit,[4] and Square.[5]

Contents  [hide] 
1	History
2	Incidents
3	Notable programs
4	See also
5	References
6	External links
History[edit]

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (February 2015) (Learn how and when to remove this template message)
The original "Bugs Bounty" program was the creation of Jarrett Ridlinghafer while working at Netscape Communications Corporation as a technical support Engineer.

Netscape encouraged its employees to push themselves and do whatever it takes to get the job done and, in early 1996, Ridlinghafer was inspired with the idea for, and coined the phrase, 'Bugs Bounty'.

He recognized that Netscape had many enthusiasts and evangelists for their products, some of whom to him seemed even fanatical, particularly for the Mosaic/Netscape/Mozilla browser. He started to investigate the phenomenon in more detail and discovered that many of Netscape's enthusiasts were actually software engineers who were fixing the product's bugs on their own and publishing the fixes or workarounds:

in the news forums that had been set up by Netscape's technical support department to enable "self-help through collaboration" (another one of Ridlinghafer's ideas during his four-year stint at Netscape); or
on the unofficial "Netscape U-FAQ" website, where every known bug and feature of the browser was listed, as well as instructions regarding workarounds and fixes.
Ridlinghafer thought the company should leverage these resources and sat down and wrote out a proposal for the 'Netscape Bugs Bounty Program', which he presented to his manager who in turn suggested that Ridlinghafer present it at the next company executive team meeting.

At the next executive team meeting, which was attended by James Barksdale, Marc Andreessen and the VPs of every department including product engineering, each member was given a copy of the 'Netscape Bugs Bounty Program' proposal and Ridlinghafer was invited to present his idea to the Netscape Executive Team.

Everyone at the meeting embraced the idea except the VP of Engineering, who did not want it to go forward believing it to be a waste of time and resources. However, the VP of Engineering was overruled and Ridlinghafer was given an initial $50k budget to run with the proposal and the first official 'Bugs Bounty' program was launched in 1995.[6]

The program was such a huge success that it is mentioned in many of books about Netscape's successes, and many organizations (including Google) have statements on their 'Bugs Bounty' pages giving credit to mozilla.org for the Bugs Bounty program.

Incidents[edit]
In August 2013, a Computer Science student named Khalil used an exploit to post a letter on the Facebook timeline of site founder Mark Zuckerberg. According to the hacker, he had tried to report the vulnerability using Facebook's bug bounty program, but because of the vague report the response team told him that his vulnerability was not actually a bug.[7]


A Facebook "White Hat" debit card, given to researchers who report security bugs
Facebook started paying researchers who find and report security bugs by issuing them custom branded “White Hat” debit cards that can be reloaded with funds each time the researchers discover new flaws. “Researchers who find bugs and security improvements are rare, and we value them and have to find ways to reward them,” Ryan McGeehan, former manager of Facebook’s security response team, told CNET in an interview. “Having this exclusive black card is another way to recognize them. They can show up at a conference and show this card and say ‘I did special work for Facebook.’”[8] In 2014, Facebook stopped issuing debit cards to researchers.

India, which has the second largest number of bug hunters in the world,[9] tops the Facebook Bug Bounty Program with the largest number of valid bugs. "Researchers in Russia earned the highest amount per report in 2013, receiving an average of $3,961 for 38 bugs. India contributed the largest number of valid bugs at 136, with an average reward of $1,353. The USA reported 92 issues and averaged $2,272 in rewards. Brazil and the UK were third and fourth by volume, with 53 bugs and 40 bugs, respectively, and average rewards of $3,792 and $2,950", Facebook quoted in a post.[10]

Yahoo! was severely criticized for sending out Yahoo! T-shirts as reward to the Security Researchers for finding and reporting security vulnerabilities in Yahoo!, sparking what came to be called T-shirt-gate.[11] High-Tech Bridge, a Geneva, Switzerland-based security testing company issued a press release saying Yahoo! offered $12.50 in credit per vulnerability, which could be used toward Yahoo-branded items such as T-shirts, cups and pens from its store. Ramses Martinez, director of Yahoo's security team claimed later in a blog post[12] that he was behind the voucher reward program, and that he basically had been paying for them out of his own pocket. Eventually, Yahoo! launched its new bug bounty program on October 31 of the same year, that allows security researchers to submit bugs and receive rewards between $250 and $15,000, depending on the severity of the bug discovered.[13]

Notable programs[edit]
In October 2013, Google announced a major change to its Vulnerability Reward Program. Previously, it had been a bug bounty program covering many Google products. With the shift, however, the program was broadened to include a selection of high-risk free software applications and libraries, primarily those designed for networking or for low-level operating system functionality. Submissions that Google found adherent to the guidelines would be eligible for rewards ranging from $500 to $3133.70.[14][15]

Similarly, Microsoft and Facebook partnered in November 2013 to sponsor The Internet Bug Bounty, a program to offer rewards for reporting hacks and exploits for a broad range of Internet-related software.[16] The software covered by the IBB includes Adobe Flash, Python, Ruby, PHP, Django, Ruby on Rails, Perl, OpenSSL, Nginx, Apache HTTP Server, and Phabricator. In addition, the program offered rewards for broader exploits affecting widely used operating systems and web browsers, as well as the Internet as a whole.[17]

See also[edit]
Bugcrowd
Bounty hunter
HackerOne
Cyber-arms industry
Knuth reward check, a bug reward program dating back (at least) to the early 1980s

HackerOne
From Wikipedia, the free encyclopedia
HackerOne
Type
Private
Industry	Cybersecurity
Founded	2012
Founders	Michiel Prins, Jobert Abma, Alex Rice and Merijn Terheggen
Headquarters	San Francisco, California
Key people
M?rten Mickos (CEO)
Website	hackerone.com
HackerOne is a vulnerability disclosure company based in San Francisco, California, which established a bug bounty platform that connects businesses with security researches.[1][2] The company has an additional office located in the Netherlands and its clients include companies such as Twitter, Slack, Adobe, Yahoo, LinkedIn and Airbnb.[1] HackerOne is one of the first companies to embrace and utilize hackers as part of its business model.[2][3]

History[edit]
In 2011, Jobert Abma, Michiel Prins, and Merijn Terheggen attempted to find vulnerabilities in 100 high-tech companies.[1] They found security vulnerabilities in Facebook, Google, Apple, Microsoft, Twitter and 95 other companies' systems.[1] Abma, Prins, and Terheggen met Alex Rice, Facebook's head of product security, after notifying the company that its system had a software vulnerability.[1] They founded HackerOne in 2012.[4] In November 2013, the company announced that it would host the Internet Bug Bounty project, a program encouraging the discovery and disclosure of internet bugs, funded by Microsoft and Facebook.[5]

In May 2014, HackerOne received $9 million in Series A funding from Benchmark.[6][7] Bill Gurley, Benchmark's general partner, joined HackerOne's board of directors as part of the deal.[6] John Hering, executive chairman of Lookout, joined the board of directors as well. Additionally, the company hired Katie Moussouris, former Microsoft lead security strategist, as Chief Policy Officer.[8][9] In 2015, HackerOne opened an office in the Netherlands on the campus of The Hague Security Delta (HSD), known as the European cyber security capital.[10]

The company announced a $25 million Series B funding round in June 2015.[2][3][11] The round was led by New Enterprise Associates and included investments from Marc Benioff, Yuri Milner, Drew Houston, Jeremy Stoppelman, David Sacks, Brandon Beck and Nicolas Berggruen.[4][12] Following the funding round, New Enterprise Associates' general partner Jon Sakoda joined HackerOne's board of directors.[3]

Operations[edit]
HackerOne develops a vulnerability coordination and bug bounty platform.[13] Companies pay hackers through the platform as a reward for identifying vulnerabilities in their systems and products.[14] The platform enables secure intelligence report sharing, payment and a reputation system for hackers.[13] By June 2015, HackerOne's platform identified approximately 10,000 vulnerabilities and paid hackers over $3 million.[4][15] At that time, the company's network of 1,500 hackers spans 150 countries.[3][15]

Facebook
From Wikipedia, the free encyclopedia
This article is about the social networking service. For the type of directory, see face book.
Facebook, Inc.
Facebook New Logo (2015).svg
Facebook (login, signup page).jpg
Facebook login/signup screen
Type	Public
Traded as	NASDAQ: FB
NASDAQ-100 Component
S&P 500 Component
Founded	February 4, 2004; 12 years ago
Headquarters	Menlo Park, California, U.S.
Coordinates	37.4848?N 122.1484?WCoordinates: 37.4848?N 122.1484?W
Area served	United States (2004–05)
Worldwide, except blocking countries (2005–present)
Founder(s)	
Mark Zuckerberg
Eduardo Saverin
Andrew McCollum
Dustin Moskovitz
Chris Hughes
Key people	Mark Zuckerberg
(Chairman and CEO)
Sheryl Sandberg
(COO)
Industry	Internet
Revenue	Increase US$17.928 billion (2015)[1]
Operating income	Increase US$6.225 billion (2015)[1]
Net income	Increase US$3.688 billion (2015)[1]
Total assets	Increase US$49.407 billion (2015)[1]
Total equity	Increase US$44.218 billion (2015)[1]
Employees	12,691 (2015)[2]
Subsidiaries	Instagram
WhatsApp
Oculus VR
PrivateCore
Website	www.facebook.com
Written in	C++, PHP (as HHVM)[3] and D language[4]
Alexa rank	Decrease 3 (May 2016)[5]
Type of site	Social networking service
Registration	Required
Users	Increase 1.65 billion monthly active users (April 27, 2016)[6]
Available in	Multilingual (140)
Current status	Active
Facebook is a corporation and an online social networking service headquartered in Menlo Park, California, in the United States. Its website was launched on February 4, 2004 by Mark Zuckerberg with his Harvard College roommates and fellow students Eduardo Saverin, Andrew McCollum, Dustin Moskovitz, and Chris Hughes.[7][8][9] The founders had initially limited the website's membership to Harvard students, but later expanded it to colleges in the Boston area, the Ivy League, and Stanford University. It gradually added support for students at various other universities and later to high school students. Since 2006, anyone in general aged 13 and older has been allowed to become a registered user of the website, though variations exist in the minimum age requirement, depending on applicable local laws.[10] Its name comes from the face book directories often given to American university students.[11]

After registering to use the site, users can create a user profile, add other users as "friends", exchange messages, post status updates and photos, share videos, use various apps, and receive notifications when others update their profiles. Additionally, users may join common-interest user groups organized by workplace, school, or other characteristics, and categorize their friends into lists such as "People From Work" or "Close Friends". Also, users can complain or block unpleasant people. Facebook had over 1.59 billion monthly active users as of August 2015.[12] Because of the large volume of data that users submit to the service, Facebook has come under scrutiny for their privacy policies. Facebook, Inc. held its initial public offering in February 2012 and began selling stock to the public three months later, reaching an original peak market capitalization of $104 billion. On July 13, 2015, Facebook became the fastest company in the Standard & Poor's 500 Index to reach a market cap of $250 billion.[13] Following its Q3 earnings call in 2015, Facebook's market cap exceeded $300 billion.

Contents  [hide] 
1	History
2	Corporate affairs
3	Website
4	Reception
5	Criticisms and controversies
6	Impact
7	In popular culture
8	See also
9	Notes
10	References
11	Further reading
12	External links
History
Main articles: History of Facebook and Timeline of Facebook
2003–06: Thefacebook, Thiel investment, and name change
Zuckerberg wrote a program called Facemash on October 28, 2003 while attending Harvard University as a sophomore (second year student). According to The Harvard Crimson, the site was comparable to Hot or Not and used "photos compiled from the online facebooks of nine houses, placing two next to each other at a time and asking users to choose the 'hotter' person"[14][15][16]

To accomplish this, Zuckerberg hacked into protected areas of Harvard's computer network and copied private dormitory ID images. Harvard did not have a student "Facebook" (a directory with photos and basic information) at the time, although individual houses had been issuing their own paper facebooks since the mid-1980s, and Harvard's longtime Freshman Yearbook was colloquially referred to as the "Freshman Facebook". Facemash attracted 450 visitors and 22,000 photo-views in its first four hours online.[14][17]

The site was quickly forwarded to several campus group list-servers, but was shut down a few days later by the Harvard administration. Zuckerberg faced expulsion and was charged by the administration with breach of security, violating copyrights, and violating individual privacy. Ultimately, the charges were dropped.[18] Zuckerberg expanded on this initial project that semester by creating a social study tool ahead of an art history final exam. He uploaded 500 Augustan images to a website, each of which was featured with a corresponding comments section.[17] He shared the site with his classmates, and people started sharing notes.


Original layout and name of Thefacebook, 2004.
The following semester, Zuckerberg began writing code for a new website in January 2004. He said that he was inspired by an editorial about the Facemash incident in The Harvard Crimson.[19] On February 4, 2004, Zuckerberg launched "Thefacebook", originally located at thefacebook.com.[20]

Six days after the site launched, Harvard seniors Cameron Winklevoss, Tyler Winklevoss, and Divya Narendra accused Zuckerberg of intentionally misleading them into believing that he would help them build a social network called HarvardConnection.com. They claimed that he was instead using their ideas to build a competing product.[21] The three complained to The Harvard Crimson and the newspaper began an investigation. They later filed a lawsuit against Zuckerberg, subsequently settling in 2008[22] for 1.2 million shares (worth $300 million at Facebook's IPO).[23]

Membership was initially restricted to students of Harvard College; within the first month, more than half the undergraduates at Harvard were registered on the service.[24] Eduardo Saverin (business aspects), Dustin Moskovitz (programmer), Andrew McCollum (graphic artist), and Chris Hughes joined Zuckerberg to help promote the website. In March 2004, Facebook expanded to the universities of Columbia, Stanford, and Yale.[25] It later opened to all Ivy League colleges, Boston University, New York University, MIT, and gradually most universities in the United States and Canada.[26][27]

In mid-2004, entrepreneur Sean Parker — an informal advisor to Zuckerberg — became the company's president.[28] In June 2004, Facebook moved its operations base to Palo Alto, California.[25] It received its first investment later that month from PayPal co-founder Peter Thiel.[29] In 2005, the company dropped "the" from its name after purchasing the domain name facebook.com for US$200,000.[30] The domain facebook.com belonged to AboutFace Corporation before the purchase. This website last appeared on April 8, 2005;[31] from April 10, 2005 to August 4, 2005, this domain gave a 403 error.[32]


Mark Zuckerberg, co-creator of Facebook, in his Harvard dorm room, 2005.
In May 2005, Accel Partners invested $12.7 million in Facebook, and Jim Breyer[33] added $1 million of his own money. A high-school version of the site was launched in September 2005, which Zuckerberg called the next logical step.[34] (At the time, high-school networks required an invitation to join.)[35] Facebook also expanded membership eligibility to employees of several companies, including Apple Inc. and Microsoft.[36]

A January 2009 Compete.com study ranked Facebook the most used social networking service by worldwide monthly active users.[37] Entertainment Weekly included the site on its end-of-the-decade "best-of" list saying, "How on earth did we stalk our exes, remember our co-workers' birthdays, bug our friends, and play a rousing game of Scrabulous before Facebook?"[38]

2006–12: public access, Microsoft alliance and rapid growth
On September 26, 2006, Facebook was opened to everyone at least 13 years old with a valid email address.[39][40][41]

In late 2007, Facebook had 100,000 business pages (pages which allowed companies to promote themselves and attract customers). These started as group pages, but a new concept called company pages was planned.[42] Pages began rolling out for businesses in May 2009.[43]

On October 24, 2007, Microsoft announced that it had purchased a 1.6% share of Facebook for $240 million, giving Facebook a total implied value of around $15 billion.[44] Microsoft's purchase included rights to place international advertisements on the social networking site.[45]

In October 2008, Facebook announced that it would set up its international headquarters in Dublin, Ireland.[46] Almost a year later, in September 2009, Facebook said that it had turned cash-flow positive for the first time.[47]

Traffic to Facebook increased steadily after 2009. The company announced 500 million users in July 2010[48] making it the largest online social network in the world at the time. According to the company's data, half of the site's membership use Facebook daily, for an average of 34 minutes, while 150 million users access the site by mobile. A company representative called the milestone a "quiet revolution."[49]

In November 2010, based on SecondMarket Inc. (an exchange for privately held companies' shares), Facebook's value was $41 billion. The company had slightly surpassed eBay to become the third largest American web company after Google and Amazon.com.[50]


Facebook headquarters entrance sign at 1 Hacker Way, Menlo Park, California
In early 2011, Facebook announced plans to move its headquarters to the former Sun Microsystems campus in Menlo Park, California.[51][52] In March 2011, it was reported that Facebook was removing approximately 20,000 profiles offline every day for violations such as spam, graphic content, and underage use, as part of its efforts to boost cyber security.[53]

Release of statistics by DoubleClick showed that Facebook reached one trillion page views in the month of June 2011, making it the most visited website tracked by DoubleClick.[54] According to a Nielsen Media Research study, released in December 2011, Facebook had become the second-most accessed website in the U.S. behind Google.[55]

2012–13: IPO, lawsuits and one-billionth user
Main article: Initial public offering of Facebook
Facebook eventually filed for an initial public offering on February 1, 2012.[56] Facebook held an initial public offering on May 17, 2012, negotiating a share price of US$38. The company was valued at $104 billion, the largest valuation to date for a newly listed public company.[57][58][59]

Facebook Inc. began selling stock to the public and trading on the NASDAQ on May 18, 2012.[60] Based on its 2012 income of $5 billion, Facebook joined the Fortune 500 list for the first time in May 2013, ranked in position 462.[61]

Facebook filed their S1 document with the Securities and Exchange Commission on February 1, 2012. The company applied for a $5 billion IPO, one of the biggest offerings in the history of technology.[62] The IPO raised $16 billion, making it the third-largest in U.S. history.[63][64]

The shares began trading on May 18; the stock struggled to stay above the IPO price for most of the day, but set a record for the trading volume of an IPO (460 million shares).[65] The first day of trading was marred by technical glitches that prevented orders from going through;[66][67] only the technical problems and artificial support from underwriters prevented the stock price from falling below the IPO price on the day.[68]

In March 2012, Facebook announced App Center, a store selling applications that operate via the site. The store was to be available on iPhones, Android devices, and mobile web users.[69]


Billboard on the Thomson Reuters building welcomes Facebook to NASDAQ, 2012
On May 22, 2012, the Yahoo! Finance website reported that Facebook's lead underwriters, Morgan Stanley (MS), JP Morgan (JPM), and Goldman Sachs (GS), cut their earnings forecasts for the company in the middle of the IPO process.[70] The stock had begun its freefall by this time, closing at 34.03 on May 21 and 31.00 on May 22. A "circuit breaker" was used in an attempt to slow down the stock price's decline.[71] Securities and Exchange Commission Chairman Mary Schapiro, and Financial Industry Regulatory Authority (FINRA) Chairman Rick Ketchum, called for a review of the circumstances surrounding the IPO.[72]

Facebook's IPO was consequently investigated, and was compared to a pump and dump scheme.[66][70][72][73] A class-action lawsuit was filed in May 2012 because of the trading glitches, which led to botched orders.[74][75] Lawsuits were filed, alleging that an underwriter for Morgan Stanley selectively revealed adjusted earnings estimates to preferred clients.[76]

The other underwriters (MS, JPM, GS), Facebook's CEO and board, and NASDAQ also faced litigation after numerous lawsuits were filed, while SEC and FINRA both launched investigations.[77] It was believed that adjustments to earnings estimates were communicated to the underwriters by a Facebook financial officer, who used the information to cash out on their positions while leaving the general public with overpriced shares.[78] By the end of May 2012, Facebook's stock lost over a quarter of its starting value, which led the Wall Street Journal to label the IPO a "fiasco".[79]

Zuckerberg announced to the media at the start of October 2012 that Facebook had passed the monthly active users mark of one billion[80]—Facebook defines active users as a logged-in member who visits the site, or accesses it through a third-party site connected to Facebook, at least once a month. Fake accounts were not mentioned in the announcement, but the company continued to remove them after it found that 8.7% of its users were not real in August 2012. The company's data also revealed 600 million mobile users, 140 billion friend connections since the inception of Facebook, and the median age of a user as 22 years.[48]

2013–present: site developments, A4AI and 10th anniversary
On January 15, 2013, Facebook announced Facebook Graph Search, which provides users with a "precise answer," rather than a link to an answer by leveraging the data present on its site.[81] Facebook emphasized that the feature would be "privacy-aware," returning only results from content already shared with the user.[82]

The company became the subject of a lawsuit by Rembrandt Social Media in February 2013, for patents involving the "Like" button.[83] On April 3, 2013, Facebook unveiled Facebook Home, a user-interface layer for Android devices offering greater integration with the site. HTC announced the HTC First, a smartphone with Home pre-loaded.[84]

On April 15, 2013, Facebook announced an alliance across 19 states with the National Association of Attorneys General, to provide teenagers and parents with information on tools to manage social networking profiles.[85] On April 19, 2013, Facebook officially modified its logo to remove the faint blue line at the bottom of the "F" icon. The letter F moved closer to the edge of the box.[86]

Following a campaign by 100 advocacy groups, Facebook agreed to update its policy on hate speech. The campaign highlighted content promoting domestic and sexual violence against women, and used over 57,000 tweets and more than 4,900 emails that caused withdrawal of advertising from the site by 15 companies, including Nissan UK, House of Burlesque and Nationwide UK. The social media website initially responded by stating that "while it may be vulgar and offensive, distasteful content on its own does not violate our policies".[87] It decided to take action on May 29, 2013, after it "become clear that our systems to identify and remove hate speech have failed to work as effectively as we would like, particularly around issues of gender-based hate."[88]

On June 12, 2013, Facebook announced on its newsroom that it was introducing clickable hashtags to help users follow trending discussions, or search what others are talking about on a topic.[89] A July 2013 Wall Street Journal article identified the Facebook IPO as the cause of a change in the U.S.' national economic statistics, as the local government area of the company's headquarters, San Mateo County, California, became the top wage-earning county in the country after the fourth quarter of 2012. The Bureau of Labor Statistics reported that the average weekly wage in the county was US$3,240, 107% higher than the previous year. It noted the wages were "the equivalent of $168,000 a year, and more than 50% higher than the next-highest county, New York County (better known as Manhattan), at $2,107 a week, or roughly $110,000 a year."[90]

Russian internet firm Mail.Ru sold its Facebook shares for US$525 million on September 5, 2013, following its initial $200 million investment in 2009. Partly owned by Russia's richest man, Alisher Usmanovhe, the firm owned a total of 14.2 million remaining shares prior to the sale.[91] In the same month, the Chinese government announced that it will lift the ban on Facebook in the Shanghai Free Trade Zone "to welcome foreign companies to invest and to let foreigners live and work happily in the free-trade zone." Facebook was first blocked in China in 2009.[92]

Facebook was announced as a member of The Alliance for Affordable Internet (A4AI) in October 2013, when the A4AI was launched. The A4AI is a coalition of public and private organisations that includes Google, Intel and Microsoft. Led by Sir Tim Berners-Lee, the A4AI seeks to make Internet access more affordable so that access is broadened in the developing world, where only 31% of people are online. Google will help to decrease Internet access prices so that they fall below the UN Broadband Commission's worldwide target of 5% of monthly income.[93]

A Reuters report, published on December 11, 2013, stated that Standard & Poor's announced the placement of Facebook on its S&P 500 index "after the close of trading on December 20."[94] Facebook announced Q4 2013 earnings of $523 million (20 cents per share), an increase of $64 million from the previous year,[95] as well as 945 million mobile users.

By January 2014, Facebook's market capitalization had risen to over $134 billion.[57][96] At the end of January 2014, 1.23 billion users were active on the website every month.

The company celebrated its 10th anniversary during the week of February 3, 2014.[97] In each of the first three months of 2014, over one billion users logged into their Facebook account on a mobile device.[98]

In February 2014, Facebook announced that it would be buying mobile messaging company Whatsapp for US$19 billion in cash and stock.[99] In June 2014, Facebook announced the acquisition of Pryte, a Finnish mobile data-plan firm that aims to make it easier for mobile phone users in underdeveloped parts of the world to use wireless Internet apps.[100]

At the start of July 2014, Facebook announced the acquisition of LiveRail, a San Francisco, California-based online video advertising company. LiveRail's technology facilitates the sale of video inventory across different devices. The terms of the deal were undisclosed, but TechCrunch reported that Facebook paid between US$400 million and $500 million.[101][102] As part of the company's second quarter results, Facebook announced in late July 2014 that mobile accounted for 62% of its advertising revenue, which is an increase of 21% from the previous year.[103]

Alongside other American technology figures like Jeff Bezos and Tim Cook, Zuckerberg hosted visiting Chinese politician Lu Wei, known as the "Internet czar" for his influence in the enforcement of China's online policy, at Facebook's headquarters on December 8, 2014. The meeting occurred after Zuckerberg participated in a Q&A session at Tsinghua University in Beijing, China, on October 23, 2014, where he attempted to converse in Mandarin—although Facebook is banned in China, Zuckerberg is highly regarded among the people and was at the university to help fuel the nation's burgeoning entrepreneur sector.[104] A book of Chinese president Xi Xinping found on Zuckerberg's office desk attracted a great deal of attention in the media, after the Facebook founder explained to Lu, "I want them [Facebook staff] to understand socialism with Chinese characteristics."[105]

Zuckerberg fielded questions during a live Q&A session at the company's headquarters in Menlo Park on December 11, 2014. The question of whether the platform would adopt a dislike button was raised again, and Zuckerberg said, "We're [Facebook] thinking about it [dislike button] … It's an interesting question," and said that he likes the idea of Facebook users being able to express a greater variety of emotions.[106][107] In October 2015, Zuckerberg said that instead of creating a dislike button, Facebook is testing emoji reactions as an alternative to the 'like' button.[108] On February 24, 2016, Facebook launched Facebook Reactions, which allows users to respond to posts with multiple reactions in addition to "liking" it.[109]

As of January 21, 2015, Facebook's algorithm is programmed to filter out false or misleading content, such as fake news stories and hoaxes, and will be supported by users who select the option to flag a story as "purposefully fake or deceitful news." According to Reuters, such content is "being spread like a wildfire" on the social media platform. Facebook maintained that "satirical" content, "intended to be humorous, or content that is clearly labeled as satire," will be taken into account and should not be intercepted.[110] The algorithm, however, has been accused of maintaining a "filter bubble", where both material the user disagrees with[111] and posts with a low level of likes, will also not be seen.[112] In 2015 November, Zuckerberg prolonged period of paternity leave from 4 weeks to 4 months.[113]

On April 12, 2016, Zuckerberg revealed a decade-long plan for Facebook in a keynote address. His speech outlined his vision, which was centered around three main pillars: artificial intelligence, increased connectivity around the world and virtual and augmented reality.[114] He also announced a new Facebook Messenger platform, which will have developers creating bots that are able to engage in automatic interactions with customers.[115]

Corporate affairs
Management
The ownership percentages of the company, as of 2012, are:

Mark Zuckerberg: 28%,[116]
Accel Partners: 10%
Mail.Ru Group: 10%[117]
Dustin Moskovitz: 6%
Eduardo Saverin: 5%
Sean Parker: 4%
Peter Thiel: 3%
Greylock Partners: between 1 and 2%
Meritech Capital Partners: between 1 and 2% each
Microsoft: 1.3%
Li Ka-shing: 0.8%
Interpublic Group: less than 0.5%
A small group of current and former employees and celebrities own less than 1% each, including Matt Cohler, Jeff Rothschild, Adam D'Angelo, Chris Hughes, and Owen Van Natta, while Reid Hoffman and Mark Pincus have sizable holdings of the company. The remaining 30% or so are owned by employees, an undisclosed number of celebrities, and outside investors.[118] Adam D'Angelo, former chief technology officer and friend of Zuckerberg, resigned in May 2008. Reports claimed that he and Zuckerberg began quarreling, and that he was no longer interested in partial ownership of the company.[119]

Key management personnel consist of: Chris Cox (Chief Product Officer), Sandberg (COO), and Zuckerberg (Chairman and CEO). Mike Vernal is considered to be the company's top engineer.[120] As of April 2011, Facebook has over 7,000 employees, and offices in 15 countries.[121] Other managers include chief financial officer David Wehner and public relations head Elliot Schrage.[122]

Facebook was named the 5th best company to work for in 2014 by company-review site Glassdoor as part of its sixth annual Employees' Choice Awards. The website stated that 93% of Facebook employees would recommend the company to a friend.[123]

Revenue
Revenues
(in millions US$)
Year	Revenue	Growth
2004	$0.4[124]	—
2005	$9[124]	2150%
2006	$48[124]	433%
2007	$153[124]	219%
2008	$280[125]	83%
2009	$775[126]	177%
2010	$2,000[127]	158%
2011	$3,711[128]	86%
2012	$5,089[129]	37%
2013	$7,872[129]	55%
2014	$12,466[130]	58%
2015	$17,928[131]	44%
Most of Facebook's revenue comes from advertising.[132][133] Facebook generally has a lower clickthrough rate (CTR) for advertisements than most major Web sites. According to BusinessWeek.com, banner advertisements on Facebook have generally received one-fifth the number of clicks compared to those on the Web as a whole,[134] although specific comparisons can reveal a much larger disparity. For example, while Google users click on the first advertisement for search results an average of 8% of the time (80,000 clicks for every one million searches),[135] Facebook's users click on advertisements an average of 0.04% of the time (400 clicks for every one million pages).[136]

Sarah Smith, who was Facebook's Online Sales Operations Manager until 2012,[137] reported that successful advertising campaigns on the site can have clickthrough rates as low as 0.05% to 0.04%, and that CTR for ads tend to fall within two weeks.[138]

The cause of Facebook's low CTR has been attributed to younger users enabling ad blocking software and their adeptness at ignoring advertising messages, as well as the site's primary purpose being social communication rather than content viewing.[139] According to digital consultancy iStrategy Labs in mid-January 2014, three million fewer users aged between 13 and 17 years were present on Facebook's Social Advertising platform compared to 2011.[140] However, Time writer and reporter Christopher Matthews stated in the wake of the iStrategy Labs results:

A big part of Facebook's pitch is that it has so much information about its users that it can more effectively target ads to those who will be responsive to the content. If Facebook can prove that theory to be true, then it may not worry so much about losing its cool cachet.[141][142]

In December 2014, a report from Frank N. Magid and Associates found that the percentage of teens aged 13 to 17 who used Facebook fell to 88% in 2014, down from 94% in 2013 and 95% in 2012.[143]

Zuckerberg, alongside other Facebook executives, have questioned the data in such reports; although, a former Facebook senior employee has commented: "Mark [Zuckerberg] is very willing to recognize the strengths in other products and the flaws in Facebook."[144]

On pages for brands and products, however, some companies have reported CTR as high as 6.49% for Wall posts.[145] A study found that, for video advertisements on Facebook, over 40% of users who viewed the videos viewed the entire video, while the industry average was 25% for in-banner video ads.[146]


Chart of Facebook's Stock
The company released its own set of revenue data at the end of January 2014 and claimed: Revenues of US$2.59 billion were generated for the three months ending December 31, 2013; earnings per share were 31 cents; revenues of US$7.87 billion were made for the entirety of 2013; and Facebook's annual profit for 2013 was US$1.5 billion. During the same time, independent market research firm eMarketer released data in which Facebook accounted for 5.7 per cent of all global digital ad revenues in 2013 (Google's share was 32.4 per cent).[97] Revenue for the June 2014 quarter rose to $2.68 billion, an increase of 67 per cent over the second quarter of 2013. Mobile advertising revenue accounted for around 62 per cent of advertising revenue, an increase of approximately 41 per cent over the comparable quarter of the previous year.

Number of advertisers
In February 2015, Facebook announced that it has reached two million active advertisers with most of the gain coming from small businesses. An active advertiser is an advertiser that has advertised on the Facebook platform in the last 28 days.[147]

Mergers and acquisitions
Main article: List of acquisitions by Facebook
On November 15, 2010, Facebook announced it had acquired the domain name fb.com from the American Farm Bureau Federation for an undisclosed amount. On January 11, 2011, the Farm Bureau disclosed $8.5 million in "domain sales income", making the acquisition of FB.com one of the ten highest domain sales in history.[148]

Offices
In early 2011, Facebook announced plans to move to its new headquarters, the former Sun Microsystems campus in Menlo Park.[149]

All users outside of the US and Canada have a contract with Facebook's Irish subsidiary "Facebook Ireland Limited". This allows Facebook to avoid US taxes for all users in Europe, Asia, Australia, Africa and South America. Facebook is making use of the Double Irish arrangement which allows it to pay just about 2–3% corporation tax on all international revenue.[150]

In 2010, Facebook opened its fourth office, in Hyderabad[151][152][153] and the first in Asia.[154]

Facebook, which in 2010 had more than 750 million active users globally including over 23 million in India, announced that its Hyderabad center would house online advertising and developer support teams and provide round-the-clock, multilingual support to the social networking site's users and advertisers globally.[155] With this, Facebook joins other giants like Google, Microsoft, Oracle, Dell, IBM and Computer Associates that have already set up shop.[156] In Hyderabad, it is registered as 'Facebook India Online Services Pvt Ltd'.[157][158][159]

Though Facebook did not specify its India investment or hiring figures, it said recruitment had already begun for a director of operations and other key positions at Hyderabad,[160] which would supplement its operations in California, Dublin in Ireland as well as at Austin, Texas.

A custom-built data center with substantially reduced ("38% less") power consumption compared to existing Facebook data centers opened in April 2011 in Prineville, Oregon.[161] In April 2012, Facebook opened a second data center in Forest City, North Carolina, US.[162] In June 2013, Facebook opened a third data center in Lule?, Sweden. In November 2014, Facebook opened a fourth data center in Altoona, Iowa, US.[163]

On October 1, 2012, CEO Zuckerberg visited Moscow to stimulate social media innovation in Russia and to boost Facebook's position in the Russian market.[164] Russia's communications minister tweeted that Prime Minister Dmitry Medvedev urged the social media giant's founder to abandon plans to lure away Russian programmers and instead consider opening a research center in Moscow. Facebook has roughly 9 million users in Russia, while domestic analogue VK has around 34 million.[165]

The functioning of a woodwork facility on the Menlo Park campus was announced at the end of August 2013. The facility, opened in June 2013, provides equipment, safety courses and woodwork learning course, while employees are required to purchase materials at the in-house store. A Facebook spokesperson explained that the intention of the facility is to encourage employees to think in an innovative manner because of the different environment, and also serves as an attractive perk for prospective employees.[166]


Entrance to Facebook's previous headquarters in the Stanford Research Park, Palo Alto, California
 
Entrance to Facebook headquarters complex in Menlo Park, California
 
Inside the Facebook headquarters in 2014
Open source contributions
Facebook is both a consumer of and contributor to free and open source software.[167] Facebook's contributions include: HipHop for PHP,[168] Fair scheduler in Apache Hadoop,[169] Apache Hive, Apache Cassandra,[170] and the Open Compute Project.[171]

Facebook also contributes to other opensource projects such as Oracle's MySQL database engine.[172][173]

Website
Main articles: Facebook features and Facebook Platform

Profile shown on Thefacebook in 2005

Previous Facebook logo in use until July 2015
Technical aspects
The website's primary color is blue as Zuckerberg is red-green colorblind, a realization that occurred after a test undertaken around 2007; he explained in 2010: "blue is the richest color for me—I can see all of blue."[174][175] Facebook is built in PHP which is compiled with HipHop for PHP, a 'source code transformer' built by Facebook engineers that turns PHP into C++.[176] The deployment of HipHop reportedly reduced average CPU consumption on Facebook servers by 50%.[177]

Facebook is developed as one monolithic application. According to an interview in 2012 with Chuck Rossi, a build engineer at Facebook, Facebook compiles into a 1.5 GB binary blob which is then distributed to the servers using a custom BitTorrent-based release system. Rossi stated that it takes approximately 15 minutes to build and 15 minutes to release to the servers. The build and release process is zero downtime and new changes to Facebook are rolled out daily.[177]

Facebook used a combination platform based on HBase to store data across distributed machines. Using a tailing architecture, new events are stored in log files, and the logs are tailed. The system rolls these events up and writes them into storage. The User Interface then pulls the data out and displays it to users. Facebook handles requests as AJAX behavior. These requests are written to a log file using Scribe (developed by Facebook).[178]

Data is read from these log files using Ptail, an internally built tool to aggregate data from multiple Scribe stores. It tails the log files and pulls data out (thus the name). Ptail data is separated out into three streams so they can eventually be sent to their own clusters in different data centers (Plugin impression, News feed impressions, Actions (plugin + news feed)). Puma is used to manage periods of high data flow (Input/Output or IO). Data is processed in batches to lessen the number of times needed to read and write under high demand periods (A hot article will generate a lot of impressions and news feed impressions which will cause huge data skews). Batches are taken every 1.5 seconds, limited by memory used when creating a hash table.[178]

After this, data is output in PHP format (compiled with HipHop for PHP). The backend is written in Java and Thrift is used as the messaging format so PHP programs can query Java services. Caching solutions are used to make the web pages display more quickly. The more and longer data is cached the less realtime it is. The data is then sent to MapReduce servers so it can be queried via Hive. This also serves as a backup plan as the data can be recovered from Hive. Raw logs are removed after a period of time.[178]

On March 20, 2014 Facebook announced a new open source programming language called Hack. Prior to public release, a large portion of Facebook was already running and "battle tested" using the new language.[179]

Facebook uses the Momentum platform from Message Systems to deliver the enormous volume of emails it sends to its users every day.[180]

History
On July 20, 2008, Facebook introduced "Facebook Beta", a significant redesign of its user interface on selected networks. The Mini-Feed and Wall were consolidated, profiles were separated into tabbed sections, and an effort was made to create a "cleaner" look.[181] After initially giving users a choice to switch, Facebook began migrating all users to the new version starting in September 2008.[182] On December 11, 2008, it was announced that Facebook was testing a simpler signup process.[183]

Notes
Facebook Notes was introduced on August 22, 2006, a blogging feature that allowed tags and embeddable images. Users were later able to import blogs from Xanga, LiveJournal, Blogger, and other blogging services.[39]
Chat
Facebook Chat was added April 6, 2008. It is a Comet-based[184] instant messaging application[185] which allows users to communicate with other Facebook users in a way similar in functionality to instant messaging software.
Gifts
Facebook launched Gifts on February 8, 2007, which allows users to send virtual gifts to their friends that appear on the recipient's profile. Gifts cost $1.00 each to purchase, and a personalized message can be attached to each gift.[186][187]
Marketplace
On May 14, 2007, Facebook launched Marketplace, which lets users post free classified ads.[188] Marketplace has been compared to Craigslist by CNET, which points out that the major difference between the two is that listings posted by a user on Marketplace are seen only by users in the same network as that user, whereas listings posted on Craigslist can be seen by anyone.[189]
Messaging
A new Messaging platform, codenamed "Project Titan", was launched on November 15, 2010. Described as a "Gmail killer" by some publications, the system allows users to directly communicate with each other via Facebook using several different methods (including a special email address, text messaging, or through the Facebook website or mobile app)—no matter what method is used to deliver a message, they are contained within single threads in a unified inbox. As with other Facebook features, users can adjust from whom they can receive messages—including just friends, friends of friends, or from anyone.[190][191] Email service was terminated in 2014 because of low uptake.[192] Aside from the Facebook website, messages can also be accessed through the site's mobile apps, or a dedicated Facebook Messenger app.[193]
Voice calls
Since April 2011, Facebook users have had the ability to make live voice calls via Facebook Chat, allowing users to chat with others from all over the world. This feature, which is provided free through T-Mobile's new Bobsled service, lets the user add voice to the current Facebook Chat as well as leave voice messages on Facebook.[194]
Video calling
On July 6, 2011, Facebook launched its video calling services using Skype as its technology partner. It allows one-to-one calling using a Skype Rest API.[195]
Video viewing
In September 2014, Facebook announced that it delivers 1 billion video views per day and that it would begin showing everyone view counts on publicly posted videos from users, Pages, and public figures. It also confirmed that it is recommending additional videos to users after they have watched a video. Sixty-five percent of Facebook's video views are coming from mobile where Facebook's user base is shifting, and views grew 50 percent from May to July, in part thanks to the viral ALS Ice Bucket Challenge finding a home on Facebook.[196]
Tor hidden service
Main article: facebookcorewwwi.onion
In October 2014, Facebook announced[197] that users could now connect to the website through a Tor hidden service using the privacy-protecting Tor browser and encrypted using SSL.[198][199][200] Announcing the feature, Alec Muffett said "Facebook's onion address provides a way to access Facebook through Tor without losing the cryptographic protections provided by the Tor cloud. […] it provides end-to-end communication, from your browser directly into a Facebook datacentre."[198] Its URL address – facebookcorewwwi.onion is a backronym, which stands for Facebook's Core WWW Infrastructure.[197]
User profile/personal timeline

Public profile of a user on Facebook in 2014 showing various social networking features of the site, including music preferences and favorite books
The format of individual user pages was revamped in late 2011 and became known as either a profile or personal timeline since that change.[201][202] Users can create profiles with photos and images, lists of personal interests, contact information, memorable life events, and other personal information, such as employment status.[203] Users can communicate with friends and other users through private or public messages, as well as a chat feature, and share content that includes website URLs, images, and video content.[204] A 2012 Pew Internet and American Life study identified that between 20 and 30 percent of Facebook users are "power users" who frequently link, poke, post and tag themselves and others.[205]

In 2007, Facebook launched Facebook Pages (also called "Fan Pages" by users) to allow "users to interact and affiliate with businesses and organizations in the same way they interact with other Facebook user profiles". On November 6, 2007, more than 100,000 Facebook pages were launched.[206]

In July 2012, Facebook added a same-sex marriage icon to its timeline feature.[207] On February 14, 2014, Facebook expanded the options for user's gender setting, adding a custom input field that allows users to choose from a wide range of gender identities. Users can also set which set of gender-specific pronouns are used in reference to them throughout the site.[208][209] The change occurs after Nepal's first openly gay politician Sunil Babu Pant sent a letter to Zuckerberg in early 2012 to request the addition of an "Other" gender option for Facebook users; Facebook's official statement on the issue: "People can already opt out of showing their sex on their profile. We're constantly innovating on our products and features and we welcome input from everyone as we explore ways to improve the Facebook experience."[210]

On June 13, 2009, Facebook introduced a "Usernames" feature, whereby pages can be linked with simpler URLs such as https://www.facebook.com/name instead of https://www.facebook.com/profile.php?id=20531316728.[211] Many new smartphones offer access to Facebook services through either their Web browsers or applications. An official Facebook application is available for the operating systems Android, iOS, webOS, and Firefox OS. Nokia and Research In Motion both provide Facebook applications for their own mobile devices. As of January 2015, 745 million active users access Facebook through mobile devices every day. Sherr, Ian (January 28, 2015). "Facebook mobile users hit new highs, revenue jumps". Cnet. Retrieved March 2, 2015.

In May 2014, Facebook introduced a feature to allow users to ask for information not disclosed by other users on their profiles. If a user does not provide key information, such as location, hometown, or relationship status, other users can use a new 'ask' button to send a message asking about that item to the user in a single click.[212]

News Feed
On September 6, 2006, News Feed was announced, which appears on every user's homepage and highlights information including profile changes, upcoming events, and birthdays of the user's friends.[213] This enabled spammers and other users to manipulate these features by creating illegitimate events or posting fake birthdays to attract attention to their profile or cause.[214] Initially, the News Feed caused dissatisfaction among Facebook users; some complained it was too cluttered and full of undesired information, others were concerned that it made it too easy for others to track individual activities (such as relationship status changes, events, and conversations with other users).[215]

In response, Zuckerberg issued an apology for the site's failure to include appropriate customizable privacy features. Since then, users have been able to control what types of information are shared automatically with friends. Users are now able to prevent user-set categories of friends from seeing updates about certain types of activities, including profile changes, Wall posts, and newly added friends.[216]

On February 23, 2010, Facebook was granted a patent[217] on certain aspects of its News Feed. The patent covers News Feeds in which links are provided so that one user can participate in the same activity of another user.[218] The patent may encourage Facebook to pursue action against websites that violate its patent, which may potentially include websites such as Twitter.[219]

One of the most popular applications on Facebook is the Photos application, where users can upload albums and photos.[220] Facebook allows users to upload an unlimited number of photos, compared with other image hosting services such as Photobucket and Flickr, which apply limits to the number of photos that a user is allowed to upload. During the first years, Facebook users were limited to 60 photos per album. As of May 2009, this limit has been increased to 200 photos per album.[221][222][223][224]

Privacy settings can be set for individual albums, limiting the groups of users that can see an album. For example, the privacy of an album can be set so that only the user's friends can see the album, while the privacy of another album can be set so that all Facebook users can see it. Another feature of the Photos application is the ability to "tag", or label, users in a photo. For instance, if a photo contains a user's friend, then the user can tag the friend in the photo. This sends a notification to the friend that they have been tagged, and provides them a link to see the photo.[225]

On June 7, 2012, Facebook launched its App Center to its users. It will help the users in finding games and other applications with ease.[226] Since the launch of the App Center, Facebook has seen 150M monthly users with 2.4 times the installation of apps.[227]

The sorting and display of stories in a user's News Feed is governed by the EdgeRank algorithm.[228]

On May 13, 2015 Facebook in association with major news portals launched a program "Instant Articles" to provide rich news experience. Instant articles provides users, access to articles on Facebook news feed without leaving the site.[229][230]

According to the technology news web site Gizmo on May 9, 2016, Facebook curators routinely suppresses or promotes news that is deemed to meet a political agenda. For example articles about Black Lives Matter would be listed even if they did not meet the trending criteria of News Feed. Likewise positive news about conservative political figures were regularly excised from Facebook pages.[231]

Like button
Main article: Facebook like button
The like button is a social networking feature, allowing users to express their appreciation of content such as status updates, comments, photos, and advertisements. It is also a social plug-in of the Facebook Platform – launched on April 21, 2010[232][233] – that enables participating Internet websites to display a similar like button.

The sheriff of Hampton, Virginia, US fired employees who liked the Facebook page of an adversary, and a federal appeals court in Virginia ruled that the US Constitution protects the rights of US citizens to like any Facebook page of their choosing. US Circuit Judge William Traxler likened the practice to displaying a "political sign in one's front yard."[234] Additionally, the United States Court of Appeals for the Second Circuit upheld a National Labor Relations Board decision which found that one employee's "liking" another employee's negative comments about their employer deserved protection under the National Labor Relations Act, alerting employers to proceed with caution when disciplining employees for Facebook activity.[235]

Following a lengthy period of calls from the public to include a dislike button on the Facebook interface, Zuckerberg explained in a Q&A session on December 11, 2014 that his hesitance was due to a concern about a tone of negativity on the platform, whereby users could "shame" others, and he offered the comment option for situations where people were unwilling to use the like function. However, he said, "We're [Facebook] thinking about it [dislike button] … It's an interesting question," and said that he likes the idea of Facebook users being able to express a greater variety of emotions.[107]

Following
On September 14, 2011, Facebook added the ability for users to provide a "Subscribe" button on their page, which allows users to subscribe to public postings by the user without needing to add them as a friend.[236] In conjunction, Facebook also introduced a system in February 2012 to verify the identity of certain accounts.[237]

In December 2012, Facebook announced that because of user confusion surrounding its function, the Subscribe button would be re-labeled as a "Follow" button—making it more similar to other social networks with similar functions.[238]

Comparison with Myspace
The media often compares Facebook to Myspace, but one significant difference between the two Web sites is the level of customization.[239] Another difference is Facebook's requirement that users give their true identity, a demand that MySpace does not make.[240] MySpace allows users to decorate their profiles using HTML and Cascading Style Sheets (CSS), while Facebook allows only plain text.[241] Facebook has a number of features with which users may interact. They include the Wall, a space on every user's profile page that allows friends to post messages for the user to see;[242] Pokes, which allows users to send a virtual "poke" to each other (a notification then tells a user that they have been poked);[243] Photos, that allows users to upload albums and photos;[244] and Status, which allows users to inform their friends of their whereabouts and actions.[245] Facebook also allows users to tag various people in photographs. Depending on privacy settings, anyone who can see a user's profile can also view that user's Wall. In July 2007, Facebook began allowing users to post attachments to the Wall, whereas the Wall was previously limited to textual content only.[242] Facebook also differs from Myspace in the form of advertising used. Facebook uses advertising in the form of banner ads, referral marketing, and games. Myspace, on the other hand, uses Google and AdSense.[246] There is also a difference in the userbase of each site. MySpace, initially, was much more popular with high school students, while Facebook was more popular among college students. A study by the American firm Nielsen Claritas showed that Facebook users are more inclined to use other professional networking sites, such as LinkedIn, than Myspace users.[246]

Privacy
Facebook enables users to choose their own privacy settings and choose who can see specific parts of their profile.[247] The website is free to its users and generates revenue from advertising, such as banner ads.[248] Facebook requires a user's name and profile picture (if applicable) to be accessible by everyone. Users can control who sees other information they have shared, as well as who can find them in searches, through their privacy settings.[249]

On November 6, 2007, Facebook launched Facebook Beacon, which was a part of Facebook's advertisement system until it was discontinued in 2009. Its purpose was to allow targeted advertisements and allowing users to share their activities with their friends.

In 2010, Facebook's security team began expanding its efforts to reduce the risks to users' privacy,[250] but privacy concerns remain.[251]

Since 2010, among other social media services, the National Security Agency has been taking publicly posted profile information from users Facebook profiles to discover who they interact with.[252]

On November 29, 2011, Facebook settled Federal Trade Commission charges that it deceived consumers by failing to keep privacy promises.[253]

In August 2013 High-Tech Bridge published a study showing that links included in Facebook messaging service messages were being accessed by Facebook.[254] In January 2014 two users filed a lawsuit against Facebook alleging that their privacy had been violated by this practice.[255]

Facebook Bug Bounty Program
On July 29, 2011, Facebook announced its Bug Bounty Program in which security researchers will be paid a minimum of $500 for reporting security holes on Facebook website. Facebook's Whitehat page for security researchers says: "If you give us a reasonable time to respond to your report before making any information public and make a good faith effort to avoid privacy violations, destruction of data, and interruption or degradation of our service during your research, we will not bring any lawsuit against you or ask law enforcement to investigate you."[256][257]


A Facebook "White Hat" debit card, given to researchers who report security bugs
Facebook started paying researchers who find and report security bugs by issuing them custom branded "White Hat" debit cards that can be reloaded with funds each time the researchers discover new flaws. "Researchers who find bugs and security improvements are rare, and we value them and have to find ways to reward them," Ryan McGeehan, former manager of Facebook's security response team, told CNET in an interview. "Having this exclusive black card is another way to recognize them. They can show up at a conference and show this card and say 'I did special work for Facebook.'"[258]

India, which has the second largest number of bug hunters in the world,[259] tops the Facebook Bug Bounty Program with the largest number of valid bugs. "Researchers in Russia earned the highest amount per report in 2013, receiving an average of $3,961 for 38 bugs. India contributed the largest number of valid bugs at 136, with an average reward of $1,353. The U.S. reported 92 issues and averaged $2,272 in rewards. Brazil and the UK were third and fourth by volume, with 53 bugs and 40 bugs, respectively, and average rewards of $3,792 and $2,950", Facebook quoted in a post.[260]

Reception

Most popular social networking sites by country
  Facebook
  Twitter
  VKontakte
  QZone
  Odnoklassniki
  Facenama
  no data
According to comScore, Facebook is the leading social networking site based on monthly unique visitors, having overtaken main competitor MySpace in April 2008.[261] ComScore reports that Facebook attracted 130 million unique visitors in May 2010, an increase of 8.6 million people.[262] According to third-party web analytics providers, Alexa and SimilarWeb, Facebook is ranked second and first globally respectively, it is the highest-read social network on the Web, with over 20 billion visitors per month, as of 2015.[263][264][265] SimilarWeb, Quantcast and Compete.com all rank the website 2nd in the U.S. in traffic.[264][266][267] The website is the most popular for uploading photos, with 50 billion uploaded cumulatively.[268] In 2010, Sophos's "Security Threat Report 2010" polled over 500 firms, 60% of which responded that they believed that Facebook was the social network that posed the biggest threat to security, well ahead of MySpace, Twitter, and LinkedIn.[250]

Facebook is the most popular social networking site in several English-speaking countries, including Canada,[269] the United Kingdom,[270] and the United States.[271][272][273][274] However, Facebook still receives limited adoption in countries such as Japan, where domestically created social networks are still largely preferred.[275] In regional Internet markets, Facebook penetration is highest in North America (69 percent), followed by Middle East-Africa (67 percent), Latin America (58 percent), Europe (57 percent), and Asia-Pacific (17 percent).[276] Some of the top competitors were listed in 2007 by Mashable.[277]

The website has won awards such as placement into the "Top 100 Classic Websites" by PC Magazine in 2007,[278] and winning the "People's Voice Award" from the Webby Awards in 2008.[279] In a 2006 study conducted by Student Monitor, a company specializing in research concerning the college student market, Facebook was named the second most popular thing among undergraduates, tied with beer and only ranked lower than the iPod.[280]

In 2010, Facebook won the Crunchie "Best Overall Startup Or Product" for the third year in a row[281] and was recognized as one of the "Hottest Silicon Valley Companies" by Lead411.[282] However, in a July 2010 survey performed by the American Customer Satisfaction Index, Facebook received a score of 64 out of 100, placing it in the bottom 5% of all private-sector companies in terms of customer satisfaction, alongside industries such as the IRS e-file system, airlines, and cable companies. The reasons why Facebook scored so poorly include privacy problems, frequent changes to the website's interface, the results returned by the News Feed, and spam.[283]

Total active users[N 1]
Date	Users
(in millions)	Days later	Monthly growth[N 2]
February 4, 2004	0	—	—
August 26, 2008	100[284]	1,665	178.38%
April 8, 2009	200[285]	225	13.33%
September 15, 2009	300[286]	160	9.38%
February 5, 2010	400[287]	143	6.99%
July 21, 2010	500[288]	166	4.52%
January 5, 2011	600[289][N 3]	168	3.57%
May 30, 2011	700[290]	145	3.45%
September 22, 2011	800[291]	115	3.73%
April 24, 2012	900[292]	215	1.74%
September 14, 2012	1,000[293]	143	2.33%
March 31, 2013	1,110[294]	198	1.5%
December 31, 2013	1,230[295]	275	0.97%
December 31, 2014	1,390[296]	365	0.64%
In December 2008, the Supreme Court of the Australian Capital Territory ruled that Facebook is a valid protocol to serve court notices to defendants. It is believed to be the world's first legal judgement that defines a summons posted on Facebook as legally binding.[297] In March 2009, the New Zealand High Court associate justice David Gendall allowed for the serving of legal papers on Craig Axe by the company Axe Market Garden via Facebook.[298][299] Employers have also used Facebook as a means to keep tabs on their employees and have even been known to fire them over posts they have made.[300]

By 2005, the use of Facebook had already become so ubiquitous that the generic verb "facebooking" had come into use to describe the process of browsing others' profiles or updating one's own.[301] In 2008, Collins English Dictionary declared "Facebook" as its new Word of the Year.[302] In December 2009, the New Oxford American Dictionary declared its word of the year to be the verb "unfriend", defined as "To remove someone as a 'friend' on a social networking site such as Facebook.[303]

In early 2010, Openbook was established, an avowed parody (and privacy advocacy) website[304] that enables text-based searches of those Wall posts that are available to "Everyone", i.e. to everyone on the Internet.

Writers for The Wall Street Journal found in 2010 that Facebook apps were transmitting identifying information to "dozens of advertising and Internet tracking companies". The apps used an HTTP referer which exposed the user's identity and sometimes their friends'. Facebook said, "We have taken immediate action to disable all applications that violate our terms".[305]

In May 2014, the countries with the most Facebook users were:[306]

United States with 151.8 million members
India with 108.9 million members
Brazil with 70.5 million members
Indonesia with 60.3 million members
Mexico with 44.4 million members
Facebook's popularity throughout the world (especially as a tool for political movements or procrastination) has led to some countries and employers blocking access to the site.

All of the above total 309 million members or about 38.6 percent of Facebook's 1 billion worldwide members.[307] As of March 2013, Facebook reported having 1.11 billion monthly active users, globally.[308]

In regards to Facebook's mobile usage, per an analyst report in early 2013, there are 192 million Android users, 147 million iPhone users, 48 million iPad users and 56 million messenger users, and a total of 604 million mobile Facebook users.[309] In January 2016, Facebook Messenger hits 800 million users.[310]


Facebook popularity. Active users of Facebook increased from just a million in 2004 to over 750 million in 2011.[311]
 

Population pyramid of Facebook users by age as of January 1, 2010[312]
Criticisms and controversies
Main article: Criticism of Facebook
Electricity usage
On April 21, 2011, Greenpeace released a report showing that of the top ten big brands in cloud computing, Facebook was most reliant on coal for electricity for its data centers. At the time, data centers consumed up to 2% of all global electricity and this amount was projected to increase. Phil Radford of Greenpeace said "we are concerned that this new explosion in electricity use could lock us into old, polluting energy sources instead of the clean energy available today."[313] On December 15, 2011, Greenpeace and Facebook announced together that Facebook would shift to use clean and renewable energy to power its own operations. Marcy Scott Lynn, of Facebook's sustainability program, said it looked forward "to a day when our primary energy sources are clean and renewable" and that the company is "working with Greenpeace and others to help bring that day closer."[314][315]

Google
In May 2011 emails were sent to journalists and bloggers making critical allegations about Google's privacy policies; however it was later discovered that PR giant Burson-Marsteller was paid for the emails by Facebook .[316]

Users violating minimum age requirements
A 2011 study in the online journal First Monday, examines how parents consistently enable children as young as 10 years old to sign up for accounts, directly violating Facebook's policy banning young visitors. This policy is in compliance with a United States law, the 1998 Children's Online Privacy Protection Act, which requires minors aged 13 or younger to gain explicit parental consent to access commercial websites. In other jurisdictions where a similar law sets a lower minimum age, Facebook enforces the lower age. Of the 1,007 households surveyed for the study, 76% of parents reported that their child joined Facebook when they were younger than 13, the minimum age in the site's terms of service. The study also reported that Facebook removes roughly 20,000 users each day for violating its minimum age policy. The study's authors also note, "Indeed, Facebook takes various measures both to restrict access to children and delete their accounts if they join." The findings of the study raise questions primarily about the shortcomings of United States federal law, but also implicitly continue to raise questions about whether or not Facebook does enough to publicize its terms of service with respect to minors. Only 53% of parents said they were aware that Facebook has a minimum signup age; 35% of these parents believe that the minimum age is merely a recommendation, or thought the signup age was 16 or 18, and not 13.[317]

Accounts hacked in Bangalore, India
In November 2011, several Facebook users in Bangalore, India reported that their accounts had been hacked and their profile pictures replaced with pornographic images. For more than a week, users' news feeds were spammed with pornographic, violent and sexual content, and it was reported that more than 200,000 accounts were affected. Facebook described the reports as inaccurate, and Bangalore police speculated that the stories may have been rumors spread by Facebook's competitors.[318][319]

Unauthorized wall posting bug
On August 19, 2013, it was reported that a Facebook user from Yatta, West Bank[320][321] Khalil Shreateh had found a bug that allowed him to post material to other users' Facebook Walls. Users are not supposed to have the ability to post material to the Facebook Walls of other users unless they are approved friends of those users that they have posted material to. To prove that he was telling the truth, Shreateh posted material to Sarah Goodin's wall, a friend of Facebook CEO Mark Zuckerberg. Following this, Shreateh contacted Facebook's security team with the proof that his bug was real, explaining in detail what was going on. Facebook has a bounty program in which it compensates people a US$500 fee for reporting bugs instead of using them to their advantage or selling them on the black market. However, it was reported that instead of fixing the bug and paying Shreateh the fee, Facebook originally told him that "this was not a bug" and dismissed him. Shreateh then tried a second time to inform Facebook, but they dismissed him yet again. On the third try, Shreateh used the bug to post a message to Mark Zuckerberg's Wall, stating "Sorry for breaking your privacy … but a couple of days ago, I found a serious Facebook exploit" and that Facebook's security team was not taking him seriously. Within minutes, a security engineer contacted Shreateh, questioned him on how he performed the move and ultimately acknowledged that it was a bug in the system. Facebook temporarily suspended Shreateh's account and fixed the bug after several days. Facebook refused to pay out the bounty to Shreateh, stating that by posting to Zuckerberg's account, Shreateh had violated one of their terms of service policies and therefore "could not be paid." Facebook also noted that in Shreateh's initial reports, he had failed to provide technical details for Facebook to act on the bug.[322][323][324]

On August 22, 2013, Yahoo News reported that Marc Maiffret, a chief technology officer of the cybersecurity firm BeyondTrust, is prompting hackers to support in raising a $10,000 reward for Khalil Shreateh. On August 20, Maiffret stated that he had already raised $9,000 in his efforts, including the $2,000 he himself contributed. He and other hackers alike have denounced Facebook for refusing Shreateh compensation. Stated Maiffret, "He is sitting there in Palestine doing this research on a five-year-old laptop that looks like it is half broken. It's something that might help him out in a big way." Facebook representatives have since responded, "We will not change our practice of refusing to pay rewards to researchers who have tested vulnerabilities against real users." Facebook representatives also claimed they'd paid out over $1 million to individuals who have discovered bugs in the past.[325]

Users quitting
A 2013 study examined the reasons users eventually quit the site. It found the most common reasons were privacy concerns (48%), general dissatisfaction with Facebook (14%), negative aspects regarding Facebook friends (13%) and the feeling of getting addicted to Facebook (6%). Facebook quitters were found to be more concerned about privacy, more addicted to the Internet and more conscientious.[251]

iPhone 'Paper' app
Following the release of the Facebook iPhone app "Paper" at the beginning of February 2014, developer company FiftyThree sent a correspondence to the social media company regarding its own app, also entitled Paper and trademarked in 2012, asking Facebook to cease using an app name that they consider their own. In response, Facebook stated that it will continue to use the Paper title but conceded that it should have informed FiftyThree at an earlier point in time. FiftyThree articulated its desired outcome in a blog post: "There's a simple fix here. We think Facebook can apply the same degree of thought they put into the app into building a brand name of their own. An app about stories shouldn't start with someone else's story. Facebook should stop using our brand name."[326][327]

Lane v. Facebook, Inc.
On March 2010, Judge Richard Seeborg issued an order approving the class settlement in Lane v. Facebook, Inc.[328] This lawsuit charged that user's private information was being posted on Facebook without consent using Facebook's Beacon program.

User influence experiments
Academic and Facebook researchers have collaborated to test if the messages people see on Facebook can influence their behavior. For instance, in "A 61-Million-Person Experiment in Social Influence And Political Mobilization," during the 2010 elections, Facebook users were given the opportunity to "tell your friends you voted" by clicking on an "I voted" button. Users were 2% more likely to click the button if it was associated with friends who had already voted.[329]

Much more controversially, a 2014 study of "Emotional Contagion Through Social Networks" manipulated the balance of positive and negative messages seen by 689,000 Facebook users.[330] The researchers concluded that they had found "some of the first experimental evidence to support the controversial claims that emotions can spread throughout a network, [though] the effect sizes from the manipulations are small."[331]

Unlike the "I voted" study, which had presumptively beneficial ends and raised few concerns, this study was criticized for both its ethics and methods/claims. As controversy about the study grew, Adam Kramer, a lead author of both studies and member of the Facebook data team, defended the work in a Facebook update. A few days later, Sheryl Sandburg, Facebook's COO, made a statement while traveling abroad. While at an Indian Chambers of Commerce event in New Delhi she stated that "This was part of ongoing research companies do to test different products, and that was what it was. It was poorly communicated and for that communication we apologize. We never meant to upset you."[332]

Shortly thereafter, on July 3, 2014, USA Today reported that the privacy watchdog group Electronic Privacy Information Center (EPIC) had filed a formal complaint with the Federal Trade claiming that Facebook had broken the law when it conducted the study on the emotions of its users without their knowledge or consent. In its complaint, EPIC alleged that Facebook had deceived it users by secretly conducting a psychological experiment on their emotions: "At the time of the experiment, Facebook did not state in the Data Use Policy that user data would be used for research purposes. Facebook also failed to inform users that their personal information would be shared with researchers."[333]

Beyond the ethical concerns, other scholars criticized the methods and reporting of the study's findings. John Grohol, writing at PsycCentral, argued that despite its title and claims of "emotional contagion," this study did not look at emotions at all. Instead, its authors used an application (called "Linguistic Inquiry and Word Count" or LIWC 2007) that simply counted positive and negative words in order to infer users' sentiments. He wrote that a shortcoming of the LIWC tool is that it does not understand negations. Hence, the tweet "I am not happy" would be scored as positive: "Since the LIWC 2007 ignores these subtle realities of informal human communication, so do the researchers." Grohol concluded that given these subtleties, the effect size of the findings are little more than a "statistical blip."

Kramer et al. (2014) found a 0.07% — that's not 7 percent, that's 1/15th of one percent!! — decrease in negative words in people's status updates when the number of negative posts on their Facebook news feed decreased. Do you know how many words you'd have to read or write before you've written one less negative word due to this effect? Probably thousands.[334]

The consequences of the controversy are pending (be it FTC or court proceedings) but it did prompt an "Editorial Expression of Concern" from its publisher, the Proceedings of the National Academy of Sciences, as well as an blog posting from OkCupid that "We experiment on human beings!" In September 2014, law professor James Grimmelmann argued that the actions of both companies were "illegal, immoral, and mood-altering" and filed notices with the Maryland Attorney General and Cornell Institutional Review Board.[335]

In the UK, the study was also criticised by the British Psychological Society which said, in a letter to The Guardian, "There has undoubtedly been some degree of harm caused, with many individuals affected by increased levels of negative emotion, with consequent potential economic costs, increase in possible mental health problems and burden on health services. The so-called 'positive' manipulation is also potentially harmful."[336]

Real-name policy controversy and compromise
Main article: Facebook real-name policy controversy
Facebook has a real-name system policy for user profiles. The real-name policy stems from the position "that way, you always know who you're connecting with. This helps keep our community safe."[337][338] Facebook's real-name system does not allow adopted names or pseudonyms, and in its enforcement has suspended accounts of legitimate users, until the user provides identification indicating the name. Facebook representatives have described these incidents as very rare.[339] A user claimed responsibility via the anonymous Android and iOS app Secret for reporting "fake names" which caused user profiles to be suspended, specifically targeting the stage names of drag queens.[340] On October 1, 2014, Chris Cox, Chief Product Officer at Facebook, offered an apology: "In the two weeks since the real-name policy issues surfaced, we've had the chance to hear from many of you in these communities and understand the policy more clearly as you experience it. We've also come to understand how painful this has been. We owe you a better service and a better experience using Facebook, and we're going to fix the way this policy gets handled so everyone affected here can go back to using Facebook as you were."[341]

On December 15, 2015, Facebook announced in a press release[342] that it would be providing a compromise to its real name policy after protests from groups such as the gay/lesbian community and abuse-victims.[343] The site is developing a protocol that will allow members to provide specifics as to their "special circumstance" or "unique situation" with a request to use pseudonyms, subject to verification of their true identities. At that time, this was already being tested in the U.S. Product manager Todd Gage and vice president of global operations Justin Osofsky also promised a new method for reducing the number of members who must go through ID verification while ensuring the safety of others on Facebook. The fake name reporting procedure will also be modified, forcing anyone who makes such an allegation to provide specifics that would be investigated and giving the accused individual time to dispute the allegation.[344]

"Free basics" controversy in India
In February 2016, TRAI ruled against differential data pricing for limited services from mobile phone operators effectively ending zero-rating platforms in India. Zero rating provides access to limited number of websites for no charge to the end user. Net-neutrality supporters from India(SaveTheInternet.in) brought out the negative implications of Facebook Free Basic program and spread awareness to the public.[345] Facebook’s Free Basics program[346] was a collaboration with Reliance Communications to launch Free Basics in India. The TRAI ruling against differential pricing marked the end of Free Basics in India.[347]

Earlier, Facebook had spent $44 million USD in advertising and it implored all of its Indian users to send an email to the Telecom Regulatory Authority to support its program.[348] TRAI later asked Facebook to provide specific responses from the supporters of Free Basics[349][350]

Safety Check bug
On March 27, 2016, following a bombing in Lahore, Pakistan, Facebook activated its "Safety Check" feature, which allows people to let friends and loved ones know they are okay following a crisis or natural disaster, to people who were never in danger, or even close to the Pakistan explosion. Some users as far as the US, UK and Egypt received notifications asking if they were okay.[351][352]

Impact

Facebook on the Ad-tech 2010
Media impact
In April 2011, Facebook launched a new portal for marketers and creative agencies to help them develop brand promotions on Facebook.[353] The company began its push by inviting a select group of British advertising leaders to meet Facebook's top executives at an "influencers' summit" in February 2010. Facebook has now been involved in campaigns for True Blood, American Idol, and Top Gear.[354] News and media outlets such as the Washington Post,[355] Financial Times[356] and ABC News[357] have used aggregated Facebook fan data to create various infographics and charts to accompany their articles. In 2012, the beauty pageant Miss Sri Lanka Online was run exclusively using Facebook.[358]

Social impact
Main articles: Social networking service  Social impact and Social impact of the Internet  Social networking and entertainment
Facebook has affected the social life and activity of people in various ways. Facebook allows people using computers or mobile phones to continuously stay in touch with friends, relatives and other acquaintances wherever they are in the world, as long as there is access to the Internet. It has reunited lost family members and friends.[359][360] It allows users to trade ideas, stay informed with local or global developments, and unite people with common interests and/or beliefs through open, closed and private groups and other pages.[361][362]

Facebook's social impact has also changed how people communicate. Rather than having to reply to others through email, Facebook allows users to broadcast or share content to others, and thereby to engage others or be engaged with others' posts.[363]

Facebook has been successful and more socially impactful than many other social media sites. David Kirkpatrick, technology journalist and author of The Facebook Effect, believes that Facebook is structured in a way that is not easily replaceable. He challenges users to consider how difficult it would be to move all the relationships and photos to an alternative. Facebook has let people participate in an atmosphere with the "over the backyard fence quality" of a small town, despite the move to larger cities.[364]

Emotional health impact
Recent studies have shown that Facebook causes negative effects on self-esteem by triggering feelings of envy, with vacation and holiday photos proving to be the largest resentment triggers. Other prevalent causes of envy include posts by friends about family happiness and images of physical beauty—such envious feelings leave people lonely and dissatisfied with their own lives. A joint study by two German universities discovered that one out of three people were more dissatisfied with their lives after visiting Facebook, and another study by Utah Valley University found that college students felt worse about their own lives following an increase in the amount of time spent on Facebook.[365][366][367]

According to professor of psychology Susan Krauss Whitbourne, although Facebook has an upside of friending people, there is also the downside of having someone unfriend or reject another person.[368] Whitbourne refers to unfriended persons on Facebook as victims of estrangement.[368] Unfriending someone is seldom a mutual decision and the person often does not know they have been unfriended.[368]

Political impact
Further information: Social media and political communication in the United States

A man during the 2011 Egyptian protests carrying a card saying "Facebook,#jan25, The Egyptian Social Network".
In February 2008, a Facebook group called "One Million Voices Against FARC" organized an event in which hundreds of thousands of Colombians marched in protest against the Revolutionary Armed Forces of Colombia, better known as the FARC (from the group's Spanish name).[369] In August 2010, one of North Korea's official government websites and the official news agency of the country, Uriminzokkiri, joined Facebook.[370]

During the Arab Spring many journalists made claims that Facebook played a major role in generating the 2011 Egyptian revolution.[371][372] On January 14, the Facebook page of "We are all khaled Said" was started by Wael Ghoniem Create Event to invite the Egyptian people to "peaceful demonstrations" on January 25. According to Mashable,[unreliable source?] in Tunisia and Egpyt, Facebook became the primary tool for connecting all protesters and led the Egyptian government of Prime Minister Nazif to ban Facebook, Twitter and another websites on January 26[373] then ban all mobile and Internet connections for all of Egypt at midnight January 28. After 18 days, the uprising forced President Mubarak to resign.

In Bahrain uprising which started on February 14, 2011, Facebook was utilized by the Bahraini regime as well as regime loyalists to identify, capture and prosecute citizens involved in the protests. A 20-year-old girl named Ayat Al Qurmezi was identified as a protester using Facebook, taken from her home by masked commandos and put in prison.[374]

In 2011, Facebook filed paperwork with the Federal Election Commission to form a political action committee under the name FB PAC.[375] In an email to The Hill, a spokesman for Facebook said "Facebook Political Action Committee will give our employees a way to make their voice heard in the political process by supporting candidates who share our goals of promoting the value of innovation to our economy while giving people the power to share and make the world more open and connected."[376]

During the Syrian civil war, the YPG, a libertarian army for Rojava has recruited westerners through Facebook in its fight against ISIL.[377][378] Dozens have joined its ranks for various reasons from religious to ideological. The Facebook page's name "The Lions of Rojava" comes from a Kurdish saying which translates as "A lion is a lion, whether it's a female or a male", reflecting the organisation's feminist ideology.[379]

United States
Facebook's role in the American political process was demonstrated in January 2008, shortly before the New Hampshire primary, when Facebook teamed up with ABC and Saint Anselm College to allow users to give live feedback about the "back to back" January 5 Republican and Democratic debates.[380][381][382] Facebook users took part in debate groups organized around specific topics, register to vote, and message questions.[383]

Over a million people installed the Facebook application "US Politics on Facebook" in order to take part, and the application measured users' responses to specific comments made by the debating candidates.[384] This debate showed the broader community what many young students had already experienced: Facebook as a popular and powerful new way to interact and voice opinions. A poll by CBS News, UWIRE and The Chronicle of Higher Education claimed to illustrate how the "Facebook effect" has affected youth voting rates, support by youth of political candidates, and general involvement by the youth population in the 2008 election.[385]

The new social media, such as Facebook and Twitter, made use first of the personal computer and the Internet, and after 2010 of the smart phones to connect hundreds of millions of people, especially those under age 35. By 2008, politicians and interest groups were experimenting with systematic use of social media to spread their message among much larger audiences than they had previously reached.[386][387]

Facebook is having an impact on local government as well. Justin Smith, a Colorado sheriff uses Facebook to disseminate his ideas on matters relating to local, state, and national concerns. He also publicizes crimes, particularly those that his department solves. He has seven thousand followers on the social medium, considered a large number. Smith said that he rarely goes out in public "when I don't get feedback from folks. … Facebook is an interesting tool because I think it holds candidates and elected officials more accountable. Voters know where someone stands."[388]

As American political strategists turn their attention to the 2016 presidential contest, they identify Facebook as an increasingly important advertising tool. Recent technical innovations have made possible more advanced divisions and subdivisions of the electorate. Most important, Facebook can now deliver video ads to small, highly targeted subsets. Television, by contrast, shows the same commercials to all viewers, and so cannot be precisely tailored.[389]

Ban
In many countries the social networking sites and mobile apps have been blocked temporarily or permanently. In Bangladesh, the government has been blocking Facebook, WhatsApp, Tango, Viber and many other sites and apps since November 18, 2015.[390]