Computer science is the scientific and practical approach to computation and its applications. It is the systematic study of the feasibility, structure, expression, and 
mechanization of the methodical procedures (or algorithms) that underlie the acquisition, representation, processing, storage, communication of, and access to 
information. An alternate, more succinct definition of computer science is the study of automating algorithmic processes that scale. A computer scientist specializes in the 
theory of computation and the design of computational systems.[1]
Its fields can be divided into a variety of theoretical and practical disciplines. Some fields, such as computational complexity theory (which explores the fundamental 
properties of computational and intractable problems), are highly abstract, while fields such as computer graphics emphasize real world visual applications. Still other 
fields focus on challenges in implementing computation. For example, programming language theory considers various approaches to the description of computation, 
while the study of computer programming itself investigates various aspects of the use of programming language and complex systems. Human computer interaction 
considers the challenges in making computers and computations useful, usable, and universally accessible to humans.
Personal computer
From Wikipedia, the free encyclopedia
A personal computer (PC) is a general-purpose computer whose size, capabilities, and original sale price make it useful for individuals, and is intended to be operated directly by an end-user with no intervening computer time-sharing models that allowed larger, more expensive minicomputer and mainframe systems to be used by many people, usually at the same time.

Software applications for most personal computers include, but are not limited to, word processing, spreadsheets, databases, web browsers and e-mail clients, digital media playback, games and many personal productivity and special-purpose software applications. Modern personal computers often have connections to the Internet, allowing access to the World Wide Web and a wide range of other resources. Personal computers may be connected to a local area network (LAN), either by a cable or a wireless connection. A personal computer may be a laptop computer or a desktop computer running an operating system such as Windows, Linux (and the various operating systems based on it), or Macintosh OS.

Early computer owners usually had to write their own programs to do anything useful with the machines, which even did not include an operating system. The very earliest microcomputers, equipped with a front panel, required hand-loading of a bootstrap program to load programs from external storage (paper tape, cassettes, or eventually diskettes). Before very long, automatic booting from permanent read-only memory became universal. Today's users have access to a wide range of commercial software, freeware and free and open-source software, which are provided in ready-to-run or ready-to-compile form. Software for personal computers, such as applications and video games, are typically developed and distributed independently from the hardware or OS manufacturers, whereas software for many mobile phones and other portable systems is approved and distributed through a centralized online store.[1][2]

Since the early 1990s, Microsoft operating systems and Intel hardware dominated much of the personal computer market, first with MS-DOS and then with Windows. Popular alternatives to Microsoft's Windows operating systems include Apple's OS X and free open-source Unix-like operating systems such as Linux and BSD. AMD provides the major alternative to Intel's processors. ARM architecture processors now outnumber Intel's (and compatibles) in smartphones and tablets, that are also personal computers, outnumbering the traditional kind.

Contents  [hide] 
1  History
1.1 Market and sales
1.2 Average selling price
2 Terminology
3 Types
3.1 Stationary
3.1.1 Workstation
3.1.2 Desktop computer
3.1.2.1 Gaming computer
3.1.2.2 Single unit
3.1.3 Nettop
3.1.4 Home theater PC
3.2 Portable
3.2.1 Laptop
3.2.1.1 Desktop replacement
3.2.2 Netbook
3.2.3 Tablet
3.2.4 Ultra-mobile PC
3.2.5 Pocket PC
4 Hardware
4.1 Computer case
4.2 Power supply unit
4.3 Processor
4.4 Motherboard
4.5 Main memory
4.6 Hard disk
4.7 Visual display unit
4.8 Video card
4.9 Keyboard
4.10  Mouse
4.11  Other components
5 Software
5.1 Operating system
5.1.1 Microsoft Windows
5.1.2 OS X
5.1.3 Linux
5.2 Applications
5.3 Gaming
6 Toxicity
6.1 Electronic waste regulation
7 See also
8 Notes
9 References
10  Further reading
11  External links
History[edit]
Main article: History of personal computers
The Programma 101 was the first commercial "desktop personal computer", produced by the Italian company Olivetti and invented by the Italian engineer Pier Giorgio Perotto, inventor of the magnetic card system. The project started in 1962. It was launched at the 1964 New York World's Fair, and volume production began in 1965, the computer retailing for  3,200.[3][unreliable source ]

NASA bought at least ten Programma 101s and used them for the calculations for the 1969 Apollo 11 Moon landing. The ABC Network used the Programma 101 to predict the presidential election of 1968, and the U.S. military used the machine to plan their operations in the Vietnam War. The Programma 101 was used in schools, hospitals, government offices. This marked the beginning of the era of the personal computer.

In 1968, Hewlett-Packard was ordered to pay about  900,000 in royalties to Olivetti after their Hewlett-Packard 9100A was ruled to have copied some of the solutions adopted in the Programma 101, including the magnetic card, the architecture and other similar components.[3]

The Soviet MIR series of computers was developed from 1965 to 1969 in a group headed by Victor Glushkov. It was designed as a relatively small-scale computer for use in engineering and scientific applications and contained a hardware implementation of a high-level programming language. Another innovative feature for that time was the user interface combining a keyboard with a monitor and light pen for correcting texts and drawing on screen.[4]

In what was later to be called the Mother of All Demos, SRI researcher Douglas Engelbart in 1968 gave a preview of what would become the staples of daily working life in the 21st century: e-mail, hypertext, word processing, video conferencing and the mouse. The demonstration required technical support staff and a mainframe time-sharing computer that were far too costly for individual business use at the time.


Commodore PET in 1983 (at American Museum of Science and Energy)
By the early 1970s, people in academic or research institutions had the opportunity for single-person use of a computer system in interactive mode for extended durations, although these systems would still have been too expensive to be owned by a single person.

Early personal computers   generally called microcomputers   were often sold in a kit form and in limited volumes, and were of interest mostly to hobbyists and technicians. Minimal programming was done with toggle switches to enter instructions, and output was provided by front panel lamps. Practical use required adding peripherals such as keyboards, computer displays, disk drives, and printers. Micral N was the earliest commercial, non-kit microcomputer based on a microprocessor, the Intel 8008. It was built starting in 1972 and about 90,000 units were sold.

In 1973 the IBM Los Gatos Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT and full function keyboard. SCAMP emulated an IBM 1130 minicomputer in order to run APL\1130.[5] In 1973 APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because SCAMP was the first to emulate APL\1130 performance on a portable, single user computer, PC Magazine in 1983 designated SCAMP a "revolutionary concept" and "the world's first personal computer".[5][6] This seminal, single user portable computer now resides in the Smithsonian Institution, Washington, D.C.. Successful demonstrations of the 1973 SCAMP prototype led to the IBM 5100 portable microcomputer launched in 1975 with the ability to be programmed in both APL and BASIC for engineers, analysts, statisticians and other business problem-solvers. In the late 1960s such a machine would have been nearly as large as two desks and would have weighed about half a ton.[5]

Another seminal product in 1973 was the Xerox Alto, developed at Xerox's Palo Alto Research Center (PARC), it had a graphical user interface (GUI) which later served as inspiration for Apple Computer's Macintosh, and Microsoft's Windows operating system. Also in 1973 Hewlett Packard introduced fully BASIC programmable microcomputers that fit entirely on top of a desk, including a keyboard, a small one-line display and printer. The Wang 2200 microcomputer of 1973 had a full-size cathode ray tube (CRT) and cassette tape storage.[7] These were generally expensive specialized computers sold for business or scientific uses. The introduction of the microprocessor, a single chip with all the circuitry that formerly occupied large cabinets, led to the proliferation of personal computers after 1975.


IBM Personal Computer XT in 1988
In 1976 Steve Jobs and Steve Wozniak sold the Apple I computer circuit board, which was fully prepared and contained about 30 chips. The Apple I computer differed from the other hobby computers of the time at the beckoning of Paul Terrell owner of the Byte Shop who gave Steve Jobs his first purchase order for 50 Apple I computers only if the computers were assembled and tested and not a kit computer so he would have computers to sell to everyone, not just people that could assemble a computer kit. The Apple I as delivered was still a kit computer as it did not have a power supply, case, or keyboard as delivered to the Byte Shop.

The first successfully mass marketed personal computer was the Commodore PET introduced in January 1977, but back-ordered and not available until later in the year.[8] At the same time, the Apple II (usually referred to as the "Apple") was introduced[9] (June 1977), and the TRS-80 from Tandy Corporation / Tandy Radio Shack in summer 1977, delivered in September in a small number. Mass-market ready-assembled computers allowed a wider range of people to use computers, focusing more on software applications and less on development of the processor hardware.


The 8-bit PMD 85 personal computer produced in 1985 1990 by the Tesla company in the former socialist Czechoslovakia. This computer was produced locally (in Pie  any) due to a lack of foreign currency with which to buy systems from the West.

IBM 5150, released in 1981
During the early 1980s, home computers were further developed for household use, with software for personal productivity, programming and games. They typically could be used with a television already in the home as the computer display, with low-detail blocky graphics and a limited color range, and text about 40 characters wide by 25 characters tall. Sinclair Research,[10] a UK company, produced the ZX Series   the ZX80 (1980), ZX81 (1981), and the ZX Spectrum; the latter was introduced in 1982, and totaled 8 million unit sold. Following came the Commodore 64, totaled 17 million units sold.[11][12]

In the same year, the NEC PC-98 was introduced, which was a very popular personal computer that sold in more than 18 million units.[13] Another famous personal computer, the revolutionary Amiga 1000, was unveiled by Commodore on July 23, 1985. The Amiga 1000 featured a multitasking, windowing operating system, color graphics with a 4096-color palette, stereo sound, Motorola 68000 CPU, 256 kB RAM, and 880 kB 3.5-inch disk drive, for US 1,295.[14]

Somewhat larger and more expensive systems (for example, running CP/M), or sometimes a home computer with additional interfaces and devices, although still low-cost compared with minicomputers and mainframes, were aimed at office and small business use, typically using "high resolution" monitors capable of at least 80 column text display, and often no graphical or color drawing capability.

Workstations were characterized by high-performance processors and graphics displays, with large-capacity local disk storage, networking capability, and running under a multitasking operating system.

Eventually, due to the influence of the IBM PC on the personal computer market, personal computers and home computers lost any technical distinction. Business computers acquired color graphics capability and sound, and home computers and game systems users used the same processors and operating systems as office workers. Mass-market computers had graphics capabilities and memory comparable to dedicated workstations of a few years before. Even local area networking, originally a way to allow business computers to share expensive mass storage and peripherals, became a standard feature of personal computers used at home.

In 1982 "The Computer" was named Machine of the Year by Time Magazine.

In the 2010s, several companies such as Hewlett-Packard and Sony sold off their PC and laptop divisions. As a result, the personal computer was declared dead several times during this time.[15]

Market and sales[edit]
See also: Market share of personal computer vendors

Personal computers worldwide in million distinguished by developed and developing world
In 2001, 125 million personal computers were shipped in comparison to 48,000 in 1977.[16] More than 500 million personal computers were in use in 2002 and one billion personal computers had been sold worldwide from the mid-1970s up to this time. Of the latter figure, 75% were professional or work related, while the rest were sold for personal or home use. About 81.5% of personal computers shipped had been desktop computers, 16.4% laptops and 2.1% servers. The United States had received 38.8% (394 million) of the computers shipped, Europe 25% and 11.7% had gone to the Asia-Pacific region, the fastest-growing market as of 2002. The second billion was expected to be sold by 2008.[17] Almost half of all households in Western Europe had a personal computer and a computer could be found in 40% of homes in United Kingdom, compared with only 13% in 1985.[18]

The global personal computer shipments were 350.9 million units in 2010,[19] 308.3 million units in 2009[20] and 302.2 million units in 2008.[21][22] The shipments were 264 million units in the year 2007, according to iSuppli,[23] up 11.2% from 239 million in 2006.[24] In 2004, the global shipments were 183 million units, an 11.6% increase over 2003.[25] In 2003, 152.6 million computers were shipped, at an estimated value of  175 billion.[26] In 2002, 136.7 million PCs were shipped, at an estimated value of  175 billion.[26] In 2000, 140.2 million personal computers were shipped, at an estimated value of  226 billion.[26] Worldwide shipments of personal computers surpassed the 100-million mark in 1999, growing to 113.5 million units from 93.3 million units in 1998.[27] In 1999, Asia had 14.1 million units shipped.[28]

As of June 2008, the number of personal computers in use worldwide hit one billion,[29] while another billion is expected to be reached by 2014. Mature markets like the United States, Western Europe and Japan accounted for 58% of the worldwide installed PCs. The emerging markets were expected to double their installed PCs by 2012 and to take 70% of the second billion PCs. About 180 million computers (16% of the existing installed base) were expected to be replaced and 35 million to be dumped into landfill in 2008. The whole installed base grew 12% annually.[30][31]

Based on International Data Corporation (IDC) data for Q2 2011, for the first time China surpassed US in PC shipments by 18.5 million and 17.7 million respectively. This trend reflects the rising of emerging markets as well as the relative stagnation of mature regions.[32]

In the developed world, there has been a vendor tradition to keep adding functions to maintain high prices of personal computers. However, since the introduction of the One Laptop per Child foundation and its low-cost XO-1 laptop, the computing industry started to pursue the price too. Although introduced only one year earlier, there were 14 million netbooks sold in 2008.[33] Besides the regular computer manufacturers, companies making especially rugged versions of computers have sprung up, offering alternatives for people operating their machines in extreme weather or environments.[34]

2014 worldwide PC vendor unit shipment estimates
Source  Date  Lenovo  HP  Dell  Acer Inc. Asus  Others
IDC[35] Q2 2014 19.6% 18.3% 14.0% 8.2%  6.2%  33.6%
Gartner[36] Q2 2014 19.2% 17.7% 13.3% 7.9%  6.9%  35.0%
In 2011, Deloitte consulting firm predicted that, smartphones and tablet computers as computing devices would surpass the PCs sales[37] (as has happened since 2012). As of 2013, worldwide sales of PCs had begun to fall as many consumers moved to tablets and smartphones for gifts and personal use. Sales of 90.3 million units in the 4th quarter of 2012 represented a 4.9% decline from sales in the 4th quarter of 2011.[38] Global PC sales fell sharply in the first quarter of 2013, according to IDC data. The 14% year-over-year decline was the largest on record since the firm began tracking in 1994, and double what analysts had been expecting.[39][40] The decline of Q2 2013 PC shipments marked the fifth straight quarter of falling sales.[41] "This is horrific news for PCs," remarked an analyst. "It s all about mobile computing now. We have definitely reached the tipping point."[39] Data from Gartner Inc. showed a similar decline for the same time period.[39] China's Lenovo Group bucked the general trend as strong sales to first time buyers in the developing world allowed the company's sales to stay flat overall.[39] Windows 8, which was designed to look similar to tablet/smartphone software, was cited as a contributing factor in the decline of new PC sales. "Unfortunately, it seems clear that the Windows 8 launch not only didn t provide a positive boost to the PC market, but appears to have slowed the market," said IDC Vice President Bob O Donnell.[40]

In August 2013, Credit Suisse published research findings that attributed around 75% of the operating profit share of the PC industry to Microsoft (operating system) and Intel (semiconductors).[42]

According to IDC, in 2013 PC shipments dropped by 9.8% as the greatest drop-ever in line with consumers trends to use mobile devices.[43]

Average selling price[edit]
Selling prices of personal computers steadily declined due to lower costs of production and manufacture, while the capabilities of computers increased. In 1975, an Altair kit sold for only around US  400, but required customers to solder components into circuit boards; peripherals required to interact with the system in alphanumeric form instead of blinking lights would add another  2,000, and the resultant system was only of use to hobbyists.[44]

At their introduction in 1981, the US  1,795 price of the Osborne 1 and its competitor Kaypro was considered an attractive price point; these systems had text-only displays and only floppy disks for storage. By 1982, Michael Dell observed that a personal computer system selling at retail for about  3,000 US was made of components that cost the dealer about  600; typical gross margin on a computer unit was around  1,000.[45] The total value of personal computer purchases in the US in 1983 was about  4 billion, comparable to total sales of pet food. By late 1998, the average selling price of personal computer systems in the United States had dropped below  1,000.[46]

For Microsoft Windows systems, the average selling price (ASP) showed a decline in 2008/2009, possibly due to low-cost netbooks, drawing  569 for desktop computers and  689 for laptops at U.S. retail in August 2008. In 2009, ASP had further fallen to  533 for desktops and to  602 for notebooks by January and to  540 and  560 in February.[47] According to research firm NPD, the average selling price of all Windows portable PCs has fallen from  659 in October 2008 to  519 in October 2009.[48]

Terminology[edit]
"PC" is an initialism for "personal computer". However, it is sometimes used in a different sense, referring to a personal computer with an Intel x86-compatible processor, very often running (but not necessarily limited to) Microsoft Windows, which is a combination sometimes also called Wintel, although large portion of PCs are not shipped with preinstalled Windows operating systems. Some PCs, including the OLPC XOs, are equipped with x86 or x64 processors but not designed to run Microsoft Windows. "PC" is used in contrast with "Mac", an Apple Macintosh computer.[49][50][51][52] This sense of the word is used in the Get a Mac advertisement campaign that ran between 2006 and 2009, as well as its rival, I'm a PC campaign, that appeared in 2008. Since Apple's transition to Intel processors starting 2005, all Macintosh computers are now PCs.[53]

Types[edit]
Stationary[edit]
Workstation[edit]

Sun SPARCstation 1+ from the early 1990s, with a 25 MHz RISC processor
Main article: Workstation
A workstation is a high-end personal computer designed for technical, mathematical, or scientific applications. Intended primarily to be used by one person at a time, they are commonly connected to a local area network and run multi-user operating systems. Workstations are used for tasks such as computer-aided design, drafting and modeling, computation-intensive scientific and engineering calculations, image processing, architectural modeling, and computer graphics for animation and motion picture visual effects.[54]

Desktop computer[edit]
Main article: Desktop computer

A Dell OptiPlex desktop computer
Prior to the widespread usage of PCs, a computer that could fit on a desk was remarkably small, leading to the "desktop" nomenclature. More recently, the phrase usually indicates a particular style of computer case. Desktop computers come in a variety of styles ranging from large vertical tower cases to small models which can be tucked behind an LCD monitor. In this sense, the term "desktop" refers specifically to a horizontally oriented case, usually intended to have the display screen placed on top to save desk space. Most modern desktop computers have separate screens and keyboards.

Gaming computer[edit]
Main article: Gaming computer
A gaming computer is a standard desktop computer that typically has high-performance hardware, such as a more powerful video card, processor and memory, in order to handle the requirements of demanding video games, which are often simply called "PC games".[55] A number of companies, such as Alienware, manufacture prebuilt gaming computers, and companies such as Razer and Logitech market mice, keyboards and headsets geared toward gamers.

Single unit[edit]
Further information: All-in-one computer
Single-unit PCs (also known as all-in-one PCs) are a subtype of desktop computers that combine the monitor and case of the computer within a single unit. The monitor often utilizes a touchscreen as an optional method of user input, but separate keyboards and mice are normally still included. The inner components of the PC are often located directly behind the monitor and many of such PCs are built similarly to laptops.

Nettop[edit]
Main article: Nettop
A subtype of desktops, called nettops, was introduced by Intel in February 2008, characterized by low cost and lean functionality. A similar subtype of laptops (or notebooks) is the netbook, described below. The product line features the new Intel Atom processor, which specifically enables nettops to consume less power and fit into small enclosures.

Home theater PC[edit]
Main article: Home theater PC

An Antec Fusion V2 home theater PC, with a keyboard placed on top of it.
A home theater PC (HTPC) is a convergence device that combines the functions of a personal computer and a digital video recorder. It is connected to a TV set or an appropriately sized computer display, and is often used as a digital photo viewer, music and video player, TV receiver, and digital video recorder. HTPCs are also referred to as media center systems or media servers. The general goal in a HTPC is usually to combine many or all components of a home theater setup into one box. More recently, HTPCs gained the ability to connect to services providing on-demand movies and TV shows.

HTPCs can be purchased pre-configured with the required hardware and software needed to add television programming to the PC, or can be cobbled together out of discrete components, what is commonly done with software support from MythTV, Windows Media Center, GB-PVR, SageTV, Famulent or LinuxMCE.

Portable[edit]
Laptop[edit]
Main article: Laptop

A modern laptop computer
A laptop computer, also called a notebook, is a small personal computer designed for portability. Usually, all of the hardware and interfaces needed to operate a laptop, such as the graphics card, audio devices or USB ports (previously parallel and serial ports), are built into a single unit. Laptops contain high-capacity batteries that can power the device for extensive periods of time, enhancing portability. Once the battery charge is depleted, it will have to be recharged through a power outlet. In the interests of saving power, weight and space, laptop graphics cards are in many cases integrated into the CPU or chipset and use system RAM, resulting in reduced graphics performance when compared to an equivalent desktop machine. For this reason, desktop or gaming computers are usually preferred to laptop PCs for gaming purposes.

One of the drawbacks of laptops is that, due to the size and configuration of components, usually relatively little can be done to upgrade the overall computer from its original design. Internal upgrades are either not manufacturer-recommended, can damage the laptop if done with poor care or knowledge, or in some cases impossible, making the desktop PC more modular. Some internal upgrades, such as memory and hard disk drive upgrades are often easily performed, while a display or keyboard upgrade is usually impossible. Just as desktops, laptops also have the same possibilities for connecting to a wide variety of devices, including external displays, mice, cameras, storage devices and keyboards, which may be attached externally through USB ports and other less common ports such as external video. Laptops are also a little more expensive compared to desktops, as the components for laptops themselves are expensive.

A subtype of notebooks, called subnotebook, has most of the features of a standard laptop computer, but with smaller physical dimensions. Subnotebooks are larger than hand-held computers, and usually run full versions of desktop or laptop operating systems. Ultra-Mobile PCs (UMPC) are usually considered subnotebooks, or more specifically, subnotebook tablet PCs, which are described below. Netbooks are sometimes considered to belong to this category, though they are sometimes separated into a category of their own (see below).

Desktop replacement[edit]
Main article: Desktop replacement computer

An Acer Aspire desktop replacement laptop
A desktop replacement computer (DTR) is a personal computer that provides the full capabilities of a desktop computer while remaining mobile. Such computers are often actually larger, bulkier laptops. Because of their increased size, this class of computers usually includes more powerful components and a larger display than generally found in smaller portable computers, and can have a relatively limited battery capacity or none at all in some cases. Some use a limited range of desktop components to provide better performance at the expense of battery life. Desktop replacement computers are sometimes called desknotes, as a portmanteau of words "desktop" and "notebook," though the term is also applied to desktop replacement computers in general.[56]

Netbook[edit]
Main article: Netbook

An HP netbook
Netbooks, also called mini notebooks or subnotebooks, are a subgroup of laptops[57] acting as a category of small, lightweight and inexpensive laptop computers suited for general computing tasks and accessing web-based applications. They are often marketed as "companion devices", with an intention to augment other ways in which a user can access computer resources.[57] Walt Mossberg called them a "relatively new category of small, light, minimalist and cheap laptops."[58] By August 2009, CNET called netbooks "nothing more than smaller, cheaper notebooks."[57]

Initially, the primary defining characteristic of netbooks was the lack of an optical disc drive, requiring it to be a separate external device. This has become less important as flash memory devices have gradually increased in capacity, replacing the writable optical disc (e.g. CD-RW, DVD-RW) as a transportable storage medium.

At their inception in late 2007   as smaller notebooks optimized for low weight and low cost[59]   netbooks omitted key features (e.g., the optical drive), featured smaller screens and keyboards, and offered reduced specifications and computing power. Over the course of their evolution, netbooks have ranged in their screen sizes from below five inches[60] to over 13 inches,[61] with weights around ~1 kg (2-3 pounds). Often significantly less expensive than other laptops,[62] by mid-2009 netbooks had been offered to users "free of charge", with an extended service contract purchase of a cellular data plan.[63]

In the short period since their appearance, netbooks have grown in size and features, converging with new smaller and lighter notebooks. By mid-2009, CNET noted that "the specs are so similar that the average shopper would likely be confused as to why one is better than the other," noting "the only conclusion is that there really is no distinction between the devices."[57]

Tablet[edit]
Main article: Tablet computer

HP Compaq tablet PC with rotating/removable keyboard
A tablet is a type of portable PC that de-emphasizes the use of traditional input devices (such as a mouse or keyboard) by using a touchscreen display, which can be controlled using either a stylus pen or finger. Some tablets may use a "hybrid" or "convertible" design, offering a keyboard that can either be removed as an attachment, or a screen that can be rotated and folded directly over top the keyboard.

Some tablets may run a traditional PC operating system such as Windows or Linux; Microsoft attempted to enter the tablet market in 2002 with its Microsoft Tablet PC specifications, for tablets and convertible laptops running Windows XP. However, Microsoft's early attempts were overshadowed by the release of Apple's iPad; following in its footsteps, most modern tablets use slate designs and run mobile operating systems such as Android and iOS, giving them functionality similar to smartphones. In response, Microsoft built its Windows 8 operating system to better accommodate these new touch-oriented devices.[64] Many tablet computers have USB ports, to which a keyboard or mouse can be connected.

Ultra-mobile PC[edit]
Main article: Ultra-mobile PC

A Samsung Q1 ultra-mobile PC
The ultra-mobile PC (UMPC) is a specification for small-configuration tablet PCs. It was developed as a joint development exercise by Microsoft, Intel and Samsung, among others. Current UMPCs typically feature the Windows XP, Windows Vista, Windows 7, or Linux operating system, and low-voltage Intel Atom or VIA C7-M processors.

Pocket PC[edit]
Main article: Pocket PC

An O2 pocket PC
A pocket PC is a hardware specification for a handheld-sized computer (personal digital assistant, PDA) that runs the Microsoft Windows Mobile operating system. It may have the capability to run an alternative operating system like NetBSD or Linux. Pocket PCs have many of the capabilities of modern desktop PCs.

Numerous applications are available for handhelds adhering to the Microsoft Pocket PC specification, many of which are freeware. Some of these devices also include mobile phone features, actually representing a smartphone. Microsoft-compliant Pocket PCs can also be used with many other add-ons like GPS receivers, barcode readers, RFID readers and cameras. In 2007, with the release of Windows Mobile 6, Microsoft dropped the name Pocket PC in favor of a new naming scheme: devices without an integrated phone are called Windows Mobile Classic instead of Pocket PC, while devices with an integrated phone and a touch screen are called Windows Mobile Professional.[65]

Hardware[edit]

An exploded view of a modern personal computer and peripherals:
Scanner
CPU (Microprocessor)
Memory (RAM)
Expansion cards (graphics cards, etc.)
Power supply
Optical disc drive
Storage (Hard disk or SSD)
Motherboard
Speakers
Monitor
System software
Application software
Keyboard
Mouse
External hard disk
Printer
Main article: Personal computer hardware
Computer hardware is a comprehensive term for all physical parts of a computer, as distinguished from the data it contains or operates on, and the software that provides instructions for the hardware to accomplish tasks. The boundary between hardware and software might be slightly blurry, with the existence of firmware that is software "built into" the hardware.

Mass-market consumer computers use highly standardized components and so are simple for an end user to assemble into a working system. A typical desktop computer consists of a computer case that holds the power supply, motherboard, hard disk drive, and often an optical disc drive. External devices such as a computer monitor or visual display unit, keyboard, and a pointing device are usually found in a personal computer.

The motherboard connects all processor, memory and peripheral devices together. The RAM, graphics card and processor are in most cases mounted directly onto the motherboard. The central processing unit (microprocessor chip) plugs into a CPU socket, while the memory modules plug into corresponding memory sockets. Some motherboards have the video display adapter, sound and other peripherals integrated onto the motherboard, while others use expansion slots for graphics cards, network cards, or other I/O devices. The graphics card or sound card may employ a break out box to keep the analog parts away from the electromagnetic radiation inside the computer case. Disk drives, which provide mass storage, are connected to the motherboard with one cable, and to the power supply through another cable. Usually, disk drives are mounted in the same case as the motherboard; expansion chassis are also made for additional disk storage. For extended amounts of data, a tape drive can be used or extra hard disks can be put together in an external case.

The keyboard and the mouse are external devices plugged into the computer through connectors on an I/O panel on the back of the computer case. The monitor is also connected to the I/O panel, either through an onboard port on the motherboard, or a port on the graphics card.

Capabilities of the personal computers hardware can sometimes be extended by the addition of expansion cards connected via an expansion bus. Standard peripheral buses often used for adding expansion cards in personal computers include PCI, PCI Express (PCIe), and AGP (a high-speed PCI bus dedicated to graphics adapters, found in older computers). Most modern personal computers have multiple physical PCI Express expansion slots, with some of the having PCI slots as well.

Computer case[edit]
Main article: Computer case

A stripped ATX case lying on its side.
A computer case is an enclosure that contains the main components of a computer. They are usually constructed from steel or aluminum combined with plastic, although other materials such as wood have been used. Cases are available in different sizes and shapes; the size and shape of a computer case is usually determined by the configuration of the motherboard that it is designed to accommodate, since this is the largest and most central component of most computers.

The most popular style for desktop computers is ATX, although microATX and similar layouts became very popular for a variety of uses. Companies like Shuttle Inc. and AOpen have popularized small cases, for which FlexATX is the most common motherboard size.

Power supply unit[edit]
Main article: Power supply unit (computer)

Computer power supply unit with top cover removed.
The power supply unit (PSU) converts general-purpose mains AC electricity to direct current (DC) for the other components of the computer. The rated output capacity of a PSU should usually be about 40% greater than the calculated system power consumption needs obtained by adding up all the system components. This protects against overloading the supply, and guards against performance degradation.

Processor[edit]
Main article: Central processing unit

AMD Athlon 64 X2 CPU.
The central processing unit, or CPU, is a part of a computer that executes instructions of a software program. In newer PCs, the CPU contains over a million transistors in one integrated circuit chip called the microprocessor. In most cases, the microprocessor plugs directly into the motherboard. The chip generates so much heat that the PC builder is required to attach a special cooling device to its surface; thus, modern CPUs are equipped with a fan attached via heat sink.

IBM PC compatible computers use an x86-compatible microprocessor, manufactured by Intel, AMD, VIA Technologies or Transmeta. Apple Macintosh computers were initially built with the Motorola 680x0 family of processors, then switched to the PowerPC series; in 2006, they switched to x86-compatible processors made by Intel.

Motherboard[edit]
Main article: Motherboard

A motherboard without processor, memory and expansion cards, cables
The motherboard, also referred to as system board or main board, is the primary circuit board within a personal computer, and other major system components plug directly into it or via a cable. A motherboard contains a microprocessor, the CPU supporting circuitry (mostly integrated circuits) that provide the interface between memory and input/output peripheral circuits, main memory, and facilities for initial setup of the computer immediately after power-on (often called boot firmware or, in IBM PC compatible computers, a BIOS or UEFI).

In many portable and embedded personal computers, the motherboard houses nearly all of the PC's core components. Often a motherboard will also contain one or more peripheral buses and physical connectors for expansion purposes. Sometimes a secondary daughter board is connected to the motherboard to provide further expandability or to satisfy space constraints.

Main memory[edit]
Main article: Primary storage

1GB DDR SDRAM PC-3200 module
A PC's main memory is a fast primary storage device that is directly accessible by the CPU, and is used to store the currently executing program and immediately needed data. PCs use semiconductor random access memory (RAM) of various kinds such as DRAM, SDRAM or SRAM as their primary storage. Which exact kind is used depends on cost/performance issues at any particular time.

Main memory is much faster than mass storage devices like hard disk drives or optical discs, but is usually volatile, meaning that it does not retain its contents (instructions or data) in the absence of power, and is much more expensive for a given capacity than is most mass storage. As a result, main memory is generally not suitable for long-term or archival data storage.

Hard disk[edit]
Main article: Hard disk drive

A Western Digital 250 GB hard disk drive
Mass storage devices store programs and data even when the power is off; they do require power to perform read and write functions during usage. Although flash memory has dropped in cost, the prevailing form of mass storage in personal computers is still the hard disk drive.

If the mass storage controller provides additional ports for expandability, a PC may also be upgraded by the addition of extra hard disk or optical disc drives. For example, BD-ROMs, DVD-RWs, and various optical disc recorders may all be added by the user to certain PCs. Standard internal storage device connection interfaces are PATA, Serial ATA and SCSI.

Solid state drives (SSDs) are a much faster replacement for traditional mechanical hard disk drives, but are also more expensive in terms of cost per gigabyte.

Visual display unit[edit]
Main article: Visual display unit
A visual display unit, computer monitor or just display, is a piece of electrical equipment, usually separate from the computer case, which displays visual images without producing a permanent computer record. A display device is usually either a CRT or some form of flat panel such as a TFT LCD. Multi-monitor setups are also quite common.

The display unit houses an electronic circuitry that generates its picture from signals received from the computer. Within the computer, either integral to the motherboard or plugged into it as an expansion card, there is pre-processing circuitry to convert the microprocessor's output data to a format compatible with the display unit's circuitry. The images from computer monitors originally contained only text, but as graphical user interfaces emerged and became common, they began to display more images and multimedia content.

The term "monitor" is also used, particularly by technicians in broadcasting television, where a picture of the broadcast data is displayed to a highly standardized reference monitor for confidence checking purposes.

Video card[edit]
Main article: Video card

An ATI Radeon video card
The video card otherwise called a graphics card, graphics adapter or video adapter processes the graphics output from the motherboard and transmits it to the display. It is an essential part of modern multimedia-enriched computing. On older models, and today on budget models, graphics circuitry may be integrated with the motherboard, but for modern and flexible machines, they are connected by the PCI, AGP, or PCI Express interface.

When the IBM PC was introduced, most existing business-oriented personal computers used text-only display adapters and had no graphics capability. Home computers at that time had graphics compatible with television signals, but with low resolution by modern standards owing to the limited memory available to the eight-bit processors available at the time.

Keyboard[edit]
Main article: Keyboard (computing)

A "Model M" IBM computer keyboard from the early 1980s. Commonly called the "Clicky Keyboard" due to its buckling spring key spring design, which gives the keyboard its iconic 'Click' sound with each keystroke.
A keyboard is an arrangement of buttons that each correspond to a function, letter, or number. They are the primary devices used for inputting text. In most cases, they contain an array of keys specifically organized with the corresponding letters, numbers, and functions printed or engraved on the button. They are generally designed around an operators language, and many different versions for different languages exist.

In English, the most common layout is the QWERTY layout, which was originally used in typewriters. They have evolved over time, and have been modified for use in computers with the addition of function keys, number keys, arrow keys, and keys specific to an operating system. Often, specific functions can be achieved by pressing multiple keys at once or in succession, such as inputting characters with accents or opening a task manager. Programs use keyboard shortcuts very differently and all use different keyboard shortcuts for different program specific operations, such as refreshing a web page in a web browser or selecting all text in a word processor.

Mouse[edit]
Main article: Mouse (computing)

A selection of computer mice built between 1986 and 2007
A computer mouse is a small handheld device that users hold and slide across a flat surface, pointing at various elements of a graphical user interface with an on-screen cursor, and selecting and moving objects using the mouse buttons. Almost all modern personal computers include a mouse; it may be plugged into a computer's rear mouse socket, or as a USB device, or, more recently, may be connected wirelessly via an USB dongle or Bluetooth link.

In the past, mice had a single button that users could press down on the device to "click" on whatever the pointer on the screen was hovering over. Modern mice have two, three or more buttons, providing a "right click" function button on the mouse, which performs a secondary action on a selected object, and a scroll wheel, which users can rotate using their fingers to "scroll" up or down. The scroll wheel can also be pressed down, and therefore be used as a third button. Some mouse wheels may be tilted from side to side to allow sideways scrolling. Different programs make use of these functions differently, and may scroll horizontally by default with the scroll wheel, open different menus with different buttons, etc. These functions may be also user-defined through software utilities.

Mice traditionally detected movement and communicated with the computer with an internal "mouse ball", and used optical encoders to detect rotation of the ball and tell the computer where the mouse has moved. However, these systems were subject to low durability, accuracy and required internal cleaning. Modern mice use optical technology to directly trace movement of the surface under the mouse and are much more accurate, durable and almost maintenance free. They work on a wider variety of surfaces and can even operate on walls, ceilings or other non-horizontal surfaces.

Other components[edit]

A proper ergonomic design of a personal computer workplace is necessary to prevent repetitive strain injuries, which can develop over time and can lead to long-term disability.[66]
All computers require either fixed or removable storage for their operating system, programs and user-generated material. Early home computers used compact audio cassettes for file storage; these were at the time a very low cost storage solution, but were displaced by floppy disk drives when manufacturing costs dropped, by the mid-1980s.

Initially, the 5.25-inch and 3.5-inch floppy drives were the principal forms of removable storage for backup of user files and distribution of software. As memory sizes increased, the capacity of the floppy did not keep pace; the Zip drive and other higher-capacity removable media were introduced but never became as prevalent as the floppy drive.

By the late 1990s, the optical drive, in CD and later DVD and Blu-ray Disc forms, became the main method for software distribution, and writeable media provided means for data backup and file interchange. As a result, floppy drives became uncommon in desktop personal computers since about 2000, and were dropped from many laptop systems even earlier.[note 1]

A second generation of tape recorders was provided when videocassette recorders were pressed into service as backup media for larger disk drives. All these systems were less reliable and slower than purpose-built magnetic tape drives. Such tape drives were uncommon in consumer-type personal computers but were a necessity in business or industrial use.

Interchange of data such as photographs from digital cameras is greatly expedited by installation of a card reader, which is often compatible with several forms of flash memory devices. It is usually faster and more convenient to move large amounts of data by removing the card from the mobile device, instead of communicating with the mobile device through a USB interface.

A USB flash drive performs much of the data transfer and backup functions formerly done with floppy drives, Zip disks and other devices. Mainstream operating systems for personal computers provide built-in support for USB flash drives, allowing interchange even between computers with different processors and operating systems. The compact size and lack of moving parts or dirt-sensitive media, combined with low cost and high capacity, have made USB flash drives a popular and useful accessory for any personal computer user.

The operating system can be located on any storage, but is typically installed on a hard disk or solid-state drive. A Live CD represents the concept of running an operating system directly from a CD. While this is slow compared to storing the operating system on a hard disk drive, it is typically used for installation of operating systems, demonstrations, system recovery, or other special purposes. Large flash memory is currently more expensive than hard disk drives of similar size (as of mid-2014) but are starting to appear in laptop computers because of their low weight, small size and low power requirements.

Computer communications involve internal modem cards, modems, network adapter cards, and routers. Common peripherals and adapter cards include headsets, joysticks, microphones, printers, scanners, sound adapter cards (as a separate card rather than located on the motherboard), speakers and webcams.

Software[edit]
Main article: Computer software

A screenshot of the OpenOffice.org Writer software
Computer software is any kind of computer program, procedure, or documentation that performs some task on a computer system.[67] The term includes application software such as word processors that perform productive tasks for users, system software such as operating systems that interface with computer hardware to provide the necessary services for application software, and middleware that controls and co-ordinates distributed systems.

Software applications are common for word processing, Internet browsing, Internet faxing, e-mail and other digital messaging, multimedia playback, playing of computer game, and computer programming. The user of a modern personal computer may have significant knowledge of the operating environment and application programs, but is not necessarily interested in programming nor even able to write programs for the computer. Therefore, most software written primarily for personal computers tends to be designed with simplicity of use, or "user-friendliness" in mind. However, the software industry continuously provide a wide range of new products for use in personal computers, targeted at both the expert and the non-expert user.

Operating system[edit]
Main article: Operating system
See also: Usage share of operating systems
An operating system (OS) manages computer resources and provides programmers with an interface used to access those resources. An operating system processes system data and user input, and responds by allocating and managing tasks and internal system resources as a service to users and programs of the system. An operating system performs basic tasks such as controlling and allocating memory, prioritizing system requests, controlling input and output devices, facilitating computer networking, and managing files.

Common contemporary desktop operating systems are Microsoft Windows, OS X, Linux, Solaris and FreeBSD. Windows, OS X, and Linux all have server and personal variants. With the exception of Microsoft Windows, the designs of each of them were inspired by or directly inherited from the Unix operating system, which was developed at Bell Labs beginning in the late 1960s and spawned the development of numerous free and proprietary operating systems.

Microsoft Windows[edit]
Main article: Microsoft Windows
Microsoft Windows is the collective brand name of several operating systems made by Microsoft which, as of 2015, are installed on PCs built by HP, Dell and Lenovo, the three remaining high volume manufacturers.[68] Microsoft first introduced an operating environment named Windows in November 1985,[69] as an add-on to MS-DOS and in response to the growing interest in graphical user interfaces (GUIs)[70][71] generated by Apple's 1984 introduction of the Macintosh.[72] As of August 2015, the most recent client and server version of Windows are Windows 10 and Windows Server 2012 R2, respectively. Windows Server 2016 is currently in Beta Testing.

OS X[edit]
Main article: OS X
OS X (formerly Mac OS X) is a line of operating systems developed, marketed and sold by Apple Inc.. OS X is the successor to the original Mac OS, which had been Apple's primary operating system since 1984. OS X is a Unix-based graphical operating system, and Snow Leopard, Leopard, Lion, Mountain Lion, Mavericks and Yosemite are its version codenames. The most recent version of OS X is codenamed El Capitan.

On iPhone, iPad and iPod, versions of iOS (which is an OS X derivative) are available from iOS 1.0 to the recent iOS 9. The iOS devices, however, are not considered PCs.

Linux[edit]
Main article: Linux

A Linux distribution running KDE Plasma Workspaces 4.
Linux is a family of Unix-like computer operating systems. Linux is one of the most prominent examples of free software and open source development: typically all underlying source code can be freely modified, used, and redistributed by anyone.[73] The name "Linux" refers to the Linux kernel, started in 1991 by Linus Torvalds. The system's utilities and libraries usually come from the GNU operating system, announced in 1983 by Richard Stallman. The GNU contribution is the basis for the alternative name GNU/Linux.[74]

Known for its use in servers, with the LAMP application stack as one of prominent examples, Linux is supported by corporations such as Dell, Hewlett-Packard, IBM, Novell, Oracle Corporation, Red Hat, Canonical Ltd. and Sun Microsystems. It is used as an operating system for a wide variety of computer hardware, including desktop computers, netbooks, supercomputers,[75] video game systems such as the Steam Machine or PlayStation 3 (until this option was removed remotely by Sony in 2010[76]), several arcade games, and embedded devices such as mobile phones, portable media players, routers, and stage lighting systems.

Applications[edit]

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (April 2014)
Main article: Application software

A screenshot of GIMP, which is a raster graphics editor
Generally, a computer user uses application software to carry out a specific task. System software supports applications and provides common services such as memory management, network connectivity and device drivers, all of which may be used by applications but are not directly of interest to the end user. A simplified analogy in the world of hardware would be the relationship of an electric light bulb (an application) to an electric power generation plant (a system): the power plant merely generates electricity, not itself of any real use until harnessed to an application like the electric light that performs a service that benefits the user.

Typical examples of software applications are word processors, spreadsheets, and media players. Multiple applications bundled together as a package are sometimes referred to as an application suite. Microsoft Office and OpenOffice.org, which bundle together a word processor, a spreadsheet, and several other discrete applications, are typical examples. The separate applications in a suite usually have a user interface that has some commonality making it easier for the user to learn and use each application. Often, they may have some capability to interact with each other in ways beneficial to the user; for example, a spreadsheet might be able to be embedded in a word processor document even though it had been created in the separate spreadsheet application.

End-user development tailors systems to meet the user's specific needs. User-written software include spreadsheet templates, word processor macros, scientific simulations, graphics and animation scripts; even email filters are a kind of user software. Users create this software themselves and often overlook how important it is.

Gaming[edit]
PC gaming is popular among the high-end PC market. According to an April 2014 market analysis, Gaming platforms like Steam (software), Uplay, Origin, and GOG.com (as well as competitive e-sports titles like League of Legends) are largely responsible for PC systems overtaking console revenue in 2013.[77]

Toxicity[edit]
Toxic chemicals found in some computer hardware include lead, mercury, cadmium, chromium, plastic (PVC), and barium. Overall, a computer is about 17% lead, copper, zinc, mercury, and cadmium; 23% is plastic, 14% is aluminum, and 20% is iron.

Lead is found in a cathode ray tube (CRT) display, and on all of the printed circuit boards and most expansion cards. Mercury is located in the screen's fluorescent lamp, in the laser light generators in the optical disk drive, and in the round, silver-looking batteries on the motherboard. Plastic is found mostly in the housing of the computation and display circuitry.

While daily end-users are not exposed to these toxic elements, the danger arises during the computer recycling process, which involves manually breaking down hardware and leads to the exposure of a measurable amount of lead or mercury. A measurable amount of lead or mercury can easily cause serious brain damage or ruin drinking water supplies. Computer recycling is best handled by the electronic waste (e-waste) industry, and kept segregated from the general community dump.

Electronic waste regulation[edit]
Main article: Computer recycling
Personal computers have become a large contributor to the 50 million tons of discarded electronic waste that is being generated annually, according to the United Nations Environment Programme. To address the electronic waste issue affecting developing countries and the environment, extended producer responsibility (EPR) acts have been implemented in various countries and states.[78]

Organizations, such as the Silicon Valley Toxics Coalition, Basel Action Network, Toxics Link India, SCOPE, and Greenpeace have contributed to these efforts. In the absence of comprehensive national legislation or regulation on the export and import of electronic waste, the Silicon Valley Toxics Coalition and BAN (Basel Action Network) teamed up with 32 electronic recyclers in the US and Canada to create an e-steward program for the orderly disposal of manufacturers and customers electronic waste. The Silicon Valley Toxics Coalition founded the Electronics TakeBack Coalition, a coalition that advocates for the production of environmentally friendly products. The TakeBack Coalition works with policy makers, recyclers, and smart businesses to get manufacturers to take full responsibility of their products.

There are organizations opposing EPR regulation, such as the Reason Foundation. They see flaws in two principal tenants of EPR: First EPR relies on the idea that if the manufacturers have to pay for environmental harm, they will adapt their practices. Second EPR assumes the current design practices are environmentally inefficient. The Reason Foundation claims that manufacturers naturally move toward reduced material and energy use.

See also[edit]
Portal icon Computer Science portal
Portal icon Electronics portal
Computer case
Computer virus
Desktop computer
Desktop replacement computer
e-waste
IBM 5100
Information and communication technologies for development
Laptop
List of computer system manufacturers
Market share of personal computer vendors
Personal Computer Museum
Portable computer
Public computer
Quiet PC
PC game
Notes[edit]
Jump up ^ The NeXT computer introduced in 1988 did not include a floppy drive, which at the time was unusual.
References[edit]
Jump up ^ Conlon, Tom (January 29, 2010), The iPad s Closed System: Sometimes I Hate Being Right, Popular Science, retrieved 2010-10-14, The iPad is not a personal computer in the sense that we currently understand.
Jump up ^ "Steve Jobs Offers World 'Freedom From Porn'", Gawker, May 15, 2010, retrieved 2010-10-14
^ Jump up to: a b "The incredible story of the first PC, from 1965". Pingdom. Retrieved August 28, 2012.
Jump up ^ Pospelov, Dmitry.               -                         [MIR series of computers. The first personal computers]. Glushkov Foundation (in Russian). Institute of Applied Informatics. Retrieved November 19, 2012.
^ Jump up to: a b c IBM Archives
Jump up ^ PC Magazine, Vol. 2, No. 6, November 1983,  SCAMP: The Missing Link in the PC's Past  
Jump up ^ Jim Battle (August 9, 2008). "The Wang 2200". Wang2200.org. Jim Battle. Retrieved November 13, 2013.
Jump up ^ What's New (February 1978), "Commodore Ships First PET Computers", BYTE (Byte Publications) 3 (2): 190 Commodore press release. "The PET computer made its debut recently as the first 100 units were shipped to waiting customers in mid-October 1977."
Jump up ^ "Apple II History". Apple II History. Retrieved May 8, 2014.
Jump up ^ "Sinclair Research website". Retrieved 2014-08-06.
Jump up ^ Reimer, Jeremy (November 2, 2009). "Personal Computer Market Share: 1975 2004". Retrieved 2009-07-17.
Jump up ^ Reimer, Jeremy (December 2, 2012). "Personal Computer Market Share: 1975 2004". Retrieved 2013-02-09.
Jump up ^ "Computing Japan". Computing Japan (LINC Japan). 54-59: 18. 1999. Retrieved February 6, 2012. ...its venerable PC 9800 series, which has sold more than 18 million units over the years, and is the reason why NEC has been the number one PC vendor in Japan for as long as anyone can remember.
Jump up ^ Polsson, Ken. "Chronology of Amiga Computers". Retrieved May 9, 2014.
Jump up ^ Angler, Martin. "Obituary: The PC is Dead". JACKED IN. Retrieved February 12, 2014.
Jump up ^ Kanellos, Michael. "PCs: More than 1 billion served". CNET News. Retrieved August 9, 2001.
Jump up ^ Kanellos, Michael (June 30, 2002). "personal computers: More than 1 billion served". CNET News. Retrieved 2010-10-14.
Jump up ^ "Computers reach one billion mark". BBC News. July 1, 2002. Retrieved 2010-10-14.
Jump up ^ Global PC shipments grew 13.8 percent in 2010   Gartner study, Jan 13, 2011, retrieved at September 12, 2011
Jump up ^ Laptop Sales Soaring Amid Wider PC Growth: Gartner, May 27, 2010, Andy Patrizio, earthweb.com, retrieved at September 12, 2011
Jump up ^ Worldwide PC Shipments in 2008, March 16, 2009, ZDNet, retrieved at September 12, 2011
Jump up ^ PC Sales Up for 2008, but Barely, January 14, 2009, Andy Patrizio, internetnews.com, retrieved at September 12, 2011
Jump up ^ ISuppli Raises 2007 Computer Sales Forecast, pcworld.com, retrieved January 13, 2009
Jump up ^ iSuppli raises 2007 computer sales forecast, Macworld UK, retrieved January 13, 2009
Jump up ^ Global PC Sales Leveling Off, newsfactor.com, retrieved January 13, 2009
^ Jump up to: a b c HP back on top of PC market, retrieved January 13, 2009
Jump up ^ Yates, Nona (January 24, 2000). "Dell Passes Compaq as Top PC Seller in U.S". Los Angeles Times. Retrieved January 13, 2009.
Jump up ^ Economic recovery bumps AP 1999 PC shipments to record high, zdnetasia.com, retrieved January 13, 2009
Jump up ^ "Worldwide PC use to reach 1 billion by 2008: report". CBC News. Retrieved June 12, 2007.
Jump up ^ "Gartner Says More than 1 Billion PCs In Use Worldwide and Headed to 2 Billion Units by 2014" (Press release). Gartner. June 23, 2008. Retrieved 2010-10-14.
Jump up ^ Tarmo Virki (June 23, 2008). "Computers in use pass 1 billion mark: Gartner". Reuters. Retrieved 2010-10-14.
Jump up ^ "China hits tech milestone: PC shipments pass US". August 23, 2011.
Jump up ^ "4P Computing   Negroponte's 14 Million Laptop Impact". OLPC News. December 11, 2008. Retrieved 2010-10-14.
Jump up ^ Conrad H. Blickenstorfer. "Rugged PC leaders". Ruggedpcreview.com. Retrieved 2010-10-14.
Jump up ^ "PC Rebound in Mature Regions Stabilizes Market, But Falls Short of Overall Growth in the Second Quarter of 2014". International Data Corporation.
Jump up ^ "After Two Years of Decline, Worldwide PC Shipments Experienced Flat Growth in Second Quarter of 2014". Gartner.
Jump up ^ Tablets, smartphones to outsell PCs http://news.yahoo.com/s/afp/20110210/tc_afp/itinternettelecomequipmentmobileconsumerproduct
Jump up ^ "Gartner Says Declining Worldwide PC Shipments in Fourth Quarter of 2012 Signal Structural Shift of PC Market". Gartner.Com (Press release). January 14, 2013. Retrieved January 18, 2013.
^ Jump up to: a b c d "Feeble PC industry stumbles to steep sales drop during 1st quarter as Windows makeover flops". Washington Times. Associated Press. April 10, 2013. Archived from the original on April 19, 2013. Retrieved April 11, 2013.
^ Jump up to: a b Nick Wingfield (April 10, 2013). "PC Sales Still in a Slump, Despite New Offerings". New York Times. Retrieved April 11, 2013.
Jump up ^ "Steve Ballmer's retirement leaves Microsoft in a replacement crisis". August 24, 2013.
Jump up ^ "The Apple Vs. Samsung Title Fight for Mobile Supremacy". The Financialist. Credit Suisse. August 8, 2013. Retrieved August 13, 2013.
Jump up ^ John Fingas (March 4, 2014). "PC shipments faced their steepest-ever drop in 2013".
Jump up ^ Marvin B. Sussman Personal Computers and the Family Routledge, 1985 ISBN 0-86656-361-X, page 90
Jump up ^ Kateri M. Drexler Icons of business: an encyclopedia of mavericks, movers, and shakers, Volume 1, Greenwood Publishing Group, 2007 ISBN 0-313-33863-9 page 102
Jump up ^ Nancy Weil, Average PC Price drops below  1000, PC World December 1998, Retrieved November 17, 2010
Jump up ^ Joe Wilcox (April 16, 2009). "Netbooks Are Destroying the Laptop Market and Microsoft Needs to Act Now". eWeek. Retrieved 2010-10-14.
Jump up ^ Shane O'Neill (December 2, 2009). "Falling PC Prices Pit Microsoft Against PC Makers". Retrieved 2010-10-14.
Jump up ^ "Mac* vs. PC Debate". intel.com. Intel. Retrieved 5 October 2014.
Jump up ^ Finnie, Scot (8 June 2007). "Mac vs. PC cost analysis: How does it all add up ". Computerworld. Computerworld, Inc. Retrieved 5 October 2014.
Jump up ^ Ackerman, Dan (22 August 2013). "Don't buy a new PC or Mac before you read this". CNET. CBS Interactive. Retrieved 5 October 2014.
Jump up ^ Haslam, Karen (11 December 2013). "Mac or PC  Ten reasons why Macs are better than PCs". Macworld. IDG. Retrieved 5 October 2014.
Jump up ^ Hoffman, Chris. "Macs Are PCs! Can We Stop Pretending They Aren t ". How-To Geek. Retrieved 15 October 2015.
Jump up ^ Ralston, Anthony; Reilly, Edwin (1993). "Workstation". Encyclopedia of Computer Science (Third ed.). New York: Van Nostrand Reinhold. ISBN 0-442-27679-6.
Jump up ^ Houghton, Andy. "Evolution of Custom Gaming PCs: What Really Made the Difference". digitalstorm.com. Retrieved 28 September 2015.
Jump up ^ Desktop notebooks stake their claim, accessed October 19, 2007
^ Jump up to: a b c d Erica Ogg (August 20, 2009). "Time to drop the Netbook label". CNN.
Jump up ^ Walt Mossberg (August 6, 2009). "New Netbook Offers Long Battery Life and Room to Type". The Wall Street Journal Online, Personal Technology.
Jump up ^ "Cheap PCs Weigh on Microsoft". Business Technologies, The Wall Street Journal. December 8, 2008.
Jump up ^ "UMID Netbook Only 4.8". Elitezoom.com. Retrieved 2010-10-14.
Jump up ^ "CES 2009   MSI Unveils the X320 "MacBook Air Clone" Netbook". Futurelooks.com. 2009-01-07. Retrieved 2010-10-14.
Jump up ^ Netbook Trends and Solid-State Technology Forecast (PDF). pricegrabber.com. p. 7. Retrieved 2009-01-28.
Jump up ^ Light and Cheap, Netbooks Are Poised to Reshape PC Industry, The New York Times, April 1, 2009, retrieved 2010-10-14, AT&T announced on Tuesday that customers in Atlanta could get a type of compact PC called a netbook for just 50 US  if they signed up for an Internet service plan... 'The era of a perfect Internet computer for 99 US  is coming this year,' said Jen-Hsun Huang, the chief executive of Nvidia, a maker of PC graphics chips that is trying to adapt to the new technological order.
Jump up ^ "Tablet PC Redux ". Paul Thurrott's Supersite for Windows. Retrieved October 6, 2013.
Jump up ^ New Windows Mobile 6 Devices :: Jun/Jul 2007 Archived March 4, 2008, at the Wayback Machine.
Jump up ^ Berkeley Lab. Integrated Safety Management: Ergonomics. Website. Retrieved July 9, 2008. Archived August 5, 2009, at the Wayback Machine.
Jump up ^ "Wordreference.com: WordNet 2.0". Princeton University, Princeton, NJ. Retrieved 2007-08-19.
Jump up ^ Tim Bajarin (October 29, 2015). "Microsoft Sees That Apple Has Been Right All Along". Re/code. Retrieved October 31, 2015. At the moment, it looks like Microsoft will only have three serious PC partners doing any volume   HP, Dell and Lenovo.
Jump up ^ "A history of Windows: Highlights from the first 25 years".
Jump up ^ Mary Bellis. "The Unusual History of Microsoft Windows". About.com. Retrieved 2010-10-14.
Jump up ^ "IDC: Consolidation to Windows won't happen". Linuxworld. Retrieved 2010-10-14.
Jump up ^ "Thirty Years of Mac: 1984   Macintosh". Apple. Retrieved May 8, 2014.
Jump up ^ "Linux Online  About the Linux Operating System". Linux.org. Retrieved 2007-07-06.
Jump up ^ Weeks, Alex (2004). "1.1". Linux System Administrator's Guide (version 0.9 ed.). Retrieved 2007-01-18.
Jump up ^ Lyons, Daniel (March 15, 2005). "Linux rules supercomputers". Forbes. Retrieved 2007-02-22.
Jump up ^ Patrick Seybold (March 28, 2010). "PS3 Firmware (v3.21) Update". PlayStation.Blog. Retrieved March 29, 2010.
Jump up ^ Mark Serrels. "PC Gaming Revenue Has Now Overtaken Console Gaming". kotaku.com.au. Retrieved June 8, 2015.
Jump up ^ Nash, Jennifer; Bosso, Christopher (2013). "Extended Producer Responsibility in the United States: Full Speed Ahead " (PDF). Journal of Industrial Ecology 17 (2   RPP-2013-04): 175 185. doi:10.1111/j.1530-9290.2012.00572.x. Retrieved August 23, 2014.
Intel
From Wikipedia, the free encyclopedia
This article is about the company. For the information gathering term, see Intelligence assessment.
Coordinates: 37 23 16.54 N 121 57 48.74 W

Intel Corporation
Intel-logo.svg
The current logo, used since December 30, 2005
Intelheadquarters.jpg
Headquarters in Santa Clara, California
Type
Public company
Traded as NASDAQ: INTC
Dow Jones Industrial Average Component
NASDAQ-100 Component
S&P 500 Component
Industry  Semiconductors
Founded July 18, 1968; 47 years ago
Founders  Gordon Moore
Robert Noyce
Headquarters  Santa Clara, California, United States
Area served
Worldwide
Key people
Andy Bryant
(Chairman)
Brian Krzanich
(CEO)
Products  Bluetooth chipsets
flash memory
microprocessors
motherboard chipsets
Network interface controllers
mobile phones
solid state drives
central processing units
Revenue Decrease US 55.4 billion (2015)[1]
Operating income
Decrease US 14.0 billion (2015)[1]
Net income
Decrease US 11.4 billion (2015)[1]
Total assets  Decrease US 103.065 billion (2015)[1]
Total equity  Decrease US 61.085 billion (2015)[1]
Number of employees
107,300, 51% located in the U.S. (2015)[1]
Slogan  Experience What's Inside.
Website www.intel.com
Intel Corporation (better known as Intel, stylized as intel) is an American multinational technology company headquartered in Santa Clara, California. Intel is one of the world's largest and highest valued semiconductor chip makers, based on revenue.[2] It is the inventor of the x86 series of microprocessors, the processors found in most personal computers. Intel supplies processors for computer system manufacturers such as Apple, Samsung, HP and Dell. Intel also makes motherboard chipsets, network interface controllers and integrated circuits, flash memory, graphics chips, embedded processors and other devices related to communications and computing.

Intel Corporation was founded on July 18, 1968 by semiconductor pioneers Robert Noyce and Gordon Moore and widely associated with the executive leadership and vision of Andrew Grove, Intel combines advanced chip design capability with a leading-edge manufacturing capability.

Intel was an early developer of SRAM and DRAM memory chips, which represented the majority of its business until 1981. Although Intel created the world's first commercial microprocessor chip in 1971, it was not until the success of the personal computer (PC) that this became its primary business. During the 1990s, Intel invested heavily in new microprocessor designs fostering the rapid growth of the computer industry. During this period Intel became the dominant supplier of microprocessors for PCs, and was known for aggressive and anti-competitive tactics in defense of its market position, particularly against Advanced Micro Devices (AMD), as well as a struggle with Microsoft for control over the direction of the PC industry.[3][4]

Intel was ranked #56 on the 2015 rankings of the world's most valuable brands published by Millward Brown Optimor.[5]

The Open Source Technology Center at Intel hosts PowerTOP and LatencyTOP, and supports other open-source projects such as Wayland, Intel Array Building Blocks, Threading Building Blocks (TBB), and Xen.[6][7]

The name "Intel" was conceived as portmanteau of the words integrated and electronics. The fact that "intel" is the term for intelligence information also made the name appropriate.[8]

Contents  [hide] 
1 Current Operations
1.1 Operating segments
1.2 Top customers
1.3 Market share
1.3.1 Market share in early 2011
1.3.2 Historical market share
1.3.3 Major competitors
2 Corporate history
2.1 Origins
2.2 Early history
2.3 Slowing demand and challenges to dominance in 2000
2.4 Litigation issues
2.5 Regaining of momentum (2005-2007)
2.6 Sale of XScale processor business (2006)
2.7 Acquisitions (2010-2015)
3 Acquisition table (2010-2016)
3.1 Expansions (2008-2011)
3.2 Opening up the foundries to other manufacturers (2013)
4 Product and market history
4.1 SRAMS and the microprocessor
4.2 From DRAM to microprocessors
4.3 Intel, x86 processors, and the IBM PC
4.3.1 386 microprocessor
4.3.2 486, Pentium, and Itanium
4.3.3 Pentium flaw
4.3.4 "Intel Inside" and other 1990s programs
4.4 Solid-state drives (SSD)
4.5 Supercomputers
4.6 Competition, antitrust and espionage
4.7 Use of Intel products by Apple Computer (2005-present)
4.8 Core 2 Duo advertisement controversy (2007)
4.9 Introduction of Classmate PC (2011)
4.10  Introduction of new mobile processor technology (2011)
4.11  Update to server chips (2011)
4.12  Introduction of Ivy Bridge 22 nm processors (2011)
4.13  Development of Personal Office Energy Monitor (POEM) (2011)
4.14  IT Manager 3: Unseen Forces
4.15  Car Security System (2011)
4.16  High-Bandwidth Digital Content Protection
4.17  Move from Wintel desktop to open mobile platforms (2013-2014)
4.18  Introduction of Haswell processors (2013)
4.19  Wearable fashion (2014)
5 Corporate affairs
5.1 Leadership and corporate structure
5.2 Employment
5.2.1 Diversity
5.3 Economic impact in Oregon in 2009
5.4 School funding in New Mexico in 1997
5.5 Ultrabook fund (2011)
5.6 Advertising and brand management
5.6.1 Intel Inside
5.6.2 Sonic logo
5.6.3 Processor naming strategy
5.7 Open source support
5.8 PC declining sales
6 Controversies
6.1 Patent infringement litigation (2006-2007)
6.2 Anti-competitive allegations and litigation (2005-2009)
6.2.1 Allegations by Japan Fair Trade Commission (2005)
6.2.2 Allegations by the European Union (2007-2008)
6.2.3 Allegations by regulators in South Korea (2007)
6.2.4 Allegations by regulators in the United States (2008-2010)
6.3 Corporate responsibility record
6.4 Religious controversy in Israel (2009)
6.5 Age discrimination complaints
6.6 Land dispute in Israel
7 See also
8 References
9 External links
Current Operations[edit]
Operating segments[edit]
Client Computing Group - 58% of 2015 revenues - produces hardware components used in desktop and notebook computers.[1]
Data Center Group - 29% of 2015 revenues - produces hardware components used in server, network, and storage platforms.[1]
Internet of Things Group - 4% of 2015 revenues - offers platforms designed for retail, transportation, industrial, buildings and home use.[1]
Software and Services - 4% of 2015 revenues - produces software, particularly security and antivirus software.[1]
Other - 5% of 2015 revenues - primarily the Non-Volatile Memory Solutions Group and the New Devices Group.[1]
Top customers[edit]
In 2015, HP Inc. and HP Enterprise collectively accounted for 18% of the company's revenue, Dell accounted for 15% of the company's revenue, and Lenovo accounted for 13% of the company's revenue.[1]

Market share[edit]
Further information: Semiconductor sales leaders by year
Market share in early 2011[edit]
According to IDC, while Intel enjoyed the biggest market share in both the overall worldwide PC microprocessor market (79.3%) and the mobile PC microprocessor (84.4%) in the second quarter of 2011, the numbers decreased by 1.5% and 1.9% compared to the first quarter of 2011.[9][10]

Historical market share[edit]
In the 1980s, Intel was among the top ten sellers of semiconductors (10th in 1987) in the world. In 1991, Intel became the biggest chip maker by revenue and has held the position ever since. Other top semiconductor companies include TSMC, Advanced Micro Devices, Samsung, Texas Instruments, Toshiba and STMicroelectronics.

Major competitors[edit]
Competitors in PC chip sets include Advanced Micro Devices, VIA Technologies, Silicon Integrated Systems, and Nvidia. Intel's competitors in networking include NXP Semiconductors, Infineon, Broadcom Limited, Marvell Technology Group and Applied Micro Circuits Corporation, and competitors in flash memory include Spansion, Samsung, Qimonda, Toshiba, STMicroelectronics, and SK Hynix.

The only major competitor in the x86 processor market is Advanced Micro Devices (AMD), with which Intel has had full cross-licensing agreements since 1976: each partner can use the other's patented technological innovations without charge after a certain time.[11] However, the cross-licensing agreement is canceled in the event of an AMD bankruptcy or takeover.[12]

Some smaller competitors such as VIA Technologies produce low-power x86 processors for small factor computers and portable equipment. However, the advent of such mobile computing devices, in particular, smartphones, has in recent years led to a decline in PC sales.[13] Since over 95% of the world's smartphones are currently powered by processors designed by ARM Holdings, ARM has become a major competitor for Intel's processor market. ARM is also planning to make inroads into the PC and server market.[14]

Intel has been involved in several disputes regarding violation of antitrust laws, which are noted below.

Corporate history[edit]
Further information: Timeline of Intel
Origins[edit]

Andy Grove, Robert Noyce and Gordon Moore (1978)

The old Intel logo used from July 18, 1968, until December 29, 2005
Intel was founded in Mountain View, California in 1968 by Gordon E. Moore (of "Moore's Law" fame), a chemist, and Robert Noyce, a physicist and co-inventor of the integrated circuit. Arthur Rock (investor and venture capitalist) helped them find investors, while Max Palevsky was on the board from an early stage.[15] Moore and Noyce had left Fairchild Semiconductor to found Intel. Rock was not an employee, but he was an investor and was chairman of the board.[16][17] The total initial investment in Intel was  2.5 million convertible debentures and  10,000 from Rock. Just 2 years later, Intel became a public company via an initial public offering (IPO), raising  6.8 million ( 23.50 per share).[16] Intel's third employee was Andy Grove,[18] a chemical engineer, who later ran the company through much of the 1980s and the high-growth 1990s.

In deciding on a name, Moore and Noyce quickly rejected "Moore Noyce",[19] homophone for "more noise"   an ill-suited name for an electronics company, since noise in electronics is usually very undesirable and typically associated with bad interference. Instead they used the name NM Electronics before renaming their company Integrated Electronics or "Intel" for short.[20] Since "Intel" was already trademarked by the hotel chain Intelco, they had to buy the rights for the name.[16][21]

Early history[edit]
At its founding, Intel was distinguished by its ability to make semiconductors. Its first product, in 1969, was the 3101 Schottky TTL bipolar 64-bit static random-access memory (SRAM), which was nearly twice as fast as earlier Schottky diode implementations by Fairchild and the Electrotechnical Laboratory in Tsukuba, Japan.[22][23] In the same year Intel also produced the 3301 Schottky bipolar 1024-bit read-only memory (ROM)[24] and the first commercial metal oxide semiconductor field-effect transistor (MOSFET) silicon gate SRAM chip, the 256-bit 1101.[16][25][26] Intel's business grew during the 1970s as it expanded and improved its manufacturing processes and produced a wider range of products, still dominated by various memory devices.


Federico Faggin, the designer of Intel 4004
While Intel created the first commercially available microprocessor (Intel 4004) in 1971[16] and one of the first microcomputers in 1972,[25][27] by the early 1980s its business was dominated by dynamic random-access memory chips. However, increased competition from Japanese semiconductor manufacturers had, by 1983, dramatically reduced the profitability of this market. The growing success of the IBM personal computer, based on an Intel microprocessor, was among factors that convinced Gordon Moore (CEO since 1975) to shift the company's focus to microprocessors, and to change fundamental aspects of that business model. Moore's decision to sole-source Intel's 386 chip played into the company's continuing success.

By the end of the 1980s, buoyed by its fortuitous position as microprocessor supplier to IBM and IBM's competitors within the rapidly growing personal computer market, Intel embarked on a 10-year period of unprecedented growth as the primary (and most profitable) hardware supplier to the PC industry, part of the winning 'Wintel' combination. Moore handed over to Andy Grove in 1987. By launching its Intel Inside marketing campaign in 1991, Intel was able to associate brand loyalty with consumer selection, so that by the end of the 1990s, its line of Pentium processors had become a household name.

Slowing demand and challenges to dominance in 2000[edit]
After 2000, growth in demand for high-end microprocessors slowed. Competitors, notably AMD (Intel's largest competitor in its primary x86 architecture market), garnered significant market share, initially in low-end and mid-range processors but ultimately across the product range, and Intel's dominant position in its core market was greatly reduced.[28] In the early 2000s then-CEO Craig Barrett attempted to diversify the company's business beyond semiconductors, but few of these activities were ultimately successful.

Litigation issues[edit]
Intel had also for a number of years been embroiled in litigation. US law did not initially recognize intellectual property rights related to microprocessor topology (circuit layouts), until the Semiconductor Chip Protection Act of 1984, a law sought by Intel and the Semiconductor Industry Association (SIA).[29] During the late 1980s and 1990s (after this law was passed), Intel also sued companies that tried to develop competitor chips to the 80386 CPU.[30] The lawsuits were noted to significantly burden the competition with legal bills, even if Intel lost the suits.[30] Antitrust allegations had been simmering since the early 1990s and had been the cause of one lawsuit against Intel in 1991. In 2004 and 2005, AMD brought further claims against Intel related to unfair competition.

Regaining of momentum (2005-2007)[edit]
In 2005, CEO Paul Otellini reorganized the company to refocus its core processor and chipset business on platforms (enterprise, digital home, digital health, and mobility).

In 2007, Intel unveiled its Core microarchitecture to widespread critical acclaim;[31] the product range was perceived as an exceptional leap in processor performance that at a stroke regained much of its leadership of the field.[32][33] In 2008, Intel had another "tick," when it introduced the Penryn microarchitecture, which was 45 nm. Later that year, Intel released a processor with the Nehalem architecture. Nehalem had positive reviews.[34]

Sale of XScale processor business (2006)[edit]
On June 27, 2006, the sale of Intel's XScale assets was announced. Intel agreed to sell the XScale processor business to Marvell Technology Group for an estimated  600 million and the assumption of unspecified liabilities. The move was intended to permit Intel to focus its resources on its core x86 and server businesses, and the acquisition completed on November 9, 2006.[35]

Acquisitions (2010-2015)[edit]
In 2010, Intel purchased McAfee, a manufacturer of computer security technology for  7.68 billion.[36] As a condition for regulatory approval of the transaction, Intel agreed to provide rival security firms with all necessary information that would allow their products to use Intel's chips and personal computers.[37] After the acquisition, Intel had about 90,000 employees, including about 12,000 software engineers.[38]

On August 30, 2010, Intel and Infineon Technologies announced that Intel would acquire Infineon's Wireless Solutions business.[39] Intel planned to use Infineon's technology in laptops, smart phones, netbooks, tablets and embedded computers in consumer products, eventually integrating its wireless modem into Intel's silicon chips.[40]

In March 2011, Intel bought most of the assets of Cairo-based SySDSoft.[41]

In July 2011, Intel announced that it had agreed to acquire Fulcrum Microsystems Inc., a company specializing in network switches.[42] The company was previously included on the EE Times list of 60 Emerging Startups.[42]

On October 1, 2011, Intel reached a deal to acquire Telmap, an Israeli-based navigation software company. The purchase price was not disclosed, but Israeli media reported values around  300 million to  350 million.[43]

In July 2012, Intel Corporation agreed to buy 10 percent shares of ASML Holding NV for  2.1 billion and another  1 billon for 5 percent shares that need shareholder approval to fund relevant research and development efforts, as part of a EUR3.3 billion ( 4.1 billion) deal to accelerate the development of 450-millimeter wafer technology and extreme ultra-violet lithography by as much as two years.[44]

In July 2013, Intel confirmed the acquisition of Omek Interactive, an Israeli company that makes technology for gesture-based interfaces, without disclosing the monetary value of the deal. An official statement from Intel read: "The acquisition of Omek Interactive will help increase Intel's capabilities in the delivery of more immersive perceptual computing experiences." One report estimated the value of the acquisition between US 30 million and  50 million.[45]

The acquisition of a Spanish natural language recognition startup named Indisys was announced on September 13, 2013. The terms of the deal were not disclosed but an email from an Intel representative stated: "Intel has acquired Indisys, a privately held company based in Seville, Spain. The majority of Indisys employees joined Intel. We signed the agreement to acquire the company on May 31 and the deal has been completed." Indysis explains that its artificial intelligence (AI) technology "is a human image, which converses fluently and with common sense in multiple languages and also works in different platforms."[46]

In December 2014, Intel bought PasswordBox.[47]

In January 2015, Intel purchased a 30% stake in Vuzix, a smart glasses manufacturer. The deal was worth  24.8 million.[48]

In February 2015, Intel announced its agreement to purchase German network chipmaker Lantiq, to aid in its expansion of its range of chips in devices with Internet connection capability.[49]

In June 2015, Intel announced its agreement to purchase FPGA design company Altera for  16.7 billion, in its largest acquisition to date.[50] The acquisition completed in December 2015.[51]

In October 2015, Intel bought cognitive computing company Saffron Technology for an undisclosed price.[52]

Acquisition table (2010-2016)[edit]
Number  Acquisition announcement date Company Business  Country Price Used as or integrated with  Ref(s).
1 June 4, 2009  Wind River Systems  Embedded Systems   US  884M Software  [53]
2 August 19, 2010 McAfee  Security   US  7.6B Software  [54]
3 August 30, 2010 Infineon (partial)  Wireless   Germany   1.4B Mobile CPUs [55]
4 March 17, 2011  Silicon Hive  DSP  Netherlands  N/A Mobile CPUs [56]
5 September 29, 2011  Telmap  Software   Israel N/A Location Services [57]
6 April 13, 2013  Mashery Cloud Software   US  180M Software  [58]
7 May 3, 2013 Aepona  SDN  Ireland  N/A Software  [59]
8 May 6, 2013 Stonesoft Corporation Security   Finland   389M Software  [60]
9 July 16, 2013 Omek Interactive  Gesture  Israel N/A Software  [45]
10  September 13, 2013  Indisys Natural language processing  Spain  N/A Software  [46]
11  March 25, 2014  BASIS Wearable   US N/A New Devices [61]
12  August 13, 2014 Avago Technologies (partial)  Semiconductor  US  650M Communications Processors [62]
13  December 1, 2014  PasswordBox Security   Canada N/A Software  [63]
14  January 5, 2015 Vuzix Wearable   US  24.8M  New Devices [64]
15  February 2, 2015  Lantiq  Telecom  Germany  undisclosed Gateways  [65]
16  June 1, 2015  Altera  Semiconductor  US  16.7B  FPGA  [50]
17  June 18, 2015 Recon Wearable   US  175M New Devices [66]
18  October 26, 2015  Saffron Technology  Cognitive computing  US undisclosed Software  [52]
19  January 4, 2016 Ascending Technologies  UAVs   DE undisclosed New Technology  [67]
20  March 9, 2016 Replay Technologies Video technology   Israel undisclosed 3D video technology [68]
21  April 5, 2016 Yogitech  IoT security and Advanced Driver Assistance Systems.   Italy  undisclosed Software  [69]
Expansions (2008-2011)[edit]
In 2008, Intel spun off key assets of a solar startup business effort to form an independent company, SpectraWatt Inc. In 2011, SpectraWatt filed for bankruptcy.[70]

In February 2011, Intel began to build a new microprocessor manufacturing facility in Chandler, Arizona, completed in 2013 at a cost of  5 billion.[71] The building was never used.[72] The company produces three-quarters of its products in the United States, although three-quarters of its revenue comes from overseas.[73]

In April 2011, Intel began a pilot project with ZTE Corporation to produce smartphones using the Intel Atom processor for China's domestic market.

In December 2011, Intel announced that it reorganized several of its business units into a new mobile and communications group[74] be responsible for the company's smartphone, tablet and wireless efforts.

Opening up the foundries to other manufacturers (2013)[edit]
Finding itself with excess fab capacity after the failure of the Ultrabook to gain market traction and with PC sales declining, in 2013 Intel reached a foundry agreement to produce chips for Altera using 14-nm process. General Manager of Intel's custom foundry division Sunit Rikhi indicated that Intel would pursue further such deals in the future.[75] This was after poor sales of Windows 8 hardware caused a major retrenchment for most of the major semiconductor manufacturers, except for Qualcomm, which continued to see healthy purchases from its largest customer, Apple.[76]

As of July 2013, five companies were using Intel's fabs via the Intel Custom Foundry division: Achronix, Tabula, Netronome, Microsemi, and Panasonic most are Field-programmable gate array (FPGA) makers, but Netronome designs network processors. Only Achronix began shipping chips made by Intel using the 22-nm Tri-Gate process.[77][78] Several other customers also exist but were not announced at the time.[79]

The Alliance for Affordable Internet (A4AI) was launched in October 2013 and Intel is part of the coalition of public and private organisations that also includes Facebook, Google, and Microsoft. Led by Sir Tim Berners-Lee, the A4AI seeks to make Internet access more affordable so that access is broadened in the developing world, where only 31% of people are online. Google will help to decrease internet access prices so that they fall below the UN Broadband Commission's worldwide target of 5% of monthly income.[80]

Product and market history[edit]
SRAMS and the microprocessor[edit]
Intel's first products were shift register memory and random-access memory integrated circuits, and Intel grew to be a leader in the fiercely competitive DRAM, SRAM, and ROM markets throughout the 1970s. Concurrently, Intel engineers Marcian Hoff, Federico Faggin, Stanley Mazor and Masatoshi Shima invented Intel's first microprocessor. Originally developed for the Japanese company Busicom to replace a number of ASICs in a calculator already produced by Busicom, the Intel 4004 was introduced to the mass market on November 15, 1971, though the microprocessor did not become the core of Intel's business until the mid-1980s. (Note: Intel is usually given credit with Texas Instruments for the almost-simultaneous invention of the microprocessor)

From DRAM to microprocessors[edit]
In 1983, at the dawn of the personal computer era, Intel's profits came under increased pressure from Japanese memory-chip manufacturers, and then-president Andy Grove focused the company on microprocessors. Grove described this transition in the book Only the Paranoid Survive. A key element of his plan was the notion, then considered radical, of becoming the single source for successors to the popular 8086 microprocessor.

Until then, the manufacture of complex integrated circuits was not reliable enough for customers to depend on a single supplier,[clarification needed] but Grove began producing processors in three geographically distinct factories,[which ] and ceased licensing the chip designs to competitors such as Zilog and AMD.[citation needed] When the PC industry boomed in the late 1980s and 1990s, Intel was one of the primary beneficiaries.

Intel, x86 processors, and the IBM PC[edit]

The die from an Intel 8742, an 8-bit microcontroller that includes a CPU running at 12 MHz, 128 bytes of RAM, 2048 bytes of EPROM, and I/O in the same chip
Despite the ultimate importance of the microprocessor, the 4004 and its successors the 8008 and the 8080 were never major revenue contributors at Intel. As the next processor, the 8086 (and its variant the 8088) was completed in 1978, Intel embarked on a major marketing and sales campaign for that chip nicknamed "Operation Crush", and intended to win as many customers for the processor as possible. One design win was the newly created IBM PC division, though the importance of this was not fully realized at the time.

IBM introduced its personal computer in 1981, and it was rapidly successful. In 1982, Intel created the 80286 microprocessor, which, two years later, was used in the IBM PC/AT. Compaq, the first IBM PC "clone" manufacturer, produced a desktop system based on the faster 80286 processor in 1985 and in 1986 quickly followed with the first 80386-based system, beating IBM and establishing a competitive market for PC-compatible systems and setting up Intel as a key component supplier.

In 1975, the company had started a project to develop a highly advanced 32-bit microprocessor, finally released in 1981 as the Intel iAPX 432. The project was too ambitious and the processor was never able to meet its performance objectives, and it failed in the marketplace. Intel extended the x86 architecture to 32 bits instead.[81][82]

386 microprocessor[edit]
During this period Andrew Grove dramatically redirected the company, closing much of its DRAM business and directing resources to the microprocessor business. Of perhaps greater importance was his decision to "single-source" the 386 microprocessor. Prior to this, microprocessor manufacturing was in its infancy, and manufacturing problems frequently reduced or stopped production, interrupting supplies to customers. To mitigate this risk, these customers typically insisted that multiple manufacturers produce chips they could use to ensure a consistent supply. The 8080 and 8086-series microprocessors were produced by several companies, notably AMD. Grove made the decision not to license the 386 design to other manufacturers, instead producing it in three geographically distinct factories: Santa Clara, California; Hillsboro, Oregon; and Chandler, a suburb of Phoenix, Arizona. He convinced customers that this would ensure consistent delivery. As the success of Compaq's Deskpro 386 established the 386 as the dominant CPU choice, Intel achieved a position of near-exclusive dominance as its supplier. Profits from this funded rapid development of both higher-performance chip designs and higher-performance manufacturing capabilities, propelling Intel to a position of unquestioned leadership by the early 1990s.

486, Pentium, and Itanium[edit]
Intel introduced the 486 microprocessor in 1989, and in 1990 formally established a second design team, designing the processors code-named "P5" and "P6" in parallel and committing to a major new processor every two years, versus the four or more years such designs had previously taken. Engineers Vinod Dham and Rajeev Chandrasekhar (Member of Parliament, India) were key figures on the core team that invented the 486 chip and later, Intel's signature Pentium chip. The P5 was earlier known as "Operation Bicycle," referring to the cycles of the processor. The P5 was introduced in 1993 as the Intel Pentium, substituting a registered trademark name for the former part number (numbers, such as 486, are hard to register as a trademark). The P6 followed in 1995 as the Pentium Pro and improved into the Pentium II in 1997. New architectures were developed alternately in Santa Clara, California and Hillsboro, Oregon.

The Santa Clara design team embarked in 1993 on a successor to the x86 architecture, codenamed "P7". The first attempt was dropped a year later, but quickly revived in a cooperative program with Hewlett-Packard engineers, though Intel soon took over primary design responsibility. The resulting implementation of the IA-64 64-bit architecture was the Itanium, finally introduced in June 2001. The Itanium's performance running legacy x86 code did not meet expectations, and it failed to compete effectively with x86-64, which was AMD's 64-bit extensions to the original x86 architecture (Intel uses the name Intel 64, previously EM64T). As of 2012, Intel continues to develop and deploy the Itanium; known planning continues into 2014.

The Hillsboro team designed the Willamette processors (initially code-named P68), which were marketed as the Pentium 4.[citation needed]

Pentium flaw[edit]
Main article: Pentium FDIV bug
In June 1994, Intel engineers discovered a flaw in the floating-point math subsection of the P5 Pentium microprocessor. Under certain data-dependent conditions, the low-order bits of the result of a floating-point division would be incorrect. The error could compound in subsequent calculations. Intel corrected the error in a future chip revision.

In October 1994, Thomas Nicely, Professor of Mathematics at Lynchburg College, independently discovered the bug. He contacted Intel, but received no response. On October 30, he posted a message on the Internet.[83] Word of the bug spread quickly and reached the industry press. The bug was easy to replicate; a user could enter specific numbers into the calculator on the operating system. Consequently, many users did not accept Intel's statements that the error was minor and "not even an erratum." During Thanksgiving, in 1994, The New York Times ran a piece by journalist John Markoff spotlighting the error. Intel changed its position and offered to replace every chip, quickly putting in place a large end-user support organization. This resulted in a  475 million charge against Intel's 1994 revenue.[84]

The "Pentium flaw" incident, Intel's response to it, and the surrounding media coverage propelled Intel from being a technology supplier generally unknown to most computer users to a household name. Dovetailing with an uptick in the "Intel Inside" campaign, the episode is considered to have been a positive event for Intel, changing some of its business practices to be more end-user focused and generating substantial public awareness, while avoiding a lasting negative impression.[85]

"Intel Inside" and other 1990s programs[edit]
During this period, Intel undertook two major supporting programs. The first is widely known: the 1991 "Intel Inside" marketing and branding campaign. The idea of ingredient branding was new at the time with only Nutrasweet and a few others making attempts to do so.[86] This campaign established Intel, which had been a component supplier little-known outside the PC industry, as a household name.

The second program is little-known: Intel's Systems Group began, in the early 1990s, manufacturing PC "motherboards", the main board component of a personal computer, and the one into which the processor (CPU) and memory (RAM) chips are plugged.[87] Shortly after, Intel began manufacturing fully configured "white box" systems for the dozens of PC clone companies that rapidly sprang up.[citation needed] At its peak in the mid-1990s, Intel manufactured over 15% of all PCs, making it the third-largest supplier at the time.[citation needed]

During the 1990s, Intel Architecture Labs (IAL) was responsible for many of the hardware innovations of the personal computer, including the PCI Bus, the PCI Express (PCIe) bus, the Universal Serial Bus (USB). IAL's software efforts met with a more mixed fate; its video and graphics software was important in the development of software digital video,[citation needed] but later its efforts were largely overshadowed by competition from Microsoft. The competition between Intel and Microsoft was revealed in testimony by IAL Vice-President Steven McGeady at the Microsoft antitrust trial.


Solid-state drives (SSD)[edit]

It has been suggested that this section be split into an article titled Intel solid-state drives. (Discuss) (January 2014)

An Intel X25-M SSD
On September 8, 2008, Intel began shipping its first mainstream solid-state drives, the X18-M and X25-M with 80GB and 160GB storage capacities.[88] Reviews measured high performance with these MLC-based drives.[89][90][91][92] Intel released its SLC-based Enterprise X25-E Extreme SSDs on October 15 that same year in capacities of 32GB and 64GB.[93]

In July 2009, Intel refreshed its X25-M and X18-M lines by moving from a 50-nanometer to a 34-nanometer process. These new drives, dubbed by the press as the X25-M and X18-M G2[94][95] (or generation 2), reduced prices by up to 60 percent while offering lower latency and improved performance.[96]

On February 1, 2010, Intel and Micron announced that they were gearing up for production of NAND flash memory using a new 25-nanometer process.[97] In March of that same year, Intel entered the budget SSD segment with its X25-V drives with an initial capacity of 40GB.[98] The SSD 310, Intel's first mSATA drive was released on December 2010, providing X25-M G2 performance in a much smaller package.[99][100]

March 2011 saw the introduction of two new SSD lines from Intel. The first, the SSD 510, used a SATA 6 Gigabit per second interface to reach speeds of up to 500 MegaBytes per second.[101] The drive, which uses a controller from Marvell Technology Group,[102] was released using 34 nm NAND Flash and came in capacities of 120GB and 250GB. The second product announcement, the SSD 320, is the successor to Intel's earlier X25-M. It uses the new 25 nm process that Intel and Micron announced in 2010, and was released in capacities of 40 GB, 80 GB, 120 GB, 160 GB, 300 GB and 600 GB.[103] Sequential read performance maxes out at 270 MB/s due to the older SATA 3 Gbit/s interface, and sequential write performance varies greatly based on the size of the drive with sequential write performance of the 40 GB model peaking at 45 MB/s and the 600 GB at 220 MB/s.[104]

Micron and Intel announced that they were producing their first 20 nm MLC NAND flash on April 14, 2011.[105]

In February 2012, Intel launched the SSD 520 series solid state drives using the SandForce SF-2200 controller with sequential read and write speeds of 550 and 520 MB/s respectively with random read and write IOPS as high as 80,000. These drives will replace the 510 series.[106] Intel has released the budget 330 series solid state drive in 60, 120, and 180GB capacities using 25 nm flash memory and a SandForce controller that have replaced the 320 series.[107][108]
Introduction of Haswell processors (2013)[edit]
In June 2013, Intel unveiled its fourth generation of Intel Core processors (Haswell) in an event named Computex in Taipei.[177]

Wearable fashion (2014)[edit]
On January 6, 2014, Intel announced that it was "teaming with the Council of Fashion Designers of America, Barneys New York and Opening Ceremony around the wearable tech field."[178]

Intel has developed a reference design for wearable smart earbuds that provide biometric and fitness information. The Intel smart earbuds provide full stereo audio, and monitor heart rate, while the applications on the user s phone keep track of run distance and calories burned.

Corporate affairs[edit]
Leadership and corporate structure[edit]

Paul Otellini, Craig Barrett and Sean Maloney (2006)
Robert Noyce was Intel's CEO at its founding in 1968, followed by co-founder Gordon Moore in 1975. Andy Grove became the company's president in 1979 and added the CEO title in 1987 when Moore became chairman. In 1998, Grove succeeded Moore as Chairman, and Craig Barrett, already company president, took over. On May 18, 2005, Barrett handed the reins of the company over to Paul Otellini, who previously was the company president and COO and who was responsible for Intel's design win in the original IBM PC. The board of directors elected Otellini as President and CEO, and Barrett replaced Grove as Chairman of the Board. Grove stepped down as chairman, but is retained as a special adviser. In May 2009, Barrett stepped down as chairman of the Board and was succeeded by Jane Shaw. In May 2012, Intel vice chairman Andy Bryant, who had previously held the posts of CFO (1994) and Chief Administrative Officer (2007) at Intel, succeeded Shaw as executive chairman.[179]

In November 2012, president and CEO Paul Otellini announced that he would step down in May 2013 at the age of 62, three years before the company's mandatory retirement age. During a six-month transition period, Intel's board of directors commenced a search process for the next CEO, in which it considered both internal managers and external candidates such as Sanjay Jha and Patrick Gelsinger.[180] Financial results revealed that, under Otellini, Intel's revenue increased by 55.8 percent (US 34.2 to 53.3 billion), while its net income increased by 46.7% (US 7.5 billion to 11 billion).[181]

On May 2, 2013, Executive Vice President and COO Brian Krzanich was elected as Intel's sixth CEO,[182] a selection that became effective on May 16, 2013 at the company's annual meeting. Reportedly, the board concluded that an insider could proceed with the role and exert an impact more quickly, without the need to learn Intel's processes, and Krzanich was selected on such a basis.[183] Intel's software head Ren e James was selected as president of the company, a role that is second to the CEO position.[184]

As of May 2013, Intel's board of directors consists of Andy Bryant, John Donahoe, Frank Yeary, Ambassador Charlene Barshefsky, Susan Decker, Reed Hundt, Paul Otellini, James Plummer, David Pottruck, and David Yoffie. The board was described by former Financial Times journalist Tom Foremski as "an exemplary example of corporate governance of the highest order" and received a rating of ten from GovernanceMetrics International, a form of recognition that has only been awarded to twenty-one other corporate boards worldwide.[185]

Employment[edit]

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2008)

Intel microprocessor facility in Costa Rica was responsible in 2006 for 20% of Costa Rican exports and 4.9% of the country's GDP.[186]
The firm promotes very heavily from within, most notably in its executive suite. The company has resisted the trend toward outsider CEOs. Paul Otellini was a 30-year veteran of the company when he assumed the role of CEO. All of his top lieutenants have risen through the ranks after many years with the firm. In many cases, Intel's top executives have spent their entire working careers with Intel.[citation needed]

Intel has a mandatory retirement policy for its CEOs when they reach age 65. Andy Grove retired at 62, while both Robert Noyce and Gordon Moore retired at 58. Grove retired as Chairman and as a member of the board of directors in 2005 at age 68.

Intel's headquarters are located in Santa Clara, California, and the company has operations around the world. Its largest workforce concentration anywhere is in Washington County, Oregon[187] (in the Portland metropolitan area's "Silicon Forest"), with 18,600 employees at several facilities.[188] Outside the United States, the company has facilities in China, Costa Rica, Malaysia, Israel, Ireland, India, Russia, Argentina and Vietnam, in 63 countries and regions internationally. In the U.S. Intel employs significant numbers of people in California, Colorado, Massachusetts, Arizona, New Mexico, Oregon, Texas, Washington and Utah. In Oregon, Intel is the state's largest private employer.[188][189] The company is the largest industrial employer in New Mexico while in Arizona the company has over 10,000 employees.[citation needed]

Intel invests heavily in research in China and about 100 researchers   or 10% of the total number of researchers from Intel   are located in Beijing.[190]

In 2011, the Israeli government offered Intel  290 million to expand in the country. As a condition, Intel will have to employ 1,500 more workers in Kiryat Gat and between 600 1000 workers in the north.[191]

In January 2014, it was reported that Intel would cut about 5,000 jobs from its work force of 107,000. The announcement was made a day after it reported earnings that missed analyst targets.[192]

In March 2014, it was reported that Intel would embark upon a  6 billion plan to expand its activities in Israel. The plan calls for continued investment in existing and new Intel plants until 2030. As of 2014 Intel employs 10,000 workers at four development centers and two production plants in Israel.[193]

Diversity[edit]
Intel has a Diversity Initiative, including employee diversity groups as well as supplier diversity programs.[194] Like many companies with employee diversity groups, they include groups based on race and nationality as well as sexual identity and religion. In 1994, Intel sanctioned one of the earliest corporate Gay, Lesbian, Bisexual, and Transgender employee groups,[195] and supports a Muslim employees group,[196] a Jewish employees group,[197] and a Bible-based Christian group.[198][199]

Intel received a 100% rating on the first Corporate Equality Index released by the Human Rights Campaign in 2002. It has maintained this rating in 2003 and 2004. In addition, the company was named one of the 100 Best Companies for Working Mothers in 2005 by Working Mother magazine.[citation needed]

In January 2015, Intel announced the investment of  300 million over the next five years to enhance gender and racial diversity in their own company as well as the technology industry as a whole.[200][201][202][203][204]

In February 2016, Intel released its Global Diversity & Inclusion 2015 Annual Report.[205] The male-female mix of US employees was reported as 75.2% men and 24.8% women. For US employees in technical roles, the mix was reported as 79.8% male and 20.1% female.[205] NPR reports that Intel is facing a retention problem (particularly for African Americans), not just a pipeline problem.[206]

Economic impact in Oregon in 2009[edit]
In 2011, ECONorthwest conducted an economic impact analysis of Intel's economic contribution to the state of Oregon. The report found that in 2009 "the total economic impacts attributed to Intel's operations, capital spending, contributions and taxes amounted to almost  14.6 billion in activity, including  4.3 billion in personal income and 59,990 jobs."[207] Through multiplier effects, every 10 Intel jobs supported, on average, was found to create 31 jobs in other sectors of the economy.[208]

School funding in New Mexico in 1997[edit]
In Rio Rancho, New Mexico, Intel is the leading employer.[209] In 1997, a community partnership between Sandoval County and Intel Corporation funded and built Rio Rancho High School.[210][211]

Ultrabook fund (2011)[edit]
In 2011, Intel Capital announced a new fund to support startups working on technologies in line with the company's concept for next generation notebooks.[212] The company is setting aside a  300 million fund to be spent over the next three to four years in areas related to ultrabooks.[212] Intel announced the ultrabook concept at Computex in 2011. The ultrabook is defined as a thin (less than 0.8 inches [~2 cm] thick[213]) notebook that utilizes Intel processors[213] and also incorporates tablet features such as a touch screen and long battery life.[212][213]

At the Intel Developers Forum in 2011, four Taiwan ODMs showed prototype ultrabooks that used Intel's Ivy Bridge chips.[214] Intel plans to improve power consumption of its chips for ultrabooks, like new Ivy Bridge processors in 2013, which will only have 10W default thermal design power.[215]

Intel's goal for Ultrabook's price is below  1000;[213] however, according to two presidents from Acer and Compaq, this goal will not be achieved if Intel does not lower the price of its chips.[216]

Advertising and brand management[edit]
Intel Inside[edit]
Intel has become one of the world's most recognizable computer brands following its long-running Intel Inside campaign. The idea for "Intel Inside" came out of a meeting between Intel and one of the major computer resellers, MicroAge.[217]

In the late 1980s, Intel's market share was being seriously eroded by upstart competitors such as American Micro Devices (now AMD), Zilog, and others who had started to sell their less expensive microprocessors to computer manufacturers. This was because, by using cheaper processors, manufacturers could make cheaper computers and gain more market share in an increasingly price-sensitive market. In 1989, Intel's Dennis Carter visited MicroAge's headquarters in Tempe, Arizona, to meet with MicroAge's VP of Marketing, Ron Mion. MicroAge had become one of the largest distributors of Compaq, IBM, HP, and others and thus was a primary   although indirect   driver of demand for microprocessors. Intel wanted MicroAge to petition its computer suppliers to favor Intel chips. However, Mion felt that the marketplace should decide which processors they wanted. Intel's counterargument was that it would be too difficult to educate PC buyers on why Intel microprocessors were worth paying more for ... and they were right.[217] But Mion felt that the public didn't really need to fully understand why Intel chips were better, they just needed to feel they were better. So Mion proposed a market test. Intel would pay for a MicroAge billboard somewhere saying, "If you're buying a personal computer, make sure it has Intel inside." In turn, MicroAge would put "Intel Inside" stickers on the Intel-based computers in their stores in that area. To make the test easier to monitor, Mion decided to do the test in Boulder, Colorado, where it had a single store. Virtually overnight, the sales of personal computers in that store dramatically shifted to Intel-based PCs. Intel very quickly adopted "Intel Inside" as its primary branding and rolled it out worldwide.[217]

As is often the case with computer lore, other tidbits have been combined to explain how things evolved. "Intel Inside" has not escaped that tendency and there are other "explanations" that had been floating around.

Intel's branding campaign started with "The Computer Inside" tagline in 1990 in US and Europe. The Japan chapter of Intel proposed an "Intel in it" tagline and kicked off the Japanese campaign by hosting EKI-KON (meaning "Station Concert" in Japanese) at the Tokyo railway station dome on Christmas Day, December 25, 1990. Several months later, "The Computer Inside" incorporated the Japan idea to become "Intel Inside" which eventually elevated to the worldwide branding campaign in 1991, by Intel marketing manager Dennis Carter.[218] The case study of the Inside Intel Inside was put together by Harvard Business School.[219] The five-note jingle was introduced in 1994 and by its tenth anniversary was being heard in 130 countries around the world. The initial branding agency for the "Intel Inside" campaign was DahlinSmithWhite Advertising of Salt Lake City. The Intel swirl logo was the work of DahlinSmithWhite art director Steve Grigg under the direction of Intel president and CEO Andy Grove.[citation needed]


The Intel Inside logo from 1991 to 2006
The Intel Inside advertising campaign sought public brand loyalty and awareness of Intel processors in consumer computers.[220] Intel paid some of the advertiser's costs for an ad that used the Intel Inside logo and xylo-marimba jingle.[221]


2009 2011 badge design
In 2008, Intel planned to shift the emphasis of its Intel Inside campaign from traditional media such as television and print to newer media such as the Internet.[222] Intel required that a minimum of 35% of the money it provided to the companies in its co-op program be used for online marketing.[222] The Intel 2010 annual financial report indicated that  1.8 billion (6% of the gross margin and nearly 16% of the total net income) was allocated to all advertising with Intel Inside being part of that.[223]

Sonic logo[edit]
The famous D   D   G   D   A  xylophone/xylomarimba jingle, sonic logo, tag, audio mnemonic was produced by Musikvergnuegen and written by Walter Werzowa, once a member of the Austrian 1980s sampling band Edelweiss.[224] The sonic Intel logo has undergone substantial changes in tone since the introduction of the Pentium III, Pentium 4, and Core processors, yet keeps the same jingle.

Processor naming strategy[edit]
In 2006, Intel expanded its promotion of open specification platforms beyond Centrino, to include the Viiv media center PC and the business desktop Intel vPro.

In mid-January 2006, Intel announced that they were dropping the long running Pentium name from their processors. The Pentium name was first used to refer to the P5 core Intel processors and was done to comply with court rulings that prevent the trademarking of a string of numbers, so competitors could not just call their processor the same name, as had been done with the prior 386 and 486 processors (both of which had copies manufactured by IBM and AMD). They phased out the Pentium names from mobile processors first, when the new Yonah chips, branded Core Solo and Core Duo, were released. The desktop processors changed when the Core 2 line of processors were released. By 2009 Intel was using a good-better-best strategy with Celeron being good, Pentium better, and the Intel Core family representing the best the company has to offer.[225]

According to spokesman Bill Calder, Intel has maintained only the Celeron brand, the Atom brand for netbooks and the vPro lineup for businesses. Since late 2009, Intel's mainstream processors have been called Celeron, Pentium, Core i3, Core i5, and Core i7, in order of performance from lowest to highest. The first generation core products carry a 3 digit name, such as i5 750, and the second generation products carry a 4 digit name, such as the i5 2500. In both cases, a K at the end of it shows that it is an unlocked processor, enabling additional overclocking abilities (for instance, 2500K). vPro products will carry the Intel Core i7 vPro processor or the Intel Core i5 vPro processor name.[226] In October 2011, Intel started to sell its Core i7-2700K "Sandy Bridge" chip to customers worldwide.[227]

Since 2010, "Centrino" is only be applied to Intel's WiMAX and Wi-Fi technologies.[226]

Open source support[edit]
Intel has a significant participation in the open source communities since 1999.[228] For example, in 2006 Intel released MIT-licensed X.org drivers for their integrated graphic cards of the i965 family of chipsets. Intel released FreeBSD drivers for some networking cards,[229] available under a BSD-compatible license,[230] which were also ported to OpenBSD.[230] Binary firmware files for non-wireless Ethernet devices were also released under a BSD licence allowing free redistribution.[231] Intel ran the Moblin project until April 23, 2009, when they handed the project over to the Linux Foundation. Intel also runs the LessWatts.org campaigns.[232]

However, after the release of the wireless products called Intel Pro/Wireless 2100, 2200BG/2225BG/2915ABG and 3945ABG in 2005, Intel was criticized for not granting free redistribution rights for the firmware that must be included in the operating system for the wireless devices to operate.[233] As a result of this, Intel became a target of campaigns to allow free operating systems to include binary firmware on terms acceptable to the open source community. Linspire-Linux creator Michael Robertson outlined the difficult position that Intel was in releasing to open source, as Intel did not want to upset their large customer Microsoft.[234] Theo de Raadt of OpenBSD also claimed that Intel is being "an Open Source fraud" after an Intel employee presented a distorted view of the situation at an open-source conference.[235] In spite of the significant negative attention Intel received as a result of the wireless dealings, the binary firmware still has not gained a license compatible with free software principles.[236]

PC declining sales[edit]
Due to PC declining sales and the condition prediction will be worse which Intel support the chipsets, so in 2016 Intel cuts 12,000 jobs. In 2014, Intel has also cut 5,000 jobs.[237]

Controversies[edit]
Patent infringement litigation (2006-2007)[edit]
In October 2006, a Transmeta lawsuit was filed against Intel for patent infringement on computer architecture and power efficiency technologies.[238] The lawsuit was settled in October 2007, with Intel agreeing to pay US 150 million initially and US 20 million per year for the next five years. Both companies agreed to drop lawsuits against each other, while Intel was granted a perpetual non-exclusive license to use current and future patented Transmeta technologies in its chips for 10 years.[239]

Anti-competitive allegations and litigation (2005-2009)[edit]
See also: AMD v. Intel
In September 2005, Intel filed a response to an AMD lawsuit,[240] disputing AMD's claims, and claiming that Intel's business practices are fair and lawful. In a rebuttal, Intel deconstructed AMD's offensive strategy and argued that AMD struggled largely as a result of its own bad business decisions, including underinvestment in essential manufacturing capacity and excessive reliance on contracting out chip foundries.[241] Legal analysts predicted the lawsuit would drag on for a number of years, since Intel's initial response indicated its unwillingness to settle with AMD.[242][243] In 2008 a court date was finally set,[244] but in 2009 Intel settled with a  1.25 billion payout to AMD (see below).[245]

On November 4, 2009, New York's attorney general filed an antitrust lawsuit against Intel Corp, claiming the company used "illegal threats and collusion" to dominate the market for computer microprocessors.

On November 12, 2009, AMD agreed to drop the antitrust lawsuit against Intel in exchange for  1.25 billion.[245] A joint press release published by the two chip makers stated "While the relationship between the two companies has been difficult in the past, this agreement ends the legal disputes and enables the companies to focus all of our efforts on product innovation and development."[246][247]

Main article: High-Tech Employee Antitrust Litigation
An antitrust lawsuit[248] and a class-action suit relating to cold calling employees of other companies has been settled. [249]

Allegations by Japan Fair Trade Commission (2005)[edit]
In 2005, the local Fair Trade Commission found that Intel violated the Japanese Antimonopoly Act. The commission ordered Intel to eliminate discounts that had discriminated against AMD. To avoid a trial, Intel agreed to comply with the order.[250][251][252][253]

Allegations by the European Union (2007-2008)[edit]
In July 2007, the European Commission accused Intel of anti-competitive practices, mostly against AMD.[254] The allegations, going back to 2003, include giving preferential prices to computer makers buying most or all of their chips from Intel, paying computer makers to delay or cancel the launch of products using AMD chips, and providing chips at below standard cost to governments and educational institutions.[255] Intel responded that the allegations were unfounded and instead qualified its market behavior as consumer-friendly.[255] General counsel Bruce Sewell responded that the Commission had misunderstood some factual assumptions as to pricing and manufacturing costs.[256]

In February 2008, Intel stated that its office in Munich had been raided by European Union regulators. Intel reported that it was cooperating with investigators.[257] Intel faced a fine of up to 10% of its annual revenue, if found guilty of stifling competition.[258] AMD subsequently launched a website promoting these allegations.[259][260] In June 2008, the EU filed new charges against Intel.[261] In May 2009, the EU found that Intel had engaged in anti-competitive practices and subsequently fined Intel 1.06 billion (US 1.44 billion), a record amount. Intel was found to have paid companies, including Acer, Dell, HP, Lenovo and NEC,[262] to exclusively use Intel chips in their products, and therefore harmed other companies including AMD.[262][263][264] The European Commission said that Intel had deliberately acted to keep competitors out of the computer chip market and in doing so had made a "serious and sustained violation of the EU's antitrust rules".[262] In addition to the fine, Intel was ordered by the Commission to immediately cease all illegal practices.[262] Intel has stated that they will appeal against the Commission's verdict. In June 2014, the General Court, which sits below the European Court of Justice, rejected the appeal.[262]

Allegations by regulators in South Korea (2007)[edit]
In September 2007, South Korean regulators accused Intel of breaking antitrust law. The investigation began in February 2006, when officials raided Intel's South Korean offices. The company risked a penalty of up to 3% of its annual sales, if found guilty.[265] In June 2008, the Fair Trade Commission ordered Intel to pay a fine of US 25.5 million for taking advantage of its dominant position to offer incentives to major Korean PC manufacturers on the condition of not buying products from AMD.[266]

Allegations by regulators in the United States (2008-2010)[edit]
New York started an investigation of Intel in January 2008 on whether the company violated antitrust laws in pricing and sales of its microprocessors.[267] In June 2008, the Federal Trade Commission also began an antitrust investigation of the case.[268] In December 2009, the FTC announced it would initiate an administrative proceeding against Intel in September 2010.[269][270][271][272]

In November 2009, following a two-year investigation, New York Attorney General Andrew Cuomo sued Intel, accusing them of bribery and coercion, claiming that Intel bribed computer makers to buy more of their chips than those of their rivals, and threatened to withdraw these payments if the computer makers were perceived as working too closely with its competitors. Intel has denied these claims.[273]

On July 22, 2010, Dell agreed to a settlement with the U.S. Securities and Exchange Commission (SEC) to pay  100M in penalties resulting from charges that Dell did not accurately disclose accounting information to investors. In particular, the SEC charged that from 2002 to 2006, Dell had an agreement with Intel to receive rebates in exchange for not using chips manufactured by AMD. These substantial rebates were not disclosed to investors, but were used to help meet investor expectations regarding the company's financial performance; "These exclusivity payments grew from 10 percent of Dell's operating income in FY 2003 to 38 percent in FY 2006, and peaked at 76 percent in the first quarter of FY 2007.".[274] Dell eventually did adopt AMD as a secondary supplier in 2006, and Intel subsequently stopped their rebates, causing Dell's financial performance to fall.[275][276][277]

Corporate responsibility record[edit]
Intel has been accused by some residents of Rio Rancho, New Mexico of allowing VOCs to be released in excess of their pollution permit. One resident claimed that a release of 1.4 tons of carbon tetrachloride was measured from one acid scrubber during the fourth quarter of 2003 but an emission factor allowed Intel to report no carbon tetrachloride emissions for all of 2003.[278]

Another resident alleges that Intel was responsible for the release of other VOCs from their Rio Rancho site and that a necropsy of lung tissue from two deceased dogs in the area indicated trace amounts of toluene, hexane, ethylbenzene, and xylene isomers,[279] all of which are solvents used in industrial settings but also commonly found in gasoline, retail paint thinners and retail solvents. During a sub-committee meeting of the New Mexico Environment Improvement Board, a resident claimed that Intel's own reports documented more than 1,580 pounds (720 kg) of VOCs were released in June and July 2006.[280]

Intel's environmental performance is published annually in their corporate responsibility report.[281]

In its 2012 rankings on the progress of consumer electronics companies relating to conflict minerals, the Enough Project rated Intel the best of 24 companies, calling it a "Pioneer of progress".[282] In 2014, chief executive Brian Krzanich urged the rest of the industry to follow Intel's lead by also shunning conflict minerals.[283]

Religious controversy in Israel (2009)[edit]
Orthodox Jews have protested against Intel operating in Israel on Saturday, Shabbat. Intel ringed its office with barbed wire before the protest, but there was no violence.[284] As of December 2009, the situation has been stable for Intel Israel while some employees reported working overtime on Shabbat.

Age discrimination complaints[edit]
Intel has faced complaints of age discrimination in firing and layoffs. Intel was sued in 1993 by nine former employees, over allegations that they were laid off because they were over the age of 40.[285]

A group called FACE Intel (Former and Current Employees of Intel) claims that Intel weeds out older employees. FACE Intel claims that more than 90 percent of people who have been laid off or fired from Intel are over the age of 40. Upside magazine requested data from Intel breaking out its hiring and firing by age, but the company declined to provide any.[286] Intel has denied that age plays any role in Intel's employment practices.[287] FACE Intel was founded by Ken Hamidi, who was fired from Intel in 1995 at the age of 47.[286] Hamidi was blocked in a 1999 court decision from using Intel's email system to distribute criticism of the company to employees,[288] which overturned in 2003 in Intel Corp. v. Hamidi.

Land dispute in Israel[edit]
According to the Christian Science Monitor, Intel's plant at Kiryat Gat was built on the site of former Palestinian villages of Al-Faluja and Iraq al-Manshiyya; and Al-Awda   the Palestinian Right to Return Coalition   a supporter of Boycott, Divestment and Sanctions, has called on Intel to close this plant.[289]

See also[edit]
Portal icon San Francisco Bay Area portal
Portal icon Companies portal
5 nm The Quantum tunneling leakage Wall
ASCI Red
AMD
Comparison of ATI Graphics Processing Units
Comparison of Intel processors
Comparison of Nvidia graphics processing units
Cyrix
Engineering sample (CPU)
Graphics Processing Unit (GPU)
Intel Driver Update Utility
Intel GMA (Graphics Media Accelerator)
Intel Museum
Intel Science Talent Search
Intel Developer Zone (Intel DZ)
List of Intel chipsets
List of Intel CPU microarchitectures
List of Intel manufacturing sites
List of Intel microprocessors
List of Semiconductor Fabrication Plants
Semiconductor sales leaders by year
Wintel
Intel related biographical articles on Wikipedia:

Andy Grove
Bill Gaede
Bob Colwell
Craig Barrett (chief executive)
Gordon Moore
Justin Rattner
Pat Gelsinger
Paul Otellini
Robert Noyce
Sean Maloney

Advanced Micro Devices
From Wikipedia, the free encyclopedia
  (Redirected from AMD)
"AMD" redirects here. For other uses, see AMD (disambiguation).
Advanced Micro Devices, Inc.
AMD Logo
Amdheadquarters.jpg
Headquarters in Sunnyvale, California
Type
Public
Traded as NASDAQ: AMD
Industry  Semiconductors
Founded May 1, 1969; 46 years ago
Founder Jerry Sanders
Headquarters  One AMD Place,[1]
Sunnyvale, California, United States
Area served
Worldwide
Key people
Lisa Su (CEO)[2]
Bruce Claflin (Executive Chairman)
Products  Microprocessors
Motherboard chipsets
Graphics processing units
Random-access memory[3]
TV tuner cards[4]
Revenue Decrease  3.991 billion (2015)[5]
Operating income
Decrease - 481 million (2015)[5]
Net income
Decrease - 660 million (2015)[5]
Total assets  Decrease US  3.109 billion (2015)[5]
Total equity  Decrease - 412 million(2015)[5]
Number of employees
9,139 (2015)[5]
Divisions SeaMicro
Slogan  Enabling today.
Inspiring tomorrow.
Website www.amd.com
Advanced Micro Devices, Inc. (AMD) is an American multinational semiconductor company based in Sunnyvale, California, United States, that develops computer processors and related technologies for business and consumer markets. While initially it manufactured its own processors, the company became fabless after GlobalFoundries was spun off in 2009. AMD's main products include microprocessors, motherboard chipsets, embedded processors and graphics processors for servers, workstations and personal computers, and embedded systems applications.

AMD is the second-largest supplier and only significant rival to Intel in the market for x86-based microprocessors. Since acquiring ATI in 2006, AMD and its competitor Nvidia have dominated the discrete graphics processor unit (GPU) market.[6]

Contents  [hide] 
1 Company history
1.1 First twelve years
1.2 Technology exchange agreement with Intel
2 History of CPUs and APUs
2.1 IBM PC and the x86 architecture
2.2 K5, K6, Athlon, Duron, and Sempron
2.3 Athlon 64, Opteron and Phenom
2.4 Fusion becomes the AMD APU, and new microarchitectures
2.5 ARM architecture-based chip
2.6 Zen based CPUs and APUs
3 Other products and technologies
3.1 Graphics products (discrete and AMD APU technology)
3.2 AMD motherboard chipsets
3.3 AMD Live!
3.4 AMD Quad FX platform
3.5 Server platform
3.6 Desktop platforms
3.7 Embedded systems
3.8 Other initiatives
3.9 Software
4 Production and fabrication
5 Corporate affairs
5.1 Partnerships
5.2 Litigation with Intel
5.3 Guinness World Record Achievement
5.4 Corporate social responsibility
6 See also
7 References
8 Notes
9 External links
Company history[edit]

AMD campus in Markham, Ontario, Canada, formerly ATI headquarters

AMD's LEED-certified Lone Star campus in Austin, Texas
First twelve years[edit]
Advanced Micro Devices was formally incorporated on May 1, 1969, by Jerry Sanders, along with seven of his colleagues from Fairchild Semiconductor.[7][8] Sanders, an electrical engineer who was the director of marketing at Fairchild, had like many Fairchild executives grown frustrated with the increasing lack of support, opportunity, and flexibility within that company, and decided to leave to start his own semiconductor company.[9] The previous year Robert Noyce, who had invented the first practical integrated circuit or microchip in 1959 at Fairchild,[10] had left Fairchild together with Gordon Moore and founded the semiconductor company Intel in July 1968.[11]

In September 1969, AMD moved from its temporary location in Santa Clara to Sunnyvale, California.[12] To immediately secure a customer base, AMD initially became a second source supplier of microchips designed by Fairchild and National Semiconductor.[13][14] AMD first focused on producing logic chips.[15] The company guaranteed quality control to United States Military Standard, an advantage in the early computer industry since unreliability in microchips was a distinct problem that customers   including computer manufacturers, the telecommunications industry, and instrument manufacturers   wanted to avoid.[13][16][17][18]

In November 1969, the company manufactured its first product, the Am9300, a 4-bit MSI shift register, which began selling in 1970.[18][19] Also in 1970, AMD produced its first proprietary product, the Am2501 logic counter, which was highly successful.[20][21] Its best-selling product in 1971 was the Am2505, the fastest multiplier available.[20][22]

In 1971, AMD entered the RAM chip market, beginning with the Am3101, a 64-bit bipolar RAM.[22][23] That year AMD also greatly increased the sales volume of its linear integrated circuits, and by year end the company's total annual sales reached  4.6 million.[20][24]

AMD went public in September 1972.[13][25][26] The company was a second source for Intel MOS/LSI circuits by 1973, with products such as Am14/1506 and Am14/1507, dual 100-bit dynamic shift registers.[27][28] By 1975, AMD was producing 212 products   of which 49 were proprietary, including the Am9102 (a static N-channel 1024-bit RAM)[29] and three low-power Schottky MSI circuits: Am25LS07, Am25LS08, and Am25LS09.[30]

Intel had created the first microprocessor, its 4-bit 4004, in 1971.[31][32] By 1975, AMD entered the microprocessor market with the Am9080, a reverse-engineered clone of the Intel 8080,[33][34][35] and the Am2900 bit-slice microprocessor family.[34] When Intel began installing microcode in its microprocessors in 1976, it entered into a cross-licensing agreement with AMD, granting AMD a copyright license to the microcode in its microprocessors and peripherals, effective October 1976.[30][36][37][38][39]

In 1977, AMD entered into a joint venture with Siemens, a German engineering conglomerate wishing to enhance its technology expertise and enter the U.S. market.[40] Siemens purchased 20% of AMD's stock, giving AMD an infusion of cash to increase its product lines.[40][41][42] That year the two companies also jointly established Advanced Micro Computers, located in Silicon Valley and in Germany, giving AMD an opportunity to enter the microcomputer development and manufacturing field,[40][43][44][45] in particular based on AMD's second-source Zilog Z8000 microprocessors.[46][47] When the two companies' vision for Advanced Micro Computers diverged, AMD bought out Siemens' stake in the U.S. division in 1979.[48][49] AMD closed its Advanced Micro Computers subsidiary in late 1981, after switching focus to manufacturing second-source Intel x86 microprocessors.[46][50][51]

Total sales in fiscal year 1978 topped  100 million,[43] and in 1979, AMD debuted on the New York Stock Exchange.[21] In 1979, production also began in AMD's new semiconductor fab in Austin;[21] the company already had overseas assembly facilities in Penang and Manila,[52] and it began construction on a semiconductor fab in San Antonio in 1981.[53] In 1980, AMD began supplying semiconductor products for telecommunications, an industry undergoing rapid expansion and innovation.[54]

Technology exchange agreement with Intel[edit]
Intel had introduced the first x86 microprocessors in 1978.[55] In 1981, IBM created its PC, and wanted Intel's x86 processors, but only under the condition that Intel also provide a second-source manufacturer for its patented x86 microprocessors.[16] Intel and AMD entered into a 10-year technology exchange agreement, first signed in October 1981[50][56] and formally executed in February 1982.[39] The terms of the agreement were that each company could acquire the right to become a second-source manufacturer for semiconductor products developed by the other; that is, each party could "earn" the right to manufacture and sell a product developed by the other, if agreed to, by exchanging the manufacturing rights to a product of equivalent technical complexity. The technical information and licenses needed to make and sell a part would be exchanged for a royalty to the developing company.[38] The 1982 agreement also extended the 1976 AMD Intel cross-licensing agreement through 1995.[38][39] The agreement included the right to invoke arbitration of disagreements, and after five years the right of either party to end the agreement with one year's notice.[38] The main result of the 1982 agreement was that AMD became a second-source manufacturer of Intel's x86 microprocessors and related chips, and Intel provided AMD with database tapes for its 8086, 80186, and 80286 chips.[39]

Beginning in 1982, AMD began volume-producing second-source Intel-licensed 8086, 8088, 80186, and 80188 processors, and by 1984 its own Am286 clone of Intel's 80286 processor, for the rapidly growing market of IBM PCs and IBM clones.[16][57] It also continued its successful concentration on proprietary bipolar chips.[58] In 1983, it introduced INT.STD.1000, the highest manufacturing quality standard in the industry.[18][53]

The company continued to spend greatly on research and development,[59] and in addition to other breakthrough products, created the world's first 512K EPROM in 1984.[60] That year AMD was listed in the book The 100 Best Companies to Work for in America,[53][61] and based on 1984 income it made the Fortune 500 list for the first time in 1985.[62][63]

By mid-1985, however, the microchip market experienced a severe downturn, mainly due to longterm aggressive trade practices (dumping) from Japan, but also due to a crowded and non-innovative chip market in the U.S.[64] AMD rode out the mid-1980s crisis by aggressively innovating and modernizing,[65] devising the Liberty Chip program of designing and manufacturing one new chip or chip set per week for 52 weeks in fiscal year 1986,[53][66] and by heavily lobbying the U.S. government until sanctions and restrictions were put into place to prevent predatory Japanese pricing.[67] During this time period, AMD withdrew from the DRAM market,[68] and at the same time made some headway into the CMOS market, which it had lagged in entering, having focused instead on bipolar chips.[69]

AMD had some success in the mid-1980s with the AMD7910 and AMD7911 "World Chip" FSK modem, one of the first multi-standard devices that covered both Bell and CCITT tones at up to 1200 baud half duplex or 300/300 full duplex.[70] Beginning in 1986, AMD embraced the perceived shift toward RISC with their own AMD Am29000 (29k) processor;[71] the 29k survived as an embedded processor.[72][73] The company also increased its EPROM memory market share in the late 1980s.[74] Throughout the 1980s, AMD was a second-source supplier of Intel x86 processors. In 1991, it introduced its own 386-compatible Am386, an AMD-designed chip. Creating its own chips, AMD began to compete directly with Intel.[75]

AMD had a large and successful flash memory business, even during the dotcom bust.[76] In 2003, to divest some manufacturing and aid its overall cash flow, which was under duress from aggressive microprocessor competition from Intel, AMD spun-off its flash memory business and manufacturing into Spansion, a joint venture with Fujitsu, which had been co-manufacturing flash memory with AMD since 1993.[77][78] AMD divested itself of Spansion in December 2005, in order to focus on the microprocessor market, and Spansion went public in an IPO.[79]

AMD announced the acquisition of the graphics processor company ATI Technologies on July 24, 2006. AMD paid  4.3 billion in cash and 58 million shares of its stock, for a total of approximately  5.4 billion. The transaction completed on October 25, 2006.[80] On August 30, 2010, AMD announced that it would retire the ATI brand name for its graphics chipsets in favor of the AMD brand name.[81][82]

In October 2008, AMD announced plans to spin off manufacturing operations in the form of a multibillion-dollar joint venture with Advanced Technology Investment Co., an investment company formed by the government of Abu Dhabi. The new venture is called GlobalFoundries Inc. The partnership and spin-off gave AMD an infusion of cash and allowed AMD to focus solely on chip design.[83] To assure the Abu Dhabi investors of the new venture's success, CEO Hector Ruiz stepped down as CEO of AMD in July 2008, while remaining Executive Chairman, in preparation to becoming Chairman of Global Foundries in March 2009.[84][85] President and COO Dirk Meyer became AMD's CEO.[86] Recessionary losses necessitated AMD cutting 1,100 jobs in 2009.[87]

In August 2011, AMD announced that former Lenovo executive Rory Read would be joining the company as CEO, replacing Meyer.[88] AMD announced in November 2011 plans to lay off more than 10% (1,400) of its employees from across all divisions worldwide.[89] In October 2012, it announced plans to lay off an additional 15% of its workforce to reduce costs in the face of declining sales revenue.[90]

AMD acquired the low-power server manufacturer SeaMicro in early 2012, with an eye to bringing out an ARM architecture server chip.[91]

On October 8, 2014, AMD announced that Rory Read had stepped down after three years as president and chief executive officer.[2] He was succeeded by Lisa Su, a key lieutenant who had been serving as chief operating officer since June.[92]

On October 16, 2014, AMD announced a new restructuring plan along with its Q3 results. Effective July 1, 2014, AMD reorganized into two business groups: Computing and Graphics, which primarily includes desktop and notebook processors and chipsets, discrete GPUs, and professional graphics; and Enterprise, Embedded and Semi-Custom, which primarily includes server and embedded processors, dense servers, semi-custom SoC products, engineering services, and royalties. As part of this restructuring AMD announced that 7% of its global workforce would be laid off by the end of 2014.[93]

History of CPUs and APUs[edit]

Early AMD 9080 Processor (AMD AM9080ADC / C8080A), 1977

AMD D8086, 1978
See also: List of AMD microprocessors
IBM PC and the x86 architecture[edit]
Main articles: Am286, Am386, Am486 and Am5x86
In February 1982, AMD signed a contract with Intel, becoming a licensed second-source manufacturer of 8086 and 8088 processors. IBM wanted to use the Intel 8088 in its IBM PC, but IBM's policy at the time was to require at least two sources for its chips. AMD later produced the Am286 under the same arrangement. In 1984 Intel, in order to shore up its advantage in the marketplace, internally decided to no longer cooperate with AMD in supplying product information, and delayed and eventually refused to convey the technical details of the Intel 80386 to AMD.[94] In 1987, AMD invoked arbitration over the issue, and Intel reacted by cancelling the 1982 technological-exchange agreement altogether.[95][96] After three years of testimony, AMD eventually won in arbitration in 1992, but Intel disputed this decision. Another long legal dispute followed, ending in 1994 when the Supreme Court of California sided with the arbitrator and AMD.[97][98]

In 1990, Intel also countersued AMD, reneging on AMD's right to use derivatives of Intel's microcode for its cloned processors.[99] In the face of uncertainty during the legal dispute, AMD was forced to develop clean room designed versions of Intel code for its x386 and x486 processors, the former long after Intel had released its own x386 in 1985.[100] In March 1991, AMD released the Am386, its clone of the Intel 386 processor.[53] By October of the same year it had sold one million units.[53]

In 1993, AMD introduced the first of the Am486 family of processors,[21] which proved popular with a large number of original equipment manufacturers, including Compaq, which signed an exclusive agreement using the Am486.[13][101][102] Another Am486-based processor, the Am5x86, was released in November 1995 and continued AMD's success as a fast, cost-effective processor.[103][104]

Finally, in an agreement effective 1996, AMD received the rights to the microcode in Intel's x386 and x486 processor families, but not the rights to the microcode in the following generations of processors.[105][106]

K5, K6, Athlon, Duron, and Sempron[edit]
Main articles: AMD K5, AMD K6, Athlon, Duron and Sempron
AMD's first in-house x86 processor was the K5, which was launched in 1996.[107] The "K" was a reference to Kryptonite. (In comic books, the only substance which could harm Superman was Kryptonite. This is a reference to Intel's hegemony over the market, i.e., an anthropomorphization of them as Superman.[108]) The numeral "5" refers to the fifth generation of x86 processors; rival Intel had previously introduced its line of fifth-generation x86 processors as Pentium because the U.S. Trademark and Patent Office had ruled that mere numbers could not be trademarked.[109]

In 1996, AMD purchased NexGen, specifically for the rights to their Nx series of x86-compatible processors. AMD gave the NexGen design team their own building, left them alone, and gave them time and money to rework the Nx686. The result was the K6 processor, introduced in 1997. Although the K6 was based on Socket 7, variants such as K6-3/450 were faster than Intel's Pentium II (sixth-generation processor).

The K7 was AMD's seventh-generation x86 processor, making its debut on June 23, 1999, under the brand name Athlon. Unlike previous AMD processors, it could not be used on the same motherboards as Intel's, due to licensing issues surrounding Intel's Slot 1 connector, and instead used a Slot A connector, referenced to the Alpha processor bus. The Duron was a lower-cost and limited version of the Athlon (64KB instead of 256KB L2 cache) in a 462-pin socketed PGA (socket A) or soldered directly onto the motherboard. Sempron was released as a lower-cost Athlon XP, replacing Duron in the socket A PGA era. It has since been migrated upward to all new sockets, up to AM3.

On October 9, 2001, the Athlon XP was released. On February 10, 2003, the Athlon XP with 512KB L2 Cache was released.[110]

Athlon 64, Opteron and Phenom[edit]
Main articles: Athlon 64, Opteron and Phenom (processor)
The K8 was a major revision of the K7 architecture, with the most notable features being the addition of a 64-bit extension to the x86 instruction set (called x86-64, AMD64, or x64), the incorporation of an on-chip memory controller, and the implementation of an extremely high performance point-to-point interconnect called HyperTransport, as part of the Direct Connect Architecture. The technology was initially launched as the Opteron server-oriented processor on April 22, 2003.[111] Shortly thereafter it was incorporated into a product for desktop PCs, branded Athlon 64.[112]

On April 21, 2005, AMD released the first dual core Opteron, an x86-based server CPU.[113] A month later, AMD released the Athlon 64 X2, the first desktop-based dual core processor family.[114] In May 2007, AMD abandoned the string "64" in its dual-core desktop product branding, becoming Athlon X2, downplaying the significance of 64-bit computing in its processors. Further updates involved improvements to the microarchitecture, and a shift of target market from mainstream desktop systems to value dual-core desktop systems. In 2008, AMD started to release dual-core Sempron processors exclusively in China, branded as the Sempron 2000 series, with lower HyperTransport speed and smaller L2 cache. Thus AMD completed its dual-core product portfolio for each market segment.

After K8 came K10. In September 2007, AMD released the first K10 processors   nine quad-core Third Generation Opteron processors   followed in November by the Phenom processor for desktop. K10 processors came in dual-core, triple-core,[115] and quad-core versions, with all cores on a single die. AMD released a new platform, codenamed "Spider", which utilized the new Phenom processor, as well as an R770 GPU and a 790 GX/FX chipset from the AMD 700 chipset series. However, AMD built the Spider at 65nm, which was uncompetitive with Intel's smaller and more power-efficient 45nm.

In January 2009, AMD released a new processor line dubbed Phenom II, a refresh of the original Phenom built using the 45 nm process. AMD's new platform, codenamed  Dragon , utilised the new Phenom II processor, and an ATI R770 GPU from the R700 GPU family, as well as a 790 GX/FX chipset from the AMD 700 chipset series. The Phenom II came in dual-core, triple-core and quad-core variants, all using the same die, with cores disabled for the triple-core and dual-core versions. The Phenom II resolved issues that the original Phenom had, including a low clock speed, a small L3 cache and a Cool'n'Quiet bug that decreased performance. The Phenom II cost less but was not performance-competitive with Intel's mid-to-high-range Core 2 Quads. The Phenom II also enhanced the Phenom's memory controller, allowing it to use DDR3 in a new native socket AM3, while maintaining backwards compatibility with AM2+, the socket used for the Phenom, and allowing the use of the DDR2 memory that was used with the platform.

In April 2010, AMD released a new Phenom II hexa-core (6-core) processor codenamed "Thuban". This was a totally new die based on the hexa-core  Istanbul  Opteron processor. It included AMD's  turbo core  technology, which allows the processor to automatically switch from 6 cores to 3 faster cores when more pure speed is needed. AMD's enthusiast platform, codenamed "Leo", utilized the new Phenom II, a new chipset from the AMD 800 chipset series and an ATI  Cypress  GPU from the Evergreen GPU series.

Ambox current red.svg
This section is outdated. Please update this article to reflect recent events or newly available information. (July 2014)
The Magny Cours and Lisbon server parts were released in 2010. The Magny Cours part came in 8 to 12 cores and the Lisbon part in 4 and 6 core parts. Magny Cours is focused on performance while the Lisbon part is focused on high performance per watt. Magny Cours is an MCM (Multi-Chip Module) with two hexa-core  Istanbul  Opteron parts. This will use a new G34 socket for dual and quad socket processors and thus will be marketed as Opteron 61xx series processors. Lisbon uses C32 socket certified for dual socket use or single socket use only and thus will be marketed as Opteron 41xx processors. Both will be built on a 45 nm SOI process.

Fusion becomes the AMD APU, and new microarchitectures[edit]
Main articles: AMD APU, AMD mobile platform, The AMD FX and Opteron Bulldozer microarchitecture, The AMD APU Bobcat microarchitecture and The Jaguar microarchitecture used in multiple processors and APUs
Following AMD's 2006 acquisition of Canadian graphics company ATI Technologies, an initiative codenamed Fusion was announced to integrate a CPU and GPU together on some of AMD's microprocessors, including a built in PCI Express link to accommodate separate PCI Express peripherals, eliminating the northbridge chip from the motherboard. The initiative intended to move some of the processing originally done on the CPU (e.g. floating-point unit operations) to the GPU, which is better optimized for some calculations. The Fusion was later renamed to the AMD APU (Accelerated Processing Unit).[116]

Llano was AMD's first APU built for laptops. Llano was the second APU released,[117] targeted at the mainstream market.[116] Incorporating a CPU and GPU on the same die, as well as northbridge functions, and using "Socket FM1" with DDR3 memory. The CPU part of the processor was based on the Phenom II "Deneb" processor. AMD suffered an unexpected decrease in revenue based on production problems for the Llano.[118]

Bulldozer is AMD's microarchitecture codename for server and desktop AMD FX processors first released on October 12, 2011. This family 15h microarchitecture is the successor to the family 10h (K10) microarchitecture design. Bulldozer is designed from scratch, not a development of earlier processors.[119] The core is specifically aimed at 10-125 W TDP computing products. AMD claims dramatic performance-per-watt efficiency improvements in high-performance computing (HPC) applications with Bulldozer cores. While hopes were very high that Bulldozer would bring AMD to be performance competitive with arch rival Intel once more, most benchmarks were disappointing. In some cases the new Bulldozer products were slower than the K10 model they were built to replace.[120][121][122]

Hondo is AMD's latest processor series used in Tablet computers.[123]

Piledriver is the name of AMD's microarchitecture used in some AMD FX processors released in 2012. This AMD FX series processor lineup is called Vishera, and targets the desktop performance market.

Jaguar is a x86-64 microarchitecture codename for a processor core that is used in various APUs from AMD aimed at the low-power/low-cost market. It is also used as the microarchitecture for the custom APUs in the PS4 and Xbox One (which contain CPU, GPU and memory).

Jaguar's predecessor, Bobcat, was revealed during a speech from AMD executive vice-president Henri Richard in Computex 2007 and was put into production Q1 2011.[117] One of the major supporters was executive vice-president Mario A. Rivas who felt it was difficult to compete in the x86 market with a single core optimized for the 10-100 W range and actively promoted the development of the simpler core with a target range of 1-10 watts. In addition, it was believed that the core could migrate into the hand-held space if the power consumption can be reduced to less than 1 W.[citation needed]

ARM architecture-based chip[edit]
Ambox current red.svg
This section is outdated. Please update this article to reflect recent events or newly available information. (April 2016)
AMD intends to release 64-bit ARM System on Chips (SoC) that will begin sampling in early 2014 and shipping in the second half of 2015. They will be for use in servers as a low-power alternative to current x86 chips. Their implementation using the ARM architecture is codenamed "Seattle", based on the Cortex A57 core design (ARMv8-A), and will contain 8 and 16 cores each. They will include the proprietary SeaMicro "Freedom Fabric", as well as support for 128 GB RAM, and 10 gigabit Ethernet.[124] This is to be followed by the custom ARM core K12 core, expected in 2016-2017.[125]

Zen based CPUs and APUs[edit]
Main article: Zen (microarchitecture)
Zen is a new architecture for x86-64 based CPUs and APUs, built from the ground up by a team led by Jim Keller, beginning with his arrival in 2012, and taping out before his departure in September 2015. Zen will be built on the 14 nm node and have a renewed focus on single-core performance and HSA compatibility.[126] Zen will be the first chip encompassing CPUs and APUs from AMD built for a single socket. It will also support DDR4. It is expected to be released mid-late 2016, following the availability of the AMD A10-7890K FM2+ desktop CPU.

Other products and technologies[edit]

AMD Radeon memory
Graphics products (discrete and AMD APU technology)[edit]
See also: List of AMD graphics processing units and List of AMD Accelerated Processing Unit microprocessors
AMD's portfolio of dedicated graphics processors as of 2015 includes product families and associated technologies aimed at the consumer, professional and high-performance computing markets, such as:

Radeon   brand for consumer line of graphics cards; the brand name originated with ATI. Mobility Radeon offers power-optimized versions of Radeon graphics chips for use in laptops.
AMD FirePro   brand for professional line of graphics cards for workstations. Succeeds the FireGL series of workstation CAD/CAM video cards, the FireMV series and the AMD FireStream series.
AMD FireStream   brand for discontinued product line targeting stream processing and GPGPU as used in various industries.
AMD FireMV   brand for discontinued product line targeting multi-monitor setups in professional environments.
As of 2015 technologies found in AMD products include:

AMD Eyefinity   facilitates multi-monitor setup of up to 6 monitors per graphics card
AMD TrueAudio   acceleration of audio calculations
Unified Video Decoder (UVD)   acceleration of video decoding
Video Coding Engine (VCE)   acceleration of video encoding
AMD Catalyst is a collection of proprietary device driver software available for Microsoft Windows and Linux.

Since 2007, AMD has participated in the development of free and open-source graphics device drivers. The programming specifications for a number of chipsets and features were published in several rounds. Employees hired by AMD for this purpose contribute code to the Direct Rendering Manager in the Linux kernel.

AMD motherboard chipsets[edit]
See also: Comparison of AMD chipsets
Before the launch of Athlon 64 processors in 2003, AMD designed chipsets for their processors spanning the K6 and K7 processor generations. The chipsets include the AMD-640, AMD-751 and the AMD-761 chipsets. The situation changed in 2003 with the release of Athlon 64 processors, and AMD chose not to further design its own chipsets for its desktop processors while opening the desktop platform to allow other firms to design chipsets. This was the  Open Platform Management Architecture  with ATI, VIA and SiS developing their own chipset for Athlon 64 processors and later Athlon 64 X2 and Athlon 64 FX processors, including the Quad FX platform chipset from Nvidia.

The initiative went further with the release of Opteron server processors as AMD stopped the design of server chipsets in 2004 after releasing the AMD-8111 chipset, and again opened the server platform for firms to develop chipsets for Opteron processors. As of today, Nvidia and Broadcom are the sole designing firms of server chipsets for Opteron processors.

As the company completed the acquisition of ATI Technologies in 2006, the firm gained the ATI design team for chipsets which previously designed the Radeon Xpress 200 and the Radeon Xpress 3200 chipsets. AMD then renamed the chipsets for AMD processors under AMD branding (for instance, the CrossFire Xpress 3200 chipset was renamed as AMD 580X CrossFire chipset). In February 2007, AMD announced the first AMD-branded chipset since 2004 with the release of the AMD 690G chipset (previously under the development codename RS690), targeted at mainstream IGP computing. It was the industry's first to implement a HDMI 1.2 port on motherboards, shipping for more than a million units. While ATI had aimed at releasing an Intel IGP chipset, the plan was scrapped and the inventories of Radeon Xpress 1250 (codenamed RS600, sold under ATI brand) was sold to two OEMs, Abit and ASRock. Although AMD stated the firm would still produce Intel chipsets, Intel had not granted the license of 1333 MHz FSB to ATI.

On November 15, 2007, AMD announced a new chipset series portfolio, the AMD 7-Series chipsets, covering from enthusiast multi-graphics segment to value IGP segment, to replace the AMD 480/570/580 chipsets and AMD 690 series chipsets, marking AMD's first enthusiast multi-graphics chipset. Discrete graphics chipsets were launched on November 15, 2007 as part of the codenamed Spider desktop platform, and IGP chipsets were launched at a later time in spring 2008 as part of the codenamed Cartwheel platform.

AMD returned to the server chipsets market with the AMD 800S series server chipsets. It includes support for up to six SATA 6.0 Gbit/s ports, the C6 power state, which is featured in Fusion processors and AHCI 1.2 with SATA FIS based switching support. This is a chipset family supporting Phenom processors and Quad FX enthusiast platform (890FX), IGP(890GX).

AMD Live![edit]
Main article: AMD Live!
As of 2007, AMD LIVE! was a platform marketing initiative focusing the consumer electronics segment, with an Active TV initiative for streaming Internet videos from web video services such as YouTube, into AMD Live! PC as well as connected digital TVs, together with a scheme for an ecosystem of certified peripherals for the ease of customers to identify peripherals for AMD LIVE! systems for digital home experience, called "AMD LIVE! Ready".[127]

AMD Quad FX platform[edit]
Main article: AMD Quad FX platform
The AMD Quad FX platform, being an extreme enthusiast platform,[clarification needed] allows two processors to connect through HyperTransport, which is a similar setup to dual-processor (2P) servers, excluding the use of buffered memory/registered memory DIMM modules, and a server motherboard, the current setup includes two Athlon 64 FX-70 series processors and a special motherboard.[citation needed] AMD pushed the platform for the surging demands for what AMD calls "megatasking",[128] the ability to do more tasks on a single system. The platform refreshes with the introduction of Phenom FX processors and the next-generation RD790 chipset, codenamed "FASN8".

Server platform[edit]
AMD's first multi-processor server platform, codenamed Fiorano, consists of AMD SR5690 + SP5100 server chipsets, supporting 45 nm, codenamed Shanghai Socket F+ processors and registered DDR2 memory. It was followed by the Maranello platform supporting 45 nm, codenamed Istanbul, Socket G34 processors with DDR3 memory. On single-processor platform, the codenamed Catalunya platform consists of codenamed Suzuka 45 nm quad-core processor with AMD SR5580 + SP5100 chipset and DDR3 support.[129]

AMD's x86 virtualization extension to the 64-bit x86 architecture is named AMD Virtualization, also known by the abbreviation AMD-V, and is sometimes referred to by the code name "Pacifica". AMD processors using Socket AM2, Socket S1, and Socket F include AMD Virtualization support. AMD Virtualization is also supported by release two (8200, 2200 and 1200 series) of the Opteron processors. The third generation (8300 and 2300 series) of Opteron processors will see an update in virtualization technology, specifically the Rapid Virtualization Indexing (also known by the development name Nested Page Tables), alongside the tagged TLB and Device Exclusion Vector (DEV).

AMD also promotes the "AMD I/O Virtualization Technology" (also known as IOMMU) for I/O virtualization.[130] The AMD IOMMU specification has been updated to version 1.2.[131] The specification describes the use of a HyperTransport architecture.

AMD's server initiatives include the following:

AMD Trinity, provides support for virtualization, security and management. Key features include AMD-V technology, codenamed Presidio trusted computing platform technology, I/O Virtualization and Open Management Partition.[132]
AMD Raiden, future clients similar to the Jack PC[133] to be connected through network to a blade server for central management, to reduce client form factor sizes with AMD Trinity features.
Torrenza, coprocessors support through interconnects such as HyperTransport, and PCI Express (though more focus was at HyperTransport enabled coprocessors), also opening processor socket architecture to other manufacturers, Sun and IBM are among the supporting consortium, with rumoured POWER7 processors would be socket-compatible to future Opteron processors. The move made rival Intel respond with the opening of front-side bus (FSB) architecture as well as Geneseo,[134] a collaboration project with IBM for coprocessors connected through PCI Express.
Various certified systems programs and platforms: AMD Commercial Stable Image Platform (CSIP), together with AMD Validated Server program, AMD True Server Solutions, AMD Thermally Tested Barebones Platforms and AMD Validated Server Program, providing certified systems for business from AMD.
Desktop platforms[edit]
Ambox current red.svg
This section is outdated. Please update this article to reflect recent events or newly available information. (December 2014)
Starting in 2007, AMD, following Intel, began using codenames for its desktop platforms such as Spider or Dragon. The platforms, unlike Intel's approach, will refresh every year, putting focus on platform specialization. The platform includes components such as AMD processors, chipsets, ATI graphics and other features, but continued to the open platform approach, and welcome components from other vendors such as VIA, SiS, and Nvidia, as well as wireless product vendors.

Updates to the platform includes the implementation of IOMMU I/O Virtualization with 45 nm generation of processors, and the AMD 800 chipset series in 2009.[135]

Embedded systems[edit]
Main articles: Alchemy (processor) and Geode (processor)
Ambox current red.svg
This section is outdated. Please update this article to reflect recent events or newly available information. (December 2014)
In February 2002, AMD acquired Alchemy Semiconductor for its Alchemy line of MIPS processors for the hand-held and portable media player markets. On June 13, 2006, AMD officially announced that the line was to be transferred to Raza Microelectronics, Inc., a designer of MIPS processors for embedded applications.[136]

In August 2003, AMD also purchased the Geode business which was originally the Cyrix MediaGX from National Semiconductor to augment its existing line of embedded x86 processor products. During the second quarter of 2004, it launched new low-power Geode NX processors based on the K7 Thoroughbred architecture with speeds of fanless processors 667 MHz and 1 GHz, and 1.4 GHz processor with fan, of TDP 25 W. This technology is used in a variety of embedded systems (Casino slot machines and customer kiosks for instance), several UMPC designs in Asia markets, as well as the OLPC XO-1 computer, an inexpensive laptop computer intended to be distributed to children in developing countries around the world. The Geode LX processor was announced in 2005 and is said will continue to be available through 2015.

For the past couple of years AMD has been introducing 64-bit processors into its embedded product line starting with the AMD Opteron processor. Leveraging the high throughput enabled through HyperTransport and the Direct Connect Architecture these server class processors have been targeted at high end telecom and storage applications. In 2007, AMD added the AMD Athlon, AMD Turion and Mobile AMD Sempron processors to its embedded product line. Leveraging the same 64-bit instruction set and Direct Connect Architecture as the AMD Opteron but at lower power levels, these processors were well suited to a variety of traditional embedded applications. Throughout 2007 and into 2008, AMD has continued to add both single-core Mobile AMD Sempron and AMD Athlon processors and dual-core AMD Athlon X2 and AMD Turion processors to its embedded product line and now offers embedded 64-bit solutions starting with 8W TDP Mobile AMD Sempron and AMD Athlon processors for fan-less designs up to multi-processor systems leveraging multi-core AMD Opteron processors all supporting longer than standard availability.[137]

The ATI acquisition included the Imageon and Xilleon product lines. In late 2008, the entire handheld division was sold off to Qualcomm, who have since produced the Adreno series. The Xilleon division was sold to Broadcom.

In April 2007, AMD announced the release of the M690T integrated graphics chipset for embedded designs. This enabled AMD to offer complete processor and chipset solutions targeted at embedded applications requiring high performance 3D and video such as emerging digital signage, kiosk and Point of Sale applications. The M690T was followed by the M690E specifically for embedded applications which removed the TV output, which required Macrovision licensing for OEMs, and enabled native support for dual TMDS outputs, enabling dual independent DVI interfaces.

In 2008, AMD announced the Radeon E2400, the first discrete GPU in their embedded product line offering the same long term availability as their other embedded products. That was followed in 2009 with the higher performance Radeon E4690 discrete GPU.

In 2009, AMD announced their first BGA packaged e64 architecture processors, known as the ASB1 family.

In 2010, AMD announced a second generation BGA platform referred to as ASB2. They also announced several new AM3 based processors with support for DDR3 memory.

In January 2011, AMD announced the AMD Embedded G-Series Accelerated Processing Unit. The first Fusion family APU for embedded applications. This announcement was followed by announcements for the high performance AMD Radeon E6760 and the value-conscious Radeon E6460 discrete GPUs. These solutions all added support for DirectX 11, OpenGL 4.1 and OpenCL 1.1.

In May 2012, AMD Announced the AMD Embedded R-Series[138] Accelerated Processing Unit. This family of products incorporates the Bulldozer CPU architecture, and Discrete-class AMD Radeon  HD 7000G Series graphics.

AMD Embedded solutions offer 5+ year product life.

Other initiatives[edit]
50x15, digital inclusion, with targeted 50% of world population to be connected through Internet via affordable computers by the year of 2015.
The Green Grid,[139] founded by AMD together with other founders, such as IBM, Sun and Microsoft, to seek lower power consumption for grids.
Codenamed SIMFIRE   interoperability testing tool for the Desktop and mobile Architecture for System Hardware (DASH) open architecture.
Software[edit]
AMD develops the AMD CodeXL tool suite which includes a GPU debugger, a GPU profiler, a CPU profiler and an OpenCL static kernel analyzer. CodeXL is freely available at AMD developer tools website.
AMD Stream SDK and AMD APP SDK (Accelerated Parallel Processing) SDK to enable AMD graphics processing cores (GPU), working in concert with the system s x86 cores (CPU), to execute heterogeneously to accelerate many applications beyond just graphics[140]
AMD has also taken an active part in developing coreboot, and open source projects aimed at replacing the proprietary BIOS firmware.
Other AMD software includes the AMD Core Math Library, and open-source software including the AMD Performance Library, and the CodeAnalyst performance profiler.
AMD contributes to open source projects, including working with Sun Microsystems to enhance OpenSolaris and Sun xVM on the AMD platform.[141] AMD also maintains its own Open64 compiler distribution and contributes its changes back to the community.[142]
In 2008, AMD released the low-level programming specifications for its GPUs, and works with the X.Org Foundation to develop drivers for AMD graphics cards.[143][144]
Extensions for software parallelism (xSP), aimed at speeding up programs to enable multi-threaded and multi-core processing, announced in Technology Analyst Day 2007. One of the initiatives being discussed since August 2007 is the Light Weight Profiling (LWP), providing internal hardware monitor with runtimes, to observe information about executing process and help the re-design of software to be optimized with multi-core and even multi-threaded programs. Another one is the extension of Streaming SIMD Extension (SSE) instruction set, the SSE5.
Production and fabrication[edit]
Main article: GlobalFoundries
Ever since the spin-off of AMD's fabrication plants in early 2009, GlobalFoundries has been responsible for producing AMD's processors.

GlobalFoundries' main microprocessor manufacturing facilities are located in Dresden, Germany. Additionally, highly integrated microprocessors are manufactured in Taiwan made by third-party manufacturers under strict license from AMD. Between 2003 and 2005, they constructed a second manufacturing plant (300 mm 90 nm process SOI) in the same complex in order to increase the number of chips they could produce, thus becoming more competitive with Intel. The new plant was named "Fab 36", in recognition of AMD's 36 years of operation, and reached full production in mid-2007. Fab 36 was renamed to "Fab 1" during the spin-off of AMD's manufacturing business during the creation of GlobalFoundries. In July 2007, AMD announced that they completed the conversion of Fab 1 Module 1 from 90 nm to 65 nm. They then shifted their focus to the 45 nm conversion.[145]

Corporate affairs[edit]
Partnerships[edit]
AMD utilizes strategic industry partnerships to further its business interests as well as to rival Intel's dominance and resources.

A partnership between AMD and Alpha Processor Inc. developed HyperTransport, a point-to-point interconnect standard which was turned over to an industry standards body for finalization. It is now used in modern motherboards that are compatible with AMD processors.

AMD also formed a strategic partnership with IBM, under which AMD gained silicon on insulator (SOI) manufacturing technology, and detailed advice on 90 nm implementation. AMD announced that the partnership would extend to 2011 for 32 nm and 22 nm fabrication-related technologies.[146]

To facilitate processor distribution and sales, AMD is loosely partnered with end-user companies, such as HP, Compaq, ASUS, Acer, and Dell.

In 1993, AMD established a 50-50 partnership with Fujitsu called FASL, and merged into a new company called FASL LLC in 2003. The joint venture went public under the name Spansion and ticker symbol SPSN in December 2005, with AMD shares drop to 37%. AMD no longer directly participates in the Flash memory devices market now as AMD entered into a non-competition agreement, as of December 21, 2005, with Fujitsu and Spansion, pursuant to which it agreed not to directly or indirectly engage in a business that manufactures or supplies standalone semiconductor devices (including single chip, multiple chip or system devices) containing only Flash memory.[147]

On May 18, 2006, Dell announced that it would roll out new servers based on AMD's Opteron chips by year's end, thus ending an exclusive relationship with Intel. In September 2006, Dell began offering AMD Athlon X2 chips in their desktop line-up.

In June 2011, HP announced new business and consumer notebooks equipped with the latest versions of AMD APUs   accelerated processing units. AMD will power HP's Intel-based business notebooks as well.[148]

In the spring of 2013, AMD announced that it would be powering all three major next-generation consoles.[149] The Xbox One and Sony PlayStation 4 are both powered by a custom-built AMD APU, and the Nintendo Wii U is powered by an AMD GPU.[150] According to AMD, having their processors in all three of these consoles will greatly assist developers with cross-platform development to competing consoles and PCs as well as increased support for their products across the board.[151]

Litigation with Intel[edit]
See also: AMD v. Intel

AMD processor with Intel logo
AMD has a long history of litigation with former partner and x86 creator Intel.[152][153][154]

In 1986, Intel broke an agreement it had with AMD to allow them to produce Intel's micro-chips for IBM; AMD filed for arbitration in 1987 and the arbitrator decided in AMD's favor in 1992. Intel disputed this, and the case ended up in the Supreme Court of California. In 1994, that court upheld the arbitrator's decision and awarded damages for breach of contract.
In 1990, Intel brought a copyright infringement action alleging illegal use of its 287 microcode. The case ended in 1994 with a jury finding for AMD and its right to use Intel's microcode in its microprocessors through the 486 generation.
In 1997, Intel filed suit against AMD and Cyrix Corp. for misuse of the term MMX. AMD and Intel settled, with AMD acknowledging MMX as a trademark owned by Intel, and with Intel granting AMD rights to market the AMD K6 MMX processor.
In 2005, following an investigation, the Japan Federal Trade Commission found Intel guilty on a number of violations. On June 27, 2005, AMD won an antitrust suit against Intel in Japan, and on the same day, AMD filed a broad antitrust complaint against Intel in the U.S. Federal District Court in Delaware. The complaint alleges systematic use of secret rebates, special discounts, threats, and other means used by Intel to lock AMD processors out of the global market. Since the start of this action, the court has issued subpoenas to major computer manufacturers including Acer, Dell, Lenovo, HP and Toshiba.
In November 2009, Intel agreed to pay AMD  1.25bn and renew a five-year patent cross-licensing agreement as part of a deal to settle all outstanding legal disputes between them.[155]
Guinness World Record Achievement[edit]
On August 31, 2011, in Austin, Texas, AMD achieved a Guinness World Record for the "Highest frequency of a computer processor": 8.429 GHz.[156] The company ran an 8-core FX-8150 processor with only one active module (two cores), and cooled with liquid helium.[157] The previous record was 8.308 GHz, with an Intel Celeron 352 (one core).

On November 1, 2011, geek.com reported that Andre Yang, an overclocker from Taiwan, used an FX-8150 to set another record: 8.461 GHz.[158]

Corporate social responsibility[edit]
In its 2012 report on progress relating to conflict minerals, the Enough Project rated AMD the fifth most progressive of 24 consumer electronics companies.[159]

See also[edit]
Portal icon San Francisco Bay Area portal
Portal icon Companies portal
Bill Gaede
3DNow!
Cool'n'Quiet
PowerNow!
Semiconductor sales leaders by year
Comparison of AMD Chipsets
Comparison of AMD graphics processing units
Comparison of ATI Chipsets
Comparison of AMD Processors
Intel PRO/Wireless
From Wikipedia, the free encyclopedia
  (Redirected from Intel Driver Update Utility)
Intel PRO/Wireless is a series of Intel wireless products.

Contents  [hide] 
1 History
2 Hardware
3 See also
4 References
5 External links
History[edit]
After the release of the wireless products called Intel Pro/Wireless 2100, 2200BG/2225BG/2915ABG and 3945ABG in 2005, Intel was criticized for not granting free redistribution rights for the firmware necessary to be included in the operating systems for the wireless devices to operate.[1] As a result of this, Intel became a target of campaigns to allow free operating systems to include binary firmwares on terms acceptable to the open source community. Linspire-Linux creator Michael Robertson outlined the difficult position that Intel was in releasing to Open Source, as Intel did not want to upset their large customer Microsoft.[2] Theo de Raadt of OpenBSD also claimed that Intel is being "an Open Source fraud" after an Intel employee presented a distorted view of the situation on an open-source conference.[3] In spite of the negative attention Intel received as a result of the wireless dealings, the binary firmware still has not gained a license compatible with free software principles.

Hardware[edit]
Model Name  Supported 802.11 protocols  Form Factor
PRO/Wireless 2011B  b PC-Card
PRO/Wireless 2100 b Mini PCI
PRO/Wireless 2100A  ab  Mini PCI
PRO/Wireless 2200BG bg  Mini PCI
PRO/Wireless 2915ABG  abg Mini PCI
PRO/Wireless 3945ABG  abg Mini PCIe
PRO/Wireless 5100ABGN abgn  Mini PCIe
The successor to the PRO/Wireless series is Intel Wireless WiFi Link.

See also[edit]
Comparison of open-source wireless drivers
FIPS 140
Wireless LAN
References[edit]
Jump up ^ Varghese, Sam (2005-03-01). "OpenBSD to support more wireless chipsets". theage.com.au (The Age Company Ltd). Retrieved 2007-08-05.
Jump up ^ Robertson, Michael (2003-03-19). "Is Intel's "Centrino" Techno-Latin for "No Linux "". michaelrobertson.com. Retrieved 2007-08-05.
Jump up ^ "Intel: Only "Open" for Business". OpenBSD Journal. 2006-09-30. Retrieved 2007-08-05. |first1= missing |last1= in Authors list (help)
Motherboard
From Wikipedia, the free encyclopedia

Motherboard for an Acer desktop personal computer, showing the typical components and interfaces that are found on a motherboard. This model was made by Foxconn in 2007, and follows the ATX layout (known as the "form factor") usually employed for desktop computers. It is designed to work with AMD's Athlon 64 processor

Intel D945GCPE A microATX Motherboard LGA775 for Intel Pentium 4, D, XE, Dual-Core, Core 2 (circa 2007)
A motherboard (sometimes alternatively known as the mainboard, system board, baseboard, planar board or logic board,[1] or colloquially, a mobo) is the main printed circuit board (PCB) found in general purpose microcomputers and other expandable systems. It holds and allows communication between many of the crucial electronic components of a system, such as the central processing unit (CPU) and memory, and provides connectors for other peripherals. Unlike a backplane, a motherboard usually contains significant sub-systems such as the central processor, the chipset's input/output and memory controllers, interface connectors, and other components integrated for general purpose use.

Motherboard specifically refers to a PCB with expansion capability and as the name suggests, this board is often referred to as the "mother" of all components attached to it, which often include peripherals, interface cards, and daughtercards: sound cards, video cards, network cards, hard drives, or other forms of persistent storage; TV tuner cards, cards providing extra USB or FireWire slots and a variety of other custom components.

Similarly, the term mainboard is applied to devices with a single board and no additional expansions or capability, such as controlling boards in laser printers, televisions, washing machines and other embedded systems with limited expansion abilities.

Contents  [hide] 
1 History
2 Design
2.1 Form factor
2.2 CPU sockets
2.3 Integrated peripherals
2.4 Peripheral card slots
2.5 Temperature and reliability
2.6 Air pollution and reliability
3 Bootstrapping using the Basic input output system
4 See also
5 References
6 External links
History[edit]
Prior to the invention of the microprocessor, a digital computer consisted of multiple printed circuit boards in a card-cage case with components connected by a backplane, a set of interconnected sockets. In very old designs, copper wires were the discrete connections between card connector pins, but printed circuit boards soon became the standard practice. The Central Processing Unit (CPU), memory, and peripherals were housed on individual printed circuit boards, which were plugged into the backplate. The ubiquitous S-100 bus of the 1970s is an example of this type of backplane system.

The most popular computers of the 1980s such as the Apple II and IBM PC had published schematic diagrams and other documentation which permitted rapid reverse-engineering and third-party replacement motherboards. Usually intended for building new computers compatible with the exemplars, many motherboards offered additional performance or other features and were used to upgrade the manufacturer's original equipment.

During the late 1980s and 1990s, it became economical to move an increasing number of peripheral functions onto the motherboard. In the late 1980s, personal computer motherboards began to include single ICs (also called Super I/O chips) capable of supporting a set of low-speed peripherals: keyboard, mouse, floppy disk drive, serial ports, and parallel ports. By the late 1990s, many personal computer motherboards included consumer grade embedded audio, video, storage, and networking functions without the need for any expansion cards at all; higher-end systems for 3D gaming and computer graphics typically retained only the graphics card as a separate component. Business PCs, workstations, and servers were more likely to need expansion cards, either for more robust functions, or for higher speeds; those systems often had fewer embedded components.

Laptop and notebook computers that were developed in the 1990s integrated the most common peripherals. This even included motherboards with no upgradeable components, a trend that would continue as smaller systems were introduced after the turn of the century (like the tablet computer and the netbook). Memory, processors, network controllers, power source, and storage would be integrated into some systems.

Design[edit]

The Octek Jaguar V motherboard from 1993.[2] This board has few onboard peripherals, as evidenced by the 6 slots provided for ISA cards and the lack of other built-in external interface connectors. Note the large AT keyboard connector at the back right is its only peripheral interface.

The motherboard of a Samsung Galaxy SII; almost all functions of the device are integrated into a very small board
A motherboard provides the electrical connections by which the other components of the system communicate. Unlike a backplane, it also contains the central processing unit and hosts other subsystems and devices.

A typical desktop computer has its microprocessor, main memory, and other essential components connected to the motherboard. Other components such as external storage, controllers for video display and sound, and peripheral devices may be attached to the motherboard as plug-in cards or via cables; in modern microcomputers it is increasingly common to integrate some of these peripherals into the motherboard itself.

An important component of a motherboard is the microprocessor's supporting chipset, which provides the supporting interfaces between the CPU and the various buses and external components. This chipset determines, to an extent, the features and capabilities of the motherboard.

Modern motherboards include:

Sockets (or slots) in which one or more microprocessors may be installed. In the case of CPUs in ball grid array packages, such as the VIA C3, the CPU is directly soldered to the motherboard.[3]
Slots into which the system's main memory is to be installed (typically in the form of DIMM modules containing DRAM chips)
A chipset which forms an interface between the CPU's front-side bus, main memory, and peripheral buses
Non-volatile memory chips (usually Flash ROM in modern motherboards) containing the system's firmware or BIOS
A clock generator which produces the system clock signal to synchronize the various components
Slots for expansion cards (the interface to the system via the buses supported by the chipset)
Power connectors, which receive electrical power from the computer power supply and distribute it to the CPU, chipset, main memory, and expansion cards. As of 2007, some graphics cards (e.g. GeForce 8 and Radeon R600) require more power than the motherboard can provide, and thus dedicated connectors have been introduced to attach them directly to the power supply.[4]
Connectors for hard drives, typically SATA only. Disk drives also connect to the power supply.
Additionally, nearly all motherboards include logic and connectors to support commonly used input devices, such as USB for mouse devices and keyboards. Early personal computers such as the Apple II or IBM PC included only this minimal peripheral support on the motherboard. Occasionally video interface hardware was also integrated into the motherboard; for example, on the Apple II and rarely on IBM-compatible computers such as the IBM PC Jr. Additional peripherals such as disk controllers and serial ports were provided as expansion cards.

Given the high thermal design power of high-speed computer CPUs and components, modern motherboards nearly always include heat sinks and mounting points for fans to dissipate excess heat.

Form factor[edit]
Main article: Comparison of computer form factors
Motherboards are produced in a variety of sizes and shapes called computer form factor, some of which are specific to individual computer manufacturers. However, the motherboards used in IBM-compatible systems are designed to fit various case sizes. As of 2007, most desktop computer motherboards use the ATX standard form factor   even those found in Macintosh and Sun computers, which have not been built from commodity components. A case's motherboard and PSU form factor must all match, though some smaller form factor motherboards of the same family will fit larger cases. For example, an ATX case will usually accommodate a microATX motherboard.

Laptop computers generally use highly integrated, miniaturized and customized motherboards. This is one of the reasons that laptop computers are difficult to upgrade and expensive to repair. Often the failure of one laptop component requires the replacement of the entire motherboard, which is usually more expensive than a desktop motherboard due to the large number of integrated components and their custom shape and size.

CPU sockets[edit]
A CPU socket (central processing unit) or slot is an electrical component that attaches to a Printed Circuit Board (PCB) and is designed to house a CPU (also called a microprocessor). It is a special type of integrated circuit socket designed for very high pin counts. A CPU socket provides many functions, including a physical structure to support the CPU, support for a heat sink, facilitating replacement (as well as reducing cost), and most importantly, forming an electrical interface both with the CPU and the PCB. CPU sockets on the motherboard can most often be found in most desktop and server computers (laptops typically use surface mount CPUs), particularly those based on the Intel x86 architecture. A CPU socket type and motherboard chipset must support the CPU series and speed.

Integrated peripherals[edit]

Block diagram of a modern motherboard, which supports many on-board peripheral functions as well as several expansion slots
With the steadily declining costs and size of integrated circuits, it is now possible to include support for many peripherals on the motherboard. By combining many functions on one PCB, the physical size and total cost of the system may be reduced; highly integrated motherboards are thus especially popular in small form factor and budget computers.

Disk controllers for a floppy disk drive, up to 2 PATA drives, and up to 6 SATA drives (including RAID 0/1 support)
integrated graphics controller supporting 2D and 3D graphics, with VGA and TV output
integrated sound card supporting 8-channel (7.1) audio and S/PDIF output
Fast Ethernet network controller for 10/100 Mbit networking
USB 2.0 controller supporting up to 12 USB ports
IrDA controller for infrared data communication (e.g. with an IrDA-enabled cellular phone or printer)
Temperature, voltage, and fan-speed sensors that allow software to monitor the health of computer components.
Peripheral card slots[edit]
A typical motherboard will have a different number of connections depending on its standard and form factor.

A standard, modern ATX motherboard will typically have two or three PCI-Express 16x connection for a graphics card, one or two legacy PCI slots for various expansion cards, and one or two PCI-E 1x (which has superseded PCI). A standard EATX motherboard will have two to four PCI-E 16x connection for graphics cards, and a varying number of PCI and PCI-E 1x slots. It can sometimes also have a PCI-E 4x slot (will vary between brands and models).

Some motherboards have two or more PCI-E 16x slots, to allow more than 2 monitors without special hardware, or use a special graphics technology called SLI (for Nvidia) and Crossfire (for AMD). These allow 2 to 4 graphics cards to be linked together, to allow better performance in intensive graphical computing tasks, such as gaming, video editing, etc.

Temperature and reliability[edit]

A motherboard of a Vaio E series laptop (right)

A microATX motherboard with some faulty capacitors
Main article: Computer cooling
Motherboards are generally air cooled with heat sinks often mounted on larger chips, such as the Northbridge, in modern motherboards.[5] Insufficient or improper cooling can cause damage to the internal components of the computer, or cause it to crash. Passive cooling, or a single fan mounted on the power supply, was sufficient for many desktop computer CPU's until the late 1990s; since then, most have required CPU fans mounted on their heat sinks, due to rising clock speeds and power consumption. Most motherboards have connectors for additional case fans and integrated temperature sensors to detect motherboard and CPU temperatures and controllable fan connectors which the BIOS or operating system can use to regulate fan speed.[6] Alternatively computers can use a water cooling system instead of many fans.

Some small form factor computers and home theater PCs designed for quiet and energy-efficient operation boast fan-less designs. This typically requires the use of a low-power CPU, as well as careful layout of the motherboard and other components to allow for heat sink placement.

A 2003 study found that some spurious computer crashes and general reliability issues, ranging from screen image distortions to I/O read/write errors, can be attributed not to software or peripheral hardware but to aging capacitors on PC motherboards.[7] Ultimately this was shown to be the result of a faulty electrolyte formulation,[8] an issue termed capacitor plague.

Motherboards use electrolytic capacitors to filter the DC power distributed around the board. These capacitors age at a temperature-dependent rate, as their water based electrolytes slowly evaporate. This can lead to loss of capacitance and subsequent motherboard malfunctions due to voltage instabilities. While most capacitors are rated for 2000 hours of operation at 105  C (221  F),[9] their expected design life roughly doubles for every 10  C (50  F) below this. At 45  C (113  F) a lifetime of 15 years can be expected. This appears reasonable for a computer motherboard. However, many manufacturers deliver substandard capacitors,[10] which significantly reduce life expectancy. Inadequate case cooling and elevated temperatures easily exacerbate this problem. It is possible, but time-consuming, to find and replace failed capacitors on personal computer motherboards.

Air pollution and reliability[edit]
High rates of motherboard failures in China and India appear to be due to "sulfurous air pollution produced by coal" that's burned to generate electricity. Air pollution corrodes the circuitry, according to Intel researchers.[11]

Bootstrapping using the Basic input output system[edit]
Motherboards contain some non-volatile memory to initialize the system and load some startup software, usually an operating system, from some external peripheral device. Microcomputers such as the Apple II and IBM PC used ROM chips mounted in sockets on the motherboard. At power-up, the central processor would load its program counter with the address of the boot ROM and start executing instructions from the ROM. These instructions initialized and tested the system hardware, displayed system information on the screen, performed RAM checks, and then loaded an initial program from an external or peripheral device. If none was available, then the computer would perform tasks from other memory stores or display an error message, depending on the model and design of the computer and the ROM version. For example, both the Apple II and the original IBM PC had Microsoft Cassette BASIC in ROM and would start that if no program could be loaded from disk.

Most modern motherboard designs use a BIOS, stored in an EEPROM chip soldered to or socketed on the motherboard, to booting an operating system. Non-operating system boot programs are still supported on modern IBM PC-descended machines, but nowadays it is assumed that the boot program will be a complex operating system such as Microsoft Windows or Linux. When power is first supplied to the motherboard, the BIOS firmware tests and configures memory, circuitry, and peripherals. This Power-On Self Test (POST) may include testing some of the following things:

Video adapter
Cards inserted into slots, such as conventional PCI
Floppy drive
Temperatures, voltages, and fan speeds for hardware monitoring
CMOS used to store BIOS setup configuration
Keyboard and Mouse
Network controller
Optical drives: CD-ROM or DVD-ROM
SCSI hard drive
IDE, EIDE, or Serial ATA Hard disk drive
Security devices, such as a fingerprint reader or the state of a latching switch to detect intrusion
USB devices, such as a memory storage device
On recent motherboards the BIOS may also patch the central processor microcode if the BIOS detects that the installed CPU is one for which errata have been published.

See also[edit]
Accelerated Graphics Port
Computer case screws
CMOS battery
Daughterboard
List of computer hardware manufacturers
Memory Reference Code   the part of the BIOS which handles memory timings on Intel motherboards
Overclocking
Single-board computer
Switched-mode power supply applications
Symmetric multiprocessing
Static random-access memory
From Wikipedia, the free encyclopedia
Not to be confused with SDRAM or "Synchronous DRAM".

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2010)

A static RAM chip from a NES clone (2K x 8 bit)
Computer memory types
Volatile
RAM
DRAM (e.g., DDR SDRAM) SRAM
In development

T-RAM Z-RAM
Historical
Williams Kilburn tube (1946 47) Delay line memory (1947) Selectron tube (1953) Dekatron
Non-volatile
ROM
Mask ROM PROM EPROM EEPROM
NVRAM
Flash memory Solid-state storage
Early stage NVRAM
nvSRAM FeRAM MRAM PRAM
Mechanical
Magnetic tape Hard disk drive Optical disc drive
In development
3D XPoint CBRAM SONOS RRAM Racetrack memory NRAM Millipede memory FJG RAM
Historical
Paper data storage (1725) Drum memory (1932) Magnetic-core memory (1949) Plated wire memory (1957) Core rope memory (1960s) Thin-film memory (1962) Twistor memory (~1968) Bubble memory (~1970)
v t e
Static random-access memory (static RAM or SRAM) is type of semiconductor memory that uses bistable latching circuitry (flip-flop) to store each bit. SRAM exhibits data remanence,[1] but it is still volatile in the conventional sense that data is eventually lost when the memory is not powered.

The term static differentiates SRAM from DRAM (dynamic random-access memory) which must be periodically refreshed. SRAM is faster and more expensive than DRAM; it is typically used for CPU cache while DRAM is used for a computer's main memory.

Contents  [hide] 
1 Applications and uses
1.1 Characteristics
1.1.1 Clock rate and power
1.1.2 Embedded use
1.1.3 In computers
1.1.4 Hobbyists
2 Types of SRAM
2.1 Non-volatile SRAM
2.2 By transistor type
2.3 By function
2.4 By feature
2.5 By flip-flop type
3 Design
4 SRAM operation
4.1 Bus behavior
5 See also
6 References
Applications and uses[edit]

SRAM cells on the die of a STM32F103VGT6 microcontroller as seen by a scanning electron microscope. Manufactured by STMicroelectronics using a 180 nanometre process.

Comparison image of 180 nanometre SRAM cells on a STM32F103VGT6 microcontroller as seen by an optical microscope.
Characteristics[edit]
SRAM is more expensive and less dense than DRAM and is therefore not used for high-capacity, low-cost applications such as the main memory in personal computers.

Clock rate and power[edit]
The power consumption of SRAM varies widely depending on how frequently it is accessed; it can be as power-hungry as dynamic RAM, when used at high frequencies, and some ICs can consume many watts at full bandwidth. On the other hand, static RAM used at a somewhat slower pace, such as in applications with moderately clocked microprocessors, draws very little power and can have a nearly negligible power consumption when sitting idle   in the region of a few micro-watts. Several techniques have been proposed to manage power consumption of SRAM-based memory structures.[2]

Static RAM exists primarily as:

general purpose products
with asynchronous interface, such as the ubiquitous 28-pin 8K   8 and 32K   8 chips (often but not always named something along the lines of 6264 and 62C256 respectively), as well as similar products up to 16 Mbit per chip
with synchronous interface, usually used for caches and other applications requiring burst transfers, up to 18 Mbit (256K   72) per chip
integrated on chip
as RAM or cache memory in micro-controllers (usually from around 32 bytes up to 128 kilobytes)
as the primary caches in powerful microprocessors, such as the x86 family, and many others (from 8 KB, up to many megabytes)
to store the registers and parts of the state-machines used in some microprocessors (see register file)
on application specific ICs, or ASICs (usually in the order of kilobytes)
in FPGAs and CPLDs
Embedded use[edit]
Many categories of industrial and scientific subsystems, automotive electronics, and similar, contain static RAM.
Some amount (kilobytes or less) is also embedded in practically all modern appliances, toys, etc. that implement an electronic user interface.
Several megabytes may be used in complex products such as digital cameras, cell phones, synthesizers, etc.
SRAM in its dual-ported form is sometimes used for realtime digital signal processing circuits.[citation needed]

In computers[edit]
SRAM is also used in personal computers, workstations, routers and peripheral equipment: CPU register files, internal CPU caches and external burst mode SRAM caches, hard disk buffers, router buffers, etc. LCD screens and printers also normally employ static RAM to hold the image displayed (or to be printed). Static RAM was used for the main memory of some early personal computers such as the ZX80, TRS-80 Model 100 and Commodore VIC-20.

Hobbyists[edit]
Hobbyists, specifically homebuilt processor enthusiasts,[3] often prefer SRAM due to the ease of interfacing. It is much easier to work with than DRAM as there are no refresh cycles and the address and data buses are directly accessible rather than multiplexed. In addition to buses and power connections, SRAM usually requires only three controls: Chip Enable (CE), Write Enable (WE) and Output Enable (OE). In synchronous SRAM, Clock (CLK) is also included.[citation needed]

Types of SRAM[edit]
Non-volatile SRAM[edit]
Non-volatile SRAMs, or nvSRAMs, have standard SRAM functionality, but they save the data when the power supply is lost, ensuring preservation of critical information. nvSRAMs are used in a wide range of situations networking, aerospace, and medical, among many others[4]  where the preservation of data is critical and where batteries are impractical.

By transistor type[edit]
Bipolar junction transistor (used in TTL and ECL)   very fast but consumes a lot of power
MOSFET (used in CMOS)   low power and very common today
By function[edit]
Asynchronous   independent of clock frequency; data in and data out are controlled by address transition
Synchronous   all timings are initiated by the clock edge(s). Address, data in and other control signals are associated with the clock signals
In 1990s asynchronous SRAM used to be employed for fast access time. Asynchronous SRAM was used as main memory for small cache-less embedded processors used in everything from industrial electronics and measurement systems to hard disks and networking equipment, among many other applications. Nowadays, synchronous SRAM (e.g. DDR SRAM) is rather employed similarly like Synchronous DRAM - DDR SDRAM memory is rather used than asynchronous DRAM (Dynamic random-access memory). Synchronous memory interface is much faster as access time can be significantly reduced by employing pipeline architecture. Furthermore as DRAM is much cheaper than SRAM, SRAM is often replaced by DRAM, especially in the case when large volume of data is required. SRAM memory is however much faster for random (not block / burst) access. Therefore SRAM memory is mainly used for CPU cache, small on-chip memory, FIFOs or other small buffers.

By feature[edit]
ZBT (ZBT stands for zero bus turnaround)   the turnaround is the number of clock cycles it takes to change access to the SRAM from write to read and vice versa. The turnaround for ZBT SRAMs or the latency between read and write cycle is zero.
syncBurst (syncBurst SRAM or synchronous-burst SRAM)   features synchronous burst write access to the SRAM to increase write operation to the SRAM
DDR SRAM   Synchronous, single read/write port, double data rate I/O
Quad Data Rate SRAM   Synchronous, separate read and write ports, quadruple data rate I/O
By flip-flop type[edit]
Binary SRAM
Ternary SRAM
Design[edit]

A six-transistor CMOS SRAM cell.
A typical SRAM cell is made up of six MOSFETs. Each bit in an SRAM is stored on four transistors (M1, M2, M3, M4) that form two cross-coupled inverters. This storage cell has two stable states which are used to denote 0 and 1. Two additional access transistors serve to control the access to a storage cell during read and write operations. In addition to such six-transistor (6T) SRAM, other kinds of SRAM chips use 4, 8, 10 (4T, 8T, 10T SRAM), or more transistors per bit.[5][6][7] Four-transistor SRAM is quite common in stand-alone SRAM devices (as opposed to SRAM used for CPU caches), implemented in special processes with an extra layer of polysilicon, allowing for very high-resistance pull-up resistors.[8] The principal drawback of using 4T SRAM is increased static power due to the constant current flow through one of the pull-down transistors.


Four transistor SRAM provides advantages in density at the cost of manufacturing complexity. The resistors must have small dimensions and large values.
This is sometimes used to implement more than one (read and/or write) port, which may be useful in certain types of video memory and register files implemented with multi-ported SRAM circuitry.

Generally, the fewer transistors needed per cell, the smaller each cell can be. Since the cost of processing a silicon wafer is relatively fixed, using smaller cells and so packing more bits on one wafer reduces the cost per bit of memory.

Memory cells that use fewer than four transistors are possible   but, such 3T[9][10] or 1T cells are DRAM, not SRAM (even the so-called 1T-SRAM).

Access to the cell is enabled by the word line (WL in figure) which controls the two access transistors M5 and M6 which, in turn, control whether the cell should be connected to the bit lines: BL and BL. They are used to transfer data for both read and write operations. Although it is not strictly necessary to have two bit lines, both the signal and its inverse are typically provided in order to improve noise margins.

During read accesses, the bit lines are actively driven high and low by the inverters in the SRAM cell. This improves SRAM bandwidth compared to DRAMs   in a DRAM, the bit line is connected to storage capacitors and charge sharing causes the bitline to swing upwards or downwards. The symmetric structure of SRAMs also allows for differential signaling, which makes small voltage swings more easily detectable. Another difference with DRAM that contributes to making SRAM faster is that commercial chips accept all address bits at a time. By comparison, commodity DRAMs have the address multiplexed in two halves, i.e. higher bits followed by lower bits, over the same package pins in order to keep their size and cost down.

The size of an SRAM with m address lines and n data lines is 2m words, or 2m   n bits. The most common word size is 8 bits, meaning that a single byte can be read or written to each of 2m different words within the SRAM chip. Several common SRAM chips have 11 address lines (thus a capacity of 2m = 2,048 = 2k words) and an 8-bit word, so they are referred to as "2k   8 SRAM".

SRAM operation[edit]
An SRAM cell has three different states: standby (the circuit is idle), reading (the data has been requested) or writing (updating the contents). SRAM operating in read mode and write modes should have "readability" and "write stability", respectively. The three different states work as follows:

Standby
If the word line is not asserted, the access transistors M5 and M6 disconnect the cell from the bit lines. The two cross-coupled inverters formed by M1   M4 will continue to reinforce each other as long as they are connected to the supply.
Reading
In theory, reading only requires asserting the word line WL and reading the SRAM cell state by a single access transistor and bit line, e.g. M6, BL. Nevertheless bit lines are relatively long and have large parasitic capacitance. To speed-up reading, a more complex process is used in practice: The read cycle is started by precharging both bit lines BL and BL, i.e. driving the bit lines to a threshold voltage (midrange voltage between logical 1 and 0) by an external module (not shown in the figures). Then asserting the word line WL, enabling both the access transistors M5 and M6 which causes the bit line BL voltage to either slightly drop (bottom NMOS transistor M3 is ON and top PMOS transistor M4 is off) or rise (top PMOS transistor M4 is on). It should be noted that if BL voltage rises, the BL voltage drops, and vice versa. Then the BL and BL lines will have a small voltage difference between them. A sense amplifier will sense which line has the higher voltage and thus determine whether there was 1 or 0 stored. The higher the sensitivity of the sense amplifier, the faster the read operation.
Writing
The write cycle begins by applying the value to be written to the bit lines. If we wish to write a 0, we would apply a 0 to the bit lines, i.e. setting BL to 1 and BL to 0. This is similar to applying a reset pulse to an SR-latch, which causes the flip flop to change state. A 1 is written by inverting the values of the bit lines. WL is then asserted and the value that is to be stored is latched in. This works because the bit line input-drivers are designed to be much stronger than the relatively weak transistors in the cell itself so they can easily override the previous state of the cross-coupled inverters. In practice, access NMOS transistors M5 and M6 have to be stronger than either bottom NMOS (M1, M3) or top PMOS (M2, M4) transistors. This is easily obtained as PMOS transistors are much weaker than NMOS when same sized. Consequently when one transistor pair (e.g. M3 and M4) is only slightly overriden by the write process, the opposite transistors pair (M1 and M2) gate voltage is also changed. This means that the M1 and M2 transistors can be easier overriden, and so on. Thus, cross-coupled inverters magnify the writing process.
Bus behavior[edit]
RAM with an access time of 70 ns will output valid data within 70 ns from the time that the address lines are valid. But the data will remain for a hold time as well (5 10 ns). Rise and fall times also influence valid timeslots with approximately 5 ns. By reading the lower part of an address range, bits in sequence (page cycle) one can read with significantly shorter access time (30 ns).[11]
Dynamic random-access memory
From Wikipedia, the free encyclopedia
"DRAM" redirects here. For other uses, see Dram (disambiguation).
Computer memory types
Volatile
RAM
DRAM (e.g., DDR SDRAM) SRAM
In development

T-RAM Z-RAM
Historical
Williams Kilburn tube (1946 47) Delay line memory (1947) Selectron tube (1953) Dekatron
Non-volatile
ROM
Mask ROM PROM EPROM EEPROM
NVRAM
Flash memory Solid-state storage
Early stage NVRAM
nvSRAM FeRAM MRAM PRAM
Mechanical
Magnetic tape Hard disk drive Optical disc drive
In development
3D XPoint CBRAM SONOS RRAM Racetrack memory NRAM Millipede memory FJG RAM
Historical
Paper data storage (1725) Drum memory (1932) Magnetic-core memory (1949) Plated wire memory (1957) Core rope memory (1960s) Thin-film memory (1962) Twistor memory (~1968) Bubble memory (~1970)
v t e

A die photograph of the Micron Technology MT4C1024 DRAM integrated circuit. It has a capacity of 1 megabit equalient of 2^20 bits or 128 kB. [1]
Dynamic random-access memory (DRAM) is a type of random-access memory that stores each bit of data in a separate capacitor within an integrated circuit. The capacitor can be either charged or discharged; these two states are taken to represent the two values of a bit, conventionally called 0 and 1. Since even "nonconducting" transistors always leak a small amount, the capacitors will slowly discharge, and the information eventually fades unless the capacitor charge is refreshed periodically. Because of this refresh requirement, it is a dynamic memory as opposed to static random-access memory (SRAM) and other static types of memory. Unlike flash memory, DRAM is volatile memory (vs. non-volatile memory), since it loses its data quickly when power is removed.

DRAM is widely used in digital electronics where low-cost and high-capacity memory is required. One of the largest applications for DRAM is the main memory (colloquially called the "RAM") in modern computers; and as the main memories of components used in these computers such as graphics cards (where the "main memory" is called the graphics memory). In contrast, SRAM, which is faster and more expensive than DRAM, is typically used where speed is of greater concern than cost, such as the cache memories in processors.

The advantage of DRAM is its structural simplicity: only one transistor and a capacitor are required per bit, compared to four or six transistors in SRAM. This allows DRAM to reach very high densities. The transistors and capacitors used are extremely small; billions can fit on a single memory chip. Due to the dynamic nature of its memory cells, DRAM consumes relatively large amounts of power, with different ways for managing the power consumption.[2]

Contents  [hide] 
1 History
2 Principles of operation
2.1 Operations to read a data bit from a DRAM storage cell
2.2 To write to memory
2.3 Refresh rate
2.4 Memory timing
2.4.1 Timing abbreviations
3 DRAM cells
3.1 Capacitor design
3.2 Historical cell designs
3.3 Proposed cell designs
4 DRAM array structures
4.1 Bitline architecture
4.1.1 Open bitline arrays
4.1.2 Folded bitline arrays
4.1.3 Future array architectures
4.2 Row and column redundancy
5 Embedded DRAM (eDRAM)
6 Error detection and correction
7 Security
8 Packaging
8.1 General DRAM formats
8.2 Common DRAM modules
8.3 Memory size of a DRAM module
9 Versions
9.1 Asynchronous DRAM
9.1.1 Principles of operation
9.1.1.1 RAS Only Refresh (ROR)
9.1.1.2 CAS before RAS refresh (CBR)
9.1.1.3 Hidden refresh
9.1.2 Page mode DRAM
9.1.2.1 Extended data out DRAM (EDO DRAM)
9.1.3 Burst EDO DRAM (BEDO DRAM)
9.2 Synchronous dynamic RAM (SDRAM)
9.2.1 Single data rate synchronous DRAM (SDR SDRAM)
9.2.2 Double data rate synchronous DRAM (DDR SDRAM)
9.2.3 Direct Rambus DRAM (DRDRAM)
9.2.4 Reduced Latency DRAM (RLDRAM)
9.3 Graphics RAM
9.3.1 Video DRAM (VRAM)
9.3.2 Window DRAM (WRAM)
9.3.3 Multibank DRAM (MDRAM)
9.3.4 Synchronous graphics RAM (SGRAM)
9.3.5 Graphics double data rate SDRAM (GDDR SDRAM)
9.4 Pseudostatic RAM (PSRAM)
10  See also
11  Notes
12  References
13  External links
History[edit]

A schematic drawing depicting the cross-section of the original one-transistor, one-capacitor NMOS DRAM cell. It was patented in 1968.
The cryptanalytic machine code-named "Aquarius" used at Bletchley Park during World War II incorporated a hard-wired dynamic memory. Paper tape was read and the characters on it "were remembered in a dynamic store. ... The store used a large bank of capacitors, which were either charged or not, a charged capacitor representing cross (1) and an uncharged capacitor dot (0). Since the charge gradually leaked away, a periodic pulse was applied to top up those still charged (hence the term 'dynamic')".[3]

In 1964, Arnold Farber and Eugene Schlig, working for IBM, created a hard-wired memory cell, using a transistor gate and tunnel diode latch. They replaced the latch with two transistors and two resistors, a configuration that became known as the Farber-Schlig cell. In 1965, Benjamin Agusta and his team at IBM created a 16-bit silicon memory chip based on the Farber-Schlig cell, with 80 transistors, 64 resistors, and 4 diodes. In 1966, DRAM was invented by Dr. Robert Dennard at the IBM Thomas J. Watson Research Center. He was granted U.S. patent number 3,387,286 in 1968. Capacitors had been used for earlier memory schemes such as the drum of the Atanasoff Berry Computer, the Williams tube and the Selectron tube.

The Toshiba "Toscal" BC-1411 electronic calculator, which was introduced in November 1966,[4] used a form of DRAM built from discrete components.[5]

In 1969 Honeywell asked Intel to make a DRAM using a three-transistor cell that they had developed. This became the Intel 1102[6] in early 1970. However, the 1102 had many problems, prompting Intel to begin work on their own improved design, in secrecy to avoid conflict with Honeywell. This became the first commercially available DRAM, the Intel 1103, in October 1970, despite initial problems with low yield until the fifth revision of the masks. The 1103 was designed by Joel Karp and laid out by Pat Earhart. The masks were cut by Barbara Maness and Judy Garcia.[7]

The first DRAM with multiplexed row and column address lines was the Mostek MK4096 4 Kbit DRAM designed by Robert Proebsting and introduced in 1973. This addressing scheme uses the same address pins to receive the low half and the high half of the address of the memory cell being referenced, switching between the two halves on alternating bus cycles. This was a radical advance, effectively halving the number of address lines required, which enabled it to fit into packages with fewer pins, a cost advantage that grew with every jump in memory size. The MK4096 proved to be a very robust design for customer applications. At the 16 Kbit density, the cost advantage increased; the 16 Kbit Mostek MK4116 DRAM, introduced in 1976, achieved greater than 75% worldwide DRAM market share. However, as density increased to 64 Kbit in the early 1980s, Mostek was overtaken by Japanese DRAM manufacturers selling higher quality DRAMs using the same multiplexing scheme at below-cost prices.[citation needed] This generated friction between Japan and the United States.

Principles of operation[edit]

The principles of operation for reading a simple 4 by 4 DRAM array.

Basic structure of a DRAM cell array.
DRAM is usually arranged in a rectangular array of charge storage cells consisting of one capacitor and transistor per data bit. The figure to the right shows a simple example with a four-by-four cell matrix. Some DRAM matrices are many thousands of cells in height and width.[8][9]

The long horizontal lines connecting each row are known as word-lines. Each column of cells is composed of two bit-lines, each connected to every other storage cell in the column (the illustration to the right does not include this important detail). They are generally known as the "+" and " " bit lines.

Operations to read a data bit from a DRAM storage cell[edit]
The sense amplifiers are disconnected.[10]
The bit-lines are precharged to exactly equal voltages that are in between high and low logic levels (e.g., 0.5 V if the two levels are 0 and 1 V). The bit-lines are physically symmetrical to keep the capacitance equal, and therefore at this time their voltages are equal.[10]
The precharge circuit is switched off. Because the bit-lines are relatively long, they have enough capacitance to maintain the precharged voltage for a brief time. This is an example of dynamic logic.[10]
The desired row's word-line is then driven high to connect a cell's storage capacitor to its bit-line. This causes the transistor to conduct, transferring charge from the storage cell to the connected bit-line (if the stored value is 1) or from the connected bit-line to the storage cell (if the stored value is 0). Since the capacitance of the bit-line is typically much higher than the capacitance of the storage cell, the voltage on the bit-line increases very slightly if the storage cell's capacitor is discharged and decreases very slightly if the storage cell is charged (e.g., 0.54 and 0.45 V in the two cases). As the other bit-line holds 0.50 V there is a small voltage difference between the two twisted bit-lines.[10]
The sense amplifiers are now connected to the bit-lines pairs. Positive feedback then occurs from the cross-connected inverters, thereby amplifying the small voltage difference between the odd and even row bit-lines of a particular column until one bit line is fully at the lowest voltage and the other is at the maximum high voltage. Once this has happened, the row is "open" (the desired cell data is available).[10]
All storage cells in the open row are sensed simultaneously, and the sense amplifier outputs latched. A column address then selects which latch bit to connect to the external data bus. Reads of different columns in the same row can be performed without a row opening delay because, for the open row, all data has already been sensed and latched.[10]
While reading of columns in an open row is occurring, current is flowing back up the bit-lines from the output of the sense amplifiers and recharging the storage cells. This reinforces (i.e. "refreshes") the charge in the storage cell by increasing the voltage in the storage capacitor if it was charged to begin with, or by keeping it discharged if it was empty. Note that due to the length of the bit-lines there is a fairly long propagation delay for the charge to be transferred back to the cell's capacitor. This takes significant time past the end of sense amplification, and thus overlaps with one or more column reads.[10]
When done with reading all the columns in the current open row, the word-line is switched off to disconnect the storage cell capacitors (the row is "closed") from the bit-lines. The sense amplifier is switched off, and the bit-lines are precharged again.[10]
To write to memory[edit]
To store data, a row is opened and a given column's sense amplifier is temporarily forced to the desired high or low voltage state, thus causing the bit-line to charge or discharge the cell storage capacitor to the desired value. Due to the sense amplifier's positive feedback configuration, it will hold a bit-line at stable voltage even after the forcing voltage is removed. During a write to a particular cell, all the columns in a row are sensed simultaneously just as during reading, so although only a single column's storage-cell capacitor charge is changed, the entire row is refreshed (written back in), as illustrated in the figure to the right.[10]


Writing to a DRAM cell.
Refresh rate[edit]
Main article: Memory refresh
See also:   Security
Typically, manufacturers specify that each row must be refreshed every 64 ms or less, as defined by the JEDEC standard.

Some systems refresh every row in a burst of activity involving all rows every 64 ms. Other systems refresh one row at a time staggered throughout the 64 ms interval. For example, a system with 213 = 8,192 rows would require a staggered refresh rate of one row every 7.8  s which is 64 ms divided by 8,192 rows. A few real-time systems refresh a portion of memory at a time determined by an external timer function that governs the operation of the rest of a system, such as the vertical blanking interval that occurs every 10 20 ms in video equipment.

The row address of the row that will be refreshed next is maintained by external logic or a counter within the DRAM. A system that provides the row address (and the refresh command) does so to have greater control over when to refresh and which row to refresh. This is done to minimize conflicts with memory accesses, since such a system has both knowledge of the memory access patterns and the refresh requirements of the DRAM. When the row address is supplied by a counter within the DRAM, the system relinquishes control over which row is refreshed and only provides the refresh command. Some modern DRAMs are capable of self-refresh; no external logic is required to instruct the DRAM to refresh or to provide a row address.

Under some conditions, most of the data in DRAM can be recovered even if the DRAM has not been refreshed for several minutes.[11]

Memory timing[edit]
Main article: Memory timings
Many parameters are required to fully describe the timing of DRAM operation. Here are some examples for two timing grades of asynchronous DRAM, from a data sheet published in 1998:[12]

"50 ns" "60 ns" Description
tRC 84 ns 104 ns  Random read or write cycle time (from one full /RAS cycle to another)
tRAC  50 ns 60 ns Access time: /RAS low to valid data out
tRCD  11 ns 14 ns /RAS low to /CAS low time
tRAS  50 ns 60 ns /RAS pulse width (minimum /RAS low time)
tRP 30 ns 40 ns /RAS precharge time (minimum /RAS high time)
tPC 20 ns 25 ns Page-mode read or write cycle time (/CAS to /CAS)
tAA 25 ns 30 ns Access time: Column address valid to valid data out (includes address setup time before /CAS low)
tCAC  13 ns 15 ns Access time: /CAS low to valid data out
tCAS  8 ns  10 ns /CAS low pulse width minimum
Thus, the generally quoted number is the /RAS access time. This is the time to read a random bit from a precharged DRAM array. The time to read additional bits from an open page is much less.

When such a RAM is accessed by clocked logic, the times are generally rounded up to the nearest clock cycle. For example, when accessed by a 100 MHz state machine (i.e. a 10 ns clock), the 50 ns DRAM can perform the first read in five clock cycles, and additional reads within the same page every two clock cycles. This was generally described as "5 2 2 2" timing, as bursts of four reads within a page were common.

When describing synchronous memory, timing is described by clock cycle counts separated by hyphens. These numbers represent tCL tRCD tRP tRAS in multiples of the DRAM clock cycle time. Note that this is half of the data transfer rate when double data rate signaling is used. JEDEC standard PC3200 timing is 3 4 4 8[13] with a 200 MHz clock, while premium-priced high performance PC3200 DDR DRAM DIMM might be operated at 2 2 2 5 timing.[14]

PC-3200 (DDR-400) PC2-6400 (DDR2-800) PC3-12800 (DDR3-1600) Description
Typical Fast  Typical Fast  Typical Fast
cycles  time  cycles  time  cycles  time  cycles  time  cycles  time  cycles  time
tCL 3 15 ns 2 10 ns 5 12.5 ns 4 10 ns 9 11.25 ns  8 10 ns /CAS low to valid data out (equivalent to tCAC)
tRCD  4 20 ns 2 10 ns 5 12.5 ns 4 10 ns 9 11.25 ns  8 10 ns /RAS low to /CAS low time
tRP 4 20 ns 2 10 ns 5 12.5 ns 4 10 ns 9 11.25 ns  8 10 ns /RAS precharge time (minimum precharge to active time)
tRAS  8 40 ns 5 25 ns 16  40 ns 12  30 ns 27  33.75 ns  24  30 ns Row active time (minimum active to precharge time)
...Minimum random access time has improved from tRAC = 50 ns to tRCD + tCL = 22.5 ns, and even the premium 20 ns variety is only 2.5 times better compared to the typical case (~2.22 times better). CAS latency has improved even less, from tCAC = 13 ns to 10 ns. However, the DDR3 memory does achieve 32 times higher bandwidth; due to internal pipelining and wide data paths, it can output two words every 1.25 ns (1600 Mword/s), while the EDO DRAM can output one word per tPC = 20 ns (50 Mword/s).

Timing abbreviations[edit]
tCL   CAS latency
tCR   Command rate
tPTP   precharge to precharge delay
tRAS   RAS active time
tRCD   RAS to CAS delay
tREF   Refresh period
tRFC   Row refresh cycle time
tRP   RAS precharge
tRRD   RAS to RAS delay
tRTP   Read to precharge delay
tRTR   Read to read delay
tRTW   Read to write delay
tWR   Write recovery time
tWTP   Write to precharge delay
tWTR   Write to read delay
tWTW   Write to write delay
DRAM cells[edit]
Each bit of data in a DRAM is stored as a positive or negative electrical charge in a capacitive structure. The structure providing the capacitance, as well as the transistors that control access to it, is collectively referred to as a DRAM cell. They are the fundamental building block in DRAM arrays. Multiple DRAM memory cell variants exist, but the most commonly used variant in modern DRAMs is the one-transistor, one-capacitor (1T1C) cell. The transistor is used to admit current into the capacitor during writes, and to discharge the capacitor during reads. The access transistor is designed to maximize drive strength and minimize transistor-transistor leakage (Kenner, pg. 34).

The capacitor has two terminals, one of which is connected to its access transistor, and the other to either ground or VCC/2. In modern DRAMs, the latter case is more common, since it allows faster operation. In modern DRAMs, a voltage of +VCC/2 across the capacitor is required to store a logic one; and a voltage of -VCC/2 across the capacitor is required to store a logic zero. The electrical charge stored in the capacitor is measured in coulombs. For a logic one, the charge is: {\textstyle Q = {V_{CC} \over 2} \cdot C}, where Q is the charge in coulombs and C is the capacitance in farads. A logic zero has a charge of: {\textstyle Q = {-V_{CC} \over 2} \cdot C}.[15]

Reading or writing a logic one requires the wordline is driven to a voltage greater than the sum of VCC and the access transistor's threshold voltage (VTH). This voltage is called VCC pumped (VCCP). The time required to discharge a capacitor thus depends on what logic value is stored in the capacitor. A capacitor containing logic one begins to discharge when the voltage at the access transistor's gate terminal is above VCCP. If the capacitor contains a logic zero, it begins to discharge when the gate terminal voltage is above VTH.[16]

Capacitor design[edit]
Up until the mid-1980s, the capacitors in DRAM cells were co-planar with the access transistor (they were constructed on the surface of the substrate), thus they were referred to as planar capacitors. The drive to increase both density, and to a lesser extent, performance, required denser designs. This was strongly motivated by economics; a major consideration for DRAM devices, especially commodity DRAMs. The minimization of DRAM cell area can produce a denser device (which could be sold at a higher price), or a lower priced device with the same capacity. Starting in the mid-1980s, the capacitor has been moved above or below the silicon substrate in order to meet these objectives. DRAM cells featuring capacitors above the substrate are referred to as stacked or folded plate capacitors; whereas those with capacitors buried beneath the substrate surface are referred to as trench capacitors. In the 2000s, manufacturers were sharply divided by the type of capacitor used by their DRAMs, and the relative cost and long-term scalability of both designs has been the subject of extensive debate. The majority of DRAMs, from major manufactures such as Hynix, Micron Technology, Samsung Electronics use the stacked capacitor structure, whereas smaller manufacturers such Nanya Technology use the trench capacitor structure (Jacob, pp. 355 357).

The capacitor in the stacked capacitor scheme is constructed above the surface of the substrate. The capacitor is constructed from an oxide-nitride-oxide (ONO) dielectric sandwiched in between two layers of polysilicon plates (the top plate is shared by all DRAM cells in an IC), and its shape can be a rectangle, a cylinder, or some other more complex shape. There are two basic variations of the stacked capacitor, based on its location relative to the bitline capacitor-over-bitline (COB) and capacitor-under-bitline (CUB). In a former variation, the capacitor is underneath the bitline, which is usually made of metal, and the bitline has a polysilicon contact that extends downwards to connect it to the access transistor's source terminal. In the latter variation, the capacitor is constructed above the bitline, which is almost always made of polysilicon, but is otherwise identical to the COB variation. The advantage the COB variant possesses is the ease of fabricating the contact between the bitline and the access transistor's source as it is physically close to the substrate surface. However, this requires the active area to be laid out at a 45-degree angle when viewed from above, which makes it difficult to ensure that the capacitor contact does not touch the bitline. CUB cells avoid this, but suffer from difficulties in inserting contacts in between bitlines, since the size of features this close to the surface are at or near the minimum feature size of the process technology (Kenner, pp. 33 42).

The trench capacitor is constructed by etching a deep hole into the silicon substrate. The substrate volume surrounding the hole is then heavily doped to produce a buried n+ plate and to reduce resistance. A layer of oxide-nitride-oxide dielectric is grown or deposited, and finally the hole is filled by depositing doped polysilicon, which forms the top plate of the capacitor. The top the capacitor is connected to the access transistor's drain terminal via a polysilicon strap (Kenner, pp. 42&ndash44). A trench capacitor's depth-to-width ratio in DRAMs of the mid-2000s can exceed 50:1 (Jacob, p. 357).

Trench capacitors have numerous advantages. Since the capacitor is buried in the bulk of the substrate instead of lying on its surface, the area it occupies can be minimized to what is required to connect it to the access transistor's drain terminal without decreasing the capacitor's size, and thus capacitance (Jacob, pp. 356 357). Alternatively, the capacitance can be increased by etching a deeper hole without any increase to surface area (Kenner, pg. 44). Another advantage of the trench capacitor is that its structure is under the layers of metal interconnect, allowing them to be more easily made planar, which enables it to be integrated in a logic-optimized process technology, which have many levels of interconnect above the substrate. The fact that the capacitor is under the logic means that it is constructed before the transistors are. This allows high-temperature processes to fabricate the capacitors, which would otherwise be degrading the logic transistors and their performance. This makes trench capacitors suitable for constructing embedded DRAM (eDRAM) (Jacob, p. 357). Disadvantages of trench capacitors are difficulties in reliably constructing the capacitor's structures within deep holes and in connecting the capacitor to the access transistor's drain terminal (Kenner, pg. 44).

Historical cell designs[edit]
First-generation DRAM ICs (those with capacities of 1 Kbit), of which the first was the Intel 1103, used a three-transistor, one-capacitor (3T1C) DRAM cell. By the second-generation, the requirement to increase density by fitting more bits in a given area, or the requirement to reduce cost by fitting the same amount of bits in a smaller area, lead to the almost universal adaptation of the 1T1C DRAM cell, although a couple of devices with 4 and 16 Kbit capacities continued to use the 3T1C cell for performance reasons (Kenner, p. 6). These performance advantages included, most significantly, the ability to read the state stored by the capacitor without discharging it, avoiding the need to write back what was read out (non-destructive read). A second performance advantage relates to the 3T1C cell has separate transistors for reading and writing; the memory controller can exploit this feature to perform atomic read-modify-writes, where a value is read, modified, and then written back as a single, indivisible operation (Jacob, p. 459).

Proposed cell designs[edit]
The drive to increase density and performance has lead to the one-transistor, zero-capacitor (1T) DRAM cell being a topic of research since the late-1990s. In 1T DRAM cells, there is a transistor for controlling access to a capacitive region used to store the bit of data, but this capacitance is not provided by a separate capacitor. Instead, the parasitic body capacitor inherent in silicon-on-insulator (SOI) transistors is used instead. Subsequently, 1T DRAM cells have the greatest density, and can be easily integrated with logic since they are constructed from the same SOI process technologies used for high-performance logic. A key difference from 1T1C DRAMs is that reads in 1T DRAM are non-destructive; the stored charge causes a detectable shift in the threshold voltage of the transistor. Refresh, however, is still required.[17] Performance-wise, access times are significantly better than capacitor-based DRAMs, but slightly worse than SRAM. Examples of such DRAMs include A-RAM and Z-RAM.

DRAM array structures[edit]
DRAM cells are laid out in a regular rectangular, grid-like pattern to facilitate their control and access via wordlines and bitlines. The physical layout of the DRAM cells in an array is typically designed so that two adjacent DRAM cells in a column share a single bitline contact to reduce their area. DRAM cell area is given as n F2, where n is a number derived from the DRAM cell design, and F is the smallest feature size of a given process technology. This scheme permits comparison of DRAM size over different process technology generations, as DRAM cell area scales at linear or near-linear rates over. The typical area for modern DRAM cells varies between 6 8 F2.

The horizontal wire, the wordline, is connected to the gate terminal of every access transistor in its row. The vertical bitline is connected to the source terminal of the transistors in its a column. The lengths of the wordlines and bitlines are limited. The wordline length is limited by the desired performance of the array, since propagation time of the signal that must transverse the wordline is determined by the RC time constant. The bitline length is limited by its capacitance (which increases with length), which must be kept within a range for proper sensing (as DRAMs operate by sensing the charge of the capacitor released onto the bitline). Bitline length is also limited by the amount of operating current the DRAM can draw and by how power can be dissipated, since these two characteristics are largely determined by the charging and discharging of the bitline.

Bitline architecture[edit]
Sense amplifiers are required to read the state contained in the DRAM cells. When the access transistor is activated, the electrical charge in the capacitor is shared with the bitline. The bitline's capacitance is much greater than that of the capacitor (approximately ten times). Thus, the change in bitline voltage is minute. Sense amplifiers are required to resolve the voltage differential into the levels specified by the logic signaling system. Modern DRAMs use differential sense amplifiers, and are accompanied by requirements as to how the DRAM arrays are constructed. Differential sense amplifiers work by driving their outputs to opposing extremes based on the relative voltages on pairs of bitlines. The sense amplifiers function effectively and efficient only if the capacitance and voltages of these bitline pairs are closely matched. Besides ensuring that the lengths of the bitlines and the number of attached DRAM cells attached to them are equal, two basic architectures to array design have emerged to provide for the requirements of the sense amplifiers: open and folded bitline arrays.

Open bitline arrays[edit]
The first generation (1 Kbit) DRAM ICs, up until the 64 Kbit generation (and some 256 Kbit generation devices) had open bitline array architectures. In these architectures, the bitlines are divided into multiple segments, and the differential sense amplifiers are placed in between bitline segments. Because the sense amplifiers are placed between bitline segments, to route their outputs outside the array, an additional layer of interconnect placed above those used to construct the wordlines and bitlines is required.

The DRAM cells that are on the edges of the array do not have adjacent segments. Since the differential sense amplifiers require identical capacitance and bitline lengths from both segments, dummy bitline segments are provided. The advantage of the open bitline array is a smaller array area, although this advantage is slightly diminished by the dummy bitline segments. The disadvantage that caused the near disappearance of this architecture is the inherent vulnerability to noise, which affects the effectiveness of the differential sense amplifiers. Since each bitline segment does not have any spatial relationship to the other, it is likely that noise would affect only one of the two bitline segments.

Folded bitline arrays[edit]
The folded bitline array architecture routes bitlines in pairs throughout the array. The close proximity of the paired bitlines provide superior common-mode noise rejection characteristics over open bitline arrays. The folded bitline array architecture began appearing in DRAM ICs during the mid-1980s, beginning with the 256 Kbit generation. This architecture is favored in modern DRAM ICs for its superior noise immunity.

This architecture is referred to as folded because it takes its basis from the open array architecture from the perspective of the circuit schematic. The folded array architecture appears to remove DRAM cells in alternate pairs (because two DRAM cells share a single bitline contact) from a column, then move the DRAM cells from an adjacent column into the voids.

The location where the bitline twists occupies additional area. To minimize area overhead, engineers select the simplest and most area-minimal twisting scheme that is able to reduce noise under the specified limit. As process technology improves to reduce minimum feature sizes, the signal to noise problem worsens, since coupling between adjacent metal wires is inversely proportional to their pitch. The array folding and bitline twisting schemes that are used must increase in complexity in order to maintain sufficient noise reduction. Schemes that have desirable noise immunity characteristics for a minimal impact in area is the topic of current research (Kenner, p. 37).

Future array architectures[edit]
Advances in process technology could result in open bitline array architectures being favored if it is able to offer better long-term area efficiencies; since folded array architectures require increasingly complex folding schemes to match any advance in process technology. The relationship between process technology, array architecture, and area efficiency is an active area of research.

Row and column redundancy[edit]
The first DRAM ICs did not have any redundancy. An IC with a defective DRAM cell would be discarded. Beginning with the 64 Kbit generation, DRAM arrays have included spare rows and columns to improve yields. Spare rows and columns provide tolerance of minor fabrication defects which have caused a small number of rows or columns to be inoperable. The defective rows and columns are physically disconnected from the rest of the array by a triggering a programmable fuse or by cutting the wire by a laser. The spare rows or columns are substituted in by remapping logic in the row and column decoders (Jacob, pp. 358 361).

Embedded DRAM (eDRAM)[edit]
Main article: EDRAM
DRAM that is integrated into an integrated circuit designed in a logic-optimized process, such as an application-specific integrated circuit (ASIC) or a microprocessor, is called embedded DRAM (eDRAM). Embedded DRAM requires DRAM cell designs that can be fabricated without preventing the fabrication of fast-switching transistors used in high-performance logic, and modification of the basic logic-optimized process technology to accommodate the process steps required to build DRAM cell structures.

Error detection and correction[edit]
Main articles: RAM parity and ECC memory
Electrical or magnetic interference inside a computer system can cause a single bit of DRAM to spontaneously flip to the opposite state. The majority of one-off ("soft") errors in DRAM chips occur as a result of background radiation, chiefly neutrons from cosmic ray secondaries, which may change the contents of one or more memory cells or interfere with the circuitry used to read/write them. Recent studies give widely varying error rates for single event upsets with over seven orders of magnitude difference, ranging from roughly one bit error, per hour, per gigabyte of memory to one bit error, per century, per gigabyte of memory.[18][19][20]

The problem can be mitigated by using redundant memory bits and additional circuitry that use these bits to detect and correct soft errors. In most cases, the detection and correction logic is performed by the memory controller; sometimes, the required logic is transparently implemented within DRAM chips or modules, enabling the ECC memory functionality for otherwise ECC-incapable systems.[21] The extra memory bits are used to record parity and to enable missing data to be reconstructed by error-correcting code (ECC). Parity allows the detection of all single-bit errors (actually, any odd number of wrong bits). The most common error-correcting code, a SECDED Hamming code, allows a single-bit error to be corrected and, in the usual configuration, with an extra parity bit, double-bit errors to be detected.

Recent studies give widely varying error rates with over seven orders of magnitude difference, ranging from 10 10 10 17 error/bit h, roughly one bit error, per hour, per gigabyte of memory to one bit error, per century, per gigabyte of memory.[18][19][20] The Schroeder et al. 2009 study reported a 32% chance that a given computer in their study would suffer from at least one correctable error per year, and provided evidence that most such errors are intermittent hard rather than soft errors.[22] A 2010 study at the University of Rochester also gave evidence that a substantial fraction of memory errors are intermittent hard errors.[23] Large scale studies on non-ECC main memory in PCs and laptops suggest that undetected memory errors account for a substantial number of system failures: the study reported a 1-in-1700 chance per 1.5% of memory tested (extrapolating to an approximately 26% chance for total memory) that a computer would have a memory error every eight months.[24]

Security[edit]
Main article: Data remanence
Although dynamic memory is only specified and guaranteed to retain its contents when supplied with power and refreshed every short period of time (often 64 ms), the memory cell capacitors often retain their values for significantly longer, particularly at low temperatures.[25] Under some conditions most of the data in DRAM can be recovered even if it has not been refreshed for several minutes.[26]

This property can be used to circumvent security and recover data stored in the main memory that is assumed to be destroyed at power-down. The computer could be quickly rebooted, and the contents of the main memory read out; or by removing a computer's memory modules, cooling them to prolong data remanence, then transferring them to a different computer to be read out. Such an attack was demonstrated to circumvent popular disk encryption systems, such as the open source TrueCrypt, Microsoft's BitLocker Drive Encryption, and Apple's FileVault.[25] This type of attack against a computer is often called a cold boot attack.

Packaging[edit]

It has been suggested that this section be merged into Memory module. (Discuss) Proposed since January 2016.
For economic reasons, the large (main) memories found in personal computers, workstations, and non-handheld game-consoles (such as PlayStation and Xbox) normally consist of dynamic RAM (DRAM). Other parts of the computer, such as cache memories and data buffers in hard disks,[citation needed] normally use static RAM (SRAM). However, since SRAM has high leakage power and low density, die-stacked DRAM has recently been used for designing multi-megabyte sized processor caches.[27]

Physically, most DRAM is packaged in black epoxy resin.

General DRAM formats[edit]

A 256 k x 4 bit 20-pin DIP DRAM on an early PC memory card (k = 1024), usually Industry Standard Architecture

Common DRAM packages. From top to bottom: DIP, SIPP, SIMM (30-pin), SIMM (72-pin), DIMM (168-pin), DDR DIMM (184-pin).

Two 8 GB DDR4-2133 288-pin ECC 1.2 V RDIMMs
Dynamic random access memory is produced as integrated circuits (ICs) bonded and mounted into plastic packages with metal pins for connection to control signals and buses. In early use individual DRAM ICs were usually either installed directly to the motherboard or on ISA expansion cards; later they were assembled into multi-chip plug-in modules (DIMMs, SIMMs, etc.). Some standard module types are:

DRAM chip (Integrated Circuit or IC)
Dual in-line Package (DIP)
DRAM (memory) modules
Single In-line Pin Package (SIPP)
Single In-line Memory Module (SIMM)
Dual In-line Memory Module (DIMM)
Rambus In-line Memory Module (RIMM), technically DIMMs but called RIMMs due to their proprietary slot.
Small outline DIMM (SO-DIMM), about half the size of regular DIMMs, are mostly used in notebooks, small footprint PCs (such as Mini-ITX motherboards), upgradable office printers and networking hardware like routers.
Small outline RIMM (SO-RIMM). Smaller version of the RIMM, used in laptops. Technically SO-DIMMs but called SO-RIMMs due to their proprietary slot.
Stacked vs. non-stacked RAM modules
Stacked RAM modules contain two or more RAM chips stacked on top of each other. This allows large modules to be manufactured using cheaper low density wafers. Stacked chip modules draw more power, and tend to run hotter than non-stacked modules. Stacked modules can be packaged using the older TSOP or the newer BGA style IC chips. Silicon dies connected with older wire bonding or newer TSV.
Several proposed stacked RAM approaches exist, with TSV and much wider interfaces, including Wide I/O, Wide I/O 2, Hybrid Memory Cube and High Bandwidth Memory.
Common DRAM modules[edit]
Common DRAM packages as illustrated to the right, from top to bottom (last three types are not present in the group picture, and the last type is available in a separate picture):

DIP 16-pin (DRAM chip, usually pre-fast page mode DRAM (FPRAM))
SIPP 30-pin (usually FPRAM)
SIMM 30-pin (usually FPRAM)
SIMM 72-pin (often extended data out DRAM (EDO DRAM) but FPRAM is not uncommon)
DIMM 168-pin (most SDRAM but were some extended data out DRAM (EDO DRAM))
DIMM 184-pin (DDR SDRAM)
RIMM 184-pin (RDRAM)
DIMM 240-pin (DDR2 SDRAM and DDR3 SDRAM)
DIMM 288-pin (DDR4 SDRAM)
Common SO-DIMM DRAM modules:

72-pin (32-bit)
144-pin (64-bit) used for SO-DIMM SDRAM
200-pin (72-bit) used for SO-DIMM DDR SDRAM and SO-DIMM DDR2 SDRAM
204-pin (64-bit) used for SO-DIMM DDR3 SDRAM
260-pin used for SO-DIMM DDR4 SDRAM
Memory size of a DRAM module[edit]
The exact number of bytes in a DRAM module is always an integral power of two. A 512 MB (as marked on a module) SDRAM DIMM, actually contains 512 MiB (mebibytes) = 512   220 bytes = 229 bytes = 536,870,912 bytes exactly, and might be made of 8 or 9 SDRAM chips, each containing exactly 512 Mib (mebibits) of storage, and each one contributing 8 bits to the DIMM's 64- or 72- bit width. For comparison, a 2 GB SDRAM module contains 2 GiB (gibibytes) = 2   230 bytes = 231 bytes = 2,147,483,648 bytes of memory, exactly. The module usually has 8 SDRAM chips of 256 MiB each.

Versions[edit]
While the fundamental DRAM cell and array has maintained the same basic structure (and performance) for many years, there have been many different interfaces for communicating with DRAM chips. When one speaks about "DRAM types", one is generally referring to the interface that is used.

DRAM can be divided into asynchronous and synchronous DRAM. In addition, graphics DRAM is specially designed for graphics tasks, and can be asynchronous or synchronous DRAM in nature. Pseudostatic RAM (PSRAM) have an architecture and interface that closely mimics the operation and interface of static RAM. Lastly, 1T DRAM uses a capacitorless design, as opposed to the usual 1T/1C (one transistor/one capacitor) designs of conventional DRAM.

Asynchronous DRAM[edit]
Principles of operation[edit]
An asynchronous DRAM chip has power connections, some number of address inputs (typically 12), and a few (typically one or four) bidirectional data lines. There are four active-low control signals:

RAS, the Row Address Strobe. The address inputs are captured on the falling edge of RAS, and select a row to open. The row is held open as long as RAS is low.
CAS, the Column Address Strobe. The address inputs are captured on the falling edge of CAS, and select a column from the currently open row to read or write.
WE, Write Enable. This signal determines whether a given falling edge of CAS is a read (if high) or write (if low). If low, the data inputs are also captured on the falling edge of CAS.
OE, Output Enable. This is an additional signal that controls output to the data I/O pins. The data pins are driven by the DRAM chip if RAS and CAS are low, WE is high, and OE is low. In many applications, OE can be permanently connected low (output always enabled), but it can be useful when connecting multiple memory chips in parallel.
This interface provides direct control of internal timing. When RAS is driven low, a CAS cycle must not be attempted until the sense amplifiers have sensed the memory state, and RAS must not be returned high until the storage cells have been refreshed. When RAS is driven high, it must be held high long enough for precharging to complete.

Although the DRAM is asynchronous, the signals are typically generated by a clocked memory controller, which limits their timing to multiples of the controller's clock cycle.

RAS Only Refresh (ROR)[edit]
Classic asynchronous DRAM is refreshed by opening each row in turn.

The refresh cycles are distributed across the entire refresh interval in such a way that all rows are refreshed within the required interval. To refresh one row of the memory array using RAS Only Refresh, the following steps must occur:

The row address of the row to be refreshed must be applied at the address input pins.
RAS must switch from high to low. CAS must remain high.
At the end of the required amount of time, RAS must return high.
This can be done by supplying a row address and pulsing RAS low; it is not necessary to perform any CAS cycles. An external counter is needed to iterate over the row addresses in turn.[28]

CAS before RAS refresh (CBR)[edit]
For convenience, the counter was quickly incorporated into the DRAM chips themselves. If the CAS line is driven low before RAS (normally an illegal operation), then the DRAM ignores the address inputs and uses an internal counter to select the row to open. This is known as CAS-before-RAS (CBR) refresh. This became the standard form of refresh for asynchronous DRAM, and is the only form generally used with SDRAM.

Hidden refresh[edit]
Given support of CAS-before-RAS refresh, it is possible to deassert RAS while holding CAS low to maintain data output. If RAS is then asserted again, this performs a CBR refresh cycle while the DRAM outputs remain valid. Because data output is not interrupted, this is known as hidden refresh.[29]

Page mode DRAM[edit]
Page mode DRAM is a minor modification to the first-generation DRAM IC interface which improved the performance of reads and writes to a row by avoiding the inefficiency of precharging and opening the same row repeatedly to access a different column. In Page mode DRAM, after a row was opened by holding RAS low, the row could be kept open, and multiple reads or writes could be performed to any of the columns in the row. Each column access was initiated by asserting CAS and presenting a column address. For reads, after a delay (tCAC), valid data would appear on the data out pins, which were held at high-Z before the appearance of valid data. For writes, the write enable signal and write data would be presented along with the column address.[30]

Page mode DRAM was later improved with a small modification which further reduced latency. DRAMs with this improvement were called fast page mode DRAMs (FPM DRAMs). In page mode DRAM, CAS was asserted before the column address was supplied. In FPM DRAM, the column address could be supplied while CAS was still deasserted. The column address propagated through the column address data path, but did not output data on the data pins until CAS was asserted. Prior to CAS being asserted, the data out pins were held at high-Z. FPM DRAM reduced tCAC latency.[31]

Static column is a variant of fast page mode in which the column address does not need to be stored in, but rather, the address inputs may be changed with CAS held low, and the data output will be updated accordingly a few nanoseconds later.[31]

Nibble mode is another variant in which four sequential locations within the row can be accessed with four consecutive pulses of CAS. The difference from normal page mode is that the address inputs are not used for the second through fourth CAS edges; they are generated internally starting with the address supplied for the first CAS edge.[31]

Extended data out DRAM (EDO DRAM)[edit]

A pair of 32 MB EDO DRAM modules.
EDO DRAM, sometimes referred to as Hyper Page Mode enabled DRAM, is similar to Fast Page Mode DRAM with the additional feature that a new access cycle can be started while keeping the data output of the previous cycle active. This allows a certain amount of overlap in operation (pipelining), allowing somewhat improved performance. It was 5% faster than FPM DRAM, which it began to replace in 1995, when Intel introduced the 430FX chipset that supported EDO DRAM.

To be precise, EDO DRAM begins data output on the falling edge of CAS, but does not stop the output when CAS rises again. It holds the output valid (thus extending the data output time) until either RAS is deasserted, or a new CAS falling edge selects a different column address.

Single-cycle EDO has the ability to carry out a complete memory transaction in one clock cycle. Otherwise, each sequential RAM access within the same page takes two clock cycles instead of three, once the page has been selected. EDO's performance and capabilities allowed it to somewhat replace the then-slow L2 caches of PCs. It created an opportunity to reduce the immense performance loss associated with a lack of L2 cache, while making systems cheaper to build. This was also good for notebooks due to difficulties with their limited form factor, and battery life limitations. An EDO system with L2 cache was tangibly faster than the older FPM/L2 combination.

Single-cycle EDO DRAM became very popular on video cards towards the end of the 1990s. It was very low cost, yet nearly as efficient for performance as the far more costly VRAM.

Burst EDO DRAM (BEDO DRAM)[edit]
An evolution of EDO DRAM, Burst EDO DRAM, could process four memory addresses in one burst, for a maximum of 5 1 1 1, saving an additional three clocks over optimally designed EDO memory. It was done by adding an address counter on the chip to keep track of the next address. BEDO also added a pipeline stage allowing page-access cycle to be divided into two parts. During a memory-read operation, the first part accessed the data from the memory array to the output stage (second latch). The second part drove the data bus from this latch at the appropriate logic level. Since the data is already in the output buffer, quicker access time is achieved (up to 50% for large blocks of data) than with traditional EDO.

Although BEDO DRAM showed additional optimization over EDO, by the time it was available the market had made a significant investment towards synchronous DRAM, or SDRAM [2]. Even though BEDO RAM was superior to SDRAM in some ways, the latter technology quickly displaced BEDO.

Synchronous dynamic RAM (SDRAM)[edit]
Main article: Synchronous dynamic random-access memory
SDRAM significantly revises the asynchronous memory interface, adding a clock (and a clock enable) line. All other signals are received on the rising edge of the clock.

The /RAS and /CAS inputs no longer act as strobes, but are instead, along with /WE, part of a 3-bit command:

SDRAM Command summary
/CS /RAS  /CAS  /WE Address Command
H x x x x Command inhibit (No operation)
L H H H x No operation
L H H L x Burst Terminate: stop a read or write burst in progress
L H L H column  Read from currently active row
L H L L column  Write to currently active row
L L H H row Activate a row for read and write
L L H L x Precharge (deactivate) the current row
L L L H x Auto refresh: Refresh one row of each bank, using an internal counter
L L L L mode  Load mode register: Address bus specifies DRAM operation mode.
The /OE line's function is extended to a per-byte "DQM" signal, which controls data input (writes) in addition to data output (reads). This allows DRAM chips to be wider than 8 bits while still supporting byte-granularity writes.

Many timing parameters remain under the control of the DRAM controller. For example, a minimum time must elapse between a row being activated and a read or write command. One important parameter must be programmed into the SDRAM chip itself, namely the CAS latency. This is the number of clock cycles allowed for internal operations between a read command and the first data word appearing on the data bus. The "Load mode register" command is used to transfer this value to the SDRAM chip. Other configurable parameters include the length of read and write bursts, i.e. the number of words transferred per read or write command.

The most significant change, and the primary reason that SDRAM has supplanted asynchronous RAM, is the support for multiple internal banks inside the DRAM chip. Using a few bits of "bank address" which accompany each command, a second bank can be activated and begin reading data while a read from the first bank is in progress. By alternating banks, an SDRAM device can keep the data bus continuously busy, in a way that asynchronous DRAM cannot.

Single data rate synchronous DRAM (SDR SDRAM)[edit]
See also: SDR SDRAM
Single data rate SDRAM (sometimes known as SDR) is a synchronous form of DRAM.

Double data rate synchronous DRAM (DDR SDRAM)[edit]
Main articles: DDR SDRAM, DDR2 SDRAM, DDR3 SDRAM and DDR4 SDRAM
Double data rate SDRAM (DDR) was a later development of SDRAM, used in PC memory beginning in 2000. Subsequent versions are numbered sequentially (DDR2, DDR3, etc.). DDR SDRAM internally performs double-width accesses at the clock rate, and uses a double data rate interface to transfer one half on each clock edge. DDR2 and DDR3 increased this factor to 4  and 8 , respectively, delivering 4-word and 8-word bursts over 2 and 4 clock cycles, respectively. The internal access rate is mostly unchanged (200 million per second for DDR-400, DDR2-800 and DDR3-1600 memory), but each access transfers more data.

Direct Rambus DRAM (DRDRAM)[edit]
Main article: RDRAM
Direct RAMBUS DRAM (DRDRAM) was developed by Rambus.

Reduced Latency DRAM (RLDRAM)[edit]
Main article: RLDRAM
Reduced Latency DRAM is a high performance double data rate (DDR) SDRAM that combines fast, random access with high bandwidth, mainly intended for networking and caching applications.

Graphics RAM[edit]
These are asynchronous and synchronous DRAMs designed for graphics-related tasks such as texture memory and framebuffers, and can be found on video cards.

Video DRAM (VRAM)[edit]
Main article: VRAM
VRAM is a dual-ported variant of DRAM that was once commonly used to store the frame-buffer in some graphics adaptors.

Window DRAM (WRAM)[edit]
WRAM is a variant of VRAM that was once used in graphics adaptors such as the Matrox Millenium and ATI 3D Rage Pro. WRAM was designed to perform better and cost less than VRAM. WRAM offered up to 25% greater bandwidth than VRAM and accelerated commonly used graphical operations such as text drawing and block fills.[32]

Multibank DRAM (MDRAM)[edit]
Multibank DRAM is a type of specialized DRAM developed by MoSys. It is constructed from small memory banks of 256 KB, which are operated in an interleaved fashion, providing bandwidths suitable for graphics cards at a lower cost to memories such as SRAM. MDRAM also allows operations to two banks in a single clock cycle, permitting multiple concurrent accesses to occur if the accesses were independent. MDRAM was primarily used in graphic cards, such as those featuring the Tseng Labs ET6x00 chipsets. Boards based upon this chipset often had the unusual capacity of 2.25 MB because of MDRAM's ability to be implemented more easily with such capacities. A graphics card with 2.25 MB of MDRAM had enough memory to provide 24-bit color at a resolution of 1024 768 a very popular setting at the time.

Synchronous graphics RAM (SGRAM)[edit]

MoSys SGRAM
SGRAM is a specialized form of SDRAM for graphics adaptors. It adds functions such as bit masking (writing to a specified bit plane without affecting the others) and block write (filling a block of memory with a single colour). Unlike VRAM and WRAM, SGRAM is single-ported. However, it can open two memory pages at once, which simulates the dual-port nature of other video RAM technologies.

Graphics double data rate SDRAM (GDDR SDRAM)[edit]
Main article: GDDR
Graphics double data rate SDRAM (GDDR SDRAM) is a type of specialized DDR SDRAM designed to be used as the main memory of graphics processing units (GPUs). GDDR SDRAM is distinct from commodity types of DDR SDRAM such as DDR3, although they share some core technologies. Their primary characteristics are higher clock frequencies for both the DRAM core and I/O interface, which provides greater memory bandwidth for GPUs. As of 2015, there are four successive generations of GDDR: GDDR2, GDDR3, GDDR4, and GDDR5.

Pseudostatic RAM (PSRAM)[edit]
PSRAM or PSDRAM is dynamic RAM with built-in refresh and address-control circuitry to make it behave similarly to static RAM (SRAM). It combines the high density of DRAM with the ease of use of true SRAM. PSRAM (made by Numonyx) is used in the Apple iPhone and other embedded systems such as XFlar Platform.[33]

Some DRAM components have a "self-refresh mode". While this involves much of the same logic that is needed for pseudo-static operation, this mode is often equivalent to a standby mode. It is provided primarily to allow a system to suspend operation of its DRAM controller to save power without losing data stored in DRAM, rather not to allow operation without a separate DRAM controller as is the case with PSRAM.

An embedded variant of PSRAM is sold by MoSys under the name 1T-SRAM. It is technically DRAM, but behaves much like SRAM. It is used in Nintendo Gamecube and Wii video game consoles.

See also[edit]
Portal icon Electronics portal
Portal icon Information technology portal
DRAM price fixing
Flash memory
List of device bit rates
Memory bank
Memory geometry
Row hammer
Flash memory is an electronic (solid-state) non-volatile computer storage medium that can be electrically erased and reprogrammed.

Toshiba developed flash memory from EEPROM (electrically erasable programmable read-only memory) in the early 1980s and introduced it to the market in 1984. The two main types of flash memory are named after the NAND and NOR logic gates. The individual flash memory cells exhibit internal characteristics similar to those of the corresponding gates.

Whereas EPROMs had to be completely erased before being rewritten, NAND-type flash memory may be written and read in blocks (or pages) which are generally much smaller than the entire device. NOR-type flash allows a single machine word (byte) to be written   to an erased location   or read independently.

The NAND type operates primarily in memory cards, USB flash drives, solid-state drives (those produced in 2009 or later), and similar products, for general storage and transfer of data. NAND or NOR flash memory is also often used to store configuration data in numerous digital products, a task previously made possible by EEPROM or battery-powered static RAM. One key disadvantage of flash memory is that it can endure relatively small number of write cycles in a specific block.[1]

Example applications of both types of flash memory include personal computers, PDAs, digital audio players, digital cameras, mobile phones, synthesizers, video games, scientific instrumentation, industrial robotics, and medical electronics. In addition to being non-volatile, flash memory offers fast read access times, although not as fast as static RAM or ROM.[2] Its mechanical shock resistance helps explain its popularity over hard disks in portable devices, as does its high durability, ability to withstand high pressure, temperature and immersion in water, etc.[3]

Although flash memory is technically a type of EEPROM, the term "EEPROM" is generally used to refer specifically to non-flash EEPROM which is erasable in small blocks, typically bytes.[citation needed] Because erase cycles are slow, the large block sizes used in flash memory erasing give it a significant speed advantage over non-flash EEPROM when writing large amounts of data. As of 2013,[needs update ] flash memory cost much less than byte-programmable EEPROM and had become the dominant memory type wherever a system required a significant amount of non-volatile solid-state storage.

Contents  [hide] 
1 History
2 Principles of operation
2.1 Floating-gate transistor
2.2 NOR flash
2.3 NAND flash
2.4 Vertical NAND
3 Limitations
3.1 Block erasure
3.2 Memory wear
3.3 Read disturb
3.4 X-ray effects
4 Low-level access
4.1 NOR memories
4.2 NAND memories
4.3 Standardization
5 Distinction between NOR and NAND flash
5.1 Write endurance
6 Flash file systems
7 Capacity
8 Transfer rates
9 Applications
9.1 Serial flash
9.2 Flash memory as a replacement for hard drives
9.3 Flash memory as RAM
9.4 Archival or long-term storage
10  Industry
11  Flash scalability
12  See also
13  References
14  External links
History[edit]

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2010)
Flash memory (both NOR and NAND types) was invented by Dr. Fujio Masuoka while working for Toshiba circa 1980.[4][5] According to Toshiba, the name "flash" was suggested by Masuoka's colleague, Sh ji Ariizumi, because the erasure process of the memory contents reminded him of the flash of a camera.[6] Masuoka and colleagues presented the invention at the IEEE 1984 International Electron Devices Meeting (IEDM) held in San Francisco.[7]

Intel Corporation saw the massive potential of the invention and introduced the first commercial NOR type flash chip in 1988.[8] NOR-based flash has long erase and write times, but provides full address and data buses, allowing random access to any memory location. This makes it a suitable replacement for older read-only memory (ROM) chips, which are used to store program code that rarely needs to be updated, such as a computer's BIOS or the firmware of set-top boxes. Its endurance may be from as little as 100 erase cycles for an on-chip flash memory,[9] to a more typical 10,000 or 100,000 erase cycles, up to 1,000,000 erase cycles.[10] NOR-based flash was the basis of early flash-based removable media; CompactFlash was originally based on it, though later cards moved to less expensive NAND flash.

NAND flash has reduced erase and write times, and requires less chip area per cell, thus allowing greater storage density and lower cost per bit than NOR flash; it also has up to 10 times the endurance of NOR flash. However, the I/O interface of NAND flash does not provide a random-access external address bus. Rather, data must be read on a block-wise basis, with typical block sizes of hundreds to thousands of bits. This makes NAND flash unsuitable as a drop-in replacement for program ROM, since most microprocessors and microcontrollers require byte-level random access. In this regard, NAND flash is similar to other secondary data storage devices, such as hard disks and optical media, and is thus, highly suitable for use in mass-storage devices, such as memory cards. The first NAND-based removable media format was SmartMedia in 1995, and many others have followed, including:

MultiMediaCard
Secure Digital
Memory Stick, and xD-Picture Card.
A new generation of memory card formats, including RS-MMC, miniSD and microSD, feature extremely small form factors. For example, the microSD card has an area of just over 1.5 cm2, with a thickness of less than 1 mm. microSD capacities range from 64 MB to 200 GB, as of March 2015.[11]


A flash memory cell.
Principles of operation[edit]
Flash memory stores information in an array of memory cells made from floating-gate transistors. In single-level cell (SLC) devices, each cell stores only one bit of information. In multi-level cell (MLC) devices, including triple-level cell (TLC) devices, can store more than one bit per cell.

The floating gate may be conductive (typically polysilicon in most kinds of flash memory) or non-conductive (as in SONOS flash memory).[12]

Floating-gate transistor[edit]
Main article: Floating-gate MOSFET
In flash memory, each memory cell resembles a standard MOSFET, except that the transistor has two gates instead of one. On top is the control gate (CG), as in other MOS transistors, but below this there is a floating gate (FG) insulated all around by an oxide layer. The FG is interposed between the CG and the MOSFET channel. Because the FG is electrically isolated by its insulating layer, electrons placed on it are trapped until they are removed by another application of electric field (e.g. Applied voltage or UV as in EPROM). Counter-intuitively, placing electrons on the FG sets the transistor to the logical "0" state. Once the FG is charged, the electrons in it screen (partially cancel) the electric field from the CG, thus, increasing the threshold voltage (VT1) of the cell. This means that now a higher voltage(VT2) must be applied to the CG to make the channel conductive. In order to read a value from the transistor, an intermediate voltage between the threshold voltages (VT1 & VT2) is applied to the CG. If the channel conducts at this intermediate voltage, the FG must be uncharged (if it were charged, we would not get conduction because the intermediate voltage is less than VT2), and hence, a logical "1" is stored in the gate. If the channel does not conduct at the intermediate voltage, it indicates that the FG is charged, and hence, a logical "0" is stored in the gate. The presence of a logical "0" or "1" is sensed by determining whether there is current flowing through the transistor when the intermediate voltage is asserted on the CG. In a multi-level cell device, which stores more than one bit per cell, the amount of current flow is sensed (rather than simply its presence or absence), in order to determine more precisely the level of charge on the FG.

Internal charge pumps[edit]
Despite the need for high programming and erasing voltages, virtually all flash chips today require only a single supply voltage, and produce the high voltages using on-chip charge pumps.

Over half the energy used by a 1.8 V NAND flash chip is lost in the charge pump itself. Since boost converters are inherently more efficient than charge pumps, researchers developing low-power SSDs have proposed returning to the dual Vcc/Vpp supply voltages used on all the early flash chips, driving the high Vpp voltage for all flash chips in a SSD with a single shared external boost converter.[13][14][15][16][17][18][19][20]

In spacecraft and other high-radiation environments, the on-chip charge pump is the first part of the flash chip to fail, although flash memories will continue to work in read-only mode at much higher radiation levels.[21]

NOR flash[edit]
In NOR flash, each cell has one end connected directly to ground, and the other end connected directly to a bit line. This arrangement is called "NOR flash" because it acts like a NOR gate: when one of the word lines (connected to the cell's CG) is brought high, the corresponding storage transistor acts to pull the output bit line low. NOR flash continues to be the technology of choice for embedded applications requiring a discrete non-volatile memory device. The low read latencies characteristic of NOR devices allow for both direct code execution and data storage in a single memory product.[22]


NOR flash memory wiring and structure on silicon

Programming a NOR memory cell (setting it to logical 0), via hot-electron injection.

Erasing a NOR memory cell (setting it to logical 1), via quantum tunneling.
Programming[edit]
A single-level NOR flash cell in its default state is logically equivalent to a binary "1" value, because current will flow through the channel under application of an appropriate voltage to the control gate, so that the bitline voltage is pulled down. A NOR flash cell can be programmed, or set to a binary "0" value, by the following procedure:

an elevated on-voltage (typically >5 V) is applied to the CG
the channel is now turned on, so electrons can flow from the source to the drain (assuming an NMOS transistor)
the source-drain current is sufficiently high to cause some high energy electrons to jump through the insulating layer onto the FG, via a process called hot-electron injection.
Erasing[edit]
To erase a NOR flash cell (resetting it to the "1" state), a large voltage of the opposite polarity is applied between the CG and source terminal, pulling the electrons off the FG through quantum tunneling. Modern NOR flash memory chips are divided into erase segments (often called blocks or sectors). The erase operation can be performed only on a block-wise basis; all the cells in an erase segment must be erased together. Programming of NOR cells, however, generally can be performed one byte or word at a time.


NAND flash memory wiring and structure on silicon
NAND flash[edit]
NAND flash also uses floating-gate transistors, but they are connected in a way that resembles a NAND gate: several transistors are connected in series, and the bit line is pulled low only if all the word lines are pulled high (above the transistors' VT). These groups are then connected via some additional transistors to a NOR-style bit line array in the same way that single transistors are linked in NOR flash.

Compared to NOR flash, replacing single transistors with serial-linked groups adds an extra level of addressing. Whereas NOR flash might address memory by page then word, NAND flash might address it by page, word and bit. Bit-level addressing suits bit-serial applications (such as hard disk emulation), which access only one bit at a time. Execute-in-place applications, on the other hand, require every bit in a word to be accessed simultaneously. This requires word-level addressing. In any case, both bit and word addressing modes are possible with either NOR or NAND flash.

To read data, first the desired group is selected (in the same way that a single transistor is selected from a NOR array). Next, most of the word lines are pulled up above the VT of a programmed bit, while one of them is pulled up to just over the VT of an erased bit. The series group will conduct (and pull the bit line low) if the selected bit has not been programmed.

Despite the additional transistors, the reduction in ground wires and bit lines allows a denser layout and greater storage capacity per chip. (The ground wires and bit lines are actually much wider than the lines in the diagrams.) In addition, NAND flash is typically permitted to contain a certain number of faults (NOR flash, as is used for a BIOS ROM, is expected to be fault-free). Manufacturers try to maximize the amount of usable storage by shrinking the size of the transistors.

Writing and erasing[edit]
NAND flash uses tunnel injection for writing and tunnel release for erasing. NAND flash memory forms the core of the removable USB storage devices known as USB flash drives, as well as most memory card formats and solid-state drives available today.

Vertical NAND[edit]
Vertical NAND (V-NAND) memory stacks memory cells vertically and uses a charge trap flash architecture. The vertical layers allow larger areal bit densities without requiring smaller individual cells.[23]

Structure[edit]
V-NAND uses a charge trap flash geometry (pioneered in 2002 by AMD)[citation needed] that stores charge on an embedded silicon nitride film. Such a film is more robust against point defects and can be made thicker to hold larger numbers of electrons. V-NAND wraps a planar charge trap cell into a cylindrical form.[23]

An individual memory cell is made up of one planar polysilicon layer containing a hole filled by multiple concentric vertical cylinders. The hole's polysilicon surface acts as the gate electrode. The outermost silicon dioxide cylinder acts as the gate dielectric, enclosing a silicon nitride cylinder that stores charge, in turn enclosing a silicon dioxide cylinder as the tunnel dielectric that surrounds a central rod of conducting polysilicon which acts as the conducting channel.[23]

Memory cells in different vertical layers do not interfere with each other, as the charges cannot move vertically through the silicon nitride storage medium, and the electric fields associated with the gates are closely confined within each layer. The vertical collection is electrically identical to the serial-linked groups in which conventional NAND flash memory is configured.[23]

Construction[edit]
Growth of a group of V-NAND cells begins with an alternating stack of conducting (doped) polysilicon layers and insulating silicon dioxide layers.[23]

The next step is to form a cylindrical hole through these layers. In practice, a 128 Gibit V-NAND chip with 24 layers of memory cells requires about 2.9 billion such holes. Next the hole's inner surface receives multiple coatings, first silicon dioxide, then silicon nitride, then a second layer of silicon dioxide. Finally, the hole is filled with conducting (doped) polysilicon.[23]

Performance[edit]
As of 2013, V-NAND flash architecture allows read and write operations twice as fast as conventional NAND and can last up to 10 times as long, while consuming 50 percent less power. They offer comparable physical bit density using 10-nm lithography, but may be able to increase bit density by up to two orders of magnitude.[23]

Limitations[edit]
Block erasure[edit]
One limitation of flash memory is that, although it can be read or programmed a byte or a word at a time in a random access fashion, it can be erased only a block at a time. This generally sets all bits in the block to 1. Starting with a freshly erased block, any location within that block can be programmed. However, once a bit has been set to 0, only by erasing the entire block can it be changed back to 1. In other words, flash memory (specifically NOR flash) offers random-access read and programming operations, but does not offer arbitrary random-access rewrite or erase operations. A location can, however, be rewritten as long as the new value's 0 bits are a superset of the over-written values. For example, a nibble value may be erased to 1111, then written as 1110. Successive writes to that nibble can change it to 1010, then 0010, and finally 0000. Essentially, erasure sets all bits to 1, and programming can only clear bits to 0. File systems designed for flash devices can make use of this capability, for example, to represent sector metadata.

Although data structures in flash memory cannot be updated in completely general ways, this allows members to be "removed" by marking them as invalid. This technique may need to be modified for multi-level cell devices, where one memory cell holds more than one bit.

Common flash devices such as USB flash drives and memory cards provide only a block-level interface, or flash translation layer (FTL), which writes to a different cell each time to wear-level the device. This prevents incremental writing within a block; however, it does not help the device from being prematurely worn out by intensive write patterns.

Memory wear[edit]
Another limitation is that flash memory has a finite number of program erase cycles (typically written as P/E cycles). Most commercially available flash products are guaranteed to withstand around 100,000 P/E cycles before the wear begins to deteriorate the integrity of the storage.[24] Micron Technology and Sun Microsystems announced an SLC NAND flash memory chip rated for 1,000,000 P/E cycles on 17 December 2008.[25]

The guaranteed cycle count may apply only to block zero (as is the case with TSOP NAND devices), or to all blocks (as in NOR). This effect is mitigated in some chip firmware or file system drivers by counting the writes and dynamically remapping blocks in order to spread write operations between sectors; this technique is called wear leveling. Another approach is to perform write verification and remapping to spare sectors in case of write failure, a technique called bad block management (BBM). For portable consumer devices, these wearout management techniques typically extend the life of the flash memory beyond the life of the device itself, and some data loss may be acceptable in these applications. For high reliability data storage, however, it is not advisable to use flash memory that would have to go through a large number of programming cycles. This limitation is meaningless for 'read-only' applications such as thin clients and routers, which are programmed only once or at most a few times during their lifetimes.

In December 2012, Taiwanese engineers from Macronix revealed their intention to announce at the 2012 IEEE International Electron Devices Meeting that it has figured out how to improve NAND flash storage read/write cycles from 10,000 to 100 million cycles using a  self-healing  process that uses a flash chip with  onboard heaters that could anneal small groups of memory cells. [26] The built-in thermal annealing replaces the usual erase cycle with a local high temperature process that not only erases the stored charge, but also repairs the electron-induced stress in the chip, giving write cycles of at least 100 million.[27] The result is a chip that can be erased and rewritten over and over, even when it should theoretically break down. As promising as Macronix s breakthrough could be for the mobile industry, however, there are no plans for a commercial product to be released any time in the near future.[28]

Read disturb[edit]
The method used to read NAND flash memory can cause nearby cells in the same memory block to change over time (become programmed). This is known as read disturb. The threshold number of reads is generally in the hundreds of thousands of reads between intervening erase operations. If reading continually from one cell, that cell will not fail but rather one of the surrounding cells on a subsequent read. To avoid the read disturb problem the flash controller will typically count the total number of reads to a block since the last erase. When the count exceeds a target limit, the affected block is copied over to a new block, erased, then released to the block pool. The original block is as good as new after the erase. If the flash controller does not intervene in time, however, a read disturb error will occur with possible data loss if the errors are too numerous to correct with an error-correcting code.[29][30]

X-ray effects[edit]
Most flash ICs come in ball grid array (BGA) packages, and even the ones that do not are often mounted on a PCB next to other BGA packages. After PCB Assembly, boards with BGA packages are often X-rayed to see if the balls are making proper connections to the proper pad, or if the BGA needs rework. These X-rays can erase programmed bits in a flash chip (convert programmed "0" bits into erased "1" bits). Erased bits ("1" bits) are not affected by X-rays.[31][32]

Some manufacturers are now making X-ray proof SD[33] and USB[34] memory devices.

Low-level access[edit]
The low-level interface to flash memory chips differs from those of other memory types such as DRAM, ROM, and EEPROM, which support bit-alterability (both zero to one and one to zero) and random access via externally accessible address buses.

NOR memory has an external address bus for reading and programming. For NOR memory, reading and programming are random-access, and unlocking and erasing are block-wise. For NAND memory, reading and programming are page-wise, and unlocking and erasing are block-wise.

NOR memories[edit]
Reading from NOR flash is similar to reading from random-access memory, provided the address and data bus are mapped correctly. Because of this, most microprocessors can use NOR flash memory as execute in place (XIP) memory, meaning that programs stored in NOR flash can be executed directly from the NOR flash without needing to be copied into RAM first. NOR flash may be programmed in a random-access manner similar to reading. Programming changes bits from a logical one to a zero. Bits that are already zero are left unchanged. Erasure must happen a block at a time, and resets all the bits in the erased block back to one. Typical block sizes are 64, 128, or 256 KiB.

Bad block management is a relatively new feature in NOR chips. In older NOR devices not supporting bad block management, the software or device driver controlling the memory chip must correct for blocks that wear out, or the device will cease to work reliably.

The specific commands used to lock, unlock, program, or erase NOR memories differ for each manufacturer. To avoid needing unique driver software for every device made, special Common Flash Memory Interface (CFI) commands allow the device to identify itself and its critical operating parameters.

Besides its use as random-access ROM, NOR flash can also be used as a storage device, by taking advantage of random-access programming. Some devices offer read-while-write functionality so that code continues to execute even while a program or erase operation is occurring in the background. For sequential data writes, NOR flash chips typically have slow write speeds, compared with NAND flash.

Typical NOR flash does not need an error correcting code.[35]

NAND memories[edit]
NAND flash architecture was introduced by Toshiba in 1989.[36] These memories are accessed much like block devices, such as hard disks. Each block consists of a number of pages. The pages are typically 512[37] or 2,048 or 4,096 bytes in size. Associated with each page are a few bytes (typically 1/32 of the data size) that can be used for storage of an error correcting code (ECC) checksum.

Typical block sizes include:

32 pages of 512+16 bytes each for a block size of 16 KiB
64 pages of 2,048+64 bytes each for a block size of 128 KiB[38]
64 pages of 4,096+128 bytes each for a block size of 256 KiB[39]
128 pages of 4,096+128 bytes each for a block size of 512 KiB.
While reading and programming is performed on a page basis, erasure can only be performed on a block basis.[40]

NAND devices also require bad block management by the device driver software, or by a separate controller chip. SD cards, for example, include controller circuitry to perform bad block management and wear leveling. When a logical block is accessed by high-level software, it is mapped to a physical block by the device driver or controller. A number of blocks on the flash chip may be set aside for storing mapping tables to deal with bad blocks, or the system may simply check each block at power-up to create a bad block map in RAM. The overall memory capacity gradually shrinks as more blocks are marked as bad.

NAND relies on ECC to compensate for bits that may spontaneously fail during normal device operation. A typical ECC will correct a one-bit error in each 2048 bits (256 bytes) using 22 bits of ECC code, or a one-bit error in each 4096 bits (512 bytes) using 24 bits of ECC code.[41] If the ECC cannot correct the error during read, it may still detect the error. When doing erase or program operations, the device can detect blocks that fail to program or erase and mark them bad. The data is then written to a different, good block, and the bad block map is updated.

Hamming codes are the most commonly used ECC for SLC NAND flash. Reed-Solomon codes and Bose-Chaudhuri-Hocquenghem codes are commonly used ECC for MLC NAND flash. Some MLC NAND flash chips internally generate the appropriate BCH error correction codes. [35]

Most NAND devices are shipped from the factory with some bad blocks. These are typically marked according to a specified bad block marking strategy. By allowing some bad blocks, the manufacturers achieve far higher yields than would be possible if all blocks had to be verified good. This significantly reduces NAND flash costs and only slightly decreases the storage capacity of the parts.

When executing software from NAND memories, virtual memory strategies are often used: memory contents must first be paged or copied into memory-mapped RAM and executed there (leading to the common combination of NAND + RAM). A memory management unit (MMU) in the system is helpful, but this can also be accomplished with overlays. For this reason, some systems will use a combination of NOR and NAND memories, where a smaller NOR memory is used as software ROM and a larger NAND memory is partitioned with a file system for use as a non-volatile data storage area.

NAND sacrifices the random-access and execute-in-place advantages of NOR. NAND is best suited to systems requiring high capacity data storage. It offers higher densities, larger capacities, and lower cost. It has faster erases, sequential writes, and sequential reads.

Standardization[edit]
A group called the Open NAND Flash Interface Working Group (ONFI) has developed a standardized low-level interface for NAND flash chips. This allows interoperability between conforming NAND devices from different vendors. The ONFI specification version 1.0[42] was released on 28 December 2006. It specifies:

a standard physical interface (pinout) for NAND flash in TSOP-48, WSOP-48, LGA-52, and BGA-63 packages
a standard command set for reading, writing, and erasing NAND flash chips
a mechanism for self-identification (comparable to the serial presence detection feature of SDRAM memory modules)
The ONFI group is supported by major NAND flash manufacturers, including Hynix, Intel, Micron Technology, and Numonyx, as well as by major manufacturers of devices incorporating NAND flash chips.[43]

One major flash device manufacturer, Toshiba, has chosen to use an interface of their own design known as Toggle Mode (and now Toggle V2.0). This interface isn't pin-to-pin compatible with the ONFI specification. The result is a product designed for one vendor's devices may not be able to use another vendor's devices.[44]

A group of vendors, including Intel, Dell, and Microsoft, formed a Non-Volatile Memory Host Controller Interface (NVMHCI) Working Group.[45] The goal of the group is to provide standard software and hardware programming interfaces for nonvolatile memory subsystems, including the "flash cache" device connected to the PCI Express bus.

Distinction between NOR and NAND flash[edit]
NOR and NAND flash differ in two important ways:

the connections of the individual memory cells are different
the interface provided for reading and writing the memory is different (NOR allows random-access for reading, NAND allows only page access)
These two are linked by the design choices made in the development of NAND flash. A goal of NAND flash development was to reduce the chip area required to implement a given capacity of flash memory, and thereby to reduce cost per bit and increase maximum chip capacity so that flash memory could compete with magnetic storage devices like hard disks.[citation needed]

NOR and NAND flash get their names from the structure of the interconnections between memory cells.[46] In NOR flash, cells are connected in parallel to the bit lines, allowing cells to be read and programmed individually. The parallel connection of cells resembles the parallel connection of transistors in a CMOS NOR gate. In NAND flash, cells are connected in series, resembling a NAND gate. The series connections consume less space than parallel ones, reducing the cost of NAND flash. It does not, by itself, prevent NAND cells from being read and programmed individually.

Each NOR flash cell is larger than a NAND flash cell   10 F2 vs 4 F2   even when using exactly the same semiconductor device fabrication and so each transistor, contact, etc. is exactly the same size because NOR flash cells require a separate metal contact for each cell.[47]

When NOR flash was developed, it was envisioned as a more economical and conveniently rewritable ROM than contemporary EPROM and EEPROM memories. Thus random-access reading circuitry was necessary. However, it was expected that NOR flash ROM would be read much more often than written, so the write circuitry included was fairly slow and could erase only in a block-wise fashion. On the other hand, applications that use flash as a replacement for disk drives do not require word-level write address, which would only add to the complexity and cost unnecessarily.[citation needed]

Because of the series connection and removal of wordline contacts, a large grid of NAND flash memory cells will occupy perhaps only 60% of the area of equivalent NOR cells[48] (assuming the same CMOS process resolution, for example, 130 nm, 90 nm, or 65 nm). NAND flash's designers realized that the area of a NAND chip, and thus the cost, could be further reduced by removing the external address and data bus circuitry. Instead, external devices could communicate with NAND flash via sequential-accessed command and data registers, which would internally retrieve and output the necessary data. This design choice made random-access of NAND flash memory impossible, but the goal of NAND flash was to replace mechanical hard disks, not to replace ROMs.

Attribute NAND  NOR
Main Application  File Storage  Code execution
Storage capacity  High  Low
Cost per bit  Better  
Active Power  Better  
Standby Power   Better
Write Speed Good  
Read Speed    Good
Write endurance[edit]
The write endurance of SLC floating-gate NOR flash is typically equal to or greater than that of NAND flash, while MLC NOR and NAND flash have similar endurance capabilities. Examples of endurance cycle ratings listed in datasheets for NAND and NOR flash are provided.[citation needed]

Type of flash memory  Endurance rating (Erases per block) Example(s) of flash memory
SLC NAND  100,000 Samsung OneNAND KFW4G16Q2M
MLC NAND  5,000 to 10,000 for medium-capacity applications;
1,000 to 3,000 for high-capacity applications Samsung K9G8G08U0M (Example for medium-capacity applications)
TLC NAND  1,000 Samsung 840
SLC (floating-gate) NOR 100,000 to 1,000,000  Numonyx M58BW (Endurance rating of 100,000 erases per block);
Spansion S29CD016J (Endurance rating of 1,000,000 erases per block)
MLC (floating-gate) NOR 100,000 Numonyx J3 flash
However, by applying certain algorithms and design paradigms such as wear leveling and memory over-provisioning, the endurance of a storage system can be tuned to serve specific requirements.[2][49]

Computation of NAND flash memory endurance is a challenging subject that depends on SLC/MLC/TLC memory type as well as the use pattern. In order to compute the longevity of the NAND flash, one must account for the size of the memory chip, the type of memory (e.g. SLC/MLC/TLC), and use pattern.

Flash file systems[edit]
Main article: Flash file system
Because of the particular characteristics of flash memory, it is best used with either a controller to perform wear leveling and error correction or specifically designed flash file systems, which spread writes over the media and deal with the long erase times of NOR flash blocks.[50] The basic concept behind flash file systems is the following: when the flash store is to be updated, the file system will write a new copy of the changed data to a fresh block, remap the file pointers, then erase the old block later when it has time.

In practice, flash file systems are used only for memory technology devices (MTDs), which are embedded flash memories that do not have a controller. Removable flash memory cards and USB flash drives have built-in controllers to perform wear leveling and error correction so use of a specific flash file system does not add any benefit.

Capacity[edit]
Multiple chips are often arrayed to achieve higher capacities[51] for use in consumer electronic devices such as multimedia players or GPSs. The capacity of flash chips generally follows Moore's Law because they are manufactured with many of the same integrated circuits techniques and equipment.

Consumer flash storage devices typically are advertised with usable sizes expressed as a small integer power of two (2, 4, 8, etc.) and a designation of megabytes (MB) or gigabytes (GB); e.g., 512 MB, 8 GB. This includes SSDs marketed as hard drive replacements, in accordance with traditional hard drives, which use decimal prefixes.[52] Thus, an SSD marked as "64 GB" is at least 64   1,0003 bytes (64 GB). Most users will have slightly less capacity than this available for their files, due to the space taken by file system metadata.

The flash memory chips inside them are sized in strict binary multiples, but the actual total capacity of the chips is not usable at the drive interface. It is considerably larger than the advertised capacity in order to allow for distribution of writes (wear leveling), for sparing, for error correction codes, and for other metadata needed by the device's internal firmware.

In 2005, Toshiba and SanDisk developed a NAND flash chip capable of storing 1 GB of data using multi-level cell (MLC) technology, capable of storing two bits of data per cell. In September 2005, Samsung Electronics announced that it had developed the world s first 2 GB chip.[53]

In March 2006, Samsung announced flash hard drives with a capacity of 4 GB, essentially the same order of magnitude as smaller laptop hard drives, and in September 2006, Samsung announced an 8 GB chip produced using a 40 nm manufacturing process.[54] In January 2008, SanDisk announced availability of their 16 GB MicroSDHC and 32 GB SDHC Plus cards.[55][56]

More recent flash drives (as of 2012) have much greater capacities, holding 64, 128, and 256 GB.[57]

A joint development at Intel and Micron will allow the production of 32 layer 3.5 terabyte (TB) NAND flash sticks and 10 TB standard-sized SSDs. The device includes 5 packages of 16 x 48 GB TLC dies, using a floating gate cell design.[58]

Flash chips continue to be manufactured with capacities under or around 1 MB, e.g., for BIOS-ROMs and embedded applications.

Transfer rates[edit]
Flash memory devices are typically much faster at reading than writing.[59] Performance also depends on the quality of storage controllers which become more critical when devices are partially full.[59] Even when the only change to manufacturing is die-shrink, the absence of an appropriate controller can result in degraded speeds.[60]

Applications[edit]
Serial flash[edit]
Serial flash is a small, low-power flash memory that uses a serial interface, typically Serial Peripheral Interface Bus (SPI), for sequential data access. When incorporated into an embedded system, serial flash requires fewer wires on the PCB than parallel flash memories, since it transmits and receives data one bit at a time. This may permit a reduction in board space, power consumption, and total system cost.

There are several reasons why a serial device, with fewer external pins than a parallel device, can significantly reduce overall cost:

Many ASICs are pad-limited, meaning that the size of the die is constrained by the number of wire bond pads, rather than the complexity and number of gates used for the device logic. Eliminating bond pads thus permits a more compact integrated circuit, on a smaller die; this increases the number of dies that may be fabricated on a wafer, and thus reduces the cost per die.
Reducing the number of external pins also reduces assembly and packaging costs. A serial device may be packaged in a smaller and simpler package than a parallel device.
Smaller and lower pin-count packages occupy less PCB area.
Lower pin-count devices simplify PCB routing.
There are two major SPI flash types. The first type is characterized by small pages and one or more internal SRAM page buffers allowing a complete page to be read to the buffer, partially modified, and then written back (for example, the Atmel AT45 DataFlash or the Micron Technology Page Erase NOR Flash). The second type has larger sectors. The smallest sectors typically found in an SPI flash are 4 kB, but they can be as large as 64 kB. Since the SPI flash lacks an internal SRAM buffer, the complete page must be read out and modified before being written back, making it slow to manage. SPI flash is cheaper than DataFlash and is therefore a good choice when the application is code shadowing.

The two types are not easily exchangeable, since they do not have the same pinout, and the command sets are incompatible.

Firmware storage[edit]
With the increasing speed of modern CPUs, parallel flash devices are often much slower than the memory bus of the computer they are connected to. Conversely, modern SRAM offers access times below 10 ns, while DDR2 SDRAM offers access times below 20 ns. Because of this, it is often desirable to shadow code stored in flash into RAM; that is, the code is copied from flash into RAM before execution, so that the CPU may access it at full speed. Device firmware may be stored in a serial flash device, and then copied into SDRAM or SRAM when the device is powered-up.[61] Using an external serial flash device rather than on-chip flash removes the need for significant process compromise (a process that is good for high-speed logic is generally not good for flash and vice versa). Once it is decided to read the firmware in as one big block it is common to add compression to allow a smaller flash chip to be used. Typical applications for serial flash include storing firmware for hard drives, Ethernet controllers, DSL modems, wireless network devices, etc.

Flash memory as a replacement for hard drives[edit]
Main article: Solid-state drive
One more recent application for flash memory is as a replacement for hard disks. Flash memory does not have the mechanical limitations and latencies of hard drives, so a solid-state drive (SSD) is attractive when considering speed, noise, power consumption, and reliability. Flash drives are gaining traction as mobile device secondary storage devices; they are also used as substitutes for hard drives in high-performance desktop computers and some servers with RAID and SAN architectures.

There remain some aspects of flash-based SSDs that make them unattractive. The cost per gigabyte of flash memory remains significantly higher than that of hard disks.[62] Also flash memory has a finite number of P/E cycles, but this seems to be currently under control since warranties on flash-based SSDs are approaching those of current hard drives.[63] In addition, deleted files on SSDs can remain for an indefinite period of time before being overwritten by fresh data; erasure or shred techniques or software that work well on magnetic hard disk drives have no effect on SSDs, compromising security and forensic examination.

For relational databases or other systems that require ACID transactions, even a modest amount of flash storage can offer vast speedups over arrays of disk drives.[64][65]

In June 2006, Samsung Electronics released the first flash-memory based PCs, the Q1-SSD and Q30-SSD, both of which used 32 GB SSDs, and were at least initially available only in South Korea.[66]

A solid-state drive was offered as an option with the first Macbook Air introduced in 2008, and from 2010 onwards, all Macbook Air laptops shipped with an SSD. Starting in late 2011, as part of Intel's Ultrabook initiative, an increasing number of ultra thin laptops are being shipped with SSDs standard.

There are also hybrid techniques such as hybrid drive and ReadyBoost that attempt to combine the advantages of both technologies, using flash as a high-speed non-volatile cache for files on the disk that are often referenced, but rarely modified, such as application and operating system executable files.

Flash memory as RAM[edit]
As of 2012, there are attempts to use flash memory as the main computer memory, DRAM.[67]

Archival or long-term storage[edit]
It is unclear how long flash memory will persist under archival conditions i.e., benign temperature and humidity with infrequent access with or without prophylactic rewrite. Anecdotal evidence[specify] suggests that the technology is reasonably robust on the scale of years.[citation needed]

Industry[edit]
One source states that, in 2008, the flash memory industry includes about US 9.1 billion in production and sales. Other sources put the flash memory market at a size of more than US 20 billion in 2006, accounting for more than eight percent of the overall semiconductor market and more than 34 percent of the total semiconductor memory market.[68] In 2012, the market was estimated at  26.8 billion.[69]

Flash scalability[edit]

The aggressive trend of the shrinking process design rule or technology node in NAND flash memory technology effectively accelerates Moore's Law.
Due to its relatively simple structure and high demand for higher capacity, NAND flash memory is the most aggressively scaled technology among electronic devices. The heavy competition among the top few manufacturers only adds to the aggressiveness in shrinking the design rule or process technology node.[30] While the expected shrink timeline is a factor of two every three years per original version of Moore's law, this has recently been accelerated in the case of NAND flash to a factor of two every two years.
A video card (also called a video adapter, display card, graphics card, graphics board, display adapter, graphics adapter or frame buffer[1]) is an expansion card which generates a feed of output images to a display (such as a computer monitor). Frequently, these are advertised as discrete or dedicated graphics cards, emphasizing the distinction between these and integrated graphics. Within the industry, video cards are sometimes called graphics add-in-boards, abbreviated as AIBs,[2] with the word "graphics" usually omitted.

Graphic cards
Connects to 
Motherboard via one of:

ISA
MCA
VLB
PCI
AGP
PCI-X
PCI Express
Others
Display via one of:

VGA connector
Digital Visual Interface
Composite video
S-Video
Component video
HDMI
DMS-59
DisplayPort
Others
Contents  [hide] 
1 History
2 Dedicated vs integrated graphics
3 Power demand
4 Size
5 Multi-card scaling
6 Device drivers
7 Industry
8 Size of market and impact of accelerated processing units on video card sales
9 Parts
9.1 Graphics Processing Unit
9.2 Heat sink
9.3 Video BIOS
9.4 Video memory
9.5 RAMDAC
9.6 Output interfaces
9.6.1 Video Graphics Array (VGA) (DE-15)
9.6.2 Digital Visual Interface (DVI)
9.6.3 Video In Video Out (VIVO) for S-Video, Composite video and Component video
9.6.4 High-Definition Multimedia Interface (HDMI)
9.6.5 DisplayPort
9.6.6 Other types of connection systems
9.7 Motherboard interfaces
10  See also
11  References
12  External links
History[edit]
Standards such as MDA, CGA, HGC, Tandy, PGC, EGA, VGA, MCGA, 8514 or XGA were introduced from 1982 to 1990 and supported by a variety of hardware manufacturers.

The majority of modern video cards are built with either AMD-sourced or Nvidia-sourced graphics chips.[2] Most video cards offer various functions such as accelerated rendering of 3D scenes and 2D graphics, MPEG-2/MPEG-4 decoding, TV output, or the ability to connect multiple monitors (multi-monitor). Video cards also have sound card capabilities to output sound - along with the video for connected TVs or monitors with integrated speakers.

Graphics cards weren't very useful for early computers, since they didn't have the capability to run graphic-based games or high resolution videos as modern computers do now.

Dedicated vs integrated graphics[edit]

Classical desktop computer architecture with a distinct graphics card over PCI Express. Typical bandwidths for given memory technologies, missing are the memory latency. Zero-copy between GPU and CPU is not possible, since both have their distinct physical memories. Data must be copied from one to the other to be shared.

Integrated graphics with partitioned main memory: a part of the system memory is allocated to the GPU exclusively. Zero-copy is not possible, data has to be copied, over the system memory bus, from one partition to the other.

Integrated graphics with unified main memory, to be found AMD "Kaveri" or PlayStation 4 (HSA).
As an alternative to the use of a video card, video hardware can be integrated into the motherboard or the CPU. Both approaches can be called integrated graphics. Motherboard-based implementations are sometimes called "on-board video" while CPU-based implementations are known as accelerated processing units (APUs). Almost all desktop computer motherboards with integrated graphics allow the disabling of the integrated graphics chip in BIOS, and have a PCI, or PCI Express (PCI-E) slot for adding a higher-performance graphics card in place of the integrated graphics. The ability to disable the integrated graphics sometimes also allows the continued use of a motherboard on which the on-board video has failed. Sometimes both the integrated graphics and a dedicated graphics card can be used simultaneously to feed separate displays. The main advantages of integrated graphics include cost, compactness, simplicity and low energy consumption. The performance disadvantage of integrated graphics arises because the graphics processor shares system resources with the CPU. A dedicated graphics card has its own random access memory (RAM), its own cooling system, and dedicated power regulators, with all components designed specifically for processing video images. Upgrading to a dedicated graphics card offloads work from the CPU and system RAM, so not only will graphics processing be faster, but the computer's overall performance may also improve.

Both of the dominant CPU makers, AMD and Intel, are moving to APUs. One of the reasons is that graphics processors are powerful parallel processors, and placing them on the CPU die allows their parallel processing ability to be harnessed for various computing tasks in addition to graphics processing. (See Heterogeneous System Architecture, which discusses AMD's implementation.) APUs are the newer integrated graphics technology and, as costs decline, will probably be used instead of integrated graphics on the motherboard in most future low and mid-priced home and business computers. As of late 2013, the best APUs provide graphics processing approaching mid-range mobile video cards[3] and are adequate for casual gaming. Users seeking the highest video performance for gaming or other graphics-intensive uses should still choose computers with dedicated graphics cards. (See Size of market and impact of accelerated processing units on video card sales, below.)

Beyond the enthusiast segment is the market for professional video cards for workstations used in the special effects industry, and in fields such as design, analysis and scientific research. Nvidia is a major player in the professional segment. In November, 2013, AMD introduced a so-called "Supercomputing" graphics card "designed for data visualization in finance, oil exploration, aeronautics and automotive, design and engineering, geophysics, life sciences, medicine and defense."[4]

Power demand[edit]
As the processing power of video cards has increased, so has their demand for electrical power. Current high-performance video cards tend to consume a great deal of power. For example, the thermal design power (TDP) for the GeForce GTX TITAN is 250 Watts.[5] While CPU and power supply makers have recently moved toward higher efficiency, power demands of GPUs have continued to rise, so the video card may be the biggest electricity user in a computer.[6][7] Although power supplies are increasing their power too, the bottleneck is due to the PCI-Express connection, which is limited to supplying 75 Watts.[8] Modern video cards with a power consumption over 75 Watts usually include a combination of six-pin (75W) or eight-pin (150W) sockets that connect directly to the power supply. Providing adequate cooling becomes a challenge in such computers. Computers with multiple video cards may need power supplies in the 1000W-1500W range. Heat extraction becomes a major design consideration for computers with two or more high end video cards.

Size[edit]
Video cards for desktop computers come in one of two size profiles, which can allow a graphics card to be added even to small form factor PCs. Some video cards are not of usual size, and are thus categorized as being low profile.[9][10] Video card profiles are based on width only, with low-profile cards taking up less than the width of a PCIe slot. Length and thickness can vary greatly, with high-end cards usually occupying two or three expansion slots, and with dual-GPU cards -such as the Nvidia GeForce GTX 690- generally exceeding 10" in length.[11]

Multi-card scaling[edit]
Some graphics cards can be linked together to allow scaling of the graphics processing across multiple cards. This is done using either the PCIe bus on the motherboard, or, more commonly, a data bridge. Generally, the cards must be of the same model to be linked, and most low power cards are not able to be linked in this way.[12] AMD and Nvidia both have proprietary methods of scaling, CrossFireX for AMD, and SLI for Nvidia. Cards from different chipset manufacturers, architectures cannot be used together for multi card scaling. If a graphics card has different sizes of memory, the lowest value will be used, with the higher values being disregarded. Currently, scaling on consumer grade cards can be done using up to four cards.[13][14][15]

Device drivers[edit]
The device driver usually supports one or multiple cards, and has to be specifically written for an operating system.

Industry[edit]
The primary suppliers of the GPUs (video chips or chipsets) used in video cards are AMD and Nvidia. In the third quarter of 2013, AMD had a 35.5% market share while Nvidia had a 64.5% market share,[16] according to Jon Peddie Research. In economics, this industry structure is termed a duopoly. AMD and Nvidia also build and sell video cards, which are termed graphics add-in-board (AIBs) in the industry. (See Comparison of Nvidia graphics processing units and Comparison of AMD graphics processing units.) In addition to marketing their own video cards, AMD and Nvidia sell their GPUs to authorized AIB suppliers, which AMD and Nvidia refer to as "partners".[2] The fact that Nvidia and AMD compete directly with their customer/partners complicates relationships in the industry. The fact that AMD and Intel are direct competitors in the CPU industry is also noteworthy, since AMD-based video cards may be used in computers with Intel CPUs. Intel's move to APUs may weaken AMD, which until now has derived a significant portion of its revenue from graphics components. As of the second quarter of 2013, there were 52 AIB suppliers.[2] These AIB suppliers may market video cards under their own brands, and/or produce video cards for private label brands and/or produce video cards for computer manufacturers. Some AIB suppliers such as MSI build both AMD-based and Nvidia-based video cards. Others, such as EVGA, build only Nvidia-based video cards, while XFX, now builds only AMD-based video cards. Several AIB suppliers are also motherboard suppliers. The largest AIB suppliers, based on global retail market share for graphics cards, include Taiwan-based Palit Microsystems, Hong Kong-based PC Partner (which markets AMD-based video cards under its Sapphire brand and Nvidia-based video cards under its Zotac brand), Taiwan-based computer-maker Asustek Computer (Asus), Taiwan-based Micro-Star International (MSI), Taiwan-based Gigabyte Technology,[17] Brea, California, USA-based EVGA (which also sells computer components such as power supplies) and Ontario, California USA-based XFX. (The parent corporation of XFX is based in Hong Kong.)

Size of market and impact of accelerated processing units on video card sales[edit]
Video card shipments totaled 14.5 million units in the third quarter of 2013, a 17% fall from Q3 2012 levels.[16] The traditional PC market is shrinking as tablet computers and smartphones gain share. Years ago, the move to integrated graphics on the motherboard greatly reduced the market for low end video cards. Now, AMD and Intel's accelerated processing units, which combine graphics processing with CPU functions on the CPU die itself, are putting further pressure on video card sales.[17] AMD introduced a line of combined processors which it calls the AMD A-Series APU Processors (A4, A6, A8, A10) while Intel, rather than marketing an exclusive line of APUs, introduced its "4th Generation Intel  Core  Processors", some of which are APUs. Those processors are described as offering "Superb visuals and graphics performance without the cost of a separate graphics card."[18] They are branded as having Intel HD Graphics or Intel Iris Pro Graphics. As an example, the Intel Core i7 4750HQ with Iris Pro Graphics 5200, an accelerated processing unit for notebook computers, allows users with mid-range graphics requirements to use a notebook computer without a video card. In a September, 2013 review of the Intel Core i7 4750HQ accelerated processing unit (which is closely related to the Intel processor with HD Graphics 5000 used in the MacBook Air,) the website hardware.info stated: "With its latest generation of integrated graphics, Intel set out to rival the performance of the mid-range mobile Nvidia GeForce GT 650M graphics card. And the tests leave no doubt about it, both 3DMark and the gaming benchmarks confirm that the [Intel] Iris Pro Graphics 5200 is on the same level of or slightly below that of the GT 650M."[3] (The GeForce GT 650M is not sold through retail channels, but an EVGA desktop GTX 650 was selling for around  120 in late 2013.[19]) Although the review notes that Intel's accelerated processing unit is not yet cost competitive, the technology is approaching competitiveness, at least with mid-range mobile dedicated video. (A video benchmarking website that tabulates user-submitted benchmarks shows Intel Iris Pro Graphics 5200, based on a very small sample of 8 submissions, scoring a G3D Mark of 912,[20] versus 1296 for the Nvidia GeForce GT 650M,[21] with higher scores being better. If the benchmark is linear, that puts the Iris Pro Graphics 5200's performance at about 70% of the GeForce GT 650M Intel was targeting. AMD's A10-5750M mobile APU with Radeon HD 8650G graphics scores 858 on this graphics benchmark.[22]) With anticipated price reductions, it is predicted that APUs will eventually replace low to mid-range dedicated video implementations. That will leave only the high-end enthusiast and professional market segments for video card vendors.

Parts[edit]

A Radeon HD 7970 with the cooler removed, showing the major components of the card.
A modern video card consists of a printed circuit board on which the components are mounted. These include:

Graphics Processing Unit[edit]
Main article: graphics processing unit
A graphics processing unit (GPU), also occasionally called visual processing unit (VPU), is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the building of images in a frame buffer intended for output to a display. Because of the large degree of programmable computational complexity for such a task, a modern video card is also a computer unto itself.

Heat sink[edit]
A heat sink is mounted on most modern graphics cards. A heat sink spreads out the heat produced by the graphics processing unit evenly throughout the heat sink and unit itself. The heat sink commonly has a fan mounted as well to cool the heat sink and the graphics processing unit. Not all cards have heat sinks, for example, some cards are liquid cooled, and instead have a waterblock; additionally, cards from the 1980s and early 1990s did not produce much heat, and did not require heatsinks.

Video BIOS[edit]
The video BIOS or firmware contains a minimal program for initial set up and control of the video card. It may contain information on the memory timing, operating speeds and voltages of the graphics processor, RAM, and other details which can sometimes be changed. The usual reason for doing this is to overclock the video card to allow faster video processing speeds, however, this has the potential to irreversibly damage the card with the possibility of cascaded damage to the motherboard.

The modern Video BIOS does not support all the functions of the video card, being only sufficient to identify and initialize the card to display one of a few frame buffer or text display modes. It does not support YUV to RGB translation, video scaling, pixel copying, compositing or any of the multitude of other 2D and 3D features of the video card.

Video memory[edit]
Type  Memory clock rate (MHz) Bandwidth (GB/s)
DDR 166   950 1.2   3.04
DDR2  2000   3600 128   200
GDDR5 900   7000  80   336.5
The memory capacity of most modern video cards ranges from 1 GB to 12 GB.[23] Since video memory needs to be accessed by the GPU and the display circuitry, it often uses special high-speed or multi-port memory, such as VRAM, WRAM, SGRAM, etc. Around 2003, the video memory was typically based on DDR technology. During and after that year, manufacturers moved towards DDR2, GDDR3, GDDR4 and GDDR5. The effective memory clock rate in modern cards is generally between 1 GHz to 7 GHz.

Video memory may be used for storing other data as well as the screen image, such as the Z-buffer, which manages the depth coordinates in 3D graphics, textures, vertex buffers, and compiled shader programs.

RAMDAC[edit]
The RAMDAC, or Random Access Memory Digital-to-Analog Converter, converts digital signals to analog signals for use by a computer display that uses analog inputs such as Cathode ray tube (CRT) displays. The RAMDAC is a kind of RAM chip that regulates the functioning of the graphics card. Depending on the number of bits used and the RAMDAC-data-transfer rate, the converter will be able to support different computer-display refresh rates. With CRT displays, it is best to work over 75 Hz and never under 60 Hz, in order to minimize flicker.[24] (With LCD displays, flicker is not a problem.[citation needed]) Due to the growing popularity of digital computer displays and the integration of the RAMDAC onto the GPU die, it has mostly disappeared as a discrete component. All current LCD/plasma monitors and TVs and projectors with only digital connections, work in the digital domain and do not require a RAMDAC for those connections. There are displays that feature analog inputs (VGA, component, SCART etc.) only. These require a RAMDAC, but they reconvert the analog signal back to digital before they can display it, with the unavoidable loss of quality stemming from this digital-to-analog-to-digital conversion.[25] With VGA standard being phased out in favor of digital, RAMDACs will begin to disappear from video cards.

Output interfaces[edit]

Video In Video Out (VIVO) for S-Video (TV-out), Digital Visual Interface (DVI) for High-definition television (HDTV), and DB-15 for Video Graphics Array (VGA)
The most common connection systems between the video card and the computer display are:

Video Graphics Array (VGA) (DE-15)[edit]

Video Graphics Array (VGA) (DE-15).
Main article: Video Graphics Array
Also known as D-sub, VGA is an analog-based standard adopted in the late 1980s designed for CRT displays, also called VGA connector. Some problems of this standard are electrical noise, image distortion and sampling error in evaluating pixels. Today, the VGA analog interface is used for high definition video including 1080p and higher. While the VGA transmission bandwidth is high enough to support even higher resolution playback, there can be picture quality degradation depending on cable quality and length. How discernible this quality difference is depends on the individual's eyesight and the display; when using a DVI or HDMI connection, especially on larger sized LCD/LED monitors or TVs, quality degradation, if present, is prominently visible. Blu-ray playback at 1080p is possible via the VGA analog interface, if Image Constraint Token (ICT) is not enabled on the Blu-ray disc.

Digital Visual Interface (DVI)[edit]

Digital Visual Interface (DVI-I).
Main article: Digital Visual Interface
Digital-based standard designed for displays such as flat-panel displays (LCDs, plasma screens, wide high-definition television displays) and video projectors. In some rare cases high end CRT monitors also use DVI. It avoids image distortion and electrical noise, corresponding each pixel from the computer to a display pixel, using its native resolution. It is worth to note that most manufacturers include DVI-I connector, allowing (via simple adapter) standard RGB signal output to an old CRT or LCD monitor with VGA input.

Video In Video Out (VIVO) for S-Video, Composite video and Component video[edit]
Included to allow the connection with televisions, DVD players, video recorders and video game consoles. They often come in two 10-pin mini-DIN connector variations, and the VIVO splitter cable generally comes with either 4 connectors (S-Video in and out + composite video in and out), or 6 connectors (S-Video in and out + component PB out + component PR out + component Y out [also composite out] + composite in).

High-Definition Multimedia Interface (HDMI)[edit]

High-Definition Multimedia Interface (HDMI)
Main article: HDMI
HDMI is a compact audio/video interface for transferring uncompressed video data and compressed/uncompressed digital audio data from an HDMI-compliant device ("the source device") to a compatible digital audio device, computer monitor, video projector, or digital television.[26] HDMI is a digital replacement for existing analog video standards. HDMI supports copy protection through HDCP.

DisplayPort[edit]

DisplayPort
Main article: DisplayPort
DisplayPort is a digital display interface developed by the Video Electronics Standards Association (VESA). The interface is primarily used to connect a video source to a display device such as a computer monitor, though it can also be used to transmit audio, USB, and other forms of data.[27] The VESA specification is royalty-free. VESA designed it to replace VGA, DVI, and LVDS. Backward compatibility to VGA and DVI by using adapter dongles enables consumers to use DisplayPort fitted video sources without replacing existing display devices. Although DisplayPort has much of the same functionality as HDMI, it is expected to complement the interface, not replace it.[28][29]

Other types of connection systems[edit]
Composite video Analog system with lower 480i resolution; it uses the RCA connector. The single pin connector carries all resolution, brightness and color information, making it the lowest quality dedicated video connection.[30]
Composite-video-cable.jpg
Component video It has three cables, each with RCA connector (YCBCR for digital component, or YPBPR for analog component); it is used in older projectors, video-game consoles, DVD players.[31] It can carry SDTV 480i and EDTV 480p resolutions, and HDTV resolutions 720p and 1080i, but not 1080p due to industry concerns about copy protection. Contrary to popular belief it looks equal to HDMI for the resolutions it carries,[32] but for best performance from Blu-ray, other 1080p sources like PPV, and 4K Ultra HD, a digital display connector is required.
Component video jack.jpg
DB13W3  An analog standard once used by Sun Microsystems, SGI and IBM.
DB13W3 Pinout.svg
DMS-59  A connector that provides two DVI or VGA outputs on a single connector. This is a DMS-59 port.
DMS-59.jpg
Motherboard interfaces[edit]
Main articles: Bus (computing) and Expansion card
Chronologically, connection systems between video card and motherboard were, mainly:

S-100 bus: Designed in 1974 as a part of the Altair 8800, it was the first industry-standard bus for the microcomputer industry.
ISA: Introduced in 1981 by IBM, it became dominant in the marketplace in the 1980s. It was an 8 or 16-bit bus clocked at 8 MHz.
NuBus: Used in Macintosh II, it was a 32-bit bus with an average bandwidth of 10 to 20 MB/s.
MCA: Introduced in 1987 by IBM it was a 32-bit bus clocked at 10 MHz.
EISA: Released in 1988 to compete with IBM's MCA, it was compatible with the earlier ISA bus. It was a 32-bit bus clocked at 8.33 MHz.
VLB: An extension of ISA, it was a 32-bit bus clocked at 33 MHz. Also referred to as VESA.
PCI: Replaced the EISA, ISA, MCA and VESA buses from 1993 onwards. PCI allowed dynamic connectivity between devices, avoiding the manual adjustments required with jumpers. It is a 32-bit bus clocked 33 MHz.
UPA: An interconnect bus architecture introduced by Sun Microsystems in 1995. It had a 64-bit bus clocked at 67 or 83 MHz.
USB: Although mostly used for miscellaneous devices, such as secondary storage devices and toys, USB displays and display adapters exist.
AGP: First used in 1997, it is a dedicated-to-graphics bus. It is a 32-bit bus clocked at 66 MHz.
PCI-X: An extension of the PCI bus, it was introduced in 1998. It improves upon PCI by extending the width of bus to 64 bits and the clock frequency to up to 133 MHz.
PCI Express: Abbreviated PCIe, it is a point to point interface released in 2004. In 2006 provided double the data-transfer rate of AGP. It should not be confused with PCI-X, an enhanced version of the original PCI specification.
In the attached table[33] is a comparison between a selection of the features of some of those interfaces.
Video Graphics Array (VGA) refers specifically to the display hardware first introduced with the IBM PS/2 line of computers in 1987,[1] but through its widespread adoption has also come to mean either an analog computer display standard, the 15-pin D-subminiature VGA connector or the 640x480 resolution itself.

VGA was the last IBM graphics standard to which the majority of PC clone manufacturers conformed, making it the lowest common denominator that virtually all post-1990 PC graphics hardware can be expected to implement. It was officially followed by IBM's Extended Graphics Array (XGA) standard, but was effectively superseded by numerous slightly different extensions to VGA made by clone manufacturers, collectively known as Super VGA.

Today, the VGA analog interface is used for high definition video, including resolutions of 1080p and higher. While the transmission bandwidth of VGA is high enough to support even higher resolution playback, there can be picture quality degradation depending on cable quality and length. How discernible this degradation is depends on the individual's eyesight and the display, though it is more noticeable when switching to and from digital inputs like HDMI or DVI.
(Total vertical sync and blanking time 1.43 ms; equivalent to line periods of A = 10, B = 2, C = 33, D = 480 and each complete frame = 525)

These timings are somewhat altered in "70Hz" mode, as although it uses the same line rate, its frame rate is not quite exactly 7/6ths that of "60Hz", despite 525 dividing cleanly into 7 - and, of course, 480/400 is itself a larger 6:5 ratio. Instead, it compromises on a 449-line frame (instead of the expected 450), with the back porch extended to 34 lines, and the front porch to 13, with an unaltered 2-line sync pulse - and the active image taking up 89% of the total scan period rather than 91%. The monitor is triggered into synchronising at the higher frame scan rate (and, with digital displays such as LCDs, the higher horizontal pixel density) by use of a positive-polarity VSync pulse, versus the negative pulse of 60 Hz mode.

Depending on manufacturer, the exact details of active period and front/back porch widths, particularly in the horizontal domain, may vary slightly. This does not usually cause a problem as the porches are merely intended to act as blanked-video buffers offering a little overscan space between the active area and the sync pulse (which triggers, in traditional CRT monitors, the phosphor beam deflection "flyback" to the upper or left hand side of the tube) and thus can be safely overrun into by a certain amount when everything else is operating correctly. The relationship between the front and back porches can also be altered within certain limits, which makes possible special features such as software-based image alignment with certain graphics cards (centering the image within the monitor frame by adjusting the location of the active screen area between the horizontal and vertical porches, rather than relying wholly upon the adjustment range offered by the monitor's own controls which can sometimes be less than satisfactory).

This buffer zone is typically what is exploited to achieve higher active resolutions in the various custom screen modes, by deliberately reducing porch widths and using the freed-up scan time for active pixels instead. This technique can achieve an absolute maximum of 704 pixels horizontally in 25 MHz mode and 792 at 28 MHz without altering the actual sync width (in real-world cases, e.g. with 800 pixel wide mode, the sync pulse would be shortened and a small porch area left in place to prevent obvious visual artefacting), and as much as 523 or 447 lines at the standard 60 and 70 Hz refresh rates (again, it is usually necessary to leave SOME porch lines intact, hence the usual maximum of 410 or 512 lines at these rates, and the 50 Hz maximum being 600 lines rather than 626). Conveniently, the practical limits of these techniques are not quite high enough to overflow the available memory capacity of typical 256KB cards (800x600 consuming 235KB, and even the theoretical 832x624 requiring "only" 254KB), so the only concerns remain those of monitor compatibility.

Typical uses of selected modes[edit]
640x400 @ 70 Hz is traditionally the video mode used for booting most VGA-compatible x86 personal computers[11] which show a graphical boot screen (text-mode boot uses 720x400 @ 70 Hz).

640x480 @ 60 Hz is the default Windows graphics mode (usually with 16 colors),[11] up to Windows 2000. It remains an option in XP and later versions via the boot menu "low resolution video" option and per-application compatibility mode settings.

320x200 @ 70 Hz is the most common mode for VGA-era PC games, using exactly the same timings as the 640x400 mode, but halving the pixel rate (and, in 256 colour mode, doubling the bit-depth of each pixel) and displaying each line of pixels twice.

The actual timings vary slightly from the defined standard. For example, for 640x480 @ 60 Hz, a 25.17  s active video time with a pixel frequency of 25.174 MHz gives 634 pixels, rather than the expected 640.

Connectors[edit]

A D-SUB connector (better known as VGA connector)
See also: VGA connector
VGA uses a DE-15 connector. This connector fits on the mounting tab of an ISA expansion card.


VGA BNC connectors
An alternative method of connecting VGA devices that maintains very high signal quality is the BNC connector, typically used as a group of five connectors, one each for Red, Green, Blue, Horizontal Sync, and Vertical Sync. With BNC, the coaxial wires are fully shielded end-to-end and through the interconnect so that no crosstalk or external interference is possible.

However, BNC connectors are relatively large compared to the DE 15, and some attention is needed to make sure each cable goes to the correct socket. Additionally, extra signal lines such as +5 V, DDC, and DDC2 are not supported using BNC connectors.

Standard text modes[edit]
See also: VGA compatible text mode
The BIOS offers some text modes for a VGA adapter, which have 80x25, 40x25, 80x43 or 80x50 text grid. Each cell may choose from one of 16 available colors for its foreground and eight colors for the background; the eight background colors allowed are the ones without the high-intensity bit set. Each character may also be made to blink; all that are set to blink will blink in unison. The blinking option for the entire screen can be exchanged for the ability to use all 16 colors for background. All of these options are the same as those on the CGA adapter as introduced by IBM.

Like EGA, VGA supports having up to 512 different simultaneous characters on screen, albeit in only 8 foreground colors, by rededicating one color bit as the highest bit of the character number. The glyphs on 80x25 mode are normally made of 9x16 pixels. Users may define their own character set by loading a custom font onto the card. As character data is only eight bits wide on VGA, just as on all of its predecessors, there is usually a blank pixel column between any two horizontally adjacent glyphs. However, some characters are normally made nine bits wide by repeating their last column instead of inserting a blank column, especially those defining horizontally connected IBM box-drawing characters. This functionality is hard-wired to the character numbers C0hex to DFhex, where all horizontally connecting characters are found in Codepage 437 and its most common derivatives. The same column-repeating trick was already used on the older MDA hardware with its 9x14 pixel glyphs, but on VGA it can be turned off when loading a font in which those character numbers do not represent box drawing characters.[12][13]

Monochrome modes[edit]
VGA adapters usually support both monochrome and color modes, though the monochrome mode is almost never used, and support for the full set of MDA text mode attributes (intense, underline) is often missing. Black and white text on nearly all modern VGA adapters is drawn by using gray colored text on a black background in color mode. VGA monochrome monitors intended primarily for text were sold, but most of them will work at least adequately with a VGA adapter in color mode. Occasionally, a faulty connection between a modern monitor and video card will cause the VGA part of the card to detect the monitor as monochrome; this will cause the BIOS and initial boot sequence to appear in greyscale. Usually, once the video card's drivers are loaded (for example, by continuing to boot into the operating system), they will override this detection and the monitor will return to color.

Addressing details[edit]

Examples of VGA images in 640x480x16 (top) and 320x200x256 modes (bottom). Dithering is used to mask color limitations.
The video memory of the VGA is mapped to the PC's memory via a window in the range between segments 0xA0000 and 0xBFFFF in the PC's real mode address space (A000:0000 and B000:FFFF in segment:offset notation). Typically, these starting segments are:

0xA0000 for EGA/VGA graphics modes (64 KB)
0xB0000 for monochrome text mode (32 KB)
0xB8000 for color text mode and CGA-compatible graphics modes (32 KB)
Due to the use of different address mappings for different modes, it is possible to have a monochrome adapter (i.e. MDA or Hercules) and a color adapter such as the VGA, EGA, or CGA installed in the same machine. At the beginning of the 1980s, this was typically used to display Lotus 1-2-3 spreadsheets in high-resolution text on a monochrome display and associated graphics on a low-resolution CGA display simultaneously. Many programmers also used such a setup with the monochrome card displaying debugging information while a program ran in graphics mode on the other card. Several debuggers, like Borland's Turbo Debugger, D86 (by Alan J. Cox) and Microsoft's CodeView could work in a dual monitor setup. Either Turbo Debugger or CodeView could be used to debug Windows. There were also DOS device drivers such as ox.sys, which implemented a serial interface simulation on the monochrome display and, for example, allowed the user to receive crash messages from debugging versions of Windows without using an actual serial terminal. It is also possible to use the "MODE MONO" command at the DOS prompt to redirect the output to the monochrome display. When a monochrome adapter was not present, it was possible to use the 0xB000 0xB7FF address space as additional memory for other programs (for example by adding the line "DEVICE=EMM386.EXE I=B000-B7FF" into config.sys, this memory would be made available to programs that can be "loaded high", that is loaded into high memory.)

Color palette[edit]
See also: List of monochrome and RGB palettes, 8-bit RGB, List of 16-bit computer hardware palettes and MCGA and VGA

VGA 256 default color palette
The VGA color system is backward compatible with the EGA and CGA adapters, and adds another level of indirection to support 256 8-bit-colors.

CGA was able to display 16 fixed colors, and EGA extended this by using 16 palette registers, each containing a color value from a 64-color palette. The default EGA palette values were chosen to look like the CGA colors, but it's possible to remap each color. (Note: This works in graphics and text modes.) The signals from the EGA palette entries will drive a set of six signal lines on the EGA output, with two lines corresponding to one color (off, dark, normal and bright, for red, green and blue).

VGA further extends this scheme by adding 256 color registers, 3 6 bit each for a total of 262,144 colors to choose from. These color registers are by default set to match the 64 default EGA colors. The values from these registers drive a DAC, which in turn drives three signal lines, one for red, green and blue. (Using analog signals allows a theoretically unlimited number of color values on a default VGA cable.)

Like EGA uses the CGA color value to address a palette entry, the VGA hardware will also use the palette entries not directly as signal levels but as indexes to the color registers. Therefore, in the 16-color-modes, the color value from the RAM will reference a palette register and that palette register will select a color register. E.g. the default palette entry for brown (on CGA: 4 (red) + 2(green), contains 0x14 (dark green + normal red) on the EGA palette. The corresponding VGA color register 0x14 is preset to (42,21,0, or #aa5500, also and off cause brown).

For 16-color-modes, using the 6-bit palette registers allows to use a block of up to 64 color registers, usually 0-63.

In the 256-color-modes, the palette registers are set to have no effect and the DAC is set to combine two 4-bit color values into the original
 8-bit-value; thereby the 8-bit color number in the video RAM is exactly the color register index to be used. Since the colors 0 ... 15 are still supposed to result in the CGA colors, the color registers are not preset to contain the EGA palette, instead it contains the 16 CGA colors in the first entries. The other entries are 16 gray levels from black to white and 9 groups of 24 color values. The 

Computer monitor
From Wikipedia, the free encyclopedia
  (Redirected from Computer display)
A computer monitor or a computer display is an electronic visual display for computers. A monitor usually comprises the display device, circuitry, casing, and power supply. The display device in modern monitors is typically a thin film transistor liquid crystal display (TFT-LCD) or a flat panel LED display, while older monitors used a cathode ray tubes (CRT). It can be connected to the computer via VGA, DVI, HDMI, DisplayPort, Thunderbolt, LVDS (Low-voltage differential signaling) or other proprietary connectors and signals.

Originally, computer monitors were used for data processing while television receivers were used for entertainment. From the 1980s onwards, computers (and their monitors) have been used for both data processing and entertainment, while televisions have implemented some computer functionality. The common aspect ratio of televisions, and computer monitors, has changed from 4:3 to 16:10, to 16:9.

Contents  [hide] 
1 History
2 Technologies
2.1 Cathode ray tube
2.2 Liquid crystal display
2.3 Organic light-emitting diode
3 Measurements of performance
3.1 Size
3.2 Aspect ratio
3.3 Resolution
4 Additional features
4.1 Power saving
4.2 Integrated accessories
4.3 Glossy screen
4.4 Curved designs
4.5 Directional screen
4.6 3D
4.7 Touch screen
4.8 Tablet screens
5 Mounting
5.1 Desktop
5.2 VESA mount
5.3 Rack mount
5.4 Panel mount
5.5 Open frame
6 Security vulnerabilities
7 See also
8 References
9 External links
History[edit]
Early electronic computers were fitted with a panel of light bulbs where the state of each particular bulb would indicate the on/off state of a particular register bit inside the computer. This allowed the engineers operating the computer to monitor the internal state of the machine, so this panel of lights came to be known as the 'monitor'. As early monitors were only capable of displaying a very limited amount of information, and were very transient, they were rarely considered for programme output. Instead, a line printer was the primary output device, while the monitor was limited to keeping track of the programme's operation.

As technology developed it was realized that the output of a CRT display was more flexible than a panel of light bulbs and eventually, by giving control of what was displayed to the programme itself, the monitor itself became a powerful output device in its own right.

Technologies[edit]
Further information: Comparison of CRT, LCD, Plasma, and OLED and History of display technology
Multiple technologies have been used for computer monitors. Until the 21st century most used cathode ray tubes but they have largely been superseded by LCD monitors.

Cathode ray tube[edit]
Main article: cathode ray tube
The first computer monitors used cathode ray tubes (CRTs). Prior to the advent of home computers in the late 1970s, it was common for a video display terminal (VDT) using a CRT to be physically integrated with a keyboard and other components of the system in a single large chassis. The display was monochrome and far less sharp and detailed than on a modern flat-panel monitor, necessitating the use of relatively large text and severely limiting the amount of information that could be displayed at one time. High-resolution CRT displays were developed for specialized military, industrial and scientific applications but they were far too costly for general use.

Some of the earliest home computers (such as the TRS-80 and Commodore PET) were limited to monochrome CRT displays, but color display capability was already a standard feature of the pioneering Apple II, introduced in 1977, and the specialty of the more graphically sophisticated Atari 800, introduced in 1979. Either computer could be connected to the antenna terminals of an ordinary color TV set or used with a purpose-made CRT color monitor for optimum resolution and color quality. Lagging several years behind, in 1981 IBM introduced the Color Graphics Adapter, which could display four colors with a resolution of 320 x 200 pixels, or it could produce 640 x 200 pixels with two colors. In 1984 IBM introduced the Enhanced Graphics Adapter which was capable of producing 16 colors and had a resolution of 640 x 350.[1]

By the end of the 1980s color CRT monitors that could clearly display 1024 x 768 pixels were widely available and increasingly affordable. During the following decade maximum display resolutions gradually increased and prices continued to fall. CRT technology remained dominant in the PC monitor market into the new millennium partly because it was cheaper to produce and offered viewing angles close to 180 degrees.[2] CRTs still offer some image quality advantages[clarification needed] over LCDs but improvements to the latter have made them much less obvious. The dynamic range of early LCD panels was very poor, and although text and other motionless graphics were sharper than on a CRT, an LCD characteristic known as pixel lag caused moving graphics to appear noticeably smeared and blurry.

Liquid crystal display[edit]
Main articles: Liquid-crystal display and Thin-film-transistor liquid-crystal display
There are multiple technologies that have been used to implement liquid crystal displays (LCD). Throughout the 1990s, the primary use of LCD technology as computer monitors was in laptops where the lower power consumption, lighter weight, and smaller physical size of LCDs justified the higher price versus a CRT. Commonly, the same laptop would be offered with an assortment of display options at increasing price points: (active or passive) monochrome, passive color, or active matrix color (TFT). As volume and manufacturing capability have improved, the monochrome and passive color technologies were dropped from most product lines.

TFT-LCD is a variant of LCD which is now the dominant technology used for computer monitors.[3]

The first standalone LCDs appeared in the mid-1990s selling for high prices. As prices declined over a period of years they became more popular, and by 1997 were competing with CRT monitors. Among the first desktop LCD computer monitors was the Eizo L66 in the mid-1990s, the Apple Studio Display in 1998, and the Apple Cinema Display in 1999. In 2003, TFT-LCDs outsold CRTs for the first time, becoming the primary technology used for computer monitors.[2] The main advantages of LCDs over CRT displays are that LCDs consume less power, take up much less space, and are considerably lighter. The now common active matrix TFT-LCD technology also has less flickering than CRTs, which reduces eye strain.[4] On the other hand, CRT monitors have superior contrast, have superior response time, are able to use multiple screen resolutions natively, and there is no discernible flicker if the refresh rate is set to a sufficiently high value. LCD monitors have now very high temporal accuracy and can be used for vision research.[5]

Organic light-emitting diode[edit]
Main article: Organic light-emitting diode
Organic light-emitting diode (OLED) monitors provide higher contrast and better viewing angles than LCDs but they require more power when displaying documents with white or bright backgrounds. In 2011, a 25-inch (64 cm) OLED monitor cost  7500, but the prices are expected to drop.[6]

Measurements of performance[edit]
The performance of a monitor is measured by the following parameters:

Luminance is measured in candelas per square meter (cd/m2 also called a Nit).
Aspect ratio is the ratio of the horizontal length to the vertical length. Monitors usually have the aspect ratio 4:3, 5:4, 16:10 or 16:9.
Viewable image size is usually measured diagonally, but the actual widths and heights are more informative since they are not affected by the aspect ratio in the same way. For CRTs, the viewable size is typically 1 in (25 mm) smaller than the tube itself.
Display resolution is the number of distinct pixels in each dimension that can be displayed. For a given display size, maximum resolution is limited by dot pitch.
Dot pitch is the distance between sub-pixels of the same color in millimeters. In general, the smaller the dot pitch, the sharper the picture will appear.
Refresh rate is the number of times in a second that a display is illuminated. Maximum refresh rate is limited by response time.
Response time is the time a pixel in a monitor takes to go from active (white) to inactive (black) and back to active (white) again, measured in milliseconds. Lower numbers mean faster transitions and therefore fewer visible image artifacts.
Contrast ratio is the ratio of the luminosity of the brightest color (white) to that of the darkest color (black) that the monitor is capable of producing.
Power consumption is measured in watts.
Delta-E: Color accuracy is measured in delta-E; the lower the delta-E, the more accurate the color representation. A delta-E of below 1 is imperceptible to the human eye. Delta-Es of 2 to 4 are considered good and require a sensitive eye to spot the difference.
Viewing angle is the maximum angle at which images on the monitor can be viewed, without excessive degradation to the image. It is measured in degrees horizontally and vertically.
Size[edit]
Main article: Display size

The area, height and width of displays with identical diagonal measurements vary dependent on aspect ratio.
On two-dimensional display devices such as computer monitors the display size or viewable image size is the actual amount of screen space that is available to display a picture, video or working space, without obstruction from the case or other aspects of the unit's design. The main measurements for display devices are: width, height, total area and the diagonal.

The size of a display is usually by monitor manufacturers given by the diagonal, i.e. the distance between two opposite screen corners. This method of measurement is inherited from the method used for the first generation of CRT television, when picture tubes with circular faces were in common use. Being circular, it was the external diameter of the glass envelope that described their size. Since these circular tubes were used to display rectangular images, the diagonal measurement of the rectangular image was smaller than the diameter of the tube's face (due to the thickness of the glass). This method continued even when cathode ray tubes were manufactured as rounded rectangles; it had the advantage of being a single number specifying the size, and was not confusing when the aspect ratio was universally 4:3.

With the introduction of flat panel technology, the diagonal measurement became the actual diagonal of the visible display. This meant that an eighteen-inch LCD had a larger visible area than an eighteen-inch cathode ray tube.

The estimation of the monitor size by the distance between opposite corners does not take into account the display aspect ratio, so that for example a 16:9 21-inch (53 cm) widescreen display has less area, than a 21-inch (53 cm) 4:3 screen. The 4:3 screen has dimensions of 16.8 in   12.6 in (43 cm   32 cm) and area 211 sq in (1,360 cm2), while the widescreen is 18.3 in   10.3 in (46 cm   26 cm), 188 sq in (1,210 cm2).

Aspect ratio[edit]
Main article: Display aspect ratio
Until about 2003, most computer monitors had a 4:3 aspect ratio and some had 5:4. Between 2003 and 2006, monitors with 16:9 and mostly 16:10 (8:5) aspect ratios became commonly available, first in laptops and later also in standalone monitors. Reasons for this transition was productive uses for such monitors, i.e. besides widescreen computer game play and movie viewing, are the word processor display of two standard letter pages side by side, as well as CAD displays of large-size drawings and CAD application menus at the same time.[7][8] In 2008 16:10 became the most common sold aspect ratio for LCD monitors and the same year 16:10 was the mainstream standard for laptops and notebook computers.[9]

In 2010 the computer industry started to move over from 16:10 to 16:9 because 16:9 was chosen to be the standard high-definition television display size, and because they were cheaper to manufacture.

In 2011 non-widescreen displays with 4:3 aspect ratios were only being manufactured in small quantities. According to Samsung this was because the "Demand for the old 'Square monitors' has decreased rapidly over the last couple of years," and "I predict that by the end of 2011, production on all 4:3 or similar panels will be halted due to a lack of demand."[10]

Resolution[edit]
Main article: Display resolution
The resolution for computer monitors has increased over time. From 320x200 during the early 1980s, to 800x600 during the late 1990s. Since 2009, the most commonly sold resolution for computer monitors is 1920x1080.[11] Before 2013 top-end consumer products were limited to 2560x1600 at 30 in (76 cm), excluding Apple products.[12] Apple introduced 2880x1800 with Retina MacBook Pro at 15.4 in (39 cm) on June 12, 2012, and introduced a 5120x2880 Retina iMac at 27 in (69 cm) on October 16, 2014. By 2015 all major display manufacturers had released 3840x2160 resolution displays.

Additional features[edit]
Power saving[edit]
Most modern monitors will switch to a power-saving mode if no video-input signal is received. This allows modern operating systems to turn off a monitor after a specified period of inactivity. This also extends the monitor's service life.

Some monitors will also switch themselves off after a time period on standby.

Most modern laptops provide a method of screen dimming after periods of inactivity or when the battery is in use. This extends battery life and reduces wear.

Integrated accessories[edit]
Many monitors have other accessories (or connections for them) integrated. This places standard ports within easy reach and eliminates the need for another separate hub, camera, microphone, or set of speakers. These monitors have advanced microprocessors which contain codec information, Windows Interface drivers and other small software which help in proper functioning of these functions.

Glossy screen[edit]
Main article: Glossy display
Some displays, especially newer LCD monitors, replace the traditional anti-glare matte finish with a glossy one. This increases color saturation and sharpness but reflections from lights and windows are very visible. Anti-reflective coatings are sometimes applied to help reduce reflections, although this only mitigates the effect.

Curved designs[edit]
In about 2009, NEC/Alienware together with Ostendo Technologies (based in Carlsbad, CA) were offering a curved (concave) 43-inch (110 cm) monitor that allows better viewing angles near the edges, covering 75% of peripheral vision. This monitor had 2880x900 resolution, LED backlight and was marketed as suitable both for gaming and office work, while for  6499 it was rather expensive.[13] While this particular monitor is no longer in production, most PC manufacturers now offer some sort of curved desktop display.

Directional screen[edit]
Narrow viewing angle screens are used in some security conscious applications.

3D[edit]
Main article: Stereo display
Newer monitors are able to display a different image for each eye, often with the help of special glasses, giving the perception of depth.

Active shutter
Main article: Active shutter 3D system
Polarized
Main article: Polarized 3D system
Autostereoscopic
Main article: Autostereoscopy
A directional screen which generates 3D images without headgear.

Touch screen[edit]
Main article: Touchscreen
These monitors use touching of the screen as an input method. Items can be selected or moved with a finger, and finger gestures may be used to convey commands. The screen will need frequent cleaning due to image degradation from fingerprints.

Tablet screens[edit]
Main article: Graphics tablet/screen hybrid
A combination of a monitor with a graphics tablet. Such devices are typically unresponsive to touch without the use of one or more special tools' pressure. Newer models however are now able to detect touch from any pressure and often have the ability to detect tilt and rotation as well.

Touch and tablet screens are used on LCDs as a substitute for the light pen, which can only work on CRTs.

Mounting[edit]
Computer monitors are provided with a variety of methods for mounting them depending on the application and environment.

Desktop[edit]
A desktop monitor is typically provided with a stand from the manufacturer which lifts the monitor up to a more ergonomic viewing height. The stand may be attached to the monitor using a proprietary method or may use, or be adaptable to, a Video Electronics Standards Association, VESA, standard mount. Using a VESA standard mount allows the monitor to be used with an after-market stand once the original stand is removed. Stands may be fixed or offer a variety of features such as height adjustment, horizontal swivel, and landscape or portrait screen orientation.

VESA mount[edit]
The Flat Display Mounting Interface (FDMI), also known as VESA Mounting Interface Standard (MIS) or colloquially as a VESA mount, is a family of standards defined by the Video Electronics Standards Association for mounting flat panel monitors, TVs, and other displays to stands or wall mounts.[14] It is implemented on most modern flat-panel monitors and TVs.

For Computer Monitors, the VESA Mount typically consists of four threaded holes on the rear of the display that will mate with an adapter bracket.

Rack mount[edit]
Rack mount computer monitors are available in two styles and are intended to be mounted into a 19-inch rack:


A fixed 19-inch (48 cm), 4:3 rack mount LCD monitor
Fixed
A fixed rack mount monitor is mounted directly to the rack with the LCD visible at all times. The height of the unit is measured in rack units (RU) and 8U or 9U are most common to fit 17-inch or 19-inch LCDs. The front sides of the unit are provided with flanges to mount to the rack, providing appropriately spaced holes or slots for the rack mounting screws. A 19-inch diagonal LCD is the largest size that will fit within the rails of a 19-inch rack. Larger LCDs may be accommodated but are 'mount-on-rack' and extend forward of the rack. There are smaller display units, typically used in broadcast environments, which fit multiple smaller LCDs side by side into one rack mount.


A 1U stowable clamshell 19-inch (48 cm), 4:3 rack mount LCD monitor with keyboard
Stowable
A stowable rack mount monitor is 1U, 2U or 3U high and is mounted on rack slides allowing the display to be folded down and the unit slid into the rack for storage. The display is visible only when the display is pulled out of the rack and deployed. These units may include only a display or may be equipped with a keyboard creating a KVM (Keyboard Video Monitor). Most common are systems with a single LCD but there are systems providing two or three displays in a single rack mount system.


A panel mount 19-inch (48 cm), 4:3 rack mount LCD monitor
Panel mount[edit]
A panel mount computer monitor is intended for mounting into a flat surface with the front of the display unit protruding just slightly. They may also be mounted to the rear of the panel. A flange is provided around the LCD, sides, top and bottom, to allow mounting. This contrasts with a rack mount display where the flanges are only on the sides. The flanges will be provided with holes for thru-bolts or may have studs welded to the rear surface to secure the unit in the hole in the panel. Often a gasket is provided to provide a water-tight seal to the panel and the front of the LCD will be sealed to the back of the front panel to prevent water and dirt contamination.

Open frame[edit]
An open frame monitor provides the LCD monitor and enough supporting structure to hold associated electronics and to minimally support the LCD. Provision will be made for attaching the unit to some external structure for support and protection. Open frame LCDs are intended to be built in to some other piece of equipment. An arcade video game would be a good example with the display mounted inside the cabinet. There is usually an open frame display inside all end-use displays with the end-use display simply providing an attractive protective enclosure. Some rack mount LCD manufacturers will purchase desk-top displays, take them apart, and discard the outer plastic parts, keeping the inner open-frame LCD for inclusion into their product.

Security vulnerabilities[edit]
According to an NSA document leaked to Der Spiegel, the NSA sometimes swaps the monitor cables on targeted computers with a bugged monitor cable in order to allow the NSA to remotely see what's displayed on the targeted computer monitor.[15]

Van Eck phreaking is the process of remotely displaying the contents of a CRT or LCD by detecting its electromagnetic emissions. It is named after Dutch computer researcher Wim van Eck, who in 1985 published the first paper on it, including proof of concept. Phreaking is the process of exploiting telephone networks, used here because of its connection to eavesdropping.[citation needed]

See also[edit]
History of display technology
Flat panel display
Multi-monitor
Vector monitor
Virtual desktopremaining 8 entries were black (see picture).[14]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks 
such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Further, algorithms for performing computations have existed 
since antiquity, even before the development of sophisticated computing equipment. The ancient Sanskrit treatise Shulba Sutras, or "Rules of the Chord", is a book of 
algorithms written in 800 BC for constructing geometric objects like altars using a peg and chord, an early precursor of the modern field of computational geometry.
Blaise Pascal designed and constructed the first working mechanical calculator, Pascal's calculator, in 1642.[2] In 1673, Gottfried Leibniz demonstrated a digital 
mechanical calculator, called the Stepped Reckoner.[3] He may be considered the first computer scientist and information theorist, for, among other reasons, documenting 
the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he released his simplified arithmometer, which was the 
first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic 
Computer A computer is a general purpose device that can be programmed to carry out a set of arithmetic or logical operations automatically. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem.

Conventionally, a computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logic operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices allow information to be retrieved from an external source, and the result of operations saved and retrieved.

Mechanical analog computers started appearing in the first century and were later used in the medieval era for astronomical calculations. In World War II, mechanical analog computers were used for specialized military applications such as calculating torpedo aiming. During this time the first electronic digital computers were developed. Originally they were the size of a large room, consuming as much power as several hundred modern personal computers (PCs).[1]

Modern computers based on integrated circuits are millions to billions of times more capable than the early machines, and occupy a fraction of the space.[2] Computers are small enough to fit into mobile devices, and mobile computers can be powered by small batteries. Personal computers in their various forms are icons of the Information Age and are generally considered as "computers". However, the embedded computers found in many devices from MP3 players to fighter aircraft and from electronic toys to industrial robots are the most numerous.The first known use of the word "computer" was in 1613 in a book called The Yong Mans Gleanings by English writer Richard Braithwait: "I haue read the truest computer of Times, and the best Arithmetician that euer breathed, and he reduceth thy dayes into a short number." It referred to a person who carried out calculations, or computations. The word continued with the same meaning until the middle of the 20th century. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.[3]

History
Main article: History of computing hardware
Pre twentieth century

The Ishango bone
Devices have been used to aid computation for thousands of years, mostly using one to one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers.[4][5] The use of counting rods is one example.


Suanpan (the number represented on this abacus is 6,302,715,408)
The abacus was initially used for arithmetic tasks. The Roman abacus was used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.


The ancient Greek designed Antikythera mechanism, dating between 150 to 100 BC, is the world's oldest analog computer.
The Antikythera mechanism is believed to be the earliest mechanical analog "computer", according to Derek J. de Solla Price.[6] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.

Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Ab  Rayh n al B r n  in the early 11th century.[7] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[8][9] and gear wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[10] Ab  Rayh n al B r n  invented the first mechanical geared lunisolar calendar astrolabe,[11] an early fixed wired knowledge processing machine[12] with a gear train and gear wheels,[13] circa 1000 AD.

The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.

The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.


A slide rule
The slide rule was invented around 1620 1630, shortly after the publication of the concept of the logarithm. It is a hand operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving time distance problems in light aircraft. To save space and for ease of reading, these are typically circular devices rather than the classic linear slide rule shape. A popular example is the E6B.

In the 1770s Pierre Jaquet Droz, a Swiss watchmaker, built a mechanical doll (automata) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically "programmed" to read instructions. Along with two other complex machines, the doll is at the Mus e d'Art et d'Histoire of Neuch tel, Switzerland, and still operates.[14]

The tide predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.

The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel and disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball and disk integrators.[15] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.

First general purpose computing device

A portion of Babbage's Difference engine.
Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer",[16] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general purpose computer that could be described in modern terms as Turing complete.[17][18]

The machine was about a century ahead of its time. All the parts for his machine had to be made by hand   this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.

Later analog computers

Sir William Thomson's third tide predicting machine design, 1879 81
During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.[19]

The first modern analog computer was a tide predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel and disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.[15]

The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious.

By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remain in use in some specialized applications such as education (control systems) and aircraft (slide rule).

Digital computer development
The principle of the modern computer was first described by mathematician and pioneering computer scientist Alan Turing, who set out the idea in his seminal 1936 paper,[20] On Computable Numbers. Turing reformulated Kurt G del's 1931 results on the limits of proof and computation, replacing G del's universal arithmetic based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the Entscheidungsproblem by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt.

He also introduced the notion of a 'Universal Machine' (now known as a Universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[21] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.

Electromechanical
By 1938 the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.


Replica of Zuse's Z3, the first fully automatic, digital (electromechanical) computer.
Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.[22]

In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer.[23][24] The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5 10 Hz.[25] Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard to implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.[26] The Z3 was Turing complete.[27][28]

Vacuum tubes and digital electronic circuits
Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[19] In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff Berry Computer (ABC) in 1942,[29] the first "automatic electronic digital computer".[30] This design was also all electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[31]


Colossus was the first electronic digital programmable computing device, and was used to break German ciphers during World War II.
During World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro mechanical bombes. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus.[31] He spent eleven months from early February 1943 designing and building the first Colossus.[32] After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944[33] and attacked its first message on 5 February.[31]

Colossus was the world's first electronic digital programmable computer.[19] It used a large number of valves (vacuum tubes). It had paper tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process.[34][35]


ENIAC was the first Turing complete device, and performed ballistics trajectory calculations for the United States Army.
The US built ENIAC[36] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing complete device and could compute any problem that would fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.

It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.[37]

Stored programs
Three tall racks containing electronic circuit boards
A section of the Manchester Small Scale Experimental Machine, the first stored program computer.
Early computing machines had fixed programs. Changing its function required the re wiring and re structuring of the machine.[31] With the proposal of the stored program computer this changed. A stored program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored program computer was laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored program digital computer. His 1945 report Proposed Electronic Calculator  was the first specification for such a device. John von Neumann at the University of Pennsylvania, also circulated his First Draft of a Report on the EDVAC in 1945.[19]


Ferranti Mark 1, c. 1951.
The Manchester Small Scale Experimental Machine, nicknamed Baby, was the world's first stored program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.[38] It was designed as a testbed for the Williams tube the first random access digital storage device.[39] Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer.[40] As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1.

The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general purpose computer.[41] Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[42] In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 [43] and ran the world's first regular routine office computer job.

Transistors

A bipolar junction transistor
The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.

At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.[44] Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955,[45] built by the electronics division of the Atomic Energy Research Establishment at Harwell.[46][47]

Integrated circuits
The next great advance in computing power came with the advent of the integrated circuit. The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.[48]

The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor.[49] Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.[50] In his patent application of 6 February 1959, Kilby described his new device as "a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated".[51][52] Noyce also came up with his own idea of an integrated circuit half a year later than Kilby.[53] His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.

This new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor", it is largely undisputed that the first single chip microprocessor was the Intel 4004,[54] designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.[55]

Mobile computers become dominant
With the continued miniaturization of computing resources, and advancements in portable battery life, portable computers grew in popularity in the 2000s.[56] The same developments that spurred the growth of laptop computers and other portable computers allowed manufacturers to integrate computing resources into cellular phones. These so called smartphones and tablets run on a variety of operating systems and have become the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013.[57]

Programs
The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language.

In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.

Stored program architecture
Main articles: Computer program and Computer programming

Replica of the Small Scale Experimental Machine (SSEM), the world's first stored program computer, at the Museum of Science and Industry in Manchester, England
This section applies to most common RAM machine based computers.

In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called "jump" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that "remembers" the location it jumped from and another instruction to return to the instruction following that jump instruction.

Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.

Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:

  begin:
  addi  8,  0, 0           # initialize sum to 0
  addi  9,  0, 1           # set first number to add = 1
  loop:
  slti  10,  9, 1000       # check if the number is less than 1000
  beq  10,  0, finish      # if odd number is greater than n then exit
  add  8,  8,  9           # update sum
  addi  9,  9, 1           # get next number
  j loop                   # repeat the summing process
  finish:
  add  2,  8,  0           # put sum in output register
Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.

Machine code
In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program[citation needed], architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.

While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,[58] it is extremely tedious and potentially error prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember   a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.


A 1970s punched card containing one line from a FORTRAN program. The card reads: "Z(1) = Y + W(1)" and is labeled "PROJ039" for identification purposes.
Programming language
Main article: Programming language
Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.

Low level languages
Main article: Low level programming language
Machine languages and the assembly languages that represent them (collectively termed low level programming languages) tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a PDA or a hand held videogame) cannot understand the machine language of an Intel Pentium or the AMD Athlon 64 computer that might be in a PC.[59]

High level languages/Third Generation Language
Main article: High level programming language
Though considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually "compiled" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.[60] High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.

Fourth Generation Languages
These 4G languages are less procedural than 3G languages. The benefit of 4GL is that it provides ways to obtain information without requiring the direct help of a programmer. Example of 4GL is SQL.

Program design

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2012)
Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies. The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.

Bugs
Main article: Software bug

The actual first computer bug, a moth found trapped on a relay of the Harvard Mark II computer
Errors in computer programs are called "bugs". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to "hang", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.[61]

Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term "bugs" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.[62]

Components
Main articles: Central processing unit and Microprocessor
File:Computer Components.webm
Video demonstrating the standard components of a "slimline" computer
A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires.

Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a "1", and when off it represents a "0" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.

Control unit
Main articles: CPU design and Control unit

Diagram showing how a particular MIPS architecture instruction would be decoded by the control system
The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.[63] Control systems in advanced computers may change the order of execution of some instructions to improve performance.

A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.[64]

The control system's function is as follows note that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:

Read the code for the next instruction from the cell indicated by the program counter.
Decode the numerical code for the instruction into a set of commands or signals for each of the other systems.
Increment the program counter so it points to the next instruction.
Read whatever data the instruction requires from cells in memory (or perhaps from an input device). The location of this required data is typically stored within the instruction code.
Provide the necessary data to an ALU or register.
If the instruction requires an ALU or specialized hardware to complete, instruct the hardware to perform the requested operation.
Write the result from the ALU back to a memory location or to a register or perhaps an output device.
Jump back to step (1).
Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as "jumps" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).

The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.

Central processing unit (CPU)
The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid 1970s CPUs have typically been constructed on a single integrated circuit called a microprocessor.

Arithmetic logic unit (ALU)
Main article: Arithmetic logic unit
The ALU is capable of performing two classes of operations: arithmetic and logic.[65]

The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other ("is 64 greater than 65 ").

Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.

Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously.[66] Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.

Memory
Main article: Computer data storage

Magnetic core memory was the computer memory of choice throughout the 1960s, until it was replaced by semiconductor memory.
A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered "address" and can store a single number. The computer can be instructed to "put the number 123 into the cell numbered 1357" or to "add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595." The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.

In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or  128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.

The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.

Computer main memory comes in two principal varieties:

random access memory or RAM
read only memory or ROM
RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.[67]

In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.

Input/output (I/O)
Main article: Input/output

Hard disk drives are common storage devices used with computers.
I/O is the means by which a computer exchanges information with the outside world.[68] Devices that provide input or output to the computer are called peripherals.[69] On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.

I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics.[citation needed] Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O.

Multitasking
Main article: Computer multitasking
While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.[70]

One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running "at the same time". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed "time sharing" since each program is allocated a "slice" of time in turn.[71]

Before the era of cheap computers, the principal use for multitasking was to allow many people to share the same computer.

Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a "time slice" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.

Multiprocessing
Main article: Multiprocessing

Cray designed many supercomputers that used multiprocessing heavily.
Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower end markets as a result.

Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored program architecture and from general purpose computers.[72] They often feature thousands of CPUs, customized high speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large scale simulation, graphics rendering, and cryptography applications, as well as with other so called "embarrassingly parallel" tasks.

Networking and the Internet
Main articles: Computer networking and Internet

Visualization of a portion of the routes on the Internet
Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large scale example of such a system, which led to a number of special purpose commercial systems such as Sabre.[73]

In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.[74] The technologies that made the Arpanet possible spread and evolved.

In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high tech environments, but in the 1990s the spread of applications like e mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. "Wireless" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.

Computer architecture paradigms
There are many types of computer architectures:

Quantum computer vs. Chemical computer
Scalar processor vs. Vector processor
Non Uniform Memory Access (NUMA) computers
Register machine vs. Stack machine
Harvard architecture vs. von Neumann architecture
Cellular architecture
Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing.[75]

Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms.

The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.

Misconceptions
Main articles: Human computer and Harvard Computers

Women as computers in NACA High Speed Flight Station "Computer Room"
A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word "computer" is synonymous with a personal electronic computer, the modern[76] definition of a computer is literally: "A device that computes, especially a programmable [usually] electronic machine that performs high speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information."[77] Any device which processes information qualifies as a computer, especially if the processing is purposeful.[citation needed]

Unconventional computing
Main article: Unconventional computing
Historically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example.[citation needed] More realistically, modern computers are made out of transistors made of photolithographed semiconductors.

Future
There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.

Further topics
Glossary of computers
Artificial intelligence
A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning.

Hardware
Main articles: Computer hardware and Personal computer hardware
The term hardware covers all of those parts of a computer that are tangible objects. Circuits, displays, power supplies, cables, keyboards, printers and mice are all hardware.

History of computing hardware
Main article: History of computing hardware
First generation (mechanical/electromechanical)  Calculators Pascal's calculator, Arithmometer, Difference engine, Quevedo's analytical machines
Programmable devices  Jacquard loom, Analytical engine, IBM ASCC/Harvard Mark I, Harvard Mark II, IBM SSEC, Z1, Z2, Z3
Second generation (vacuum tubes)  Calculators Atanasoff Berry Computer, IBM 604, UNIVAC 60, UNIVAC 120
Programmable devices  Colossus, ENIAC, Manchester Small Scale Experimental Machine, EDSAC, Manchester Mark 1, Ferranti Pegasus, Ferranti Mercury, CSIRAC, EDVAC, UNIVAC I, IBM 701, IBM 702, IBM 650, Z22
Third generation (discrete transistors and SSI, MSI, LSI integrated circuits) Mainframes  IBM 7090, IBM 7080, IBM System/360, BUNCH
Minicomputer  HP 2116A, IBM System/32, IBM System/36, LINC, PDP 8, PDP 11
Fourth generation (VLSI integrated circuits)  Minicomputer  VAX, IBM System i
4 bit microcomputer Intel 4004, Intel 4040
8 bit microcomputer Intel 8008, Intel 8080, Motorola 6800, Motorola 6809, MOS Technology 6502, Zilog Z80
16 bit microcomputer  Intel 8088, Zilog Z8000, WDC 65816/65802
32 bit microcomputer  Intel 80386, Pentium, Motorola 68000, ARM
64 bit microcomputer[78]  Alpha, MIPS, PA RISC, PowerPC, SPARC, x86 64, ARMv8 A
Embedded computer Intel 8048, Intel 8051
Personal computer Desktop computer, Home computer, Laptop computer, Personal digital assistant (PDA), Portable computer, Tablet PC, Wearable computer
Theoretical/experimental  Quantum computer, Chemical computer, DNA computing, Optical computer, Spintronics based computer
Other hardware topics
Peripheral device (input/output)  Input Mouse, keyboard, joystick, image scanner, webcam, graphics tablet, microphone
Output  Monitor, printer, loudspeaker
Both  Floppy disk drive, hard disk drive, optical disc drive, teleprinter
Computer buses  Short range RS 232, SCSI, PCI, USB
Long range (computer networking)  Ethernet, ATM, FDDI
Software
Main article: Computer software
Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. When software is stored in hardware that cannot easily be modified (such as BIOS ROM in an IBM PC compatible), it is sometimes called "firmware".

Operating system /System Software Unix and BSD  UNIX System V, IBM AIX, HP UX, Solaris (SunOS), IRIX, List of BSD operating systems
GNU/Linux List of Linux distributions, Comparison of Linux distributions
Microsoft Windows Windows 95, Windows 98, Windows NT, Windows 2000, Windows Me, Windows XP, Windows Vista, Windows 7, Windows 8, Windows 10
DOS 86 DOS (QDOS), IBM PC DOS, MS DOS, DR DOS, FreeDOS
Mac OS  Mac OS classic, Mac OS X
Embedded and real time  List of embedded operating systems
Experimental  Amoeba, Oberon/Bluebottle, Plan 9 from Bell Labs
Library Multimedia  DirectX, OpenGL, OpenAL, Vulkan_(API)
Programming library C standard library, Standard Template Library
Data  Protocol  TCP/IP, Kermit, FTP, HTTP, SMTP
File format HTML, XML, JPEG, MPEG, PNG
User interface  Graphical user interface (WIMP) Microsoft Windows, GNOME, KDE, QNX Photon, CDE, GEM, Aqua
Text based user interface Command line interface, Text user interface
Application Software  Office suite  Word processing, Desktop publishing, Presentation program, Database management system, Scheduling & Time management, Spreadsheet, Accounting software
Internet Access Browser, E mail client, Web server, Mail transfer agent, Instant messaging
Design and manufacturing  Computer aided design, Computer aided manufacturing, Plant management, Robotic manufacturing, Supply chain management
Graphics  Raster graphics editor, Vector graphics editor, 3D modeler, Animation editor, 3D computer graphics, Video editing, Image processing
Audio Digital audio editor, Audio playback, Mixing, Audio synthesis, Computer music
Software engineering  Compiler, Assembler, Interpreter, Debugger, Text editor, Integrated development environment, Software performance analysis, Revision control, Software configuration management
Educational Edutainment, Educational game, Serious game, Flight simulator
Games Strategy, Arcade, Puzzle, Simulation, First person shooter, Platform, Massively multiplayer, Interactive fiction
Misc  Artificial intelligence, Antivirus software, Malware scanner, Installer/Package management systems, File manager
Languages
There are thousands of different programming languages some intended to be general purpose, others useful only for highly specialized applications.

Programming languages
Lists of programming languages  Timeline of programming languages, List of programming languages by category, Generational list of programming languages, List of programming languages, Non English based programming languages
Commonly used assembly languages  ARM, MIPS, x86
Commonly used high level programming languages  Ada, BASIC, C, C++, C#, COBOL, Fortran, PL/1, REXX, Java, Lisp, Pascal, Object Pascal
Commonly used scripting languages Bourne script, JavaScript, Python, Ruby, PHP, Perl
Firmware
Firmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it.

Types of computers
Computers are typically classified based on their uses:

Based on uses
Analog computer
Digital computer
Hybrid computer
Based on sizes
Micro computer
Personal computer
Mini Computer
Mainframe computer
Super computer
Input Devices
When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand operated or automated. The act of processing is mainly regulated by the CPU. Some examples of hand operated input devices are:

Overlay keyboard
Trackball
Joystick
Digital camera
Microphone
Touchscreen
Digital video
Image scanner
Graphics tablet
Computer keyboard
Mouse
Output Devices
The means through which computer gives output are known as output devices. Some examples of output devices are:

Computer monitor
Printer
Projector
Sound card
PC speaker
Video card
Professions and organizations
As the use of computers has spread throughout society, there are an increasing number of careers involving computers.

Computer related professions
Hardware related  Electrical engineering, Electronic engineering, Computer engineering, Telecommunications engineering, Optical engineering, Nanoengineering
Software related  Computer science, Computer engineering, Desktop publishing, Human computer interaction, Information technology, Information systems, Computational science, Software engineering, Video game industry, Web design

mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[4] He 
started developing this machine in 1834 and "in less than two years he had sketched out many of the salient features of the modern computer".[5] "A crucial step was the 
adoption of a punched card system derived from the Jacquard loom"[5] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the 
Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first 
computer program.[6] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became 
part of IBM. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and 
was also in the calculator business[7] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used 
cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[8]
During the 1940s, as new and more powerful computing machines were developed, the term computer came to refer to the machines rather than their human 
predecessors.[9] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study 
computation in general. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[10][11] The world's first computer 
science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science 
degree program in the United States was formed at Purdue University in 1962.[12] Since practical computers became available, many applications of computing have 
become distinct areas of study in their own rights.
Although many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted 
among the greater academic population.[13][14] It is the now well known IBM brand that formed part of the computer science revolution during this time. IBM (short for 
International Business Machines) released the IBM 704[15] and later the IBM 709[16] computers, which were widely used during the exploration period of such devices. 
"Still, working with the IBM [computer] was frustrating [] if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to 
start the whole process over again".[13] During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were 
commonplace.[14]
Time has seen significant improvements in the usability and effectiveness of computing technology.[17] Modern society has seen a significant shift in the users of 
computer technology, from usage only by experts and professionals, to a near ubiquitous user base. Initially, computers were quite costly, and some degree of human aid 
was needed for efficient use,in part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was 
needed for common usage.
Despite its short history as a formal academic discipline, computer science has made a number of fundamental contributions to science and society,in fact, along with 
electronics, it is a founding science of the current epoch of human history called the Information Age and a driver of the Information Revolution, seen as the third major 
leap in human technological progress after the Industrial Revolution (1750 1850 CE) and the Agricultural Revolution (8000 5000 BC).
These contributions include:
The start of the "digital revolution", which includes the current Information Age and the Internet.[19]
A formal definition of computation and computability, and proof that there are computationally unsolvable and intractable problems.[20]
The concept of a programming language, a tool for the precise expression of methodological information at various levels of abstraction.[21]
In cryptography, breaking the Enigma code was an important factor contributing to the Allied victory in World War II.[18]
Scientific computing enabled practical evaluation of processes and situations of great complexity, as well as experimentation entirely by s oftware. It also enabled 
advanced study of the mind, and mapping of the human genome became possible with the Human Genome Project.[19] Distributed computing projects such as 
Folding@home explore protein folding.
Algorithmic trading has increased the efficiency and liquidity of financial markets by using artificial intelligence, machine learning, and other statistical and numerical 
techniques on a large scale.[22] High frequency algorithmic trading can also exacerbate volatility.[23]
Computer graphics and computer generated imagery have become ubiquitous in modern entertainment, particularly in television, cinema, advertising, animation and video 
games. Even films that feature no explicit CGI are usually "filmed" now on digital cameras, or edited or post processed using a digital video editor.[24][25]
Simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations 
(notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and 
electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated 
circuits.[citation needed]
Artificial intelligence is becoming increasingly important as it gets more efficient and complex. There are many applications of AI, some of which can be seen at home, 
such as robotic vacuum cleaners. It is also present in video games and on the modern battlefield in drones, anti missile systems, and squad support robots.
Main article: Philosophy of computer science
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are 
science, technology, and mathematics.[26] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[27] Amnon H. Eden described 
them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs 
deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific 
paradigm" (which approaches computer related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).[28]
Name of the field[edit]
Although first proposed in 1956,[14] the term "computer science" appears in a 1959 article in Communications of the ACM,[29] in which Louis Fein argues for the creation 
of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921,[30] justifying the name by arguing that, like management 
science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[29] His efforts, and those of others such 
as numerical analyst George Forsythe, were rewarded: universities went on to create such programs, starting with Purdue in 1962.[31] Despite its name, a significant 
amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[32] Certain departments 
of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[33] to reflect the 
fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the 
Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the 
Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a distinct field of data analysis, including statistics and 
databases.
Also, in the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM,turingineer, 
turologist, flow charts man, applied meta mathematician, and applied epistemologist.[34] Three months later in the same journal, comptologist was suggested, followed next 
year by hypologist.[35] The term computics has also been suggested.[36] In Europe, terms derived from contracted translations of the expression "automatic information" 
(e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), 
inform tica (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (           , which means informatics) in Greek. Similar words have also 
been adopted in the UK (as in the School of Informatics of the University of Edinburgh).[37]
A folkloric quotation, often attributed to,but almost certainly not first formulated by,Edsger Dijkstra, states that "computer science is no more about computers than 
astronomy is about telescopes."[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than 
computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems 
and their deployment is often called information technology or information systems. However, there has been much cross fertilization of ideas between the various 
computer related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, 
biology, statistics, and logic.
Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that 
computing is a mathematical science.[10] Early computer science was strongly influenced by the work of mathematicians such as Kurt G del and Alan Turing, and there 
continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[14]
The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term "software 
engineering" means, and how computer science is defined.[38] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has 
claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of 
specific computations to achieve practical goals, making the two separate but complementary disciplines.[39]
The academic, political, and funding aspects of computer science tend to depend on whether a department formed with a mathematical emphasis or with an engineering 
emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of 
departments tend to make efforts to bridge the field educationally if not across all research.
Areas of computer science[edit]
Further information: Outline of computer science
As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing 
computing systems in hardware and software.[40][41] CSAB, formerly called Computing Sciences Accreditation Board,which is made up of representatives of the 
Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[42],identifies four areas that it considers crucial to the discipline of computer 
science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these 
four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel 
computation, distributed computation, human computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important 
areas of computer science.[40]
Theoretical computer science[edit]
Main article: Theoretical computer science
The broader field of theoretical computer science encompasses both the classical theory of computation and a wide range of other topics that focus on the more abstract, 
logical, and mathematical aspects of computing.
Theory of computation[edit]
Main article: Theory of computation
According to Peter Denning, the fundamental question underlying computer science is, "What can be (efficiently) automated "[10] Theory of computation is focused on 
answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first 
question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by 
computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP  problem, one of the Millennium Prize Problems,[43] is an open problem in the theory of computation.
Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable computer programs. 
Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness 
and resources consumption, and implementation (commonly referred to as coding[1][2]) of algorithms in a target programming language. Source code is written in one or 
more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem. 
The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms and formal 
logic.

Related tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine 
code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the 
term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software 
development practices.
Within software engineering, programming (the implementation) is regarded as one phase in a software development process.

There is an ongoing debate on the extent to which the writing of programs is an art form, a craft, or an engineering discipline.[3] In general, good programming is 
considered to be the measured application of all three, with the goal of producing an efficient and evolvable software solution (the criteria for "efficient" and "evolvable" 
vary considerably). The discipline differs from many other technical professions in that programmers, in general, do not need to be licensed or pass any standardized (or 
governmentally regulated) certification tests in order to call themselves "programmers" or even "software engineers." Because the discipline covers many areas, which 
may or may not include critical applications, it is debatable whether licensing is required for the profession as a whole. In most cases, the discipline is self governed by 
the entities which require the programming, and sometimes very strict environments are defined (e.g. United States Air Force use of AdaCore and security clearance). 
However, representing oneself as a "professional software engineer" without a license from an accredited institution is illegal in many parts of the world.

Another ongoing debate is the extent to which the programming language used in writing computer programs affects the form that the final program takes.[citation needed] 
This debate is analogous to that surrounding the Sapir,Whorf hypothesis[4] in linguistics and cognitive science, which postulates that a particular spoken language's 
nature influences the habitual thought of its speakers. Different language patterns yield different patterns of thought. This idea challenges the possibility of representing the 
world perfectly with language, because it acknowledges that the mechanisms of any language condition the thoughts of its speaker community.
Ancient cultures seemed to have no conception of computing beyond arithmetic, algebra, and geometry, occasionally devising computational systems with elements of 
calculus (e.g. the method of exhaustion). The only mechanical device that existed for numerical computation at the beginning of human history was the abacus, invented in 
Sumeria circa 2500 BC. Later, the Antikythera mechanism, invented some time around 100 BC in ancient Greece, is the first known mechanical calculator utilizing gears 
of various sizes and configuration to perform calculations,[5] which tracked the metonic cycle still used in lunar to solar calendars, and which is consistent for calculating 
the dates of the Olympiads.[6]

The Kurdish medieval scientist Al Jazari built programmable automata in 1206 AD. One system employed in these devices was the use of pegs and cams placed into a 
wooden drum at specific locations, which would sequentially trigger levers that in turn operated percussion instruments. The output of this device was a small drummer 
playing various rhythms and drum patterns.[7] The Jacquard loom, which Joseph Marie Jacquard developed in 1801, uses a series of pasteboard cards with holes 
punched in them. The hole pattern represented the pattern that the loom had to follow in weaving cloth. The loom could produce entirely different weaves using different 
sets of cards.

Charles Babbage adopted the use of punched cards around 1830 to control his Analytical Engine. Mathematician Ada Lovelace, a friend of Babbage, between 1842 and 
1843 translated an article by Italian military engineer Luigi Menabrea on the engine,[8] which she supplemented with a set of notes, simply called Notes. These notes 
include algorithm to calculate a sequence of Bernoulli numbers,[9] intended to be carried out by a machine. Despite controversy over scope of her contribution, many 
consider this algorithm to be the first computer program.[8]


Data and instructions were once stored on external punched cards, which were kept in order and arranged in program decks.
In the 1880s, Herman Hollerith invented the recording of data on a medium that could then be read by a machine. Prior uses of machine readable media, above, had been 
for lists of instructions (not data) to drive programmed machines such as Jacquard looms and mechanized musical instruments. "After some initial trials with paper tape, 
he settled on punched cards..."[10] To process these punched cards, first known as "Hollerith cards" he invented the keypunch, sorter, and tabulator unit record 
machines.[11] These inventions were the foundation of the data processing industry. In 1896 he founded the Tabulating Machine Company (which later became the core 
of IBM). The addition of a control panel (plugboard) to his 1906 Type I Tabulator allowed it to do different jobs without having to be physically rebuilt. By the late 1940s, 
there were several unit record calculators, such as the IBM 602 and IBM 604, whose control panels specified a sequence (list) of operations and thus were programmable 
machines.

The invention of the von Neumann architecture allowed computer programs to be stored in computer memory. Early programs had to be painstakingly crafted using the 
instructions (elementary operations) of the particular machine, often in binary notation. Every model of computer would likely use different instructions (machine language) 
to do the same task. Later, assembly languages were developed that let the programmer specify each instruction in a text format, entering abbreviations for each operation 
code instead of a number and specifying addresses in symbolic form (e.g., ADD X, TOTAL). Entering a program in assembly language is usually more convenient, faster, 
and less prone to human error than using machine language, but because an assembly language is little more than a different notation for a machine language, any two 
machines with different instruction sets also have different assembly languages.


Wired control panel for an IBM 402 Accounting Machine
The synthesis of numerical calculation, predetermined operation and output, along with a way to organize and input instructions in a manner relatively easy for humans to 
conceive and produce, led to the modern development of computer programming. In 1954, FORTRAN was invented; it was the first widely used high level programming 
language to have a functional implementation, as opposed to just a design on paper.[12][13] (A high level language is, in very general terms, any programming language 
that allows the programmer to write programs in terms that are more abstract than assembly language instructions, i.e. at a level of abstraction "higher" than that of an 
assembly language.) It allowed programmers to specify calculations by entering a formula directly (e.g. Y = X*2 + 5*X + 9). The program text, or source, is converted into 
machine instructions using a special program called a compiler, which translates the FORTRAN program into machine language. In fact, the name FORTRAN stands for 
"Formula Translation". Many other languages were developed, including some for commercial programming, such as COBOL. Programs were mostly still entered using 
punched cards or paper tape. (See computer programming in the punch card era). By the late 1960s, data storage devices and computer terminals became inexpensive 
enough that programs could be created by typing directly into the computers. Text editors were developed that allowed changes and corrections to be made much more 
easily than with punched cards. (Usually, an error in punching a card meant that the card had to be discarded and a new one punched to replace it.)

As time has progressed, computers have made giant leaps in processing power, which have allowed the development of programming languages that are more abstracted 
from the underlying hardware. Popular programming languages of the modern era include ActionScript, C, C++, C#, Haskell, Java, JavaScript, Objective C, Perl, PHP, 
Python, Ruby, Smalltalk, SQL, Visual Basic, and dozens more.[14] Although these high level languages usually incur greater overhead, the increase in speed of modern 
computers has made the use of these languages much more practical than in the past. These increasingly abstracted languages are typically easier to learn and allow the 
programmer to develop applications much more efficiently and with less source code. However, high level languages are still impractical for a few programs, such as those 
where low level hardware control is necessary or where maximum processing speed is vital. Computer programming has become a popular career in the developed world, 
particularly in the United States, Europe, and Japan. Due to the high labor cost of programmers in these countries, some forms of programming have been increasingly 
subject to outsourcing (importing software and services from other countries, usually at a lower wage), making programming career decisions in developed countries more 
complicated, while increasing economic opportunities for programmers in less developed areas, particularly China and India.Quality requirements[edit]
Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:

Reliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms, and minimization of programming mistakes, such as 
mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off by one errors).
Robustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of 
needed resources such as memory, operating system services and network connections, user error, and unexpected power outages.
Usability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such 
issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical and sometimes hardware elements that improve the 
clarity, intuitiveness, cohesiveness and completeness of a program's user interface.
Portability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends 
on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware 
and operating system, and availability of platform specific compilers (and sometimes libraries) for the language of the source code.
Maintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or customizations, fix bugs and security 
holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end 
user but it can significantly affect the fate of a program over the long term.
Efficiency/performance: the amount of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to 
some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating 
memory leaks.Readability of source code[edit]
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects 
the aspects of quality above, including portability, usability and most importantly maintainability.

Readability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new 
source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study[15] found that a few simple readability transformations made code shorter 
and drastically reduced the time to understand it.

Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with 
the ability of the computer to efficiently compile and execute the code, contribute to readability.[16] Some of these factors include:

Different indentation styles (whitespace)
Comments
Decomposition
Naming conventions for objects (such as variables, classes, procedures, etc.)
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non traditional approaches to code structure 
and display. Techniques like Code refactoring can enhance readability.

Algorithmic complexity[edit]
The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for 
a given class of problem. For this purpose, algorithms are classified into orders using so called Big O notation, which expresses resource use, such as execution time or 
memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well established algorithms and their respective complexities and 
use this knowledge to choose algorithms that are best suited to the circumstances..

Methodologies[edit]
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure 
elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many 
programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a 
few weeks rather than years. There are many approaches to the Software development process.

Popular modeling techniques include Object Oriented Analysis and Design (OOAD) and Model Driven Architecture (MDA). The Unified Modeling Language (UML) is a 
notation used for both the OOAD and MDA.

A similar technique used for database design is Entity Relationship Modeling (ER Modeling).

Implementation techniques include imperative languages (object oriented or procedural), functional languages, and logic languages.

Measuring language usage[edit]
Main article: Measuring programming language popularity
It is very difficult to determine what are the most popular of modern programming languages. Methods of measuring programming language popularity include: counting 
the number of job advertisements that mention the language,[17] the number of books sold and courses teaching the language (this overestimates the importance of newer 
languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as 
COBOL).

Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, 
COBOL is still strong in corporate data centers[18] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and 
C in embedded software. Many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a 
prior language with new functionality added, (for example C++ adds object orientation to C, and Java adds memory management and bytecode to C++, but as a result, 
loses efficiency and the ability for low level manipulation).

Debugging[edit]

The bug from 1947 which is at the origin of a popular (but incorrect) etymology for the common term for a software defect.
Main article: Debugging
Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some 
languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static 
code analysis tool can help detect some possible problems.

Debugging is often done with IDEs like Eclipse, Visual Studio, Kdevelop, NetBeans and Code::Blocks. Standalone debuggers like gdb are also used, and these often 
provide less of a visual environment, usually using a command line.

Programming languages[edit]
Main articles: Programming language and List of programming languages
Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many 
considerations, such as company policy, suitability to task, availability of third party packages, or individual preference. Ideally, the programming language best suited for 
the task at hand will be selected. Trade offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that 
language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low level" to "high level"; "low level" 
languages are typically more machine oriented and faster to execute, whereas "high level" languages are more abstract and easier to use but execute less quickly. It is 
usually easier to code in "high level" languages than in "low level" ones.

Allen Downey, in his book How To Think Like A Computer Scientist, writes:

The details look different in different languages, but a few basic instructions appear in just about every language:
Input: Gather data from the keyboard, a file, or some other device.
Output: Display data on the screen or send data to a file or other device.
Arithmetic: Perform basic arithmetical operations like addition and multiplication.
Conditional Execution: Check for certain conditions and execute the appropriate sequence of statements.In mathematics and computer science, an algorithm 
is a self contained step by step set of operations to be performed. Algorithms exist that perform calculation, data processing, and automated reasoning.

The words 'algorithm' and 'algorism' come from the name al Khw rizm . Al Khw rizm  (Persian:          , c. 780 850) was a Persian mathematician, astronomer, 
geographer, and scholar.

An algorithm is an effective method that can be expressed within a finite amount of space and time[1] and in a well defined formal language[2] for calculating a function.[3] 
Starting from an initial state and initial input (perhaps empty),[4] the instructions describe a computation that, when executed, proceeds through a finite[5] number of well 
defined successive states, eventually producing "output"[6] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; 
some algorithms, known as randomized algorithms, incorporate random input.[7]

The concept of algorithm has existed for centuries, however a partial formalization of what would become the modern algorithm began with attempts to solve the 
Entscheidungsproblem (the "decision problem") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define "effective calculability"[8] or 
"effective method";[9] those formalizations included the G del,,Herbrand,Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, 
Emil Post's "Formulation 1" of 1936, and Alan Turing's Turing machines of 1936,7 and 1939. Giving a formal definition of algorithms, corresponding to the intuitive notion, 
remains a challenging problem.[10]
Informal definition[edit]
For a detailed presentation of the various points of view on the definition of "algorithm", see Algorithm characterizations.
An informal definition could be "a set of rules that precisely defines a sequence of operations."[11] which would include all computer programs, including programs that do 
not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.[12]

A prototypical example of an algorithm is Euclid's algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the 
flow chart above and as an example in a later section.

Boolos & Jeffrey (1974, 1999) offer an informal meaning of the word in the following quotation:

No human being can write fast enough, or long enough, or small enough  (  "smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on 
electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in 
the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be 
given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on 
symbols.[13]

An "enumerably infinite set" is one whose elements can be put into one to one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm 
implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can 
be an algebraic equation such as y = m + n,two arbitrary "input variables" m and n that produce an output y. But various authors' attempts to define the notion indicate
that the word implies much more than this, something on the order of (for the addition example):

Precise instructions (in language understood by "the computer")[14] for a fast, efficient, "good"[15] process that specifies the "moves" of "the computer" (machine or 
human, equipped with the necessary internally contained information and capabilities)[16] to find, decode, and then process arbitrary input integers/symbols m and n, 
symbols + and = ... and "effectively"[17] produce, in a "reasonable" time,[18] output integer y at a specified place and in a specified format.
The concept of algorithm is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set 
of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related with our customary physical dimension. 
From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage 
of the term.

Formalization[edit]
Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform 
(in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to 
be any sequence of operations that can be simulated by a Turing complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich 
(2000):

Minsky: "But we will also maintain, with Turing . . . that any procedure which could "naturally" be called effective, can in fact be realized by a (simple) machine. Although 
this may seem extreme, the arguments . . . in its favor are hard to refute".[19]

Gurevich: "...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine ... according to Savage 
[1987], an algorithm is a computational process defined by a Turing machine".[20]

Typically, when an algorithm is associated with processing information, data are read from an input source, written to an output device, and/or stored for further 
processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.

For some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any 
conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).

Because an algorithm is a precise list of precise steps, the order of computation is always critical to the functioning of the algorithm. Instructions are usually assumed to 
be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by flow of control.

So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to 
describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives 
from the intuition of "memory" as a scratchpad. There is an example below of such an assignment.

For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.

Expressing algorithms[edit]
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon charts, programming languages or control tables 
(processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. 
Pseudocode, flowcharts, drakon charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language 
statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define 
or document algorithms.

There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite state 
machine, state transition table and control table), as flowcharts and drakon charts (see more at state diagram), or as a form of rudimentary machine code or assembly 
code called "sets of quadruples" (see more at Turing machine).

Representations of algorithms can be classed into three accepted levels of Turing machine description:[21]

1 High level description
"...prose to describe an algorithm, ignoring the implementation details. At this level we do not need to mention how the machine manages its tape or head."
2 Implementation description
"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level we do not give details of states or transition 
function."
3 Formal description
Most detailed, "lowest level", gives the Turing machine's "state table".
For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.

Implementation[edit]

Logical NAND algorithm implemented electronically in 7400 chip
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network 
(for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.

Computer algorithms[edit]

Flowchart examples of the canonical B hm Jacopini structures: the SEQUENCE (rectangles descending the page), the WHILE DO and the IF THEN ELSE. The three 
structures are made of the primitive conditional GOTO (IF test=true THEN GOTO step xxx) (a diamond), the unconditional GOTO (rectangle), various assignment operators 
(rectangle), and HALT (rectangle). Nesting of these structures inside assignment blocks result in complex diagrams (cf Tausworthe 1977:100,114).
In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended "target" computer(s) to 
produce output from given input (perhaps null). An optimal algorithm, even running in old hardware, would produce faster results than a non optimal (higher time 
complexity) algorithm for the same purpose, running in more efficient hardware; that is why the algorithms, like computer hardware, are considered technology.

"Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:

Knuth: ". . .we want good algorithms in some loosely defined aesthetic sense. One criterion . . . is the length of time taken to perform the algorithm . . .. Other criteria are 
adaptability of the algorithm to computers, its simplicity and elegance, etc"[22]
Chaitin: " . . . a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does"[23]
Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant'",such a proof would solve the Halting problem (ibid).

Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set 
available to the programmer. Rogers observes that "It is . . . important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable 
by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".[24]

Unfortunately there may be a tradeoff between goodness (speed) and elegance (compactness),an elegant program may take more steps to complete a computation than
one less elegant. An example that uses Euclid's algorithm appears below.

Computers (and computors), models of computation: A computer (or human "computor"[25]) is a restricted type of machine, a "discrete deterministic mechanical 
device"[26] that blindly follows its instructions.[27] Melzak's and Lambek's primitive models[28] reduced this notion to four elements: (i) discrete, distinguishable locations, 
(ii) discrete, indistinguishable counters[29] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[30]

Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability".[31] Minsky's machine proceeds sequentially 
through its five (or six depending on how one counts) instructions unless either a conditional IF,THEN GOTO or an unconditional GOTO changes program flow out of 
sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution)[32] operations: ZERO (e.g. the contents of location replaced by 0: L   
0), SUCCESSOR (e.g. L   L+1), and DECREMENT (e.g. L   L   1).[33] Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows 
(as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, 
assignment/replacement/substitution, and HALT.[34]

Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and 
paper and work through an example".[35] But what about a simulation or execution of the real thing  The programmer must translate the algorithm into a language that the 
simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to 
take a square root. If they don't then for the algorithm to be effective it must provide a set of rules for extracting a square root.[36]

This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).

But what model should be used for the simulation  Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of 
the choice of a model remains. It is at this point that the notion of simulation enters".[37] When speed is being measured, the instruction set matters. For example, the 
subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" (division) instruction available rather than just 
subtraction (or worse: just Minsky's "decrement").

Structured programming, canonical structures: Per the Church,Turing thesis any algorithm can be computed by a model known to be Turing complete, and per Minsky's 
demonstrations Turing completeness requires only four instruction types,conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that
while "undisciplined" use of unconditional GOTOs and conditional IF THEN GOTOs can result in "spaghetti code" a programmer can write structured programs using these 
instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language".[38] Tausworthe augments the three 
B hm Jacopini canonical structures:[39] SEQUENCE, IF THEN ELSE, and WHILE DO, with two more: DO WHILE and CASE.[40] An additional benefit of a structured 
program is that it lends itself to proofs of correctness using mathematical induction.[41]

Canonical flowchart symbols[42]: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program of one). Like 
program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only 4: the directed arrow showing program 
flow, the rectangle (SEQUENCE, GOTO), the diamond (IF THEN ELSE), and the dot (OR tie). The B hm Jacopini canonical structures are made of these primitive 
shapes. Sub structures can "nest" in rectangles but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are 
shown in the diagram.

Examples[edit]
Further information: List of algorithms
Algorithm example[edit]

An animation of the quicksort algorithm sorting an array of randomized values. The red bars mark the pivot element; at the start of the animation, the element farthest to the 
right hand side is chosen as the pivot.
One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding solution requires looking at every number in the list. From this 
follows a simple algorithm, which can be stated in a high level description English prose, as:

High level description:

If there are no numbers in the set then there is no highest number.
Assume the first number in the set is the largest number in the set.
For each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.
When there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.
(Quasi )formal description: Written in prose but much closer to the high level language of a computer program, the following is the more formal coding of the algorithm in 
pseudocode or pidgin code:

Algorithm LargestNumber
  Input: A list of numbers L.
  Output: The largest number in the list L.
  if L.size = 0 return null
  largest   L[0]
  for each item in L, do
    if item > largest, then
      largest   item
  return largest
" " is a shorthand for "changes to". For instance, "largest   item" means that the value of largest changes to the value of item.
"return" terminates the algorithm and outputs the value that follows.
Euclid s algorithm[edit]
Further information: Euclid algorithm

The example diagram of Euclid's algorithm from T.L. Heath 1908 with more detail added. Euclid does not go beyond a third measuring and gives no numerical examples. 
Nicomachus gives the example of 49 and 21: "I subtract the less from the greater; 28 is left; then again I subtract from this the same 21 (for this is possible); 7 is left; I 
subtract this from 21, 14 is left; from which I again subtract 7 (for this is possible); 7 is left, but 7 cannot be subtracted from 7." Heath comments that, "The last phrase is 
curious, but the meaning of it is obvious enough, as also the meaning of the phrase about ending 'at one and the same number'."(Heath 1908:300).
Euclid s algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his Elements.
[43] Euclid poses the problem: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed 
of units": a counting number, a positive integer not including 0. And to "measure" is to place a shorter measuring length s successively (q times) along longer length l until 
the remaining portion r is less than the shorter length s.[44] In modern words, remainder r = l   q*s, q being the quotient, or remainder r is the "modulus", the integer 
fractional part left over after the division.[45]

For Euclid s method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be 0, AND (ii) the subtraction must be  proper , a test must 
guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields 0).

Euclid's original proof adds a third: the two lengths are not prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two 
numbers' common measure is in fact the greatest.[46] While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another it yields the 
number "1" for their common measure. So to be precise the following is really Nicomachus' algorithm.


A graphical expression of Euclid's algorithm to find the greatest common divisor for 1599 and 650.
 1599 = 650*2 + 299
 650 = 299*2 + 52
 299 = 52*5 + 39
 52 = 39*1 + 13
 39 = 13*3 + 0

Computer language for Euclid's algorithm[edit]
Only a few instruction types are required to execute Euclid's algorithm,some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and
subtraction.

A location is symbolized by upper case letter(s), e.g. S, A, etc.
The varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might 
contain the number l = 3009.
An inelegant program for Euclid's algorithm[edit]

"Inelegant" is a translation of Knuth's version of the algorithm with a subtraction based remainder loop replacing his use of division (or a "modulus" instruction). Derived 
from Knuth 1973:2,4. Depending on the two numbers "Inelegant" may compute the g.c.d. in fewer steps than "Elegant".
The following algorithm is framed as Knuth's 4 step version of Euclid's and Nicomachus', but rather than using division to find the remainder it uses successive 
subtractions of the shorter length s from the remaining length r until r is less than s. The high level description, shown in boldface, is adapted from Knuth 1973:2,4:

INPUT:

1 [Into two locations L and S put the numbers l and s that represent the two lengths]:
  INPUT L, S
2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:
  R   L
E0: [Ensure r   s.]

3 [Ensure the smaller of the two numbers is in S and the larger in R]: 
  IF R > S THEN 
    the contents of L is the larger number so skip over the exchange steps 4, 5 and 6: 
    GOTO step 6 
  ELSE 
    swap the contents of R and S.
4   L   R (this first step is redundant, but is useful for later discussion).
5   R   S
6   S   L
E1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in 
R.

7 IF S > R THEN 
    done measuring so 
    GOTO 10 
  ELSE 
    measure again,
8   R   R   S
9   [Remainder loop]: 
    GOTO 7.
E2: [Is the remainder 0 ]: EITHER (i) the last measure was exact and the remainder in R is 0 program can halt, OR (ii) the algorithm must continue: the last measure left a 
remainder in R less than measuring number in S.

10 IF R = 0 THEN 
     done so 
     GOTO step 15 
   ELSE 
     CONTINUE TO step 11,
E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s:; L serves as a temporary location.

11  L   R
12  R   S
13  S   L
14  [Repeat the measuring process]: 
    GOTO 7
OUTPUT:

15 [Done. S contains the greatest common divisor]: 
   PRINT S
DONE:

16 HALT, END, STOP.
An elegant program for Euclid's algorithm[edit]
The following version of Euclid's algorithm requires only 6 core instructions to do what 13 are required to do by "Inelegant"; worse, "Inelegant" requires more types of 
instructions. The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language the steps are numbered, and the instruction LET [] = [] 
is the assignment instruction symbolized by  .

  5 REM Euclid's algorithm for greatest common divisor
  6 PRINT "Type two integers greater than 0"
  10 INPUT A,B
  20 IF B=0 THEN GOTO 80
  30 IF A > B THEN GOTO 60
  40 LET B=B A
  50 GOTO 20
  60 LET A=A B
  70 GOTO 20
  80 PRINT A
  90 END
How "Elegant" works: In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co loops", an A > B loop that computes A   A   B, and a B   A loop 
that computes B   B   A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend   Subtrahend), the minuend 
can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the "sense" of the subtraction reverses.

Testing the Euclid algorithms[edit]
Does an algorithm do what its author wants it to do  A few test cases usually suffice to confirm core functionality. One source[47] uses 3009 and 884. Knuth suggested 
40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.

But exceptional cases must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S  Ditto for "Elegant": B > A, A > B, A = B  (Yes to all). 
What happens when one number is zero, both numbers are zero  ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if 
negative numbers are entered  Fractional numbers  If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive 
integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable 
failure due to exceptions is the Ariane V rocket failure.

Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's 
algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".[48] Tausworthe proposes that a measure of the complexity of a program 
be the length of its correctness proof.[49]

Measuring and improving the Euclid algorithms[edit]
Elegance (compactness) versus goodness (speed): With only 6 core instructions, "Elegant" is the clear winner compared to "Inelegant" at 13 instructions. However, 
"Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis[50] indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, 
whereas "Inelegant" only does one. As the algorithm (usually) requires many loop throughs, on average much time is wasted doing a "B = 0 " test that is needed only after 
the remainder is computed.

Can the algorithms be improved : Once the programmer judges a program "fit" and "effective",that is, it computes the function intended by its author,then the question
becomes, can it be improved 

The compactness of "Inelegant" can be improved by the elimination of 5 steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized 
algorithm;[51] rather, it can only be done heuristically, i.e. by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of 
inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps together with steps 
2 and 3 can be eliminated. This reduces the number of core instructions from 13 to 8, which makes it "more elegant" than "Elegant" at 9 steps.

The speed of "Elegant" can be improved by moving the B=0  test outside of the two subtraction loops. This change calls for the addition of 3 instructions (B=0 , A=0 , 
GOTO). Now "Elegant" computes the example numbers faster; whether for any given A, B and R, S this is always the case would require a detailed analysis.

Algorithmic analysis[edit]
Main article: Analysis of algorithms
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed 
for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O 
notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input 
list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.

Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search 
algorithm usually outperforms a brute force sequential search when used for table lookups on sorted lists.

Formal versus empirical[edit]
Main articles: Empirical algorithmics, Profiling (computer programming) and Program optimization
The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or 
implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the 
specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most 
algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution 
of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast 
interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.

Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential 
improvements to an algorithm after program optimization.

Execution efficiency[edit]
Main article: Algorithmic efficiency
To illustrate the potential improvements possible even in well established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of 
image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[52] In general, speed improvements depend on special 
properties of the problem, which are very common in practical applications.[53] Speedups of this magnitude enable computing devices that make extensive use of image 
processing (like digital cameras and medical equipment) to consume less power.

Classification[edit]
There are various ways to classify algorithms, each with its own merits.

By implementation[edit]
One way to classify algorithms is by implementation means.

Recursion
A recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method 
common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given 
problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every 
recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.
Logical
An algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control.[54] The logic component expresses the axioms 
that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic 
programming paradigm. In pure logic programming languages the control component is fixed and algorithms are specified by supplying only the logic component. The 
appeal of this approach is the elegant semantics: a change in the axioms has a well defined change in the algorithm.
Serial, parallel or distributed
Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial 
computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms take 
advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines 
connected with a network. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. 
The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some 
sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no 
parallel algorithms, and are called inherently serial problems.
Deterministic or non deterministic
Deterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non deterministic algorithms solve problems via guessing although 
typical guesses are made more accurate through the use of heuristics.
Exact or approximate
While many algorithms reach an exact solution, approximation algorithms seek an approximation that is close to the true solution. Approximation may use either a 
deterministic or a random strategy. Such algorithms have practical value for many hard problems.
Quantum algorithm
They run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of 
quantum computation such as quantum superposition or quantum entanglement.
By design paradigm[edit]
Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, 
each of these categories include many different types of algorithms. Some common paradigms are:

Brute force or exhaustive search
This is the naive method of trying every possible solution to see which is best.[55]
Divide and conquer
A divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances 
are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into 
segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease and 
conquer algorithm, that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into 
multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of decrease and conquer algorithm is the binary 
search algorithm.
Search and enumeration
Many problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for 
such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.
Randomized algorithm
Such algorithms make some choices randomly (or pseudo randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions 
can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness.[56] Whether 
randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are 
two large classes of such algorithms:
Monte Carlo algorithms return a correct answer with high probability. E.g. RP is the subclass of these that run in polynomial time.
Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.
Reduction of complexity
This technique involves solving a difficult problem by transforming it into a better known problem for which we have (hopefully) asymptotically optimal algorithms. The goal 
is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an 
unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known 
as transform and conquer.
Optimization problems[edit]
For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories 
described above as well as into one of the following:

Linear programming
When searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in 
producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm.[57] Problems that can be solved 
with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer 
then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are 
superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, 
depending on the difficulty of the problem.
Dynamic programming
When a problem shows optimal substructures , meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems , and overlapping
subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing 
solutions that have already been computed. For example, Floyd,Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using 
the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and 
divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference 
between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no 
repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of 
subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.
The greedy method
A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such 
algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they 
can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular 
use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are 
greedy algorithms that can solve this optimization problem.
The heuristic method
In optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These 
algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. 
Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated 
annealing, and genetic algorithms. Some of them, like simulated annealing, are non deterministic algorithms while others, like tabu search, are deterministic. When a 
bound on the error of the non optimal solution is known, the algorithm is further categorized as an approximation algorithm.
By field of study[edit]
See also: List of algorithms
Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search 
algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, 
medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.

Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic 
programming was invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.

By complexity[edit]
See also: Complexity class and Parameterized complexity
Algorithms can be classified by the amount of time they need to complete compared to their input size. There is a wide variety: some algorithms complete in linear time 
relative to input size, some do so in an exponential amount of time or even worse, and some never halt. Additionally, some problems may have multiple algorithms of 
differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. 
Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best 
possible algorithms for them.

Burgin (2005, p. 24) uses a generalized definition of algorithms that relaxes the common requirement that the output of the algorithm that computes a function must be 
determined after a finite number of steps. He defines a super recursive class of algorithms as "a class of algorithms in which it is possible to compute functions not 
computable by any Turing machine" (Burgin 2005, p. 107). This is closely related to the study of methods of hypercomputation.

Continuous algorithms[edit]
The adjective "continuous" when applied to the word "algorithm" can mean:

An algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations,such algorithms are studied in
numerical analysis; or
An algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.[58]
Legal issues[edit]
See also: Software patents for a general overview of the patentability of software, including computer implemented algorithms.
Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals 
does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However, practical applications of algorithms are 
sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. 
The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW 
patent.

Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).

Etymology[edit]
The words 'algorithm' and 'algorism' come from the name al Khw rizm . Al Khw rizm  (Persian:          , c. 780 850) was a Persian mathematician, astronomer, 
geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarezm', a region that was part of Greater Iran and is now in 
Uzbekistan.[59][60] About 825, he wrote a treatise in the Arabic language, which was translated into Latin in the 12th century under the title Algoritmi de numero Indorum. 
This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al Khwarizmi's name.[61] Al Khwarizmi was the most widely 
read mathematician in Europe in the late Middle Ages, primarily through his other book, the Algebra.[62] In late medieval Latin, algorismus, English 'algorism', the 
corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word         'number' (cf. 'arithmetic'), the Latin 
word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.
[63]

History: Development of the notion of "algorithm"[edit]
Ancient Near East[edit]
Algorithms were used in ancient Greece. Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus,[64][65]:Ch 9.2 
and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).[65]:Ch 9.1 Babylonian clay tablets describe and employ algorithmic procedures 
to compute the time and place of significant astronomical events.[66]

Discrete and distinguishable symbols[edit]
Tally marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks, or making 
discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16,41). Tally 
marks appear prominently in unary numeral system arithmetic used in Turing machine and Post,Turing machine computations.

Manipulation of symbols as "place holders" for numbers: algebra[edit]
The work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al Khwarizmi (from whose name 
the terms "algorism" and "algorithm" are derived), and Western European mathematicians culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):

A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner 
that ordinary algebra specifies the rules for manipulating numbers.[67]

Mechanical contrivances with discrete states[edit]
The clock: Bolter credits the invention of the weight driven clock as "The key invention [of Europe in the Middle Ages]", in particular the verge escapement[68] that 
provides us with the tick and tock of a mechanical clock. "The accurate automatic machine"[69] led immediately to "mechanical automata" beginning in the 13th century 
and finally to "computational machines",the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid 19th century.[70] Lovelace is
credited with the first creation of an algorithm intended for processing on a computer   Babbage's analytical engine, the first device considered a real Turing complete 
computer instead of just a calculator   and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not 
be realized until decades after her lifetime.

Logical machines 1870,Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form
similar to what are now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class 
of the [logical] combinations can be picked out mechanically . . . More recently however I have reduced the system to a completely mechanical form, and have thus 
embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and 
"at the foot are 21 keys like those of a piano [etc] . . .". With this machine he could analyze a "syllogism or any other simple logical argument".[71]

This machine he displayed in 1870 before the Fellows of the Royal Society.[72] Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye 
to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances 
at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented 
"a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be 
described. I prefer to call it merely a logical diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".
[73]

Jacquard loom, Hollerith punch cards, telegraphy and telephony,the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor
to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers.[74] By the mid 
19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a 
common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 
1910) with its punched paper use of Baudot code on tape.

Telephone switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he 
worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... 
When the tinkering was over, Stibitz had constructed a binary adding device".[75]

Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):

It was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had 
envisioned."[76]
Mathematics during the 19th century up to the mid 20th century[edit]
Symbols and rules: In rapid succession the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888,1889) reduced arithmetic to a 
sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was "the first attempt at an axiomatization of 
mathematics in a symbolic language".[77]

But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a " 'formula language', that is a 
lingua characterica, a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are 
manipulated according to definite rules".[78] The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia 
Mathematica (1910,1913).

The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular the Burali Forti paradox (1897), the Russell paradox (1902,03), 
and the Richard Paradox.[79] The resultant considerations led to Kurt G del's paper (1931),he specifically cites the paradox of the liar,that completely reduces rules of
recursion to numbers.

Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an 
"effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, 
Stephen Kleene and J.B. Rosser's   calculus[80] a finely honed definition of "general recursion" from the work of G del acting on suggestions of Jacques Herbrand (cf. 
G del's Princeton lectures of 1934) and subsequent simplifications by Kleene.[81] Church's proof[82] that the Entscheidungsproblem was unsolvable, Emil Post's definition 
of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a 
paper or observe the paper and make a yes no decision about the next instruction.[83] Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his 
"a  [automatic ] machine"[84],in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine".[85] S. C.
Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I",[86] and a few years later Kleene's renaming his Thesis "Church's Thesis"[87] and proposing 
"Turing's Thesis".[88]

Emil Post (1936) and Alan Turing (1936,37, 1939)[edit]
Here is a remarkable coincidence of two men not knowing each other but describing a process of men as computers working on computations,and they yield virtually
identical definitions.

Emil Post (1936) described the actions of a "computer" (human being) as follows:

"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.
His symbol space would be

"a two way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but 
one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.
"One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with 
a stroke. Likewise the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes....
"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to 
the direction of type (C ) [i.e., STOP]".[89] See more at Post,Turing machine

Alan Turing's statue at Bletchley Park.
Alan Turing's work[90] preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a 
typewriter like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter; and he could well have begun by 
asking himself what was meant by calling a typewriter 'mechanical'".[91] Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we 
might conjecture that all were influences.

Turing,his model of computation is now called a Turing machine,begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of
basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.[92]

"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book....I assume then that 
the computation is carried out on one dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is 
finite....
"The behaviour of the computer at any moment is determined by the symbols which he is observing, and his "state of mind" at that moment. We may suppose that there is 
a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We 
will also suppose that the number of states of mind which need be taken into account is finite...
"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further 
divided."[93]
Turing's reduction yields the following:

"The simple operations must therefore include:
"(a) Changes of the symbol on one of the observed squares
"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.
"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must therefore be taken to be one of the following:

"(A) A possible change (a) of symbol together with a possible change of state of mind.
"(B) A possible change (b) of observed squares, together with a possible change of state of mind"
"We may now construct a machine to do the work of this computer."[93]
A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:

"A function is said to be "effectively calculable" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, 
it is nevertheless desirable to have some more definite, mathematical expressible definition . . . [he discusses the history of the definition pretty much as presented above 
with respect to G del, Herbrand, Kleene, Church, Turing and Post] . . . We may take this statement literally, understanding by a purely mechanical process one which 
could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of 
these ideas leads to the author's definition of a computable function, and to an identification of computability   with effective calculability . . . .
"  We shall use the expression "computable function" to mean a function calculable by a machine, and we let "effectively calculable" refer to the intuitive idea without 
particular identification with any one of these definitions".[94]
J. B. Rosser (1939) and S. C. Kleene (1943)[edit]
J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):

"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite 
number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest 
of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then 
solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't 
matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one." (Rosser 1939:225,6)
Rosser's footnote #5 references the work of (1) Church and Kleene and their definition of   definability, in particular Church's use of it in his An Unsolvable Problem of 
Elementary Number Theory (1936); (2) Herbrand and G del and their use of recursion in particular G del's use in his famous paper On Formally Undecidable 
Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936,7) in their mechanism models of computation.

Stephen C. Kleene defined as his now famous "Thesis I" known as the Church Turing thesis. But he did this in the following context (boldface in original):

"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent 
variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, "yes" or "no," to the question, "is the 
predicate value true "" (Kleene 1943:273)
History after 1950[edit]
A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on going because of issues surrounding, in particular, 
foundations of mathematics (especially the Church,,Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm 
characterizations.
Repetition: Perform some action repeatedly, usually with some variation.
Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run time 
conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Programmers[edit]
Main article: Programmer
See also: Software developer and Software engineer
Computer programmers are those who write computer software. Their jobs usually involve:
Coding
Debugging
Documentation
Integration
Maintenance
Requirements analysis
Software architecture
Software testing
Specification
Raspberry Pi
From Wikipedia, the free encyclopedia
"RPi" redirects here. For other uses, see RPI.

Raspberry Pi logo
Raspberry Pi 1
Raspberry Pi B+ top.jpg
Raspberry Pi 1 model B+
Release date	February 2012; 4 years ago
Introductory price	US$25 (model A, B+[1]), US$20 (model A+), US$35 (RPi 1 model B, RPi 2 model B, RPi 3), US$30 (CM)
Operating system	Linux (e.g. Raspbian), RISC OS, FreeBSD, NetBSD, Plan 9, Inferno, AROS
CPU	700 MHz single-core ARM1176JZF-S (model A, A+, B, B+, CM)[2]
Memory	256 MB[3] (model A, A+ rev 1, B rev 1)
512 MB (model A+ rev 2,[4] B rev 2, B+, CM)
Storage	SDHC slot (model A and B), MicroSDHC slot (model A+ and B+), 4 GB eMMC IC chip (model CM)
Graphics	Broadcom VideoCore IV[2]
Power	1.5 W (model A), 1.0 W (model A+), 3.5 W (model B), 3.0 W (model B+) or 0.8 W (model Zero)
Raspberry Pi 2
Raspberry Pi 2 Model B v1.1 top new (bg cut out).jpg
Raspberry Pi 2 model B
Release date	February 2015; 1 year ago
Introductory price	US$35
Operating system	Same as for Raspberry Pi 1 plus Windows 10 IoT Core[5] and additional distributions of Linux such as Raspbian
CPU	900 MHz quad-core ARM Cortex-A7
Memory	1 GB RAM
Storage	MicroSDHC slot
Graphics	Broadcom VideoCore IV
Power	4.0 W
Raspberry Pi 3
Raspberry Pi 3 Model B.png
Raspberry Pi 3 model B
Release date	29 February 2016; 2 months ago
Introductory price	US$35
Operating system	Raspbian
Ubuntu MATE
Snappy Ubuntu Core
Windows 10 IoT Core[5]
RISC OS
Debian
Arch Linux ARM
CPU	1200 MHz quad-core ARM Cortex-A53
Memory	1 GB RAM
Storage	MicroSDHC slot
Graphics	Broadcom VideoCore IV at higher clock frequencies than previous that run at 250 MHz
Power	4.0 W
Raspberry Pi Zero
Pi Zero.png
Raspberry Pi Zero
Release date	November 2015; 6 months ago
Introductory price	US$5
Operating system	Linux (Raspbian[6]) or the same as for Raspberry Pi 1
CPU	1000 MHz single-core ARM1176JZF-S
Memory	512 MB RAM
Storage	MicroSDHC slot
Power	0.8 W
The Raspberry Pi is a series of credit cardsized single-board computers developed in the United Kingdom by the Raspberry Pi Foundation with the intent to promote the teaching of basic computer science in schools and developing countries.[7][8][9] The original Raspberry Pi and Raspberry Pi 2 are manufactured in several board configurations through licensed manufacturing agreements with Newark element14 (Premier Farnell), RS Components and Egoman.[10] The hardware is the same across all manufacturers.

Several generations of Raspberry Pi's have been released. The first generation (Pi 1) was released in February 2012 in basic model A and a higher specification model B. A+ and B+ models were released a year later. Raspberry Pi 2 model B was released in February 2015 and Raspberry Pi 3 model B in February 2016. These boards are priced between US$20 and US$35. A cut down compute model was released in April 2014 and a Pi Zero with smaller footprint and limited IO (GPIO) capabilities released in November 2015 for US$5.

All models feature a Broadcom system on a chip (SOC) which include an ARM compatible CPU and an on chip graphics processing unit GPU (a VideoCore IV). CPU speed range from 700 MHz to 1.2 GHz for the Pi 3 and on board memory range from 256 MB to 1 GB RAM. Secure Digital SD cards are used to store the operating system and program memory in either the SDHC or MicroSDHC sizes. Most boards have between one and four USB slots, HDMI and composite video output, and a 3.5 mm phono jack for audio. Lower level output is provided by a number of GPIO pins which support common protocols like I2C. Some models have an RJ45 Ethernet port and the Pi 3 has on board WiFi 802.11n and Bluetooth.

The Foundation provides Debian and Arch Linux ARM distributions for download,[11] and promotes Python as the main programming language, with support for BBC BASIC[12] (via the RISC OS image or the Brandy Basic clone for Linux),[13] C, C++, Java,[14] Perl, Ruby,[15] Squeak Smalltalk and more also available.

In February 2016, the Raspberry Pi Foundation announced that they had sold eight million devices, making it the best selling UK personal computer, ahead of the Amstrad PCW.[16][17]

Contents  [hide] 
1	Hardware
1.1	Processor
1.1.1	Performance of first generation models
1.1.2	Overclocking
1.2	RAM
1.3	Networking
1.4	Peripherals
1.5	Video
1.6	Real-time clock
1.7	Specifications
1.8	Connectors
1.9	General purpose input-output (GPIO) connector
1.10	Accessories
2	Software
2.1	Operating systems
2.2	Driver APIs
2.3	Third party application software
2.4	Software development tools
2.5	Tracking Raspberry Pi online on a global map
3	Reception and use
3.1	Community
3.2	Use in education
4	Astro Pi
5	Reviews
6	History
6.1	Pre-launch
6.2	Launch
6.3	Post-launch
7	See also
8	References
9	Further reading
10	External links
Hardware[edit]
The Raspberry Pi hardware has evolved through several versions that feature variations in memory capacity and peripheral-device support.

Raspberrypi block function v01.svg
This block diagram depicts models A, B, A+, and B+. Model A, A+, and Zero lack the Ethernet and USB hub components. The Ethernet adapter is connected to an additional USB port. In model A and A+ the USB port is connected directly to the SoC. On model B+ and later models the USB/Ethernet chip contains a five-point USB hub, of which four ports are available, while model B only provides two. On the model Zero, the USB port is also connected directly to the SoC, but it uses a micro USB (OTG) port.

Processor[edit]
The system on a chip (SoC) used in the first generation Raspberry Pi is somewhat equivalent to the chip used in older smartphones (such as iPhone, 3G, 3GS). The Raspberry Pi is based on the Broadcom BCM2835 SoC,[2] which includes an 700 MHz ARM1176JZF-S processor, VideoCore IV graphics processing unit (GPU),[18] and RAM. It has a Level 1 cache of 16 KB and a Level 2 cache of 128 KB. The Level 2 cache is used primarily by the GPU. The SoC is stacked underneath the RAM chip, so only its edge is visible.

The Raspberry Pi 2 uses a Broadcom BCM2836 SoC with a 900 MHz 32-bit quad-core ARM Cortex-A7 processor, with 256 KB shared L2 cache.

The Raspberry Pi 3 uses a Broadcom BCM2837 SoC with a 1.2 GHz 64-bit quad-core ARM Cortex-A53 processor, with 512 KB shared L2 cache.[19]

Performance of first generation models[edit]
While operating at 700 MHz by default, the first generation Raspberry Pi provided a real-world performance roughly equivalent to 0.041 GFLOPS.[20][21] On the CPU level the performance is similar to a 300 MHz Pentium II of 199799. The GPU provides 1 Gpixel/s or 1.5 Gtexel/s of graphics processing or 24 GFLOPS of general purpose computing performance. The graphics capabilities of the Raspberry Pi are roughly equivalent to the level of performance of the Xbox of 2001.

The LINPACK single node compute benchmark results in a mean single precision performance of 0.065 GFLOPS and a mean double precision performance of 0.041 GFLOPS for one Raspberry Pi Model-B board.[22] A cluster of 64 Raspberry Pi Model-B computers, labeled "Iridis-pi", achieved a LINPACK HPL suite result of 1.14 GFLOPS (n=10240) at 216 watts for c. US$4,000.[22]

Raspberry Pi 2 is based on Broadcom BCM2836 SoC, which includes a quad-core Cortex-A7 CPU running at 900 MHz and 1 GB RAM. It is described as 46 times more powerful than its predecessor. The GPU is identical to the original.

Overclocking[edit]
The first generation Raspberry Pi chip operated at 700 MHz by default, and did not become hot enough to need a heat sink or special cooling unless the chip was overclocked. The second generation runs at 900 MHz by default; it also does not become hot enough to need a heatsink or special cooling, although overclocking may heat up the SoC more than usual.

Most Raspberry Pi chips could be overclocked to 800 MHz and some even higher to 1000 MHz. There are reports the second generation can be similarly overclocked, in extreme cases, even to 1500 MHz (discarding all safety features and over voltage limitations). In the Raspbian Linux distro the overclocking options on boot can be done by a software command running "sudo raspi-config" without voiding the warranty.[23] In those cases the Pi automatically shuts the overclocking down in case the chip reaches 85 ?C (185 ?F), but it is possible to overrule automatic over voltage and overclocking settings (voiding the warranty). In that case, an appropriately sized heatsink is needed to keep the chip from heating up far above 85 ?C.

Newer versions of the firmware contain the option to choose between five overclock ("turbo") presets that when turned on try to get the most performance out of the SoC without impairing the lifetime of the Pi. This is done by monitoring the core temperature of the chip, and the CPU load, and dynamically adjusting clock speeds and the core voltage. When the demand is low on the CPU, or it is running too hot, the performance is throttled, but if the CPU has much to do, and the chip's temperature is acceptable, performance is temporarily increased, with clock speeds of up to 1 GHz, depending on the individual board, and on which of the turbo settings is used. The seven settings are:

none; 700 MHz ARM, 250 MHz core, 400 MHz SDRAM, 0 overvolt,
modest; 800 MHz ARM, 250 MHz core, 400 MHz SDRAM, 0 overvolt,
medium; 900 MHz ARM, 250 MHz core, 450 MHz SDRAM, 2 overvolt,
high; 950 MHz ARM, 250 MHz core, 450 MHz SDRAM, 6 overvolt,
turbo; 1000 MHz ARM, 500 MHz core, 600 MHz SDRAM, 6 overvolt,
Pi2; 1000 MHz ARM, 500 MHz core, 500 MHz SDRAM, 2 overvolt,
Pi3; 1100 MHz ARM, 550 MHz core, 500 MHz SDRAM, 6 overvolt. In system information CPU speed will appear as 1200 MHz. When in idle speed lowers to 600 MHz.
[24][25]

In the highest (turbo) preset the SDRAM clock was originally 500 MHz, but this was later changed to 600 MHz because 500 MHz sometimes causes SD card corruption. Simultaneously in high mode the core clock speed was lowered from 450 to 250 MHz, and in medium mode from 333 to 250 MHz.

The Raspberry Pi Zero runs at 1 GHz.

RAM[edit]
On the older beta model B boards, 128 MB was allocated by default to the GPU, leaving 128 MB for the CPU.[26] On the first 256 MB release model B (and model A), three different splits were possible. The default split was 192 MB (RAM for CPU), which should be sufficient for standalone 1080p video decoding, or for simple 3D, but probably not for both together. 224 MB was for Linux only, with only a 1080p framebuffer, and was likely to fail for any video or 3D. 128 MB was for heavy 3D, possibly also with video decoding (e.g. XBMC).[27] Comparatively the Nokia 701 uses 128 MB for the Broadcom VideoCore IV.[28] For the new model B with 512 MB RAM initially there were new standard memory split files released( arm256_start.elf, arm384_start.elf, arm496_start.elf) for 256 MB, 384 MB and 496 MB CPU RAM (and 256 MB, 128 MB and 16 MB video RAM). But a week or so later the RPF released a new version of start.elf that could read a new entry in config.txt (gpu_mem=xx) and could dynamically assign an amount of RAM (from 16 to 256 MB in 8 MB steps) to the GPU, so the older method of memory splits became obsolete, and a single start.elf worked the same for 256 and 512 MB Raspberry Pis.[29]

The Raspberry Pi 2 and the Raspberry Pi 3 have 1 GB of RAM. The Raspberry PI Zero has 512 MB of RAM.

Networking[edit]
Though the model A and A+ and Zero do not have an 8P8C ("RJ45") Ethernet port, they can be connected to a network using an external user-supplied USB Ethernet or Wi-Fi adapter. On the model B and B+ the Ethernet port is provided by a built-in USB Ethernet adapter using the SMSC LAN9514 chip.[30] The Raspberry Pi 3 is equipped with 2.4 GHz WiFi 802.11n (600 Mbit/s) and Bluetooth 4.1 (24 Mbit/s) in addition to the 10/100 Ethernet port.

Peripherals[edit]
The Raspberry Pi may be operated with any generic USB computer keyboard and mouse.[31]

Video[edit]
The video controller is capable of standard modern TV resolutions, such as HD and Full HD, and higher or lower monitor resolutions and older standard CRT TV resolutions. As shipped (i.e. without custom overclocking) it is capable of the following: 640?350 EGA; 640?480 VGA; 800?600 SVGA; 1024?768 XGA; 1280?720 720p HDTV; 1280?768 WXGA variant; 1280?800 WXGA variant; 1280?1024 SXGA; 1366?768 WXGA variant; 1400?1050 SXGA+; 1600?1200 UXGA; 1680?1050 WXGA+; 1920?1080 1080p HDTV; 1920?1200 WUXGA.[32]

Higher resolutions, such as, up to 2048?1152, may work[33][34] or even 3840?2160 at 15 Hz (too low a framerate for convincing video).[35] Note also that allowing the highest resolutions does not imply that the GPU can decode video formats at those; in fact, the Pis are known to not work reliably for H.265 (at those high resolution, at least), commonly used for very high resolutions (most formats, commonly used, up to full HD, do work).

Although the new Raspberry Pi 3 does not have H.265 decoding hardware, the OSMC project has this to say on February launch date on the possibility of decoding more videos encoded in that way using software, due to the more advanced CPU architecture:

The new BCM2837 is based on 64-bit ARMv8 architecture is backwards compatible with the Raspberry Pi 2 as well as the original. While the new CPU is 64-bit, the Pi retains the original VideoCore IV GPU which has a 32-bit design. It will be a few months before work is done to establish 64-bit pointer interfacing from the kernel and userland on the ARM to the 32-bit GPU. As such, for the time being, we will be offering a single Raspberry Pi image for Raspberry Pi 2 and the new Raspberry Pi 3. Only when 64-bit support is ready, and beneficial to OSMC users, will we offer a separate image. The new quad core CPU will bring smoother GUI performance. There have also been recent improvements to H265 decoding. While not hardware accelerated on the Raspberry Pi, the new CPU will enable more H265 content to be played back on the Raspberry Pi than before.

?Raspberry Pi 3 announced with OSMC support[36]
The Pi 3's GPU has higher clock frequencies 300 MHz and 400 MHz of different parts that in previous versions ran at 250 MHz.[37][38]

The Pis can also generate 576i and 480i composite video signals, as used on old-style (CRT) TV screens, (through non-standard connectors, different kind depending on models) for PAL-BGHID, PAL-M, PAL-N, NTSC and NTSC-J.[39]

Real-time clock[edit]
The Raspberry Pi does not come with a real-time clock, which means it cannot keep track of the time of day while it is not powered on.

As alternatives, a program running on the Pi can get the time from a network time server or user input at boot time.

A real-time clock (such as the DS1307, which is fully binary coded) with battery backup may be added (often via the I?C interface).

Specifications[edit]
Raspberry Pi 1
Model A	Raspberry Pi 1
Model A+	Raspberry Pi 1
Model B	Raspberry Pi 1
Model B+	Raspberry Pi 2
Model B	Raspberry Pi 3
Model B	Compute Module*	Raspberry Pi Zero
Release date	February 2013	November 2014[40]	AprilJune 2012	July 2014[41]	February 2015[42]	February 2016[43]	April 2014[44]	November 2015[45]
Target price	US$25	US$20[46]	US$35[47]	US$25	US$35	US$35	US$30 (in batches of 100)[44]	US$5[45]
SoC	Broadcom BCM2835[2]	Broadcom BCM2836	Broadcom BCM2837	Broadcom BCM2835[44]
CPU	700 MHz single-core ARM1176JZF-S[2]	900 MHz 32-bit quad-core ARM Cortex-A7	1.2 GHz 64-bit quad-core ARM Cortex-A53	700 MHz single-core ARM1176JZF-S	1 GHz ARM1176JZF-S single-core[45]
GPU	Broadcom VideoCore IV @ 250 MHz (BCM2837: 3D part of GPU @ 300 MHz, video part of GPU @ 400 MHz)[48][49]
OpenGL ES 2.0 (BCM2835, BCM2836: 24 GFLOPS / BCM2837: 28.8 GFLOPS)
MPEG-2 and VC-1 (with license),[50] 1080p30 H.264/MPEG-4 AVC high-profile decoder and encoder[2] (BCM2837: 1080p60)
Memory (SDRAM)	256 MB (shared with GPU)	512 MB (shared with GPU) as of 4 May 2016. Older boards had 256 MB (shared with GPU)[4]	1 GB (shared with GPU)	512 MB (shared with GPU)
USB 2.0 ports[31]	1 (direct from BCM2835 chip)	2 (via the on-board 3-port USB hub)[51]	4 (via the on-board 5-port USB hub)[41][30]	1 (direct from BCM2835 chip)	1 Micro-USB (direct from BCM2835 chip)
Video input	15-pin MIPI camera interface (CSI) connector, used with the Raspberry Pi camera or Raspberry Pi NoIR camera[52]	2? MIPI camera interface (CSI)[44][53][54]	
Video outputs	HDMI (rev 1.3 & 1.4),[32][55] composite video (RCA jack)	HDMI (rev 1.3 & 1.4), composite video (3.5 mm TRRS jack)	HDMI (rev 1.3 & 1.4), composite video (RCA jack)	HDMI (rev 1.3 & 1.4), composite video (3.5 mm TRRS jack)	HDMI, 2? MIPI display interface (DSI) for raw LCD panels,[44][54][56][57] composite video[53][58]	Mini-HDMI, 1080p60,[45] composite video via GPIO[59]
Audio inputs	As of revision 2 boards via I?S[60]
Audio outputs	Analog via 3.5 mm phone jack; digital via HDMI and, as of revision 2 boards, I?S	Analog, HDMI, I?S	Mini-HDMI, stereo audio through PWM on GPIO
On-board storage[31]	SD / MMC / SDIO card slot (3.3 V with card power only)	MicroSDHC slot[41]	SD / MMC / SDIO card slot	MicroSDHC slot	4 GB eMMC flash memory chip;[44]	MicroSDHC
On-board network[31]	None[61]	10/100 Mbit/s Ethernet (8P8C) USB adapter on the USB hub[51]	10/100 Mbit/s Ethernet
802.11n wireless
Bluetooth 4.1		None
Low-level peripherals	8? GPIO[62] plus the following, which can also be used as GPIO: UART, I?C bus, SPI bus with two chip selects, I?S audio[63] +3.3 V, +5 V, ground[48][64]
17? GPIO plus the same specific functions, and HAT ID bus	8? GPIO plus the following, which can also be used as GPIO: UART, I?C bus, SPI bus with two chip selects, I?S audio +3.3 V, +5 V, ground.
An additional 4? GPIO are available on the P5 pad if the user is willing to make solder connections

17? GPIO plus the same specific functions, and HAT ID bus	46? GPIO, some of which can be used for specific functions including I?C, SPI, UART, PCM, PWM[65]	40? GPIO ("unpopulated header")[45]
Power ratings	300 mA (1.5 W)[66]	200 mA (1 W)[67]	700 mA (3.5 W)	600 mA (3.0 W)[41]	800 mA[68] (4.0 W)[69]	200 mA (1 W)	~160 mA[45] (0.8 W)
Power source	5 V via MicroUSB or GPIO header
Size	85.60 mm ? 56.5 mm (3.370 in ? 2.224 in), not including protruding connectors	65 mm ? 56.5 mm ? 10 mm (2.56 in ? 2.22 in ? 0.39 in), same as HAT board	85.60 mm ? 56.5 mm (3.370 in ? 2.224 in), not including protruding connectors	67.6 mm ? 30 mm (2.66 in ? 1.18 in)	65 mm ? 30 mm ? 5 mm (2.56 in ? 1.18 in ? 0.20 in)
Weight	45 g (1.6 oz)	23 g (0.81 oz)	45 g (1.6 oz)	7 g (0.25 oz)[70]	9 g (0.32 oz)[71]
Console	Micro-USB cable[61] or a serial cable with optional GPIO power connector[72]	
Model A	Model A+	Model B	Model B+	Raspberry Pi 2
Model B	Raspberry Pi 3
Model B	Compute Module
Zero
* - all interfaces are via 200-pin DDR2 SO-DIMM connector.

Connectors[edit]

Location of connectors and main ICs on Raspberry Pi Zero
 

Location of connectors and main ICs on Raspberry Pi 1 model A+ revision 1.1
 

Location of connectors and main ICs on Raspberry Pi 1 model B revision 2
 

Location of connectors and main ICs on Raspberry Pi 1 model B+ revision 1.2, Raspberry Pi 2 model B and Raspberry Pi 3 model B
General purpose input-output (GPIO) connector[edit]
RPi A+, B+, 2B and Zero GPIO J8 40-pin pinout.,[73] Model 3 has 40 pins as well, but someone will need to confirm that the pin layout is the same as its predecessor. Models A and B have only the first 26 pins.

GPIO#	2nd func.	Pin#		Pin#	2nd func.	GPIO#
+3.3 V	1		2	+5 V	
2	SDA1 (I2C)	3		4	+5 V	
3	SCL1 (I2C)	5		6	GND	
4	GCLK	7		8	TXD0 (UART)	14
GND	9		10	RXD0 (UART)	15
17	GEN0	11		12	GEN1	18
27	GEN2	13		14	GND	
22	GEN3	15		16	GEN4	23
+3.3 V	17		18	GEN5	24
10	MOSI (SPI)	19		20	GND	
9	MISO (SPI)	21		22	GEN6	25
11	SCLK (SPI)	23		24	CE0_N (SPI)	8
GND	25		26	CE1_N (SPI)	7
(RPi 1 Models A and B stop here)
EEPROM	ID_SD	27		28	ID_SC	EEPROM
5	N/A	29		30	GND	
6	N/A	31		32		12
13	N/A	33		34	GND	
19	N/A	35		36	N/A	16
26	N/A	37		38	Digital IN	20
GND	39		40	Digital OUT	21
Model B rev. 2 also has a pad (called P5 on the board and P6 on the schematics) of 8 pins offering access to an additional 4 GPIO connections.[74]

Function	2nd func.	Pin#		Pin#	2nd func.	Function
N/A	+5 V	1		2	+3.3 V	N/A
GPIO28	GPIO_GEN7	3		4	GPIO_GEN8	GPIO29
GPIO30	GPIO_GEN9	5		6	GPIO_GEN10	GPIO31
N/A	GND	7		8	GND	N/A
Models A and B provide GPIO access to the ACT status LED using GPIO 16. Models A+ and B+ provide GPIO access to the ACT status LED using GPIO 47, and the power status LED using GPIO 35.

Accessories[edit]
Camera  On 14 May 2013, the foundation and the distributors RS Components & Premier Farnell/Element 14 launched the Raspberry Pi camera board with a firmware update to accommodate it.[75] The camera board is shipped with a flexible flat cable that plugs into the CSI connector located between the Ethernet and HDMI ports. In Raspbian, one enables the system to use the camera board by the installing or upgrading to the latest version of the operating system (OS) and then running Raspi-config and selecting the camera option. The cost of the camera module is 20 in Europe (9 September 2013).[76] It can produce 1080p, 720p and 640x480p video. The dimensions are 25 mm x 20 mm x 9 mm.[76]
Gertboard  A Raspberry Pi Foundation sanctioned device, designed for educational purposes, that expands the Raspberry Pi's GPIO pins to allow interface with and control of LEDs, switches, analog signals, sensors and other devices. It also includes an optional Arduino compatible controller to interface with the Pi.[77]
Infrared Camera  In October 2013, the foundation announced that they would begin producing a camera module without an infrared filter, called the Pi NoIR.[78]
HAT (Hardware Attached on Top) expansion boards  Together with the model B+, inspired by the Arduino shield boards, the interface for HAT boards was devised by the Raspberry Pi Foundation. Each HAT board carries a small EEPROM (typically a CAT24C32WI-GT3)[79] containing the relevant details of the board,[80] so that the Raspberry Pi's OS is informed of the HAT, and the technical details of it, relevant to the OS using the HAT.[81] Mechanical details of a HAT board, that use the four mounting holes in their rectangular formation.[82][83]
Software[edit]
Operating systems[edit]
The Raspberry Pi primarily uses Linux-kernel-based operating systems.

The ARM11 chip at the heart of the Pi (first generation models) is based on version 6 of the ARM. The primary supported operating system is Raspbian,[84] although it is compatible with many others. The current release of Ubuntu supports the Raspberry Pi 2,[85] while Ubuntu, and several popular versions of Linux, do not support the older[86] Raspberry Pi 1 that runs on the ARM11. Raspberry Pi 2 can also run the Windows 10 IoT Core operating system,[87] while no version of the Pi can run traditional Windows.[88] The Raspberry Pi 2 currently also supports OpenELEC and RISC OS.[89]

The install manager for the Raspberry Pi is NOOBS. The operating systems included with NOOBS are:

Arch Linux ARM
OpenELEC[90]
OSMC[91] (formerly Raspbmc[92]) and the Kodi open source digital media center[93]
Pidora (Fedora Remix)
Puppy Linux[94]
RISC OS[95]  is the operating system of the first ARM-based computer.
Raspbian (recommended for Raspberry Pi 1)[96]  is maintained independently of the Foundation;[97] based on the Debian ARM hard-float (armhf) architecture port originally designed for ARMv7 and later processors (with Jazelle RCT/ThumbEE and VFPv3), compiled for the more limited ARMv6 instruction set of the Raspberry Pi 1. A minimum size of 4 GB SD card is required for the Raspbian images provided by the Raspberry Pi Foundation. There is a Pi Store for exchanging programs.[98][99]
The Raspbian Server Edition is a stripped version with fewer software packages bundled as compared to the usual desktop computer oriented Raspbian.[100][101]
The Wayland display server protocol enables efficient use of the GPU for hardware accelerated GUI drawing functions.[102] On 16 April 2014, a GUI shell for Weston called Maynard was released.
PiBang Linux  is derived from Raspbian.[103]
Raspbian for Robots  is a fork of Raspbian for robotics projects with Lego, Grove, and Arduino.[104]
Other operating systems
Xbian[105]  using the Kodi (formerly XBMC) open source digital media center
openSUSE[106]
Raspberry Pi Fedora Remix[107]
Gentoo Linux[108]
Ubuntu MATE
CentOS for Raspberry Pi 2 and later
RedSleeve (a RHEL port) for Raspberry Pi 1
Slackware ARM  version 13.37 and later runs on the Raspberry Pi without modification.[109][110][111][112] The 128496 MB of available memory on the Raspberry Pi is at least twice the minimum requirement of 64 MB needed to run Slackware Linux on an ARM or i386 system.[113] (Whereas the majority of Linux systems boot into a graphical user interface, Slackware's default user environment is the textual shell / command line interface.[114]) The Fluxbox window manager running under the X Window System requires an additional 48 MB of RAM.[115]
FreeBSD[116]
NetBSD.[117][118]
Plan 9 from Bell Labs[119][120] and Inferno[121] (in beta)
Moebius[122]  is a light ARM HF distribution based on Debian. It uses Raspbian repository, but it fits in a 128 MB SD card.[123] It has only minimal services and its memory use is optimized to be small.
OpenWrt  is primarily used on embedded devices to route network traffic.
Kali Linux  is a Debian-derived distro designed for digital forensics and penetration testing.
Pardus ARM[124]  is a Debian-based operating system which is the light version of the Pardus (operating system).
Instant WebKiosk  is an operating system for digital signage purposes (web and media views).
Ark OS  is designed for website and email self-hosting.
ROKOS[125]  is a Raspbian-based operating system with integrated clients for the Bitcoin and OKCash cryptocurrencies.
MinePeon  is a dedicated operating system for mining cryptocurrency.
Kano OS[126]
Nard SDK[127]  is a software development kit (SDK) for industrial embedded systems.
Sailfish OS with Raspberry Pi 2 (due to use ARM Cortex-A7 CPU; Raspberry Pi 1 uses different ARMv6 architecture and Sailfish requires ARMv7.)[128]
Tiny Core Linux  a minimal Linux operating system focused on providing a base system using BusyBox and FLTK. Designed to run primarily in RAM.
Windows 10 IoT Core  a free edition of Windows 10 offered by Microsoft that runs natively on the Raspberry Pi 2.[129]
WTware for Raspberry Pi 2[130]  is a free operating system for creating Windows thin client from Pi 2.
IPFire  is a dedicated firewall/router distribution for the protection of a SOHO LAN; runs only on a Raspberry Pi 1; porting to the Raspberry Pi 2 is not planned for now.[131]
xv6[132]  is a modern reimplementation of Sixth Edition Unix OS for teaching purposes; it is ported to Raspberry Pi from MIT xv6; this xv6 port can boot from NOOBS.
Alpine Linux  is a Linux distribution based on musl and BusyBox, primarily designed for "power users who appreciate security, simplicity and resource efficiency".
Void Linux  a rolling release Linux distribution which was designed and implemented from scratch, provides images based on musl or glibc.
Tingbot OS[133]  based on Raspbian, primarily designed for use with the Tingbot addon and running Tide apps.[134]
Media center operating systems:
OSMC  https://osmc.tv
OpenELEC  http://openelec.tv
Xbian  http://www.xbian.org
Rasplex  http://www.rasplex.com)
Audio operating systems :
Volumio  https://volumio.org
Pimusicbox  http://www.pimusicbox.com
Runeaudio  http://www.runeaudio.com
moOdeaudio  http://www.moodeaudio.org
Retrogaming operating systems:
Retropie  http://blog.petrockblock.com/2015/08/11/retropie-3-0-is-released/
Recalbox  http://www.recalbox.com/
Happi game center  http://happi-game-center.com/
Lakka  http://www.lakka.tv/
ChameleonPi
Piplay  http://piplay.org/
Planned operating systems
Haiku  This open source BeOS clone has been targeted for the Raspberry Pi and several other ARM boards.[135] Work began in 2011 on model 1, but only the model 2 will be supported.
Driver APIs[edit]

Scheme of the implemented APIs: OpenMAX, OpenGL ES and OpenVG
Raspberry Pi can use a VideoCore IV GPU via a binary blob, which is loaded into the GPU at boot time from the SD-card, and additional software, that initially was closed source.[136] This part of the driver code was later released,[137] however much of the actual driver work is done using the closed source GPU code. Application software use calls to closed source run-time libraries (OpenMax, OpenGL ES or OpenVG) which in turn calls an open source driver inside the Linux kernel, which then calls the closed source VideoCore IV GPU driver code. The API of the kernel driver is specific for these closed libraries. Video applications use OpenMAX, 3D applications use OpenGL ES and 2D applications use OpenVG which both in turn use EGL. OpenMAX and EGL use the open source kernel driver in turn.[138]

Third party application software[edit]
AstroPrint  Since August 2014, AstroPrint's Wireless 3D Printing software can be run on the Pi 2[139]
Mathematica & the Wolfram Language  Since 21 November 2013, Raspbian includes a full installation of this proprietary software for free.[140][141][142] As of 24 August 2015, the version is Mathematica 10.2.[143] Programs can be run either from a command line interface or from a Notebook interface. There are Wolfram Language functions for accessing connected devices.[144] There is also a Wolfram Language desktop development kit allowing development for Raspberry Pi in Mathematica from desktop machines.[145]
Minecraft  Released 11 February 2013, a version for the Raspberry Pi, in which you can modify the game world with code, the only official version of the game in which you can do so.[146]
UserGate Web Filter  On 20 September 2013, Florida-based security vendor Entensys announced porting UserGate Web Filter to Raspberry Pi platform.[147]
Software development tools[edit]
AlgoIDE  Learn programming for kids and beginners.
BlueJ  For teaching Java to beginners.
Fawlty Language  A freely usable IDL (programming language) clone for Pi 2.
Greenfoot  Greenfoot teaches object orientation with Java. Create 'actors' which live in 'worlds' to build games, simulations, and other graphical programs.
Julia  Since May 2015, the interactive and cross-platform programming language/environment, Julia, runs on the Pi 2 (and the original).[148]
Lazarus  The professional Free Pascal RAD IDE.
LiveCode  educational RAD IDE descended from HyperCard using English-like language to write event-handlers for WYSIWYG widgets runnable on desktop, mobile and Raspberry Pi platforms
Object Pascal[149]
Ninja-IDE  A cross-platform integrated development environment (IDE) for Python.
Xojo  A cross-platform, professional RAD tool that can create desktop, web and console apps for Pi 2.
V-Play Game Engine  A cross-platform development framework that supports mobile game and app development with the V-Play Game Engine, V-Play Apps and V-Play Plugins.
Tracking Raspberry Pi online on a global map[edit]
Ryan Walmsley, a UK school student, created a site in 2012 to register and track any Raspberry Pi across the globe.[150] It became very popular soon after its launch.[151] The current site is powered by Google Maps and Digital Ocean and is free. It has a limitation of registering only one Raspberry Pi per unique email id. It uses IP based basic location tracking and is fairly accurate up to Locale or City level.

Reception and use[edit]
Technology writer Glyn Moody described the project in May 2011 as a "potential BBC Micro 2.0", not by replacing PC compatible machines but by supplementing them.[152] In March 2012 Stephen Pritchard echoed the BBC Micro successor sentiment in ITPRO.[153] Alex Hope, co-author of the Next Gen report, is hopeful that the computer will engage children with the excitement of programming.[154] Co-author Ian Livingstone suggested that the BBC could be involved in building support for the device, possibly branding it as the BBC Nano.[98] Chris Williams, writing in The Register sees the inclusion of programming languages such as Kids Ruby, Scratch and BASIC as a "good start" to equip kids with the skills needed in the future  although it remains to be seen how effective their use will be.[155] The Centre for Computing History strongly supports the Raspberry Pi project, feeling that it could "usher in a new era".[156] Before release, the board was showcased by ARM's CEO Warren East at an event in Cambridge outlining Google's ideas to improve UK science and technology education.[157]

Harry Fairhead, however, suggests that more emphasis should be put on improving the educational software available on existing hardware, using tools such as Google App Inventor to return programming to schools, rather than adding new hardware choices.[158] Simon Rockman, writing in a ZDNet blog, was of the opinion that teens will have "better things to do", despite what happened in the 1980s.[159]

In October 2012, the Raspberry Pi won T3's Innovation of the Year award,[160] and futurist Mark Pesce cited a (borrowed) Raspberry Pi as the inspiration for his ambient device project MooresCloud.[161] In October 2012, the British Computer Society reacted to the announcement of enhanced specifications by stating, "it's definitely something we'll want to sink our teeth into."[162]

In February 2015, a switched-mode power supply chip, designated U16, of the Raspberry Pi 2 model B version 1.1 (the initially released version) was found to be vulnerable to flashes of light,[163] particularly the light from xenon camera flashes and green[164] and red laser pointers. However, other bright lights, particularly ones that are on continuously, were found to have no effect. The symptom was the Raspberry Pi 2 spontaneously rebooting or turning off when these lights were flashed at the chip. Initially, some users and commenters suspected that the electromagnetic pulse from the xenon flash tube was causing the problem by interfering with the computer's digital circuitry, but this was ruled out by tests where the light was either blocked by a card or aimed at the other side of the Raspberry Pi 2, both of which did not cause a problem. The problem was narrowed down to the U16 chip by covering first the system on a chip (main processor) and then U16 with opaque poster mounting compound. Light being the sole culprit, instead of EMP, was further confirmed by the laser pointer tests,[164] where it was also found that less opaque covering was needed to shield against the laser pointers than to shield against the xenon flashes.[163] The U16 chip seems to be bare silicon without a plastic cover (i.e. a chip-scale package or wafer-level package), which would, if present, block the light. Based on the facts that the chip, like all semiconductors, is light-sensitive (photovoltaic effect), that silicon is transparent to infrared light, and that xenon flashes emit more infrared light than laser pointers (therefore requiring more light shielding),[163] it is currently thought that this combination of factors allows the sudden bright infrared light to cause an instability in the output voltage of the power supply, triggering shutdown or restart of the Raspberry Pi 2. Unofficial workarounds include covering U16 with opaque material (such as electrical tape,[163][164] lacquer, poster mounting compound, or even balled-up bread[163]), putting the Raspberry Pi 2 in a case,[164] and avoiding taking photos of the top side of the board with a xenon flash. This issue was not caught before the release of the Raspberry Pi 2 because while commercial electronic devices are routinely subjected to tests of susceptibility to radio interference, it is not standard or common practice to test their susceptibility to optical interference.[163]

Community[edit]
The Raspberry Pi community was described by Jamie Ayre of FLOSS software company AdaCore as one of the most exciting parts of the project.[165] Community blogger Russell Davis said that the community strength allows the Foundation to concentrate on documentation and teaching.[165] The community developed a fanzine around the platform called The MagPi[166] which in 2015, was handed over to the Raspberry Pi Foundation by its volunteers to be continued in-house.[167] A series of community Raspberry Jam events have been held across the UK and around the world.[168]

Use in education[edit]
As of January 2012, enquiries about the board in the United Kingdom have been received from schools in both the state and private sectors, with around five times as much interest from the latter. It is hoped that businesses will sponsor purchases for less advantaged schools.[169] The CEO of Premier Farnell said that the government of a country in the Middle East has expressed interest in providing a board to every schoolgirl, in order to enhance her employment prospects.[170][171]

In 2014, the Raspberry Pi Foundation hired a number of its community members including ex-teachers and software developers to launch a set of free learning resources for its website.[172] The resources are freely licensed under Creative Commons, and contributions and collaborations are encouraged on social coding platform GitHub.

The Foundation also started a teacher training course called Picademy with the aim of helping teachers prepare for teaching the new computing curriculum using the Raspberry Pi in the classroom.[173] The continued professional development course is provided free for teachers and is run by the Foundation's education team.

Astro Pi[edit]
A project was launched in December 2014 at an event held by the UK Space Agency. The Astro Pi competition was officially opened in January and was opened to all primary and secondary school aged children who were residents of the United Kingdom. During his mission, British ESA Astronaut Tim Peake plans to deploy the computers on board the International Space Station. He will then load up the winning code while in orbit, collect the data generated and then send this to Earth where it will be distributed to the winning teams. The themes of Spacecraft Sensors, Satellite Imaging, Space Measurements, Data Fusion and Space Radiation were devised to stimulate creative and scientific thinking.

The organisations involved in the Astro Pi competition include the UK Space Agency, UKspace, Raspberry Pi, ESERO-UK and ESA.

Reviews[edit]
Raspberry Pi model B rev. 1 was rated 4/5 by PCMag, while Raspberry Pi model B rev. 2 was rated 4.1/5 by Board-DB.org.

History[edit]

This section is in a list format that may be better presented using prose. You can help by converting this section to prose, if appropriate. Editing help is available. (February 2015)

An early alpha-test board in operation using different layout from later beta and production boards
In 2006, early concepts of the Raspberry Pi were based on the Atmel ATmega644 microcontroller. Its schematics and PCB layout are publicly available.[174] Foundation trustee Eben Upton assembled a group of teachers, academics and computer enthusiasts to devise a computer to inspire children.[169] The computer is inspired by Acorn's BBC Micro of 1981.[175][176] Pi's model A, model B and model B+ are references to the original models of the British educational BBC Micro computer, developed by Acorn Computers.[155] The first ARM prototype version of the computer was mounted in a package the same size as a USB memory stick.[177] It had a USB port on one end and an HDMI port on the other.

The Foundation's goal was to offer two versions, priced at US$25 and US$35. They started accepting orders for the higher priced model B on 29 February 2012,[178] the lower cost model A on 4 February 2013.[179] and the even lower cost (US$20) A+ on 10 November 2014.[46] On November 26, 2015, the cheapest Raspberry PI yet, the Raspberry PI Zero was launched at US$5 or ?4.[180]

Pre-launch[edit]
July 2011: Trustee Eben Upton publicly approached the RISC OS Open community in July 2011 to enquire about assistance with a port.[181] Adrian Lees at Broadcom has since worked on the port,[182][183] with his work being cited in a discussion regarding the graphics drivers.[184] This port is now included in NOOBS.
August 2011  50 alpha boards are manufactured. These boards were functionally identical to the planned model B,[185] but they were physically larger to accommodate debug headers. Demonstrations of the board showed it running the LXDE desktop on Debian, Quake 3 at 1080p,[186] and Full HD MPEG-4 video over HDMI.[187]
October 2011  A version of RISC OS 5 was demonstrated in public, and following a year of development the port was released for general consumption in November 2012.[95][188][189][190]
December 2011  Twenty-five model B Beta boards were assembled and tested[191] from one hundred unpopulated PCBs.[192] The component layout of the Beta boards was the same as on production boards. A single error was discovered in the board design where some pins on the CPU were not held high; it was fixed for the first production run.[193] The Beta boards were demonstrated booting Linux, playing a 1080p movie trailer and the Rightware Samurai OpenGL ES benchmark.[194]
Early 2012  During the first week of the year, the first 10 boards were put up for auction on eBay.[195][196] One was bought anonymously and donated to the museum at The Centre for Computing History in Cambridge, England.[156][197] The ten boards (with a total retail price of ?220) together raised over ?16,000,[198] with the last to be auctioned, serial number No. 01, raising ?3,500.[199] In advance of the anticipated launch at the end of February 2012, the Foundation's servers struggled to cope with the load placed by watchers repeatedly refreshing their browsers.[200]
Launch[edit]

Raspberry Pi model A
19 February 2012  The first proof of concept SD card image that could be loaded onto an SD card to produce a preliminary operating system is released. The image was based on Debian 6.0 (Squeeze), with the LXDE desktop and the Midori browser, plus various programming tools. The image also runs on QEMU allowing the Raspberry Pi to be emulated on various other platforms.[201][202]
29 February 2012  Initial sales commence 29 February 2012[203] at 06:00 UTC;. At the same time, it was announced that the model A, originally to have had 128 MB of RAM, was to be upgraded to 256 MB before release.[178] The Foundation's website also announced: "Six years after the project's inception, we're nearly at the end of our first run of development  although it's just the beginning of the Raspberry Pi story."[204] The web-shops of the two licensed manufacturers selling Raspberry Pi's within the United Kingdom, Premier Farnell and RS Components, had their websites stalled by heavy web traffic immediately after the launch (RS Components briefly going down completely).[205][206] Unconfirmed reports suggested that there were over two million expressions of interest or pre-orders.[207] The official Raspberry Pi Twitter account reported that Premier Farnell sold out within a few minutes of the initial launch, while RS Components took over 100,000 pre orders on day one.[178] Manufacturers were reported in March 2012 to be taking a "healthy number" of pre-orders.[165]
March 2012  Shipping delays for the first batch were announced in March 2012, as the result of installation of an incorrect Ethernet port,[208][209] but the Foundation expected that manufacturing quantities of future batches could be increased with little difficulty if required.[210] "We have ensured we can get them [the Ethernet connectors with magnetics] in large numbers and Premier Farnell and RS Components [the two distributors] have been fantastic at helping to source components," Upton said. The first batch of 10,000 boards was manufactured in Taiwan and China.[211][212]
8 March 2012  Release Raspberry Pi Fedora Remix, the recommended Linux distribution,[213] developed at Seneca College in Canada.[214]
March 2012  The Debian port is initiated by Mike Thompson, former CTO of Atomz. The effort was largely carried out by Thompson and Peter Green, a volunteer Debian developer, with some support from the Foundation, who tested the resulting binaries that the two produced during the early stages (neither Thompson nor Green had physical access to the hardware, as boards were not widely accessible at the time due to demand).[215] While the preliminary proof of concept image distributed by the Foundation before launch was also Debian-based, it differed from Thompson and Green's Raspbian effort in a couple of ways. The POC image was based on then-stable Debian Squeeze, while Raspbian aimed to track then-upcoming Debian Wheezy packages.[202] Aside from the updated packages that would come with the new release, Wheezy was also set to introduce the armhf architecture,[216] which became the raison d'?tre for the Raspbian effort. The Squeeze-based POC image was limited to the armel architecture, which was, at the time of Squeeze's release, the latest attempt by the Debian project to have Debian run on the newest ARM embedded-application binary interface (EABI).[217] The armhf architecture in Wheezy intended to make Debian run on the ARM VFP hardware floating-point unit, while armel was limited to emulating floating point operations in software.[218][219] Since the Raspberry Pi included a VFP, being able to make use of the hardware unit would result in performance gains and reduced power use for floating point operations.[215] The armhf effort in mainline Debian, however, was orthogonal to the work surrounding the Pi and only intended to allow Debian to run on ARMv7 at a minimum, which would mean the Pi, an ARMv6 device, would not benefit.[216] As a result, Thompson and Green set out to build the 19,000 Debian packages for the device using a custom build cluster.[215]
Post-launch[edit]
16 April 2012  Reports appear from the first buyers who had received their Raspberry Pi.[220][221]
20 April 2012  The schematics for the model A and model B are released.[222]
18 May 2012  The Foundation reported on its blog about a prototype camera module they had tested.[223] The prototype used a 14-megapixel module.
22 May 2012  Over 20,000 units had been shipped.[150]
16 July 2012  It was announced that 4,000 units were being manufactured per day, allowing Raspberry Pis to be bought in bulk.[224][225]
24 August 2012  Hardware accelerated video (H.264) encoding becomes available after it became known that the existing license also covered encoding. Formerly it was thought that encoding would be added with the release of the announced camera module.[226][227] However, no stable software exists for hardware H.264 encoding.[228] At the same time the Foundation released two additional codecs that can be bought separately, MPEG-2 and Microsoft's VC-1. Also it was announced that the Pi will implement CEC, enabling it to be controlled with the television's remote control.[50]
July 2012  Release of Raspbian.[229]
5 September 2012  The Foundation announced a second revision of the Raspberry Pi Model B.[230] A revision 2.0 board is announced, with a number of minor corrections and improvements.[231]
6 September 2012  Announcement that in future the bulk of Raspberry Pi units would be manufactured in the UK, at Sony's manufacturing facility in Pencoed, Wales. The Foundation estimated that the plant would produce 30,000 units per month, and would create about 30 new jobs.[232][233]
15 October 2012  It is announced that new Raspberry Pi Model Bs are to be fitted with 512 MB instead of 256 MB RAM.[234]
24 October 2012  The Foundation announces that "all of the VideoCore driver code which runs on the ARM" had been released as free software under a BSD-style license, making it "the first ARM-based multimedia SoC with fully-functional, vendor-provided (as opposed to partial, reverse engineered) fully open-source drivers", although this claim has not been universally accepted.[137] On 28 February 2014, they also announced the release of full documentation for the VideoCore IV graphics core, and a complete source release of the graphics stack under a 3-clause BSD license[235][236]
October 2012  It was reported that some customers of one of the two main distributors had been waiting more than six months for their orders. This was reported to be due to difficulties in sourcing the CPU and conservative sales forecasting by this distributor.[237]
17 December 2012  The Foundation, in collaboration with IndieCity and Velocix, opens the Pi Store, as a "one-stop shop for all your Raspberry Pi (software) needs". Using an application included in Raspbian, users can browse through several categories and download what they want. Software can also be uploaded for moderation and release.[238]
3 June 2013  'New Out Of Box Software or NOOBS is introduced. This makes the Raspberry Pi easier to use by simplifying the installation of an operating system. Instead of using specific software to prepare an SD card, a file is unzipped and the contents copied over to a FAT formatted (4 GB or bigger) SD card. That card can then be booted on the Raspberry Pi and a choice of six operating systems is presented for installation on the card. The system also contains a recovery partition that allows for the quick restoration of the installed OS, tools to modify the config.txt and an online help button and web browser which directs to the Raspberry Pi Forums.[239]
October 2013  The Foundation announces that the one millionth Pi had been manufactured in the United Kingdom.[240]
November 2013: they announce that the two millionth Pi shipped between 24 and 31 October.[241]
28 February 2014  On the day of the second anniversary of the Raspberry Pi, Broadcom, together with the Raspberry PI foundation, announced the release of full documentation for the VideoCore IV graphics core[clarification needed], and a complete source release of the graphics stack under a 3-clause BSD license.[235][236]

Raspberry Pi Compute Module

Raspberry Pi Model B
7 April 2014  The official Raspberry Pi blog announced the Raspberry Pi Compute Module, a device in a 200-pin DDR2 SO-DIMM-configured memory module (though not in any way compatible with such RAM), intended for consumer electronics designers to use as the core of their own products.[44]
June 2014  The official Raspberry Pi blog mentioned that the three millionth Pi shipped in early May 2014.[242]
14 July 2014  The official Raspberry Pi blog announced the Raspberry Pi Model B+, "the final evolution of the original Raspberry Pi. For the same price as the original Raspberry Pi model B, but incorporating numerous small improvements people have been asking for".[41]
10 November 2014  The official Raspberry Pi blog announced the Raspberry Pi Model A+.[46] It is the smallest and cheapest (US$20) Raspberry Pi so far and has the same processor and RAM as the model A. Like the A, it has no Ethernet port, and only one USB port, but does have the other innovations of the B+, like lower power, micro-SD-card slot, and 40-pin HAT compatible GPIO.
2 February 2015  The official Raspberry Pi blog announced the Raspberry Pi 2. Looking like a Model B+, it has a 900 MHz quad-core ARMv7 Cortex-A7 CPU, twice the memory (for a total of 1 GB) and complete compatibility with the original generation of Raspberry Pis.[243]
14 May 2015  The price of Model B+ was decreased from US$35 to US$25, purportedly as a "side effect of the production optimizations" from the Pi 2 development.[244] Industry observers have skeptically noted, however, that the price drop appeared to be a direct response to the "C.H.I.P.", a lower-priced competitor.[245]
26 November 2015  The Raspberry Pi Foundation launched the Raspberry Pi Zero, the smallest and cheapest member of the Raspberry Pi family yet, at 65 mm ? 30 mm, and US$5. The Zero is similar to the model A+ without camera and LCD connectors, while smaller and uses less power. It was given away with the Raspberry PI magazine Magpi #40 that was distributed in the UK and USA that day  the MagPi was sold out at almost every retailer internationally due to the freebie.[45]
29 February 2016  Raspberry Pi 3 with a BCM2837 1.2 GHz 64-bit quad processor based on the ARMv8 Cortex A53, with built-in Wi-Fi BCM43438 802.11n 2.4 GHz and Bluetooth 4.1 Low Energy (BLE). Starting with a 32-bit Raspbian version.[246]
25 April 2016 - Raspberry Pi Camera v2.1 announced with 8 Mpixels, in normal and NoIR (can receive IR) versions. The camera uses the Sony IMX219 chip. The specific resolution is 3280 ? 2464 to make use of the new resolution the software has to be updated.[247]

