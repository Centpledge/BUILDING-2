Computer science is the scientific and practical approach to computation and its applications. It is the systematic study of the feasibility, structure, expression, and 
mechanization of the methodical procedures (or algorithms) that underlie the acquisition, representation, processing, storage, communication of, and access to 
information. An alternate, more succinct definition of computer science is the study of automating algorithmic processes that scale. A computer scientist specializes in the 
theory of computation and the design of computational systems.[1]
Its fields can be divided into a variety of theoretical and practical disciplines. Some fields, such as computational complexity theory (which explores the fundamental 
properties of computational and intractable problems), are highly abstract, while fields such as computer graphics emphasize real world visual applications. Still other 
fields focus on challenges in implementing computation. For example, programming language theory considers various approaches to the description of computation, 
while the study of computer programming itself investigates various aspects of the use of programming language and complex systems. Human computer interaction 
considers the challenges in making computers and computations useful, usable, and universally accessible to humans.
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks 
such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Further, algorithms for performing computations have existed 
since antiquity, even before the development of sophisticated computing equipment. The ancient Sanskrit treatise Shulba Sutras, or "Rules of the Chord", is a book of 
algorithms written in 800 BC for constructing geometric objects like altars using a peg and chord, an early precursor of the modern field of computational geometry.
Blaise Pascal designed and constructed the first working mechanical calculator, Pascal's calculator, in 1642.[2] In 1673, Gottfried Leibniz demonstrated a digital 
mechanical calculator, called the Stepped Reckoner.[3] He may be considered the first computer scientist and information theorist, for, among other reasons, documenting 
the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he released his simplified arithmometer, which was the 
first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic 
Computer A computer is a general purpose device that can be programmed to carry out a set of arithmetic or logical operations automatically. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem.

Conventionally, a computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logic operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices allow information to be retrieved from an external source, and the result of operations saved and retrieved.

Mechanical analog computers started appearing in the first century and were later used in the medieval era for astronomical calculations. In World War II, mechanical analog computers were used for specialized military applications such as calculating torpedo aiming. During this time the first electronic digital computers were developed. Originally they were the size of a large room, consuming as much power as several hundred modern personal computers (PCs).[1]

Modern computers based on integrated circuits are millions to billions of times more capable than the early machines, and occupy a fraction of the space.[2] Computers are small enough to fit into mobile devices, and mobile computers can be powered by small batteries. Personal computers in their various forms are icons of the Information Age and are generally considered as "computers". However, the embedded computers found in many devices from MP3 players to fighter aircraft and from electronic toys to industrial robots are the most numerous.The first known use of the word "computer" was in 1613 in a book called The Yong Mans Gleanings by English writer Richard Braithwait: "I haue read the truest computer of Times, and the best Arithmetician that euer breathed, and he reduceth thy dayes into a short number." It referred to a person who carried out calculations, or computations. The word continued with the same meaning until the middle of the 20th century. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.[3]

History
Main article: History of computing hardware
Pre twentieth century

The Ishango bone
Devices have been used to aid computation for thousands of years, mostly using one to one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers.[4][5] The use of counting rods is one example.


Suanpan (the number represented on this abacus is 6,302,715,408)
The abacus was initially used for arithmetic tasks. The Roman abacus was used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.


The ancient Greek designed Antikythera mechanism, dating between 150 to 100 BC, is the world's oldest analog computer.
The Antikythera mechanism is believed to be the earliest mechanical analog "computer", according to Derek J. de Solla Price.[6] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.

Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Ab? Rayh?n al B?r?n? in the early 11th century.[7] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[8][9] and gear wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[10] Ab? Rayh?n al B?r?n? invented the first mechanical geared lunisolar calendar astrolabe,[11] an early fixed wired knowledge processing machine[12] with a gear train and gear wheels,[13] circa 1000 AD.

The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.

The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.


A slide rule
The slide rule was invented around 1620 1630, shortly after the publication of the concept of the logarithm. It is a hand operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving time distance problems in light aircraft. To save space and for ease of reading, these are typically circular devices rather than the classic linear slide rule shape. A popular example is the E6B.

In the 1770s Pierre Jaquet Droz, a Swiss watchmaker, built a mechanical doll (automata) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically "programmed" to read instructions. Along with two other complex machines, the doll is at the Mus?e d'Art et d'Histoire of Neuch?tel, Switzerland, and still operates.[14]

The tide predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.

The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel and disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball and disk integrators.[15] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.

First general purpose computing device

A portion of Babbage's Difference engine.
Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer",[16] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general purpose computer that could be described in modern terms as Turing complete.[17][18]

The machine was about a century ahead of its time. All the parts for his machine had to be made by hand   this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.

Later analog computers

Sir William Thomson's third tide predicting machine design, 1879 81
During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.[19]

The first modern analog computer was a tide predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel and disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.[15]

The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious.

By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remain in use in some specialized applications such as education (control systems) and aircraft (slide rule).

Digital computer development
The principle of the modern computer was first described by mathematician and pioneering computer scientist Alan Turing, who set out the idea in his seminal 1936 paper,[20] On Computable Numbers. Turing reformulated Kurt G?del's 1931 results on the limits of proof and computation, replacing G?del's universal arithmetic based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the Entscheidungsproblem by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt.

He also introduced the notion of a 'Universal Machine' (now known as a Universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[21] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.

Electromechanical
By 1938 the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.


Replica of Zuse's Z3, the first fully automatic, digital (electromechanical) computer.
Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.[22]

In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer.[23][24] The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5 10 Hz.[25] Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard to implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.[26] The Z3 was Turing complete.[27][28]

Vacuum tubes and digital electronic circuits
Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[19] In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff Berry Computer (ABC) in 1942,[29] the first "automatic electronic digital computer".[30] This design was also all electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[31]


Colossus was the first electronic digital programmable computing device, and was used to break German ciphers during World War II.
During World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro mechanical bombes. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus.[31] He spent eleven months from early February 1943 designing and building the first Colossus.[32] After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944[33] and attacked its first message on 5 February.[31]

Colossus was the world's first electronic digital programmable computer.[19] It used a large number of valves (vacuum tubes). It had paper tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process.[34][35]


ENIAC was the first Turing complete device, and performed ballistics trajectory calculations for the United States Army.
The US built ENIAC[36] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing complete device and could compute any problem that would fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.

It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.[37]

Stored programs
Three tall racks containing electronic circuit boards
A section of the Manchester Small Scale Experimental Machine, the first stored program computer.
Early computing machines had fixed programs. Changing its function required the re wiring and re structuring of the machine.[31] With the proposal of the stored program computer this changed. A stored program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored program computer was laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored program digital computer. His 1945 report ‘Proposed Electronic Calculator  was the first specification for such a device. John von Neumann at the University of Pennsylvania, also circulated his First Draft of a Report on the EDVAC in 1945.[19]


Ferranti Mark 1, c. 1951.
The Manchester Small Scale Experimental Machine, nicknamed Baby, was the world's first stored program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.[38] It was designed as a testbed for the Williams tube the first random access digital storage device.[39] Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer.[40] As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1.

The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general purpose computer.[41] Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[42] In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 [43] and ran the world's first regular routine office computer job.

Transistors

A bipolar junction transistor
The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.

At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.[44] Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955,[45] built by the electronics division of the Atomic Energy Research Establishment at Harwell.[46][47]

Integrated circuits
The next great advance in computing power came with the advent of the integrated circuit. The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.[48]

The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor.[49] Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.[50] In his patent application of 6 February 1959, Kilby described his new device as "a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated".[51][52] Noyce also came up with his own idea of an integrated circuit half a year later than Kilby.[53] His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.

This new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor", it is largely undisputed that the first single chip microprocessor was the Intel 4004,[54] designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.[55]

Mobile computers become dominant
With the continued miniaturization of computing resources, and advancements in portable battery life, portable computers grew in popularity in the 2000s.[56] The same developments that spurred the growth of laptop computers and other portable computers allowed manufacturers to integrate computing resources into cellular phones. These so called smartphones and tablets run on a variety of operating systems and have become the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013.[57]

Programs
The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language.

In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.

Stored program architecture
Main articles: Computer program and Computer programming

Replica of the Small Scale Experimental Machine (SSEM), the world's first stored program computer, at the Museum of Science and Industry in Manchester, England
This section applies to most common RAM machine based computers.

In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called "jump" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that "remembers" the location it jumped from and another instruction to return to the instruction following that jump instruction.

Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.

Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:

  begin:
  addi $8, $0, 0           # initialize sum to 0
  addi $9, $0, 1           # set first number to add = 1
  loop:
  slti $10, $9, 1000       # check if the number is less than 1000
  beq $10, $0, finish      # if odd number is greater than n then exit
  add $8, $8, $9           # update sum
  addi $9, $9, 1           # get next number
  j loop                   # repeat the summing process
  finish:
  add $2, $8, $0           # put sum in output register
Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.

Machine code
In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program[citation needed], architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.

While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,[58] it is extremely tedious and potentially error prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember   a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.


A 1970s punched card containing one line from a FORTRAN program. The card reads: "Z(1) = Y + W(1)" and is labeled "PROJ039" for identification purposes.
Programming language
Main article: Programming language
Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.

Low level languages
Main article: Low level programming language
Machine languages and the assembly languages that represent them (collectively termed low level programming languages) tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a PDA or a hand held videogame) cannot understand the machine language of an Intel Pentium or the AMD Athlon 64 computer that might be in a PC.[59]

High level languages/Third Generation Language
Main article: High level programming language
Though considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually "compiled" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.[60] High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.

Fourth Generation Languages
These 4G languages are less procedural than 3G languages. The benefit of 4GL is that it provides ways to obtain information without requiring the direct help of a programmer. Example of 4GL is SQL.

Program design

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2012)
Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies. The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.

Bugs
Main article: Software bug

The actual first computer bug, a moth found trapped on a relay of the Harvard Mark II computer
Errors in computer programs are called "bugs". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to "hang", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.[61]

Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term "bugs" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.[62]

Components
Main articles: Central processing unit and Microprocessor
File:Computer Components.webm
Video demonstrating the standard components of a "slimline" computer
A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires.

Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a "1", and when off it represents a "0" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.

Control unit
Main articles: CPU design and Control unit

Diagram showing how a particular MIPS architecture instruction would be decoded by the control system
The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.[63] Control systems in advanced computers may change the order of execution of some instructions to improve performance.

A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.[64]

The control system's function is as follows note that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:

Read the code for the next instruction from the cell indicated by the program counter.
Decode the numerical code for the instruction into a set of commands or signals for each of the other systems.
Increment the program counter so it points to the next instruction.
Read whatever data the instruction requires from cells in memory (or perhaps from an input device). The location of this required data is typically stored within the instruction code.
Provide the necessary data to an ALU or register.
If the instruction requires an ALU or specialized hardware to complete, instruct the hardware to perform the requested operation.
Write the result from the ALU back to a memory location or to a register or perhaps an output device.
Jump back to step (1).
Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as "jumps" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).

The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.

Central processing unit (CPU)
The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid 1970s CPUs have typically been constructed on a single integrated circuit called a microprocessor.

Arithmetic logic unit (ALU)
Main article: Arithmetic logic unit
The ALU is capable of performing two classes of operations: arithmetic and logic.[65]

The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other ("is 64 greater than 65 ").

Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.

Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously.[66] Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.

Memory
Main article: Computer data storage

Magnetic core memory was the computer memory of choice throughout the 1960s, until it was replaced by semiconductor memory.
A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered "address" and can store a single number. The computer can be instructed to "put the number 123 into the cell numbered 1357" or to "add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595." The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.

In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or ?128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.

The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.

Computer main memory comes in two principal varieties:

random access memory or RAM
read only memory or ROM
RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.[67]

In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.

Input/output (I/O)
Main article: Input/output

Hard disk drives are common storage devices used with computers.
I/O is the means by which a computer exchanges information with the outside world.[68] Devices that provide input or output to the computer are called peripherals.[69] On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.

I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics.[citation needed] Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O.

Multitasking
Main article: Computer multitasking
While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.[70]

One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running "at the same time". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed "time sharing" since each program is allocated a "slice" of time in turn.[71]

Before the era of cheap computers, the principal use for multitasking was to allow many people to share the same computer.

Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a "time slice" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.

Multiprocessing
Main article: Multiprocessing

Cray designed many supercomputers that used multiprocessing heavily.
Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower end markets as a result.

Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored program architecture and from general purpose computers.[72] They often feature thousands of CPUs, customized high speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large scale simulation, graphics rendering, and cryptography applications, as well as with other so called "embarrassingly parallel" tasks.

Networking and the Internet
Main articles: Computer networking and Internet

Visualization of a portion of the routes on the Internet
Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large scale example of such a system, which led to a number of special purpose commercial systems such as Sabre.[73]

In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.[74] The technologies that made the Arpanet possible spread and evolved.

In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high tech environments, but in the 1990s the spread of applications like e mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. "Wireless" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.

Computer architecture paradigms
There are many types of computer architectures:

Quantum computer vs. Chemical computer
Scalar processor vs. Vector processor
Non Uniform Memory Access (NUMA) computers
Register machine vs. Stack machine
Harvard architecture vs. von Neumann architecture
Cellular architecture
Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing.[75]

Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms.

The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.

Misconceptions
Main articles: Human computer and Harvard Computers

Women as computers in NACA High Speed Flight Station "Computer Room"
A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word "computer" is synonymous with a personal electronic computer, the modern[76] definition of a computer is literally: "A device that computes, especially a programmable [usually] electronic machine that performs high speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information."[77] Any device which processes information qualifies as a computer, especially if the processing is purposeful.[citation needed]

Unconventional computing
Main article: Unconventional computing
Historically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example.[citation needed] More realistically, modern computers are made out of transistors made of photolithographed semiconductors.

Future
There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.

Further topics
Glossary of computers
Artificial intelligence
A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning.

Hardware
Main articles: Computer hardware and Personal computer hardware
The term hardware covers all of those parts of a computer that are tangible objects. Circuits, displays, power supplies, cables, keyboards, printers and mice are all hardware.

History of computing hardware
Main article: History of computing hardware
First generation (mechanical/electromechanical)  Calculators Pascal's calculator, Arithmometer, Difference engine, Quevedo's analytical machines
Programmable devices  Jacquard loom, Analytical engine, IBM ASCC/Harvard Mark I, Harvard Mark II, IBM SSEC, Z1, Z2, Z3
Second generation (vacuum tubes)  Calculators Atanasoff Berry Computer, IBM 604, UNIVAC 60, UNIVAC 120
Programmable devices  Colossus, ENIAC, Manchester Small Scale Experimental Machine, EDSAC, Manchester Mark 1, Ferranti Pegasus, Ferranti Mercury, CSIRAC, EDVAC, UNIVAC I, IBM 701, IBM 702, IBM 650, Z22
Third generation (discrete transistors and SSI, MSI, LSI integrated circuits) Mainframes  IBM 7090, IBM 7080, IBM System/360, BUNCH
Minicomputer  HP 2116A, IBM System/32, IBM System/36, LINC, PDP 8, PDP 11
Fourth generation (VLSI integrated circuits)  Minicomputer  VAX, IBM System i
4 bit microcomputer Intel 4004, Intel 4040
8 bit microcomputer Intel 8008, Intel 8080, Motorola 6800, Motorola 6809, MOS Technology 6502, Zilog Z80
16 bit microcomputer  Intel 8088, Zilog Z8000, WDC 65816/65802
32 bit microcomputer  Intel 80386, Pentium, Motorola 68000, ARM
64 bit microcomputer[78]  Alpha, MIPS, PA RISC, PowerPC, SPARC, x86 64, ARMv8 A
Embedded computer Intel 8048, Intel 8051
Personal computer Desktop computer, Home computer, Laptop computer, Personal digital assistant (PDA), Portable computer, Tablet PC, Wearable computer
Theoretical/experimental  Quantum computer, Chemical computer, DNA computing, Optical computer, Spintronics based computer
Other hardware topics
Peripheral device (input/output)  Input Mouse, keyboard, joystick, image scanner, webcam, graphics tablet, microphone
Output  Monitor, printer, loudspeaker
Both  Floppy disk drive, hard disk drive, optical disc drive, teleprinter
Computer buses  Short range RS 232, SCSI, PCI, USB
Long range (computer networking)  Ethernet, ATM, FDDI
Software
Main article: Computer software
Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. When software is stored in hardware that cannot easily be modified (such as BIOS ROM in an IBM PC compatible), it is sometimes called "firmware".

Operating system /System Software Unix and BSD  UNIX System V, IBM AIX, HP UX, Solaris (SunOS), IRIX, List of BSD operating systems
GNU/Linux List of Linux distributions, Comparison of Linux distributions
Microsoft Windows Windows 95, Windows 98, Windows NT, Windows 2000, Windows Me, Windows XP, Windows Vista, Windows 7, Windows 8, Windows 10
DOS 86 DOS (QDOS), IBM PC DOS, MS DOS, DR DOS, FreeDOS
Mac OS  Mac OS classic, Mac OS X
Embedded and real time  List of embedded operating systems
Experimental  Amoeba, Oberon/Bluebottle, Plan 9 from Bell Labs
Library Multimedia  DirectX, OpenGL, OpenAL, Vulkan_(API)
Programming library C standard library, Standard Template Library
Data  Protocol  TCP/IP, Kermit, FTP, HTTP, SMTP
File format HTML, XML, JPEG, MPEG, PNG
User interface  Graphical user interface (WIMP) Microsoft Windows, GNOME, KDE, QNX Photon, CDE, GEM, Aqua
Text based user interface Command line interface, Text user interface
Application Software  Office suite  Word processing, Desktop publishing, Presentation program, Database management system, Scheduling & Time management, Spreadsheet, Accounting software
Internet Access Browser, E mail client, Web server, Mail transfer agent, Instant messaging
Design and manufacturing  Computer aided design, Computer aided manufacturing, Plant management, Robotic manufacturing, Supply chain management
Graphics  Raster graphics editor, Vector graphics editor, 3D modeler, Animation editor, 3D computer graphics, Video editing, Image processing
Audio Digital audio editor, Audio playback, Mixing, Audio synthesis, Computer music
Software engineering  Compiler, Assembler, Interpreter, Debugger, Text editor, Integrated development environment, Software performance analysis, Revision control, Software configuration management
Educational Edutainment, Educational game, Serious game, Flight simulator
Games Strategy, Arcade, Puzzle, Simulation, First person shooter, Platform, Massively multiplayer, Interactive fiction
Misc  Artificial intelligence, Antivirus software, Malware scanner, Installer/Package management systems, File manager
Languages
There are thousands of different programming languages some intended to be general purpose, others useful only for highly specialized applications.

Programming languages
Lists of programming languages  Timeline of programming languages, List of programming languages by category, Generational list of programming languages, List of programming languages, Non English based programming languages
Commonly used assembly languages  ARM, MIPS, x86
Commonly used high level programming languages  Ada, BASIC, C, C++, C#, COBOL, Fortran, PL/1, REXX, Java, Lisp, Pascal, Object Pascal
Commonly used scripting languages Bourne script, JavaScript, Python, Ruby, PHP, Perl
Firmware
Firmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it.

Types of computers
Computers are typically classified based on their uses:

Based on uses
Analog computer
Digital computer
Hybrid computer
Based on sizes
Micro computer
Personal computer
Mini Computer
Mainframe computer
Super computer
Input Devices
When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand operated or automated. The act of processing is mainly regulated by the CPU. Some examples of hand operated input devices are:

Overlay keyboard
Trackball
Joystick
Digital camera
Microphone
Touchscreen
Digital video
Image scanner
Graphics tablet
Computer keyboard
Mouse
Output Devices
The means through which computer gives output are known as output devices. Some examples of output devices are:

Computer monitor
Printer
Projector
Sound card
PC speaker
Video card
Professions and organizations
As the use of computers has spread throughout society, there are an increasing number of careers involving computers.

Computer related professions
Hardware related  Electrical engineering, Electronic engineering, Computer engineering, Telecommunications engineering, Optical engineering, Nanoengineering
Software related  Computer science, Computer engineering, Desktop publishing, Human computer interaction, Information technology, Information systems, Computational science, Software engineering, Video game industry, Web design

mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[4] He 
started developing this machine in 1834 and "in less than two years he had sketched out many of the salient features of the modern computer".[5] "A crucial step was the 
adoption of a punched card system derived from the Jacquard loom"[5] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the 
Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first 
computer program.[6] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became 
part of IBM. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and 
was also in the calculator business[7] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used 
cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[8]
During the 1940s, as new and more powerful computing machines were developed, the term computer came to refer to the machines rather than their human 
predecessors.[9] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study 
computation in general. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[10][11] The world's first computer 
science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science 
degree program in the United States was formed at Purdue University in 1962.[12] Since practical computers became available, many applications of computing have 
become distinct areas of study in their own rights.
Although many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted 
among the greater academic population.[13][14] It is the now well known IBM brand that formed part of the computer science revolution during this time. IBM (short for 
International Business Machines) released the IBM 704[15] and later the IBM 709[16] computers, which were widely used during the exploration period of such devices. 
"Still, working with the IBM [computer] was frustrating […] if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to 
start the whole process over again".[13] During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were 
commonplace.[14]
Time has seen significant improvements in the usability and effectiveness of computing technology.[17] Modern society has seen a significant shift in the users of 
computer technology, from usage only by experts and professionals, to a near ubiquitous user base. Initially, computers were quite costly, and some degree of human aid 
was needed for efficient use,in part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was 
needed for common usage.
Despite its short history as a formal academic discipline, computer science has made a number of fundamental contributions to science and society,in fact, along with 
electronics, it is a founding science of the current epoch of human history called the Information Age and a driver of the Information Revolution, seen as the third major 
leap in human technological progress after the Industrial Revolution (1750 1850 CE) and the Agricultural Revolution (8000 5000 BC).
These contributions include:
The start of the "digital revolution", which includes the current Information Age and the Internet.[19]
A formal definition of computation and computability, and proof that there are computationally unsolvable and intractable problems.[20]
The concept of a programming language, a tool for the precise expression of methodological information at various levels of abstraction.[21]
In cryptography, breaking the Enigma code was an important factor contributing to the Allied victory in World War II.[18]
Scientific computing enabled practical evaluation of processes and situations of great complexity, as well as experimentation entirely by s oftware. It also enabled 
advanced study of the mind, and mapping of the human genome became possible with the Human Genome Project.[19] Distributed computing projects such as 
Folding@home explore protein folding.
Algorithmic trading has increased the efficiency and liquidity of financial markets by using artificial intelligence, machine learning, and other statistical and numerical 
techniques on a large scale.[22] High frequency algorithmic trading can also exacerbate volatility.[23]
Computer graphics and computer generated imagery have become ubiquitous in modern entertainment, particularly in television, cinema, advertising, animation and video 
games. Even films that feature no explicit CGI are usually "filmed" now on digital cameras, or edited or post processed using a digital video editor.[24][25]
Simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations 
(notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and 
electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated 
circuits.[citation needed]
Artificial intelligence is becoming increasingly important as it gets more efficient and complex. There are many applications of AI, some of which can be seen at home, 
such as robotic vacuum cleaners. It is also present in video games and on the modern battlefield in drones, anti missile systems, and squad support robots.
Main article: Philosophy of computer science
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are 
science, technology, and mathematics.[26] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[27] Amnon H. Eden described 
them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs 
deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific 
paradigm" (which approaches computer related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).[28]
Name of the field[edit]
Although first proposed in 1956,[14] the term "computer science" appears in a 1959 article in Communications of the ACM,[29] in which Louis Fein argues for the creation 
of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921,[30] justifying the name by arguing that, like management 
science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[29] His efforts, and those of others such 
as numerical analyst George Forsythe, were rewarded: universities went on to create such programs, starting with Purdue in 1962.[31] Despite its name, a significant 
amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[32] Certain departments 
of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[33] to reflect the 
fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the 
Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the 
Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a distinct field of data analysis, including statistics and 
databases.
Also, in the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM,turingineer, 
turologist, flow charts man, applied meta mathematician, and applied epistemologist.[34] Three months later in the same journal, comptologist was suggested, followed next 
year by hypologist.[35] The term computics has also been suggested.[36] In Europe, terms derived from contracted translations of the expression "automatic information" 
(e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), 
inform tica (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (           , which means informatics) in Greek. Similar words have also 
been adopted in the UK (as in the School of Informatics of the University of Edinburgh).[37]
A folkloric quotation, often attributed to,but almost certainly not first formulated by,Edsger Dijkstra, states that "computer science is no more about computers than 
astronomy is about telescopes."[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than 
computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems 
and their deployment is often called information technology or information systems. However, there has been much cross fertilization of ideas between the various 
computer related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, 
biology, statistics, and logic.
Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that 
computing is a mathematical science.[10] Early computer science was strongly influenced by the work of mathematicians such as Kurt G del and Alan Turing, and there 
continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[14]
The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term "software 
engineering" means, and how computer science is defined.[38] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has 
claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of 
specific computations to achieve practical goals, making the two separate but complementary disciplines.[39]
The academic, political, and funding aspects of computer science tend to depend on whether a department formed with a mathematical emphasis or with an engineering 
emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of 
departments tend to make efforts to bridge the field educationally if not across all research.
Areas of computer science[edit]
Further information: Outline of computer science
As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing 
computing systems in hardware and software.[40][41] CSAB, formerly called Computing Sciences Accreditation Board,which is made up of representatives of the 
Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[42],identifies four areas that it considers crucial to the discipline of computer 
science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these 
four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel 
computation, distributed computation, human computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important 
areas of computer science.[40]
Theoretical computer science[edit]
Main article: Theoretical computer science
The broader field of theoretical computer science encompasses both the classical theory of computation and a wide range of other topics that focus on the more abstract, 
logical, and mathematical aspects of computing.
Theory of computation[edit]
Main article: Theory of computation
According to Peter Denning, the fundamental question underlying computer science is, "What can be (efficiently) automated "[10] Theory of computation is focused on 
answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first 
question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by 
computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP  problem, one of the Millennium Prize Problems,[43] is an open problem in the theory of computation.
Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable computer programs. 
Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness 
and resources consumption, and implementation (commonly referred to as coding[1][2]) of algorithms in a target programming language. Source code is written in one or 
more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem. 
The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms and formal 
logic.

Related tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine 
code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the 
term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software 
development practices.
Within software engineering, programming (the implementation) is regarded as one phase in a software development process.

There is an ongoing debate on the extent to which the writing of programs is an art form, a craft, or an engineering discipline.[3] In general, good programming is 
considered to be the measured application of all three, with the goal of producing an efficient and evolvable software solution (the criteria for "efficient" and "evolvable" 
vary considerably). The discipline differs from many other technical professions in that programmers, in general, do not need to be licensed or pass any standardized (or 
governmentally regulated) certification tests in order to call themselves "programmers" or even "software engineers." Because the discipline covers many areas, which 
may or may not include critical applications, it is debatable whether licensing is required for the profession as a whole. In most cases, the discipline is self governed by 
the entities which require the programming, and sometimes very strict environments are defined (e.g. United States Air Force use of AdaCore and security clearance). 
However, representing oneself as a "professional software engineer" without a license from an accredited institution is illegal in many parts of the world.

Another ongoing debate is the extent to which the programming language used in writing computer programs affects the form that the final program takes.[citation needed] 
This debate is analogous to that surrounding the Sapir,Whorf hypothesis[4] in linguistics and cognitive science, which postulates that a particular spoken language's 
nature influences the habitual thought of its speakers. Different language patterns yield different patterns of thought. This idea challenges the possibility of representing the 
world perfectly with language, because it acknowledges that the mechanisms of any language condition the thoughts of its speaker community.
Ancient cultures seemed to have no conception of computing beyond arithmetic, algebra, and geometry, occasionally devising computational systems with elements of 
calculus (e.g. the method of exhaustion). The only mechanical device that existed for numerical computation at the beginning of human history was the abacus, invented in 
Sumeria circa 2500 BC. Later, the Antikythera mechanism, invented some time around 100 BC in ancient Greece, is the first known mechanical calculator utilizing gears 
of various sizes and configuration to perform calculations,[5] which tracked the metonic cycle still used in lunar to solar calendars, and which is consistent for calculating 
the dates of the Olympiads.[6]

The Kurdish medieval scientist Al Jazari built programmable automata in 1206 AD. One system employed in these devices was the use of pegs and cams placed into a 
wooden drum at specific locations, which would sequentially trigger levers that in turn operated percussion instruments. The output of this device was a small drummer 
playing various rhythms and drum patterns.[7] The Jacquard loom, which Joseph Marie Jacquard developed in 1801, uses a series of pasteboard cards with holes 
punched in them. The hole pattern represented the pattern that the loom had to follow in weaving cloth. The loom could produce entirely different weaves using different 
sets of cards.

Charles Babbage adopted the use of punched cards around 1830 to control his Analytical Engine. Mathematician Ada Lovelace, a friend of Babbage, between 1842 and 
1843 translated an article by Italian military engineer Luigi Menabrea on the engine,[8] which she supplemented with a set of notes, simply called Notes. These notes 
include algorithm to calculate a sequence of Bernoulli numbers,[9] intended to be carried out by a machine. Despite controversy over scope of her contribution, many 
consider this algorithm to be the first computer program.[8]


Data and instructions were once stored on external punched cards, which were kept in order and arranged in program decks.
In the 1880s, Herman Hollerith invented the recording of data on a medium that could then be read by a machine. Prior uses of machine readable media, above, had been 
for lists of instructions (not data) to drive programmed machines such as Jacquard looms and mechanized musical instruments. "After some initial trials with paper tape, 
he settled on punched cards..."[10] To process these punched cards, first known as "Hollerith cards" he invented the keypunch, sorter, and tabulator unit record 
machines.[11] These inventions were the foundation of the data processing industry. In 1896 he founded the Tabulating Machine Company (which later became the core 
of IBM). The addition of a control panel (plugboard) to his 1906 Type I Tabulator allowed it to do different jobs without having to be physically rebuilt. By the late 1940s, 
there were several unit record calculators, such as the IBM 602 and IBM 604, whose control panels specified a sequence (list) of operations and thus were programmable 
machines.

The invention of the von Neumann architecture allowed computer programs to be stored in computer memory. Early programs had to be painstakingly crafted using the 
instructions (elementary operations) of the particular machine, often in binary notation. Every model of computer would likely use different instructions (machine language) 
to do the same task. Later, assembly languages were developed that let the programmer specify each instruction in a text format, entering abbreviations for each operation 
code instead of a number and specifying addresses in symbolic form (e.g., ADD X, TOTAL). Entering a program in assembly language is usually more convenient, faster, 
and less prone to human error than using machine language, but because an assembly language is little more than a different notation for a machine language, any two 
machines with different instruction sets also have different assembly languages.


Wired control panel for an IBM 402 Accounting Machine
The synthesis of numerical calculation, predetermined operation and output, along with a way to organize and input instructions in a manner relatively easy for humans to 
conceive and produce, led to the modern development of computer programming. In 1954, FORTRAN was invented; it was the first widely used high level programming 
language to have a functional implementation, as opposed to just a design on paper.[12][13] (A high level language is, in very general terms, any programming language 
that allows the programmer to write programs in terms that are more abstract than assembly language instructions, i.e. at a level of abstraction "higher" than that of an 
assembly language.) It allowed programmers to specify calculations by entering a formula directly (e.g. Y = X*2 + 5*X + 9). The program text, or source, is converted into 
machine instructions using a special program called a compiler, which translates the FORTRAN program into machine language. In fact, the name FORTRAN stands for 
"Formula Translation". Many other languages were developed, including some for commercial programming, such as COBOL. Programs were mostly still entered using 
punched cards or paper tape. (See computer programming in the punch card era). By the late 1960s, data storage devices and computer terminals became inexpensive 
enough that programs could be created by typing directly into the computers. Text editors were developed that allowed changes and corrections to be made much more 
easily than with punched cards. (Usually, an error in punching a card meant that the card had to be discarded and a new one punched to replace it.)

As time has progressed, computers have made giant leaps in processing power, which have allowed the development of programming languages that are more abstracted 
from the underlying hardware. Popular programming languages of the modern era include ActionScript, C, C++, C#, Haskell, Java, JavaScript, Objective C, Perl, PHP, 
Python, Ruby, Smalltalk, SQL, Visual Basic, and dozens more.[14] Although these high level languages usually incur greater overhead, the increase in speed of modern 
computers has made the use of these languages much more practical than in the past. These increasingly abstracted languages are typically easier to learn and allow the 
programmer to develop applications much more efficiently and with less source code. However, high level languages are still impractical for a few programs, such as those 
where low level hardware control is necessary or where maximum processing speed is vital. Computer programming has become a popular career in the developed world, 
particularly in the United States, Europe, and Japan. Due to the high labor cost of programmers in these countries, some forms of programming have been increasingly 
subject to outsourcing (importing software and services from other countries, usually at a lower wage), making programming career decisions in developed countries more 
complicated, while increasing economic opportunities for programmers in less developed areas, particularly China and India.Quality requirements[edit]
Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:

Reliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms, and minimization of programming mistakes, such as 
mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off by one errors).
Robustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of 
needed resources such as memory, operating system services and network connections, user error, and unexpected power outages.
Usability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such 
issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical and sometimes hardware elements that improve the 
clarity, intuitiveness, cohesiveness and completeness of a program's user interface.
Portability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends 
on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware 
and operating system, and availability of platform specific compilers (and sometimes libraries) for the language of the source code.
Maintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or customizations, fix bugs and security 
holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end 
user but it can significantly affect the fate of a program over the long term.
Efficiency/performance: the amount of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to 
some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating 
memory leaks.Readability of source code[edit]
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects 
the aspects of quality above, including portability, usability and most importantly maintainability.

Readability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new 
source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study[15] found that a few simple readability transformations made code shorter 
and drastically reduced the time to understand it.

Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with 
the ability of the computer to efficiently compile and execute the code, contribute to readability.[16] Some of these factors include:

Different indentation styles (whitespace)
Comments
Decomposition
Naming conventions for objects (such as variables, classes, procedures, etc.)
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non traditional approaches to code structure 
and display. Techniques like Code refactoring can enhance readability.

Algorithmic complexity[edit]
The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for 
a given class of problem. For this purpose, algorithms are classified into orders using so called Big O notation, which expresses resource use, such as execution time or 
memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well established algorithms and their respective complexities and 
use this knowledge to choose algorithms that are best suited to the circumstances..

Methodologies[edit]
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure 
elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many 
programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a 
few weeks rather than years. There are many approaches to the Software development process.

Popular modeling techniques include Object Oriented Analysis and Design (OOAD) and Model Driven Architecture (MDA). The Unified Modeling Language (UML) is a 
notation used for both the OOAD and MDA.

A similar technique used for database design is Entity Relationship Modeling (ER Modeling).

Implementation techniques include imperative languages (object oriented or procedural), functional languages, and logic languages.

Measuring language usage[edit]
Main article: Measuring programming language popularity
It is very difficult to determine what are the most popular of modern programming languages. Methods of measuring programming language popularity include: counting 
the number of job advertisements that mention the language,[17] the number of books sold and courses teaching the language (this overestimates the importance of newer 
languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as 
COBOL).

Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, 
COBOL is still strong in corporate data centers[18] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and 
C in embedded software. Many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a 
prior language with new functionality added, (for example C++ adds object orientation to C, and Java adds memory management and bytecode to C++, but as a result, 
loses efficiency and the ability for low level manipulation).

Debugging[edit]

The bug from 1947 which is at the origin of a popular (but incorrect) etymology for the common term for a software defect.
Main article: Debugging
Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some 
languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static 
code analysis tool can help detect some possible problems.

Debugging is often done with IDEs like Eclipse, Visual Studio, Kdevelop, NetBeans and Code::Blocks. Standalone debuggers like gdb are also used, and these often 
provide less of a visual environment, usually using a command line.

Programming languages[edit]
Main articles: Programming language and List of programming languages
Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many 
considerations, such as company policy, suitability to task, availability of third party packages, or individual preference. Ideally, the programming language best suited for 
the task at hand will be selected. Trade offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that 
language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low level" to "high level"; "low level" 
languages are typically more machine oriented and faster to execute, whereas "high level" languages are more abstract and easier to use but execute less quickly. It is 
usually easier to code in "high level" languages than in "low level" ones.

Allen Downey, in his book How To Think Like A Computer Scientist, writes:

The details look different in different languages, but a few basic instructions appear in just about every language:
Input: Gather data from the keyboard, a file, or some other device.
Output: Display data on the screen or send data to a file or other device.
Arithmetic: Perform basic arithmetical operations like addition and multiplication.
Conditional Execution: Check for certain conditions and execute the appropriate sequence of statements.In mathematics and computer science, an algorithm 
is a self contained step by step set of operations to be performed. Algorithms exist that perform calculation, data processing, and automated reasoning.

The words 'algorithm' and 'algorism' come from the name al Khw rizm . Al Khw rizm  (Persian:          , c. 780 850) was a Persian mathematician, astronomer, 
geographer, and scholar.

An algorithm is an effective method that can be expressed within a finite amount of space and time[1] and in a well defined formal language[2] for calculating a function.[3] 
Starting from an initial state and initial input (perhaps empty),[4] the instructions describe a computation that, when executed, proceeds through a finite[5] number of well 
defined successive states, eventually producing "output"[6] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; 
some algorithms, known as randomized algorithms, incorporate random input.[7]

The concept of algorithm has existed for centuries, however a partial formalization of what would become the modern algorithm began with attempts to solve the 
Entscheidungsproblem (the "decision problem") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define "effective calculability"[8] or 
"effective method";[9] those formalizations included the G del,,Herbrand,Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, 
Emil Post's "Formulation 1" of 1936, and Alan Turing's Turing machines of 1936,7 and 1939. Giving a formal definition of algorithms, corresponding to the intuitive notion, 
remains a challenging problem.[10]
Informal definition[edit]
For a detailed presentation of the various points of view on the definition of "algorithm", see Algorithm characterizations.
An informal definition could be "a set of rules that precisely defines a sequence of operations."[11] which would include all computer programs, including programs that do 
not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.[12]

A prototypical example of an algorithm is Euclid's algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the 
flow chart above and as an example in a later section.

Boolos & Jeffrey (1974, 1999) offer an informal meaning of the word in the following quotation:

No human being can write fast enough, or long enough, or small enough  (  "smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on 
electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in 
the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be 
given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on 
symbols.[13]

An "enumerably infinite set" is one whose elements can be put into one to one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm 
implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can 
be an algebraic equation such as y = m + n,two arbitrary "input variables" m and n that produce an output y. But various authors' attempts to define the notion indicate
that the word implies much more than this, something on the order of (for the addition example):

Precise instructions (in language understood by "the computer")[14] for a fast, efficient, "good"[15] process that specifies the "moves" of "the computer" (machine or 
human, equipped with the necessary internally contained information and capabilities)[16] to find, decode, and then process arbitrary input integers/symbols m and n, 
symbols + and = ... and "effectively"[17] produce, in a "reasonable" time,[18] output integer y at a specified place and in a specified format.
The concept of algorithm is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set 
of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related with our customary physical dimension. 
From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage 
of the term.

Formalization[edit]
Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform 
(in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to 
be any sequence of operations that can be simulated by a Turing complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich 
(2000):

Minsky: "But we will also maintain, with Turing . . . that any procedure which could "naturally" be called effective, can in fact be realized by a (simple) machine. Although 
this may seem extreme, the arguments . . . in its favor are hard to refute".[19]

Gurevich: "...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine ... according to Savage 
[1987], an algorithm is a computational process defined by a Turing machine".[20]

Typically, when an algorithm is associated with processing information, data are read from an input source, written to an output device, and/or stored for further 
processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.

For some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any 
conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).

Because an algorithm is a precise list of precise steps, the order of computation is always critical to the functioning of the algorithm. Instructions are usually assumed to 
be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by flow of control.

So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to 
describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives 
from the intuition of "memory" as a scratchpad. There is an example below of such an assignment.

For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.

Expressing algorithms[edit]
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon charts, programming languages or control tables 
(processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. 
Pseudocode, flowcharts, drakon charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language 
statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define 
or document algorithms.

There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite state 
machine, state transition table and control table), as flowcharts and drakon charts (see more at state diagram), or as a form of rudimentary machine code or assembly 
code called "sets of quadruples" (see more at Turing machine).

Representations of algorithms can be classed into three accepted levels of Turing machine description:[21]

1 High level description
"...prose to describe an algorithm, ignoring the implementation details. At this level we do not need to mention how the machine manages its tape or head."
2 Implementation description
"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level we do not give details of states or transition 
function."
3 Formal description
Most detailed, "lowest level", gives the Turing machine's "state table".
For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.

Implementation[edit]

Logical NAND algorithm implemented electronically in 7400 chip
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network 
(for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.

Computer algorithms[edit]

Flowchart examples of the canonical B hm Jacopini structures: the SEQUENCE (rectangles descending the page), the WHILE DO and the IF THEN ELSE. The three 
structures are made of the primitive conditional GOTO (IF test=true THEN GOTO step xxx) (a diamond), the unconditional GOTO (rectangle), various assignment operators 
(rectangle), and HALT (rectangle). Nesting of these structures inside assignment blocks result in complex diagrams (cf Tausworthe 1977:100,114).
In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended "target" computer(s) to 
produce output from given input (perhaps null). An optimal algorithm, even running in old hardware, would produce faster results than a non optimal (higher time 
complexity) algorithm for the same purpose, running in more efficient hardware; that is why the algorithms, like computer hardware, are considered technology.

"Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:

Knuth: ". . .we want good algorithms in some loosely defined aesthetic sense. One criterion . . . is the length of time taken to perform the algorithm . . .. Other criteria are 
adaptability of the algorithm to computers, its simplicity and elegance, etc"[22]
Chaitin: " . . . a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does"[23]
Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant'",such a proof would solve the Halting problem (ibid).

Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set 
available to the programmer. Rogers observes that "It is . . . important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable 
by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".[24]

Unfortunately there may be a tradeoff between goodness (speed) and elegance (compactness),an elegant program may take more steps to complete a computation than
one less elegant. An example that uses Euclid's algorithm appears below.

Computers (and computors), models of computation: A computer (or human "computor"[25]) is a restricted type of machine, a "discrete deterministic mechanical 
device"[26] that blindly follows its instructions.[27] Melzak's and Lambek's primitive models[28] reduced this notion to four elements: (i) discrete, distinguishable locations, 
(ii) discrete, indistinguishable counters[29] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[30]

Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability".[31] Minsky's machine proceeds sequentially 
through its five (or six depending on how one counts) instructions unless either a conditional IF,THEN GOTO or an unconditional GOTO changes program flow out of 
sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution)[32] operations: ZERO (e.g. the contents of location replaced by 0: L  
0), SUCCESSOR (e.g. L  L+1), and DECREMENT (e.g. L  L   1).[33] Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows 
(as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, 
assignment/replacement/substitution, and HALT.[34]

Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and 
paper and work through an example".[35] But what about a simulation or execution of the real thing  The programmer must translate the algorithm into a language that the 
simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to 
take a square root. If they don't then for the algorithm to be effective it must provide a set of rules for extracting a square root.[36]

This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).

But what model should be used for the simulation  Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of 
the choice of a model remains. It is at this point that the notion of simulation enters".[37] When speed is being measured, the instruction set matters. For example, the 
subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" (division) instruction available rather than just 
subtraction (or worse: just Minsky's "decrement").

Structured programming, canonical structures: Per the Church,Turing thesis any algorithm can be computed by a model known to be Turing complete, and per Minsky's 
demonstrations Turing completeness requires only four instruction types,conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that
while "undisciplined" use of unconditional GOTOs and conditional IF THEN GOTOs can result in "spaghetti code" a programmer can write structured programs using these 
instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language".[38] Tausworthe augments the three 
B hm Jacopini canonical structures:[39] SEQUENCE, IF THEN ELSE, and WHILE DO, with two more: DO WHILE and CASE.[40] An additional benefit of a structured 
program is that it lends itself to proofs of correctness using mathematical induction.[41]

Canonical flowchart symbols[42]: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program of one). Like 
program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only 4: the directed arrow showing program 
flow, the rectangle (SEQUENCE, GOTO), the diamond (IF THEN ELSE), and the dot (OR tie). The B hm Jacopini canonical structures are made of these primitive 
shapes. Sub structures can "nest" in rectangles but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are 
shown in the diagram.

Examples[edit]
Further information: List of algorithms
Algorithm example[edit]

An animation of the quicksort algorithm sorting an array of randomized values. The red bars mark the pivot element; at the start of the animation, the element farthest to the 
right hand side is chosen as the pivot.
One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding solution requires looking at every number in the list. From this 
follows a simple algorithm, which can be stated in a high level description English prose, as:

High level description:

If there are no numbers in the set then there is no highest number.
Assume the first number in the set is the largest number in the set.
For each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.
When there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.
(Quasi )formal description: Written in prose but much closer to the high level language of a computer program, the following is the more formal coding of the algorithm in 
pseudocode or pidgin code:

Algorithm LargestNumber
  Input: A list of numbers L.
  Output: The largest number in the list L.
  if L.size = 0 return null
  largest  L[0]
  for each item in L, do
    if item > largest, then
      largest  item
  return largest
"" is a shorthand for "changes to". For instance, "largest  item" means that the value of largest changes to the value of item.
"return" terminates the algorithm and outputs the value that follows.
Euclid s algorithm[edit]
Further information: Euclid algorithm

The example diagram of Euclid's algorithm from T.L. Heath 1908 with more detail added. Euclid does not go beyond a third measuring and gives no numerical examples. 
Nicomachus gives the example of 49 and 21: "I subtract the less from the greater; 28 is left; then again I subtract from this the same 21 (for this is possible); 7 is left; I 
subtract this from 21, 14 is left; from which I again subtract 7 (for this is possible); 7 is left, but 7 cannot be subtracted from 7." Heath comments that, "The last phrase is 
curious, but the meaning of it is obvious enough, as also the meaning of the phrase about ending 'at one and the same number'."(Heath 1908:300).
Euclid s algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his Elements.
[43] Euclid poses the problem: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed 
of units": a counting number, a positive integer not including 0. And to "measure" is to place a shorter measuring length s successively (q times) along longer length l until 
the remaining portion r is less than the shorter length s.[44] In modern words, remainder r = l   q*s, q being the quotient, or remainder r is the "modulus", the integer 
fractional part left over after the division.[45]

For Euclid s method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be 0, AND (ii) the subtraction must be “proper”, a test must 
guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields 0).

Euclid's original proof adds a third: the two lengths are not prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two 
numbers' common measure is in fact the greatest.[46] While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another it yields the 
number "1" for their common measure. So to be precise the following is really Nicomachus' algorithm.


A graphical expression of Euclid's algorithm to find the greatest common divisor for 1599 and 650.
 1599 = 650*2 + 299
 650 = 299*2 + 52
 299 = 52*5 + 39
 52 = 39*1 + 13
 39 = 13*3 + 0

Computer language for Euclid's algorithm[edit]
Only a few instruction types are required to execute Euclid's algorithm,some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and
subtraction.

A location is symbolized by upper case letter(s), e.g. S, A, etc.
The varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might 
contain the number l = 3009.
An inelegant program for Euclid's algorithm[edit]

"Inelegant" is a translation of Knuth's version of the algorithm with a subtraction based remainder loop replacing his use of division (or a "modulus" instruction). Derived 
from Knuth 1973:2,4. Depending on the two numbers "Inelegant" may compute the g.c.d. in fewer steps than "Elegant".
The following algorithm is framed as Knuth's 4 step version of Euclid's and Nicomachus', but rather than using division to find the remainder it uses successive 
subtractions of the shorter length s from the remaining length r until r is less than s. The high level description, shown in boldface, is adapted from Knuth 1973:2,4:

INPUT:

1 [Into two locations L and S put the numbers l and s that represent the two lengths]:
  INPUT L, S
2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:
  R  L
E0: [Ensure r   s.]

3 [Ensure the smaller of the two numbers is in S and the larger in R]: 
  IF R > S THEN 
    the contents of L is the larger number so skip over the exchange steps 4, 5 and 6: 
    GOTO step 6 
  ELSE 
    swap the contents of R and S.
4   L  R (this first step is redundant, but is useful for later discussion).
5   R  S
6   S  L
E1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in 
R.

7 IF S > R THEN 
    done measuring so 
    GOTO 10 
  ELSE 
    measure again,
8   R  R   S
9   [Remainder loop]: 
    GOTO 7.
E2: [Is the remainder 0 ]: EITHER (i) the last measure was exact and the remainder in R is 0 program can halt, OR (ii) the algorithm must continue: the last measure left a 
remainder in R less than measuring number in S.

10 IF R = 0 THEN 
     done so 
     GOTO step 15 
   ELSE 
     CONTINUE TO step 11,
E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s:; L serves as a temporary location.

11  L  R
12  R  S
13  S  L
14  [Repeat the measuring process]: 
    GOTO 7
OUTPUT:

15 [Done. S contains the greatest common divisor]: 
   PRINT S
DONE:

16 HALT, END, STOP.
An elegant program for Euclid's algorithm[edit]
The following version of Euclid's algorithm requires only 6 core instructions to do what 13 are required to do by "Inelegant"; worse, "Inelegant" requires more types of 
instructions. The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language the steps are numbered, and the instruction LET [] = [] 
is the assignment instruction symbolized by .

  5 REM Euclid's algorithm for greatest common divisor
  6 PRINT "Type two integers greater than 0"
  10 INPUT A,B
  20 IF B=0 THEN GOTO 80
  30 IF A > B THEN GOTO 60
  40 LET B=B A
  50 GOTO 20
  60 LET A=A B
  70 GOTO 20
  80 PRINT A
  90 END
How "Elegant" works: In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co loops", an A > B loop that computes A  A   B, and a B   A loop 
that computes B  B   A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend   Subtrahend), the minuend 
can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the "sense" of the subtraction reverses.

Testing the Euclid algorithms[edit]
Does an algorithm do what its author wants it to do  A few test cases usually suffice to confirm core functionality. One source[47] uses 3009 and 884. Knuth suggested 
40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.

But exceptional cases must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S  Ditto for "Elegant": B > A, A > B, A = B  (Yes to all). 
What happens when one number is zero, both numbers are zero  ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if 
negative numbers are entered  Fractional numbers  If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive 
integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable 
failure due to exceptions is the Ariane V rocket failure.

Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's 
algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".[48] Tausworthe proposes that a measure of the complexity of a program 
be the length of its correctness proof.[49]

Measuring and improving the Euclid algorithms[edit]
Elegance (compactness) versus goodness (speed): With only 6 core instructions, "Elegant" is the clear winner compared to "Inelegant" at 13 instructions. However, 
"Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis[50] indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, 
whereas "Inelegant" only does one. As the algorithm (usually) requires many loop throughs, on average much time is wasted doing a "B = 0 " test that is needed only after 
the remainder is computed.

Can the algorithms be improved : Once the programmer judges a program "fit" and "effective",that is, it computes the function intended by its author,then the question
becomes, can it be improved 

The compactness of "Inelegant" can be improved by the elimination of 5 steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized 
algorithm;[51] rather, it can only be done heuristically, i.e. by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of 
inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps together with steps 
2 and 3 can be eliminated. This reduces the number of core instructions from 13 to 8, which makes it "more elegant" than "Elegant" at 9 steps.

The speed of "Elegant" can be improved by moving the B=0  test outside of the two subtraction loops. This change calls for the addition of 3 instructions (B=0 , A=0 , 
GOTO). Now "Elegant" computes the example numbers faster; whether for any given A, B and R, S this is always the case would require a detailed analysis.

Algorithmic analysis[edit]
Main article: Analysis of algorithms
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed 
for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O 
notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input 
list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.

Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search 
algorithm usually outperforms a brute force sequential search when used for table lookups on sorted lists.

Formal versus empirical[edit]
Main articles: Empirical algorithmics, Profiling (computer programming) and Program optimization
The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or 
implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the 
specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most 
algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution 
of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast 
interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.

Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential 
improvements to an algorithm after program optimization.

Execution efficiency[edit]
Main article: Algorithmic efficiency
To illustrate the potential improvements possible even in well established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of 
image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[52] In general, speed improvements depend on special 
properties of the problem, which are very common in practical applications.[53] Speedups of this magnitude enable computing devices that make extensive use of image 
processing (like digital cameras and medical equipment) to consume less power.

Classification[edit]
There are various ways to classify algorithms, each with its own merits.

By implementation[edit]
One way to classify algorithms is by implementation means.

Recursion
A recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method 
common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given 
problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every 
recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.
Logical
An algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control.[54] The logic component expresses the axioms 
that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic 
programming paradigm. In pure logic programming languages the control component is fixed and algorithms are specified by supplying only the logic component. The 
appeal of this approach is the elegant semantics: a change in the axioms has a well defined change in the algorithm.
Serial, parallel or distributed
Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial 
computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms take 
advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines 
connected with a network. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. 
The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some 
sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no 
parallel algorithms, and are called inherently serial problems.
Deterministic or non deterministic
Deterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non deterministic algorithms solve problems via guessing although 
typical guesses are made more accurate through the use of heuristics.
Exact or approximate
While many algorithms reach an exact solution, approximation algorithms seek an approximation that is close to the true solution. Approximation may use either a 
deterministic or a random strategy. Such algorithms have practical value for many hard problems.
Quantum algorithm
They run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of 
quantum computation such as quantum superposition or quantum entanglement.
By design paradigm[edit]
Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, 
each of these categories include many different types of algorithms. Some common paradigms are:

Brute force or exhaustive search
This is the naive method of trying every possible solution to see which is best.[55]
Divide and conquer
A divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances 
are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into 
segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease and 
conquer algorithm, that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into 
multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of decrease and conquer algorithm is the binary 
search algorithm.
Search and enumeration
Many problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for 
such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.
Randomized algorithm
Such algorithms make some choices randomly (or pseudo randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions 
can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness.[56] Whether 
randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are 
two large classes of such algorithms:
Monte Carlo algorithms return a correct answer with high probability. E.g. RP is the subclass of these that run in polynomial time.
Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.
Reduction of complexity
This technique involves solving a difficult problem by transforming it into a better known problem for which we have (hopefully) asymptotically optimal algorithms. The goal 
is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an 
unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known 
as transform and conquer.
Optimization problems[edit]
For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories 
described above as well as into one of the following:

Linear programming
When searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in 
producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm.[57] Problems that can be solved 
with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer 
then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are 
superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, 
depending on the difficulty of the problem.
Dynamic programming
When a problem shows optimal substructures , meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems , and overlapping
subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing 
solutions that have already been computed. For example, Floyd,Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using 
the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and 
divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference 
between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no 
repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of 
subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.
The greedy method
A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such 
algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they 
can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular 
use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are 
greedy algorithms that can solve this optimization problem.
The heuristic method
In optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These 
algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. 
Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated 
annealing, and genetic algorithms. Some of them, like simulated annealing, are non deterministic algorithms while others, like tabu search, are deterministic. When a 
bound on the error of the non optimal solution is known, the algorithm is further categorized as an approximation algorithm.
By field of study[edit]
See also: List of algorithms
Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search 
algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, 
medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.

Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic 
programming was invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.

By complexity[edit]
See also: Complexity class and Parameterized complexity
Algorithms can be classified by the amount of time they need to complete compared to their input size. There is a wide variety: some algorithms complete in linear time 
relative to input size, some do so in an exponential amount of time or even worse, and some never halt. Additionally, some problems may have multiple algorithms of 
differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. 
Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best 
possible algorithms for them.

Burgin (2005, p. 24) uses a generalized definition of algorithms that relaxes the common requirement that the output of the algorithm that computes a function must be 
determined after a finite number of steps. He defines a super recursive class of algorithms as "a class of algorithms in which it is possible to compute functions not 
computable by any Turing machine" (Burgin 2005, p. 107). This is closely related to the study of methods of hypercomputation.

Continuous algorithms[edit]
The adjective "continuous" when applied to the word "algorithm" can mean:

An algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations,such algorithms are studied in
numerical analysis; or
An algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.[58]
Legal issues[edit]
See also: Software patents for a general overview of the patentability of software, including computer implemented algorithms.
Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals 
does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However, practical applications of algorithms are 
sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. 
The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW 
patent.

Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).

Etymology[edit]
The words 'algorithm' and 'algorism' come from the name al Khw rizm . Al Khw rizm  (Persian:          , c. 780 850) was a Persian mathematician, astronomer, 
geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarezm', a region that was part of Greater Iran and is now in 
Uzbekistan.[59][60] About 825, he wrote a treatise in the Arabic language, which was translated into Latin in the 12th century under the title Algoritmi de numero Indorum. 
This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al Khwarizmi's name.[61] Al Khwarizmi was the most widely 
read mathematician in Europe in the late Middle Ages, primarily through his other book, the Algebra.[62] In late medieval Latin, algorismus, English 'algorism', the 
corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word         'number' (cf. 'arithmetic'), the Latin 
word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.
[63]

History: Development of the notion of "algorithm"[edit]
Ancient Near East[edit]
Algorithms were used in ancient Greece. Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus,[64][65]:Ch 9.2 
and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).[65]:Ch 9.1 Babylonian clay tablets describe and employ algorithmic procedures 
to compute the time and place of significant astronomical events.[66]

Discrete and distinguishable symbols[edit]
Tally marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks, or making 
discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16,41). Tally 
marks appear prominently in unary numeral system arithmetic used in Turing machine and Post,Turing machine computations.

Manipulation of symbols as "place holders" for numbers: algebra[edit]
The work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al Khwarizmi (from whose name 
the terms "algorism" and "algorithm" are derived), and Western European mathematicians culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):

A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner 
that ordinary algebra specifies the rules for manipulating numbers.[67]

Mechanical contrivances with discrete states[edit]
The clock: Bolter credits the invention of the weight driven clock as "The key invention [of Europe in the Middle Ages]", in particular the verge escapement[68] that 
provides us with the tick and tock of a mechanical clock. "The accurate automatic machine"[69] led immediately to "mechanical automata" beginning in the 13th century 
and finally to "computational machines",the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid 19th century.[70] Lovelace is
credited with the first creation of an algorithm intended for processing on a computer   Babbage's analytical engine, the first device considered a real Turing complete 
computer instead of just a calculator   and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not 
be realized until decades after her lifetime.

Logical machines 1870,Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form
similar to what are now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class 
of the [logical] combinations can be picked out mechanically . . . More recently however I have reduced the system to a completely mechanical form, and have thus 
embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and 
"at the foot are 21 keys like those of a piano [etc] . . .". With this machine he could analyze a "syllogism or any other simple logical argument".[71]

This machine he displayed in 1870 before the Fellows of the Royal Society.[72] Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye 
to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances 
at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented 
"a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be 
described. I prefer to call it merely a logical diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".
[73]

Jacquard loom, Hollerith punch cards, telegraphy and telephony,the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor
to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers.[74] By the mid 
19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a 
common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 
1910) with its punched paper use of Baudot code on tape.

Telephone switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he 
worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... 
When the tinkering was over, Stibitz had constructed a binary adding device".[75]

Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):

It was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had 
envisioned."[76]
Mathematics during the 19th century up to the mid 20th century[edit]
Symbols and rules: In rapid succession the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888,1889) reduced arithmetic to a 
sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was "the first attempt at an axiomatization of 
mathematics in a symbolic language".[77]

But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a " 'formula language', that is a 
lingua characterica, a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are 
manipulated according to definite rules".[78] The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia 
Mathematica (1910,1913).

The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular the Burali Forti paradox (1897), the Russell paradox (1902,03), 
and the Richard Paradox.[79] The resultant considerations led to Kurt G del's paper (1931),he specifically cites the paradox of the liar,that completely reduces rules of
recursion to numbers.

Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an 
"effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, 
Stephen Kleene and J.B. Rosser's   calculus[80] a finely honed definition of "general recursion" from the work of G del acting on suggestions of Jacques Herbrand (cf. 
G del's Princeton lectures of 1934) and subsequent simplifications by Kleene.[81] Church's proof[82] that the Entscheidungsproblem was unsolvable, Emil Post's definition 
of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a 
paper or observe the paper and make a yes no decision about the next instruction.[83] Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his 
"a  [automatic ] machine"[84],in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine".[85] S. C.
Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I",[86] and a few years later Kleene's renaming his Thesis "Church's Thesis"[87] and proposing 
"Turing's Thesis".[88]

Emil Post (1936) and Alan Turing (1936,37, 1939)[edit]
Here is a remarkable coincidence of two men not knowing each other but describing a process of men as computers working on computations,and they yield virtually
identical definitions.

Emil Post (1936) described the actions of a "computer" (human being) as follows:

"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.
His symbol space would be

"a two way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but 
one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.
"One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with 
a stroke. Likewise the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes....
"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to 
the direction of type (C ) [i.e., STOP]".[89] See more at Post,Turing machine

Alan Turing's statue at Bletchley Park.
Alan Turing's work[90] preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a 
typewriter like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter; and he could well have begun by 
asking himself what was meant by calling a typewriter 'mechanical'".[91] Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we 
might conjecture that all were influences.

Turing,his model of computation is now called a Turing machine,begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of
basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.[92]

"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book....I assume then that 
the computation is carried out on one dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is 
finite....
"The behaviour of the computer at any moment is determined by the symbols which he is observing, and his "state of mind" at that moment. We may suppose that there is 
a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We 
will also suppose that the number of states of mind which need be taken into account is finite...
"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further 
divided."[93]
Turing's reduction yields the following:

"The simple operations must therefore include:
"(a) Changes of the symbol on one of the observed squares
"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.
"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must therefore be taken to be one of the following:

"(A) A possible change (a) of symbol together with a possible change of state of mind.
"(B) A possible change (b) of observed squares, together with a possible change of state of mind"
"We may now construct a machine to do the work of this computer."[93]
A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:

"A function is said to be "effectively calculable" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, 
it is nevertheless desirable to have some more definite, mathematical expressible definition . . . [he discusses the history of the definition pretty much as presented above 
with respect to G del, Herbrand, Kleene, Church, Turing and Post] . . . We may take this statement literally, understanding by a purely mechanical process one which 
could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of 
these ideas leads to the author's definition of a computable function, and to an identification of computability   with effective calculability . . . .
"  We shall use the expression "computable function" to mean a function calculable by a machine, and we let "effectively calculable" refer to the intuitive idea without 
particular identification with any one of these definitions".[94]
J. B. Rosser (1939) and S. C. Kleene (1943)[edit]
J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):

"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite 
number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest 
of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then 
solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't 
matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one." (Rosser 1939:225,6)
Rosser's footnote #5 references the work of (1) Church and Kleene and their definition of   definability, in particular Church's use of it in his An Unsolvable Problem of 
Elementary Number Theory (1936); (2) Herbrand and G del and their use of recursion in particular G del's use in his famous paper On Formally Undecidable 
Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936,7) in their mechanism models of computation.

Stephen C. Kleene defined as his now famous "Thesis I" known as the Church Turing thesis. But he did this in the following context (boldface in original):

"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent 
variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, "yes" or "no," to the question, "is the 
predicate value true "" (Kleene 1943:273)
History after 1950[edit]
A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on going because of issues surrounding, in particular, 
foundations of mathematics (especially the Church,,Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm 
characterizations.
Repetition: Perform some action repeatedly, usually with some variation.
Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run time 
conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Programmers[edit]
Main article: Programmer
See also: Software developer and Software engineer
Computer programmers are those who write computer software. Their jobs usually involve:
Coding
Debugging
Documentation
Integration
Maintenance
Requirements analysis
Software architecture
Software testing
Specification

